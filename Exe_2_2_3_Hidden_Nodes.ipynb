{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmtASavFnvbx2YJxxJ/FVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhivyaaSP/Deep-Learning/blob/main/Exe_2_2_3_Hidden_Nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R6P2-vHHrPjd",
        "outputId": "73d4851b-66f4-4ab9-e129-f4e39a241fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 4/1000, Training Loss: 0.0030093861627721753, Test Loss: 0.002492603081874634\n",
            "Epoch 5/1000, Training Loss: 0.0030073767622905336, Test Loss: 0.0024870695135940316\n",
            "Epoch 6/1000, Training Loss: 0.003005992011121842, Test Loss: 0.0024837925807603607\n",
            "Epoch 7/1000, Training Loss: 0.0030048969570399407, Test Loss: 0.002481600222410331\n",
            "Epoch 8/1000, Training Loss: 0.0030039671883505623, Test Loss: 0.002479986114347598\n",
            "Epoch 9/1000, Training Loss: 0.0030031406461420063, Test Loss: 0.0024787049791723053\n",
            "Epoch 10/1000, Training Loss: 0.0030023808808026103, Test Loss: 0.002477628692001488\n",
            "Epoch 11/1000, Training Loss: 0.0030016650787553566, Test Loss: 0.002476686280230136\n",
            "Epoch 12/1000, Training Loss: 0.003000978680253929, Test Loss: 0.0024758362468169036\n",
            "Epoch 13/1000, Training Loss: 0.003000312345029329, Test Loss: 0.002475052992219491\n",
            "Epoch 14/1000, Training Loss: 0.00299966006600235, Test Loss: 0.0024743198722566996\n",
            "Epoch 15/1000, Training Loss: 0.002999017963046866, Test Loss: 0.002473625525050321\n",
            "Epoch 16/1000, Training Loss: 0.0029983835089782774, Test Loss: 0.0024729618587729083\n",
            "Epoch 17/1000, Training Loss: 0.002997755034551941, Test Loss: 0.002472322905639519\n",
            "Epoch 18/1000, Training Loss: 0.002997131413271594, Test Loss: 0.0024717041404156174\n",
            "Epoch 19/1000, Training Loss: 0.0029965118614590537, Test Loss: 0.0024711020561371676\n",
            "Epoch 20/1000, Training Loss: 0.0029958958118237815, Test Loss: 0.002470513887667392\n",
            "Epoch 21/1000, Training Loss: 0.0029952828337006885, Test Loss: 0.0024699374238421556\n",
            "Epoch 22/1000, Training Loss: 0.002994672582817877, Test Loss: 0.002469370875081437\n",
            "Epoch 23/1000, Training Loss: 0.002994064769697814, Test Loss: 0.0024688127772600145\n",
            "Epoch 24/1000, Training Loss: 0.002993459139788943, Test Loss: 0.002468261920241405\n",
            "Epoch 25/1000, Training Loss: 0.0029928554609673307, Test Loss: 0.0024677172937746907\n",
            "Epoch 26/1000, Training Loss: 0.0029922535156608047, Test Loss: 0.002467178045966715\n",
            "Epoch 27/1000, Training Loss: 0.002991653095868052, Test Loss: 0.0024666434510702004\n",
            "Epoch 28/1000, Training Loss: 0.0029910539999886475, Test Loss: 0.002466112884295261\n",
            "Epoch 29/1000, Training Loss: 0.002990456030785053, Test Loss: 0.002465585801987571\n",
            "Epoch 30/1000, Training Loss: 0.002989858994052117, Test Loss: 0.0024650617259495077\n",
            "Epoch 31/1000, Training Loss: 0.0029892626977291183, Test Loss: 0.002464540230984816\n",
            "Epoch 32/1000, Training Loss: 0.002988666951289282, Test Loss: 0.0024640209349666015\n",
            "Epoch 33/1000, Training Loss: 0.0029880715653040125, Test Loss: 0.0024635034908899807\n",
            "Epoch 34/1000, Training Loss: 0.002987476351117957, Test Loss: 0.0024629875804916333\n",
            "Epoch 35/1000, Training Loss: 0.0029868811205951734, Test Loss: 0.0024624729091102856\n",
            "Epoch 36/1000, Training Loss: 0.0029862856859116742, Test Loss: 0.002461959201532559\n",
            "Epoch 37/1000, Training Loss: 0.002985689859378895, Test Loss: 0.0024614461986230737\n",
            "Epoch 38/1000, Training Loss: 0.0029850934532883754, Test Loss: 0.002460933654580048\n",
            "Epoch 39/1000, Training Loss: 0.0029844962797714804, Test Loss: 0.0024604213346907998\n",
            "Epoch 40/1000, Training Loss: 0.0029838981506701855, Test Loss: 0.002459909013487524\n",
            "Epoch 41/1000, Training Loss: 0.002983298877416275, Test Loss: 0.002459396473224283\n",
            "Epoch 42/1000, Training Loss: 0.0029826982709171635, Test Loss: 0.0024588835026122845\n",
            "Epoch 43/1000, Training Loss: 0.0029820961414470694, Test Loss: 0.002458369895763311\n",
            "Epoch 44/1000, Training Loss: 0.002981492298542596, Test Loss: 0.002457855451301351\n",
            "Epoch 45/1000, Training Loss: 0.002980886550901999, Test Loss: 0.0024573399716104824\n",
            "Epoch 46/1000, Training Loss: 0.002980278706287569, Test Loss: 0.0024568232621934773\n",
            "Epoch 47/1000, Training Loss: 0.0029796685714306593, Test Loss: 0.0024563051311206822\n",
            "Epoch 48/1000, Training Loss: 0.0029790559519389612, Test Loss: 0.002455785388552728\n",
            "Epoch 49/1000, Training Loss: 0.002978440652205695, Test Loss: 0.0024552638463239264\n",
            "Epoch 50/1000, Training Loss: 0.0029778224753204373, Test Loss: 0.0024547403175756853\n",
            "Epoch 51/1000, Training Loss: 0.002977201222981335, Test Loss: 0.0024542146164314048\n",
            "Epoch 52/1000, Training Loss: 0.0029765766954085076, Test Loss: 0.002453686557705886\n",
            "Epoch 53/1000, Training Loss: 0.0029759486912584497, Test Loss: 0.0024531559566436808\n",
            "Epoch 54/1000, Training Loss: 0.002975317007539301, Test Loss: 0.0024526226286817458\n",
            "Epoch 55/1000, Training Loss: 0.0029746814395268505, Test Loss: 0.002452086389232719\n",
            "Epoch 56/1000, Training Loss: 0.0029740417806811835, Test Loss: 0.0024515470534857502\n",
            "Epoch 57/1000, Training Loss: 0.0029733978225638833, Test Loss: 0.0024510044362223552\n",
            "Epoch 58/1000, Training Loss: 0.0029727493547557315, Test Loss: 0.002450458351645264\n",
            "Epoch 59/1000, Training Loss: 0.0029720961647748635, Test Loss: 0.002449908613218508\n",
            "Epoch 60/1000, Training Loss: 0.0029714380379953385, Test Loss: 0.0024493550335173554\n",
            "Epoch 61/1000, Training Loss: 0.002970774757566116, Test Loss: 0.0024487974240868845\n",
            "Epoch 62/1000, Training Loss: 0.0029701061043304233, Test Loss: 0.002448235595308206\n",
            "Epoch 63/1000, Training Loss: 0.002969431856745525, Test Loss: 0.0024476693562714954\n",
            "Epoch 64/1000, Training Loss: 0.0029687517908029005, Test Loss: 0.00244709851465512\n",
            "Epoch 65/1000, Training Loss: 0.002968065679948849, Test Loss: 0.0024465228766102636\n",
            "Epoch 66/1000, Training Loss: 0.0029673732950055644, Test Loss: 0.0024459422466505364\n",
            "Epoch 67/1000, Training Loss: 0.002966674404092685, Test Loss: 0.0024453564275461258\n",
            "Epoch 68/1000, Training Loss: 0.002965968772549403, Test Loss: 0.002444765220222127\n",
            "Epoch 69/1000, Training Loss: 0.0029652561628571283, Test Loss: 0.0024441684236607077\n",
            "Epoch 70/1000, Training Loss: 0.002964536334562804, Test Loss: 0.002443565834806858\n",
            "Epoch 71/1000, Training Loss: 0.002963809044202893, Test Loss: 0.00244295724847747\n",
            "Epoch 72/1000, Training Loss: 0.0029630740452281246, Test Loss: 0.0024423424572735507\n",
            "Epoch 73/1000, Training Loss: 0.0029623310879290433, Test Loss: 0.002441721251495392\n",
            "Epoch 74/1000, Training Loss: 0.0029615799193624327, Test Loss: 0.002441093419060537\n",
            "Epoch 75/1000, Training Loss: 0.0029608202832786916, Test Loss: 0.002440458745424449\n",
            "Epoch 76/1000, Training Loss: 0.0029600519200502244, Test Loss: 0.002439817013503724\n",
            "Epoch 77/1000, Training Loss: 0.002959274566600928, Test Loss: 0.0024391680036018105\n",
            "Epoch 78/1000, Training Loss: 0.0029584879563368492, Test Loss: 0.0024385114933371125\n",
            "Epoch 79/1000, Training Loss: 0.002957691819078095, Test Loss: 0.002437847257573453\n",
            "Epoch 80/1000, Training Loss: 0.0029568858809920716, Test Loss: 0.0024371750683528336\n",
            "Epoch 81/1000, Training Loss: 0.002956069864528149, Test Loss: 0.002436494694830432\n",
            "Epoch 82/1000, Training Loss: 0.0029552434883538124, Test Loss: 0.0024358059032118474\n",
            "Epoch 83/1000, Training Loss: 0.00295440646729241, Test Loss: 0.0024351084566925476\n",
            "Epoch 84/1000, Training Loss: 0.002953558512262572, Test Loss: 0.002434402115399506\n",
            "Epoch 85/1000, Training Loss: 0.0029526993302193844, Test Loss: 0.0024336866363350385\n",
            "Epoch 86/1000, Training Loss: 0.0029518286240974158, Test Loss: 0.0024329617733228207\n",
            "Epoch 87/1000, Training Loss: 0.0029509460927556767, Test Loss: 0.0024322272769561146\n",
            "Epoch 88/1000, Training Loss: 0.002950051430924604, Test Loss: 0.0024314828945481933\n",
            "Epoch 89/1000, Training Loss: 0.002949144329155151, Test Loss: 0.002430728370084999\n",
            "Epoch 90/1000, Training Loss: 0.00294822447377008, Test Loss: 0.002429963444180035\n",
            "Epoch 91/1000, Training Loss: 0.002947291546817538, Test Loss: 0.0024291878540315267\n",
            "Epoch 92/1000, Training Loss: 0.00294634522602699, Test Loss: 0.002428401333381873\n",
            "Epoch 93/1000, Training Loss: 0.002945385184767615, Test Loss: 0.0024276036124794118\n",
            "Epoch 94/1000, Training Loss: 0.002944411092009225, Test Loss: 0.0024267944180425267\n",
            "Epoch 95/1000, Training Loss: 0.002943422612285801, Test Loss: 0.002425973473226129\n",
            "Epoch 96/1000, Training Loss: 0.002942419405661706, Test Loss: 0.0024251404975905457\n",
            "Epoch 97/1000, Training Loss: 0.0029414011277006692, Test Loss: 0.0024242952070728293\n",
            "Epoch 98/1000, Training Loss: 0.0029403674294375937, Test Loss: 0.002423437313960547\n",
            "Epoch 99/1000, Training Loss: 0.0029393179573532673, Test Loss: 0.002422566526868044\n",
            "Epoch 100/1000, Training Loss: 0.002938252353352038, Test Loss: 0.002421682550715252\n",
            "Epoch 101/1000, Training Loss: 0.002937170254742498, Test Loss: 0.002420785086709022\n",
            "Epoch 102/1000, Training Loss: 0.0029360712942212673, Test Loss: 0.002419873832327057\n",
            "Epoch 103/1000, Training Loss: 0.0029349550998598833, Test Loss: 0.0024189484813044323\n",
            "Epoch 104/1000, Training Loss: 0.0029338212950948826, Test Loss: 0.0024180087236227495\n",
            "Epoch 105/1000, Training Loss: 0.002932669498721085, Test Loss: 0.002417054245501939\n",
            "Epoch 106/1000, Training Loss: 0.00293149932488814, Test Loss: 0.0024160847293947197\n",
            "Epoch 107/1000, Training Loss: 0.002930310383100345, Test Loss: 0.002415099853983757\n",
            "Epoch 108/1000, Training Loss: 0.002929102278219783, Test Loss: 0.002414099294181509\n",
            "Epoch 109/1000, Training Loss: 0.0029278746104727683, Test Loss: 0.002413082721132784\n",
            "Epoch 110/1000, Training Loss: 0.002926626975459649, Test Loss: 0.0024120498022200316\n",
            "Epoch 111/1000, Training Loss: 0.0029253589641679446, Test Loss: 0.0024110002010713396\n",
            "Epoch 112/1000, Training Loss: 0.002924070162988829, Test Loss: 0.0024099335775711827\n",
            "Epoch 113/1000, Training Loss: 0.002922760153736964, Test Loss: 0.002408849587873891\n",
            "Epoch 114/1000, Training Loss: 0.0029214285136736576, Test Loss: 0.0024077478844198603\n",
            "Epoch 115/1000, Training Loss: 0.00292007481553333, Test Loss: 0.0024066281159544857\n",
            "Epoch 116/1000, Training Loss: 0.00291869862755328, Test Loss: 0.002405489927549822\n",
            "Epoch 117/1000, Training Loss: 0.0029172995135067054, Test Loss: 0.002404332960628953\n",
            "Epoch 118/1000, Training Loss: 0.002915877032738952, Test Loss: 0.002403156852993083\n",
            "Epoch 119/1000, Training Loss: 0.002914430740206947, Test Loss: 0.002401961238851308\n",
            "Epoch 120/1000, Training Loss: 0.002912960186521771, Test Loss: 0.0024007457488530743\n",
            "Epoch 121/1000, Training Loss: 0.0029114649179943284, Test Loss: 0.002399510010123307\n",
            "Epoch 122/1000, Training Loss: 0.002909944476684041, Test Loss: 0.0023982536463001925\n",
            "Epoch 123/1000, Training Loss: 0.002908398400450524, Test Loss: 0.0023969762775756\n",
            "Epoch 124/1000, Training Loss: 0.00290682622300818, Test Loss: 0.0023956775207381186\n",
            "Epoch 125/1000, Training Loss: 0.002905227473983626, Test Loss: 0.0023943569892187155\n",
            "Epoch 126/1000, Training Loss: 0.002903601678975911, Test Loss: 0.0023930142931389738\n",
            "Epoch 127/1000, Training Loss: 0.002901948359619429, Test Loss: 0.0023916490393619243\n",
            "Epoch 128/1000, Training Loss: 0.0029002670336494662, Test Loss: 0.00239026083154544\n",
            "Epoch 129/1000, Training Loss: 0.002898557214970303, Test Loss: 0.0023888492701981813\n",
            "Epoch 130/1000, Training Loss: 0.0028968184137257956, Test Loss: 0.002387413952738105\n",
            "Epoch 131/1000, Training Loss: 0.0028950501363723576, Test Loss: 0.0023859544735535154\n",
            "Epoch 132/1000, Training Loss: 0.002893251885754261, Test Loss: 0.002384470424066652\n",
            "Epoch 133/1000, Training Loss: 0.0028914231611811936, Test Loss: 0.002382961392799835\n",
            "Epoch 134/1000, Training Loss: 0.002889563458507983, Test Loss: 0.0023814269654441524\n",
            "Epoch 135/1000, Training Loss: 0.002887672270216412, Test Loss: 0.0023798667249307144\n",
            "Epoch 136/1000, Training Loss: 0.0028857490854990674, Test Loss: 0.0023782802515044817\n",
            "Epoch 137/1000, Training Loss: 0.002883793390345138, Test Loss: 0.0023766671228006805\n",
            "Epoch 138/1000, Training Loss: 0.002881804667628093, Test Loss: 0.002375026913923849\n",
            "Epoch 139/1000, Training Loss: 0.0028797823971951865, Test Loss: 0.002373359197529513\n",
            "Epoch 140/1000, Training Loss: 0.002877726055958719, Test Loss: 0.002371663543908555\n",
            "Epoch 141/1000, Training Loss: 0.002875635117988987, Test Loss: 0.0023699395210742867\n",
            "Epoch 142/1000, Training Loss: 0.0028735090546088814, Test Loss: 0.002368186694852294\n",
            "Epoch 143/1000, Training Loss: 0.0028713473344900643, Test Loss: 0.002366404628973083\n",
            "Epoch 144/1000, Training Loss: 0.0028691494237506813, Test Loss: 0.0023645928851675803\n",
            "Epoch 145/1000, Training Loss: 0.0028669147860545586, Test Loss: 0.0023627510232655568\n",
            "Epoch 146/1000, Training Loss: 0.0028646428827118335, Test Loss: 0.0023608786012970207\n",
            "Epoch 147/1000, Training Loss: 0.002862333172780971, Test Loss: 0.002358975175596644\n",
            "Epoch 148/1000, Training Loss: 0.00285998511317213, Test Loss: 0.0023570403009112928\n",
            "Epoch 149/1000, Training Loss: 0.002857598158751816, Test Loss: 0.0023550735305107075\n",
            "Epoch 150/1000, Training Loss: 0.0028551717624487882, Test Loss: 0.002353074416301433\n",
            "Epoch 151/1000, Training Loss: 0.0028527053753611537, Test Loss: 0.002351042508944013\n",
            "Epoch 152/1000, Training Loss: 0.002850198446864617, Test Loss: 0.0023489773579735557\n",
            "Epoch 153/1000, Training Loss: 0.002847650424721796, Test Loss: 0.002346878511923707\n",
            "Epoch 154/1000, Training Loss: 0.0028450607551925634, Test Loss: 0.002344745518454078\n",
            "Epoch 155/1000, Training Loss: 0.0028424288831453226, Test Loss: 0.0023425779244811862\n",
            "Epoch 156/1000, Training Loss: 0.002839754252169137, Test Loss: 0.002340375276312942\n",
            "Epoch 157/1000, Training Loss: 0.0028370363046866126, Test Loss: 0.002338137119786709\n",
            "Epoch 158/1000, Training Loss: 0.002834274482067425, Test Loss: 0.0023358630004109474\n",
            "Epoch 159/1000, Training Loss: 0.0028314682247423495, Test Loss: 0.002333552463510441\n",
            "Epoch 160/1000, Training Loss: 0.0028286169723176647, Test Loss: 0.002331205054375106\n",
            "Epoch 161/1000, Training Loss: 0.0028257201636897257, Test Loss: 0.0023288203184123157\n",
            "Epoch 162/1000, Training Loss: 0.002822777237159545, Test Loss: 0.00232639780130272\n",
            "Epoch 163/1000, Training Loss: 0.002819787630547137, Test Loss: 0.002323937049159437\n",
            "Epoch 164/1000, Training Loss: 0.002816750781305363, Test Loss: 0.002321437608690556\n",
            "Epoch 165/1000, Training Loss: 0.0028136661266330277, Test Loss: 0.002318899027364766\n",
            "Epoch 166/1000, Training Loss: 0.002810533103586846, Test Loss: 0.0023163208535799725\n",
            "Epoch 167/1000, Training Loss: 0.0028073511491919765, Test Loss: 0.002313702636834697\n",
            "Epoch 168/1000, Training Loss: 0.002804119700550675, Test Loss: 0.0023110439279019936\n",
            "Epoch 169/1000, Training Loss: 0.0028008381949486366, Test Loss: 0.002308344279005621\n",
            "Epoch 170/1000, Training Loss: 0.0027975060699585263, Test Loss: 0.0023056032439981475\n",
            "Epoch 171/1000, Training Loss: 0.0027941227635401517, Test Loss: 0.002302820378540596\n",
            "Epoch 172/1000, Training Loss: 0.002790687714136668, Test Loss: 0.002299995240283232\n",
            "Epoch 173/1000, Training Loss: 0.0027872003607661666, Test Loss: 0.0022971273890470103\n",
            "Epoch 174/1000, Training Loss: 0.0027836601431079303, Test Loss: 0.0022942163870051544\n",
            "Epoch 175/1000, Training Loss: 0.002780066501582563, Test Loss: 0.002291261798864299\n",
            "Epoch 176/1000, Training Loss: 0.002776418877425173, Test Loss: 0.0022882631920445357\n",
            "Epoch 177/1000, Training Loss: 0.002772716712750689, Test Loss: 0.0022852201368576816\n",
            "Epoch 178/1000, Training Loss: 0.0027689594506103347, Test Loss: 0.002282132206682995\n",
            "Epoch 179/1000, Training Loss: 0.0027651465350382307, Test Loss: 0.0022789989781395077\n",
            "Epoch 180/1000, Training Loss: 0.002761277411086994, Test Loss: 0.002275820031254079\n",
            "Epoch 181/1000, Training Loss: 0.002757351524851167, Test Loss: 0.0022725949496242327\n",
            "Epoch 182/1000, Training Loss: 0.002753368323477219, Test Loss: 0.0022693233205746937\n",
            "Epoch 183/1000, Training Loss: 0.0027493272551587815, Test Loss: 0.002266004735306589\n",
            "Epoch 184/1000, Training Loss: 0.00274522776911575, Test Loss: 0.002262638789038089\n",
            "Epoch 185/1000, Training Loss: 0.002741069315555752, Test Loss: 0.0022592250811352774\n",
            "Epoch 186/1000, Training Loss: 0.0027368513456164823, Test Loss: 0.002255763215231947\n",
            "Epoch 187/1000, Training Loss: 0.002732573311287286, Test Loss: 0.0022522527993369267\n",
            "Epoch 188/1000, Training Loss: 0.002728234665308341, Test Loss: 0.0022486934459275093\n",
            "Epoch 189/1000, Training Loss: 0.0027238348610457027, Test Loss: 0.002245084772027482\n",
            "Epoch 190/1000, Training Loss: 0.002719373352340451, Test Loss: 0.0022414263992681714\n",
            "Epoch 191/1000, Training Loss: 0.0027148495933301065, Test Loss: 0.0022377179539308976\n",
            "Epoch 192/1000, Training Loss: 0.002710263038240438, Test Loss: 0.0022339590669691474\n",
            "Epoch 193/1000, Training Loss: 0.0027056131411457495, Test Loss: 0.00223014937400873\n",
            "Epoch 194/1000, Training Loss: 0.002700899355695704, Test Loss: 0.0022262885153241443\n",
            "Epoch 195/1000, Training Loss: 0.002696121134806713, Test Loss: 0.0022223761357893458\n",
            "Epoch 196/1000, Training Loss: 0.0026912779303158957, Test Loss: 0.0022184118848010328\n",
            "Epoch 197/1000, Training Loss: 0.002686369192595628, Test Loss: 0.002214395416172613\n",
            "Epoch 198/1000, Training Loss: 0.0026813943701266708, Test Loss: 0.0022103263879969166\n",
            "Epoch 199/1000, Training Loss: 0.002676352909027915, Test Loss: 0.00220620446247577\n",
            "Epoch 200/1000, Training Loss: 0.0026712442525407675, Test Loss: 0.0022020293057144753\n",
            "Epoch 201/1000, Training Loss: 0.0026660678404662893, Test Loss: 0.00219780058747935\n",
            "Epoch 202/1000, Training Loss: 0.0026608231085531856, Test Loss: 0.0021935179809163827\n",
            "Epoch 203/1000, Training Loss: 0.002655509487834872, Test Loss: 0.002189181162229146\n",
            "Epoch 204/1000, Training Loss: 0.002650126403913866, Test Loss: 0.0021847898103141603\n",
            "Epoch 205/1000, Training Loss: 0.0026446732761918907, Test Loss: 0.0021803436063519152\n",
            "Epoch 206/1000, Training Loss: 0.0026391495170441474, Test Loss: 0.0021758422333518104\n",
            "Epoch 207/1000, Training Loss: 0.002633554530936375, Test Loss: 0.002171285375649413\n",
            "Epoch 208/1000, Training Loss: 0.0026278877134834067, Test Loss: 0.0021666727183544112\n",
            "Epoch 209/1000, Training Loss: 0.002622148450448154, Test Loss: 0.0021620039467478438\n",
            "Epoch 210/1000, Training Loss: 0.00261633611668005, Test Loss: 0.0021572787456272157\n",
            "Epoch 211/1000, Training Loss: 0.0026104500749922453, Test Loss: 0.002152496798598281\n",
            "Epoch 212/1000, Training Loss: 0.002604489674976993, Test Loss: 0.0021476577873123775\n",
            "Epoch 213/1000, Training Loss: 0.0025984542517589297, Test Loss: 0.0021427613906483457\n",
            "Epoch 214/1000, Training Loss: 0.002592343124686179, Test Loss: 0.0021378072838382483\n",
            "Epoch 215/1000, Training Loss: 0.0025861555959594655, Test Loss: 0.0021327951375362296\n",
            "Epoch 216/1000, Training Loss: 0.002579890949199699, Test Loss: 0.002127724616830059\n",
            "Epoch 217/1000, Training Loss: 0.002573548447954774, Test Loss: 0.002122595380195098\n",
            "Epoch 218/1000, Training Loss: 0.0025671273341466485, Test Loss: 0.0021174070783905766\n",
            "Epoch 219/1000, Training Loss: 0.002560626826460059, Test Loss: 0.0021121593532983313\n",
            "Epoch 220/1000, Training Loss: 0.002554046118674592, Test Loss: 0.002106851836704315\n",
            "Epoch 221/1000, Training Loss: 0.0025473843779421396, Test Loss: 0.002101484149023436\n",
            "Epoch 222/1000, Training Loss: 0.002540640743012159, Test Loss: 0.00209605589796846\n",
            "Epoch 223/1000, Training Loss: 0.0025338143224075066, Test Loss: 0.002090566677164006\n",
            "Epoch 224/1000, Training Loss: 0.002526904192553997, Test Loss: 0.0020850160647067845\n",
            "Epoch 225/1000, Training Loss: 0.00251990939586723, Test Loss: 0.002079403621673558\n",
            "Epoch 226/1000, Training Loss: 0.002512828938800619, Test Loss: 0.002073728890578456\n",
            "Epoch 227/1000, Training Loss: 0.0025056617898589913, Test Loss: 0.0020679913937815112\n",
            "Epoch 228/1000, Training Loss: 0.002498406877582493, Test Loss: 0.0020621906318505447\n",
            "Epoch 229/1000, Training Loss: 0.0024910630885060184, Test Loss: 0.0020563260818786872\n",
            "Epoch 230/1000, Training Loss: 0.0024836292650997445, Test Loss: 0.002050397195760065\n",
            "Epoch 231/1000, Training Loss: 0.002476104203696872, Test Loss: 0.0020444033984264066\n",
            "Epoch 232/1000, Training Loss: 0.0024684866524150436, Test Loss: 0.002038344086047487\n",
            "Epoch 233/1000, Training Loss: 0.0024607753090784153, Test Loss: 0.0020322186241985563\n",
            "Epoch 234/1000, Training Loss: 0.002452968819147801, Test Loss: 0.002026026345998086\n",
            "Epoch 235/1000, Training Loss: 0.002445065773666814, Test Loss: 0.0020197665502193762\n",
            "Epoch 236/1000, Training Loss: 0.0024370647072323574, Test Loss: 0.0020134384993797084\n",
            "Epoch 237/1000, Training Loss: 0.002428964095998401, Test Loss: 0.002007041417810989\n",
            "Epoch 238/1000, Training Loss: 0.00242076235572242, Test Loss: 0.002000574489715953\n",
            "Epoch 239/1000, Training Loss: 0.0024124578398644634, Test Loss: 0.0019940368572142486\n",
            "Epoch 240/1000, Training Loss: 0.0024040488377493428, Test Loss: 0.0019874276183828813\n",
            "Epoch 241/1000, Training Loss: 0.0023955335728030346, Test Loss: 0.0019807458252957388\n",
            "Epoch 242/1000, Training Loss: 0.0023869102008749737, Test Loss: 0.0019739904820671333\n",
            "Epoch 243/1000, Training Loss: 0.0023781768086585655, Test Loss: 0.0019671605429045376\n",
            "Epoch 244/1000, Training Loss: 0.0023693314122229083, Test Loss: 0.0019602549101759626\n",
            "Epoch 245/1000, Training Loss: 0.002360371955669412, Test Loss: 0.0019532724324977267\n",
            "Epoch 246/1000, Training Loss: 0.002351296309927758, Test Loss: 0.0019462119028486843\n",
            "Epoch 247/1000, Training Loss: 0.0023421022717063948, Test Loss: 0.0019390720567173342\n",
            "Epoch 248/1000, Training Loss: 0.0023327875626136265, Test Loss: 0.0019318515702886764\n",
            "Epoch 249/1000, Training Loss: 0.002323349828466175, Test Loss: 0.0019245490586780945\n",
            "Epoch 250/1000, Training Loss: 0.0023137866388030525, Test Loss: 0.001917163074220099\n",
            "Epoch 251/1000, Training Loss: 0.0023040954866235, Test Loss: 0.0019096921048203238\n",
            "Epoch 252/1000, Training Loss: 0.0022942737883687647, Test Loss: 0.0019021345723797913\n",
            "Epoch 253/1000, Training Loss: 0.002284318884168543, Test Loss: 0.00189448883130124\n",
            "Epoch 254/1000, Training Loss: 0.0022742280383739478, Test Loss: 0.0018867531670880276\n",
            "Epoch 255/1000, Training Loss: 0.0022639984404000247, Test Loss: 0.0018789257950470523\n",
            "Epoch 256/1000, Training Loss: 0.0022536272059019364, Test Loss: 0.0018710048591080975\n",
            "Epoch 257/1000, Training Loss: 0.0022431113783100984, Test Loss: 0.0018629884307730473\n",
            "Epoch 258/1000, Training Loss: 0.0022324479307506955, Test Loss: 0.0018548745082095836\n",
            "Epoch 259/1000, Training Loss: 0.002221633768379178, Test Loss: 0.0018466610155052421\n",
            "Epoch 260/1000, Training Loss: 0.002210665731155401, Test Loss: 0.001838345802099022\n",
            "Epoch 261/1000, Training Loss: 0.0021995405970902347, Test Loss: 0.0018299266424092633\n",
            "Epoch 262/1000, Training Loss: 0.002188255085994396, Test Loss: 0.0018214012356779588\n",
            "Epoch 263/1000, Training Loss: 0.0021768058637612743, Test Loss: 0.001812767206053455\n",
            "Epoch 264/1000, Training Loss: 0.0021651895472162624, Test Loss: 0.0018040221029350832\n",
            "Epoch 265/1000, Training Loss: 0.002153402709565869, Test Loss: 0.0017951634016052365\n",
            "Epoch 266/1000, Training Loss: 0.0021414418864803336, Test Loss: 0.0017861885041762448\n",
            "Epoch 267/1000, Training Loss: 0.00212930358284386, Test Loss: 0.0017770947408814086\n",
            "Epoch 268/1000, Training Loss: 0.002116984280206601, Test Loss: 0.0017678793717416136\n",
            "Epoch 269/1000, Training Loss: 0.0021044804449724206, Test Loss: 0.00175853958864104\n",
            "Epoch 270/1000, Training Loss: 0.0020917885373559213, Test Loss: 0.0017490725178476052\n",
            "Epoch 271/1000, Training Loss: 0.0020789050211414304, Test Loss: 0.001739475223015928\n",
            "Epoch 272/1000, Training Loss: 0.002065826374275469, Test Loss: 0.0017297447087127463\n",
            "Epoch 273/1000, Training Loss: 0.002052549100322543, Test Loss: 0.001719877924506779\n",
            "Epoch 274/1000, Training Loss: 0.002039069740812108, Test Loss: 0.0017098717696670898\n",
            "Epoch 275/1000, Training Loss: 0.0020253848885019472, Test Loss: 0.00169972309851592\n",
            "Epoch 276/1000, Training Loss: 0.0020114912015801382, Test Loss: 0.0016894287264837401\n",
            "Epoch 277/1000, Training Loss: 0.001997385418824173, Test Loss: 0.0016789854369159102\n",
            "Epoch 278/1000, Training Loss: 0.0019830643757315133, Test Loss: 0.0016683899886817022\n",
            "Epoch 279/1000, Training Loss: 0.0019685250216310898, Test Loss: 0.0016576391246375679\n",
            "Epoch 280/1000, Training Loss: 0.001953764437779772, Test Loss: 0.0016467295809973839\n",
            "Epoch 281/1000, Training Loss: 0.001938779856441661, Test Loss: 0.001635658097662717\n",
            "Epoch 282/1000, Training Loss: 0.0019235686809413442, Test Loss: 0.0016244214295662704\n",
            "Epoch 283/1000, Training Loss: 0.0019081285066747547, Test Loss: 0.0016130163590810998\n",
            "Epoch 284/1000, Training Loss: 0.001892457143053154, Test Loss: 0.0016014397095471262\n",
            "Epoch 285/1000, Training Loss: 0.0018765526363469188, Test Loss: 0.0015896883599648224\n",
            "Epoch 286/1000, Training Loss: 0.001860413293386382, Test Loss: 0.001577759260903527\n",
            "Epoch 287/1000, Training Loss: 0.0018440377060667957, Test Loss: 0.0015656494516687704\n",
            "Epoch 288/1000, Training Loss: 0.0018274247765936642, Test Loss: 0.001553356078768902\n",
            "Epoch 289/1000, Training Loss: 0.0018105737433932784, Test Loss: 0.0015408764157164839\n",
            "Epoch 290/1000, Training Loss: 0.0017934842076011508, Test Loss: 0.0015282078841938585\n",
            "Epoch 291/1000, Training Loss: 0.00177615616002827, Test Loss: 0.0015153480766052383\n",
            "Epoch 292/1000, Training Loss: 0.0017585900084916618, Test Loss: 0.0015022947800291963\n",
            "Epoch 293/1000, Training Loss: 0.0017407866053816589, Test Loss: 0.0014890460015755489\n",
            "Epoch 294/1000, Training Loss: 0.0017227472753232969, Test Loss: 0.0014755999951390566\n",
            "Epoch 295/1000, Training Loss: 0.0017044738427736683, Test Loss: 0.0014619552895288066\n",
            "Epoch 296/1000, Training Loss: 0.001685968659380435, Test Loss: 0.0014481107179363992\n",
            "Epoch 297/1000, Training Loss: 0.001667234630909211, Test Loss: 0.0014340654486876916\n",
            "Epoch 298/1000, Training Loss: 0.0016482752435288397, Test Loss: 0.001419819017201303\n",
            "Epoch 299/1000, Training Loss: 0.001629094589223867, Test Loss: 0.0014053713590522115\n",
            "Epoch 300/1000, Training Loss: 0.0016096973900823265, Test Loss: 0.0013907228440095238\n",
            "Epoch 301/1000, Training Loss: 0.0015900890211846402, Test Loss: 0.0013758743108838129\n",
            "Epoch 302/1000, Training Loss: 0.0015702755317955686, Test Loss: 0.0013608271029802372\n",
            "Epoch 303/1000, Training Loss: 0.0015502636645362583, Test Loss: 0.0013455831039088481\n",
            "Epoch 304/1000, Training Loss: 0.0015300608721873505, Test Loss: 0.001330144773452359\n",
            "Epoch 305/1000, Training Loss: 0.0015096753317476639, Test Loss: 0.0013145151831341927\n",
            "Epoch 306/1000, Training Loss: 0.0014891159553464784, Test Loss: 0.0012986980510658439\n",
            "Epoch 307/1000, Training Loss: 0.0014683923975821456, Test Loss: 0.0012826977755831745\n",
            "Epoch 308/1000, Training Loss: 0.0014475150588368593, Test Loss: 0.0012665194671073396\n",
            "Epoch 309/1000, Training Loss: 0.0014264950840986296, Test Loss: 0.0012501689775894812\n",
            "Epoch 310/1000, Training Loss: 0.0014053443568090524, Test Loss: 0.0012336529268220354\n",
            "Epoch 311/1000, Training Loss: 0.0013840754872517195, Test Loss: 0.001216978724827005\n",
            "Epoch 312/1000, Training Loss: 0.001362701795004015, Test Loss: 0.0012001545894675955\n",
            "Epoch 313/1000, Training Loss: 0.0013412372849976601, Test Loss: 0.0011831895583798019\n",
            "Epoch 314/1000, Training Loss: 0.0013196966167736887, Test Loss: 0.0011660934942909723\n",
            "Epoch 315/1000, Training Loss: 0.0012980950665787581, Test Loss: 0.0011488770827899869\n",
            "Epoch 316/1000, Training Loss: 0.0012764484820340144, Test Loss: 0.0011315518216449274\n",
            "Epoch 317/1000, Training Loss: 0.001254773229217016, Test Loss: 0.0011141300008352091\n",
            "Epoch 318/1000, Training Loss: 0.0012330861321320468, Test Loss: 0.0010966246725809927\n",
            "Epoch 319/1000, Training Loss: 0.0012114044047033833, Test Loss: 0.001079049610816017\n",
            "Epoch 320/1000, Training Loss: 0.0011897455756071519, Test Loss: 0.0010614192597612965\n",
            "Epoch 321/1000, Training Loss: 0.001168127406455769, Test Loss: 0.001043748671513368\n",
            "Epoch 322/1000, Training Loss: 0.0011465678040575515, Test Loss: 0.001026053432855452\n",
            "Epoch 323/1000, Training Loss: 0.0011250847276849285, Test Loss: 0.0010083495818229166\n",
            "Epoch 324/1000, Training Loss: 0.0011036960924872292, Test Loss: 0.0009906535148921614\n",
            "Epoch 325/1000, Training Loss: 0.0010824196703678776, Test Loss: 0.00097298188599845\n",
            "Epoch 326/1000, Training Loss: 0.0010612729897992474, Test Loss: 0.0009553514989047231\n",
            "Epoch 327/1000, Training Loss: 0.001040273236161525, Test Loss: 0.0009377791947224853\n",
            "Epoch 328/1000, Training Loss: 0.001019437154255331, Test Loss: 0.000920281736609179\n",
            "Epoch 329/1000, Training Loss: 0.000998780954645263, Test Loss: 0.0009028756938198355\n",
            "Epoch 330/1000, Training Loss: 0.0009783202254395266, Test Loss: 0.0008855773273630261\n",
            "Epoch 331/1000, Training Loss: 0.0009580698509996174, Test Loss: 0.0008684024794963314\n",
            "Epoch 332/1000, Training Loss: 0.0009380439389072743, Test Loss: 0.000851366469194029\n",
            "Epoch 333/1000, Training Loss: 0.0009182557563013145, Test Loss: 0.000834483995534907\n",
            "Epoch 334/1000, Training Loss: 0.0008987176764446007, Test Loss: 0.0008177690507011223\n",
            "Epoch 335/1000, Training Loss: 0.0008794411361037479, Test Loss: 0.0008012348439644137\n",
            "Epoch 336/1000, Training Loss: 0.0008604366040351691, Test Loss: 0.0007848937376816303\n",
            "Epoch 337/1000, Training Loss: 0.0008417135605843199, Test Loss: 0.0007687571959462857\n",
            "Epoch 338/1000, Training Loss: 0.0008232804881336536, Test Loss: 0.0007528357461661783\n",
            "Epoch 339/1000, Training Loss: 0.0008051448718899056, Test Loss: 0.0007371389534767416\n",
            "Epoch 340/1000, Training Loss: 0.0007873132102919727, Test Loss: 0.0007216754075711241\n",
            "Epoch 341/1000, Training Loss: 0.0007697910341526114, Test Loss: 0.0007064527212431821\n",
            "Epoch 342/1000, Training Loss: 0.0007525829335239318, Test Loss: 0.0006914775397068075\n",
            "Epoch 343/1000, Training Loss: 0.0007356925911982565, Test Loss: 0.0006767555595786761\n",
            "Epoch 344/1000, Training Loss: 0.0007191228217205122, Test Loss: 0.0006622915562923971\n",
            "Epoch 345/1000, Training Loss: 0.000702875614791975, Test Loss: 0.0006480894186480221\n",
            "Epoch 346/1000, Training Loss: 0.0006869521819820481, Test Loss: 0.0006341521891864211\n",
            "Epoch 347/1000, Training Loss: 0.0006713530057291888, Test Loss: 0.0006204821091072418\n",
            "Epoch 348/1000, Training Loss: 0.0006560778896967572, Test Loss: 0.0006070806665134493\n",
            "Epoch 349/1000, Training Loss: 0.0006411260096485982, Test Loss: 0.000593948646857172\n",
            "Epoch 350/1000, Training Loss: 0.0006264959641162122, Test Loss: 0.0005810861845724693\n",
            "Epoch 351/1000, Training Loss: 0.0006121858242390786, Test Loss: 0.0005684928150029062\n",
            "Epoch 352/1000, Training Loss: 0.0005981931822681831, Test Loss: 0.0005561675258596665\n",
            "Epoch 353/1000, Training Loss: 0.0005845151983257222, Test Loss: 0.0005441088075726806\n",
            "Epoch 354/1000, Training Loss: 0.0005711486451097252, Test Loss: 0.0005323147020196276\n",
            "Epoch 355/1000, Training Loss: 0.0005580899503185075, Test Loss: 0.000520782849231676\n",
            "Epoch 356/1000, Training Loss: 0.0005453352366458092, Test Loss: 0.0005095105317783607\n",
            "Epoch 357/1000, Training Loss: 0.0005328803592628462, Test Loss: 0.0004984947166259765\n",
            "Epoch 358/1000, Training Loss: 0.0005207209407584457, Test Loss: 0.0004877320943435466\n",
            "Epoch 359/1000, Training Loss: 0.0005088524035532391, Test Loss: 0.00047721911559760985\n",
            "Epoch 360/1000, Training Loss: 0.0004972699998398473, Test Loss: 0.00046695202493301884\n",
            "Epoch 361/1000, Training Loss: 0.00048596883912849193, Test Loss: 0.0004569268918815265\n",
            "Epoch 362/1000, Training Loss: 0.0004749439134978349, Test Loss: 0.00044713963947496527\n",
            "Epoch 363/1000, Training Loss: 0.000464190120665111, Test Loss: 0.00043758607026611883\n",
            "Epoch 364/1000, Training Loss: 0.0004537022849985656, Test Loss: 0.0004282618899790882\n",
            "Epoch 365/1000, Training Loss: 0.00044347517660011244, Test Loss: 0.0004191627289235169\n",
            "Epoch 366/1000, Training Loss: 0.0004335035285873614, Test Loss: 0.0004102841613140781\n",
            "Epoch 367/1000, Training Loss: 0.0004237820527030069, Test Loss: 0.00040162172263970584\n",
            "Epoch 368/1000, Training Loss: 0.0004143054533761886, Test Loss: 0.00039317092522658933\n",
            "Epoch 369/1000, Training Loss: 0.00040506844035574104, Test Loss: 0.00038492727213596987\n",
            "Epoch 370/1000, Training Loss: 0.0003960657400294423, Test Loss: 0.00037688626953292695\n",
            "Epoch 371/1000, Training Loss: 0.0003872921055369844, Test Loss: 0.00036904343765603934\n",
            "Epoch 372/1000, Training Loss: 0.0003787423257775871, Test Loss: 0.0003613943205106671\n",
            "Epoch 373/1000, Training Loss: 0.0003704112334064063, Test Loss: 0.00035393449440100075\n",
            "Epoch 374/1000, Training Loss: 0.0003622937119069878, Test Loss: 0.0003466595754080166\n",
            "Epoch 375/1000, Training Loss: 0.0003543847018203897, Test Loss: 0.00033956522591253516\n",
            "Epoch 376/1000, Training Loss: 0.00034667920620534714, Test Loss: 0.00033264716025494163\n",
            "Epoch 377/1000, Training Loss: 0.0003391722953977283, Test Loss: 0.00032590114961544506\n",
            "Epoch 378/1000, Training Loss: 0.0003318591111319502, Test Loss: 0.0003193230261917396\n",
            "Epoch 379/1000, Training Loss: 0.00032473487008176887, Test Loss: 0.0003129086867441535\n",
            "Epoch 380/1000, Training Loss: 0.0003177948668729754, Test Loss: 0.00030665409557208546\n",
            "Epoch 381/1000, Training Loss: 0.00031103447661607285, Test Loss: 0.0003005552869797286\n",
            "Epoch 382/1000, Training Loss: 0.00030444915700286924, Test Loss: 0.0002946083672836268\n",
            "Epoch 383/1000, Training Loss: 0.00029803445000720893, Test Loss: 0.0002888095164098056\n",
            "Epoch 384/1000, Training Loss: 0.00029178598322656535, Test Loss: 0.000283154989123563\n",
            "Epoch 385/1000, Training Loss: 0.00028569947089816133, Test Loss: 0.00027764111593101096\n",
            "Epoch 386/1000, Training Loss: 0.0002797707146203494, Test Loss: 0.0002722643036876285\n",
            "Epoch 387/1000, Training Loss: 0.00027399560380746266, Test Loss: 0.0002670210359457775\n",
            "Epoch 388/1000, Training Loss: 0.0002683701159039077, Test Loss: 0.000261907873069974\n",
            "Epoch 389/1000, Training Loss: 0.0002628903163811759, Test Loss: 0.0002569214521460246\n",
            "Epoch 390/1000, Training Loss: 0.00025755235853945754, Test Loss: 0.00025205848670753305\n",
            "Epoch 391/1000, Training Loss: 0.000252352483133712, Test Loss: 0.0002473157663010487\n",
            "Epoch 392/1000, Training Loss: 0.0002472870178424953, Test Loss: 0.00024269015590912678\n",
            "Epoch 393/1000, Training Loss: 0.00024235237659623931, Test Loss: 0.0002381785952485816\n",
            "Epoch 394/1000, Training Loss: 0.00023754505878041275, Test Loss: 0.00023377809795972608\n",
            "Epoch 395/1000, Training Loss: 0.0002328616483276201, Test Loss: 0.0002294857507006488\n",
            "Epoch 396/1000, Training Loss: 0.00022829881271164856, Test Loss: 0.0002252987121594622\n",
            "Epoch 397/1000, Training Loss: 0.00022385330185530604, Test Loss: 0.00022121421199596802\n",
            "Epoch 398/1000, Training Loss: 0.00021952194696301405, Test Loss: 0.00021722954972325736\n",
            "Epoch 399/1000, Training Loss: 0.00021530165928811032, Test Loss: 0.00021334209353862836\n",
            "Epoch 400/1000, Training Loss: 0.00021118942884408527, Test Loss: 0.0002095492791123371\n",
            "Epoch 401/1000, Training Loss: 0.00020718232306810903, Test Loss: 0.00020584860834181595\n",
            "Epoch 402/1000, Training Loss: 0.00020327748544459213, Test Loss: 0.00020223764807830082\n",
            "Epoch 403/1000, Training Loss: 0.0001994721340957642, Test Loss: 0.00019871402883201933\n",
            "Epoch 404/1000, Training Loss: 0.00019576356034574238, Test Loss: 0.00019527544346158252\n",
            "Epoch 405/1000, Training Loss: 0.00019214912726391045, Test Loss: 0.0001919196458525392\n",
            "Epoch 406/1000, Training Loss: 0.00018862626819297654, Test Loss: 0.0001886444495895958\n",
            "Epoch 407/1000, Training Loss: 0.00018519248526654988, Test Loss: 0.00018544772662653627\n",
            "Epoch 408/1000, Training Loss: 0.0001818453479206544, Test Loss: 0.00018232740595740748\n",
            "Epoch 409/1000, Training Loss: 0.00017858249140314874, Test Loss: 0.0001792814722921509\n",
            "Epoch 410/1000, Training Loss: 0.0001754016152846946, Test Loss: 0.00017630796473955664\n",
            "Epoch 411/1000, Training Loss: 0.00017230048197450692, Test Loss: 0.00017340497550001193\n",
            "Epoch 412/1000, Training Loss: 0.00016927691524380975, Test Loss: 0.00017057064857027598\n",
            "Epoch 413/1000, Training Loss: 0.00016632879875961873, Test Loss: 0.0001678031784622179\n",
            "Epoch 414/1000, Training Loss: 0.00016345407463120597, Test Loss: 0.00016510080893723542\n",
            "Epoch 415/1000, Training Loss: 0.0001606507419713158, Test Loss: 0.00016246183175780968\n",
            "Epoch 416/1000, Training Loss: 0.000157916855473959, Test Loss: 0.00015988458545746704\n",
            "Epoch 417/1000, Training Loss: 0.00015525052401044127, Test Loss: 0.00015736745413027332\n",
            "Epoch 418/1000, Training Loss: 0.00015264990924502474, Test Loss: 0.00015490886624074752\n",
            "Epoch 419/1000, Training Loss: 0.0001501132242714469, Test Loss: 0.00015250729345497866\n",
            "Epoch 420/1000, Training Loss: 0.00014763873227138427, Test Loss: 0.0001501612494935856\n",
            "Epoch 421/1000, Training Loss: 0.00014522474519573818, Test Loss: 0.00014786928900701967\n",
            "Epoch 422/1000, Training Loss: 0.00014286962246951731, Test Loss: 0.00014563000647362475\n",
            "Epoch 423/1000, Training Loss: 0.000140571769720957, Test Loss: 0.00014344203512073634\n",
            "Epoch 424/1000, Training Loss: 0.00013832963753535073, Test Loss: 0.00014130404586903424\n",
            "Epoch 425/1000, Training Loss: 0.00013614172023403097, Test Loss: 0.00013921474630027565\n",
            "Epoch 426/1000, Training Loss: 0.00013400655467876217, Test Loss: 0.00013717287964844541\n",
            "Epoch 427/1000, Training Loss: 0.00013192271910178674, Test Loss: 0.00013517722381433642\n",
            "Epoch 428/1000, Training Loss: 0.0001298888319616365, Test Loss: 0.00013322659040346962\n",
            "Epoch 429/1000, Training Loss: 0.00012790355082474325, Test Loss: 0.00013131982378721463\n",
            "Epoch 430/1000, Training Loss: 0.00012596557127287158, Test Loss: 0.00012945580018697888\n",
            "Epoch 431/1000, Training Loss: 0.00012407362583627082, Test Loss: 0.0001276334267812271\n",
            "Epoch 432/1000, Training Loss: 0.00012222648295241774, Test Loss: 0.0001258516408350847\n",
            "Epoch 433/1000, Training Loss: 0.00012042294595018188, Test Loss: 0.0001241094088522544\n",
            "Epoch 434/1000, Training Loss: 0.00011866185205919956, Test Loss: 0.0001224057257489445\n",
            "Epoch 435/1000, Training Loss: 0.00011694207144416991, Test Loss: 0.00012073961404946829\n",
            "Epoch 436/1000, Training Loss: 0.00011526250626380956, Test Loss: 0.00011911012310317192\n",
            "Epoch 437/1000, Training Loss: 0.00011362208975412495, Test Loss: 0.00011751632832232704\n",
            "Epoch 438/1000, Training Loss: 0.00011201978533567258, Test Loss: 0.00011595733044061545\n",
            "Epoch 439/1000, Training Loss: 0.00011045458574440905, Test Loss: 0.00011443225479178021\n",
            "Epoch 440/1000, Training Loss: 0.0001089255121857745, Test Loss: 0.00011294025060810125\n",
            "Epoch 441/1000, Training Loss: 0.00010743161351157558, Test Loss: 0.00011148049033822114\n",
            "Epoch 442/1000, Training Loss: 0.0001059719654192698, Test Loss: 0.00011005216898396316\n",
            "Epoch 443/1000, Training Loss: 0.0001045456696732099, Test Loss: 0.00010865450345568986\n",
            "Epoch 444/1000, Training Loss: 0.00010315185334741399, Test Loss: 0.00010728673194580293\n",
            "Epoch 445/1000, Training Loss: 0.0001017896680894079, Test Loss: 0.00010594811331993805\n",
            "Epoch 446/1000, Training Loss: 0.00010045828940470825, Test Loss: 0.00010463792652547744\n",
            "Epoch 447/1000, Training Loss: 9.91569159614685e-05, Test Loss: 0.00010335547001692107\n",
            "Epoch 448/1000, Training Loss: 9.788476891484277e-05, Test Loss: 0.00010210006119770608\n",
            "Epoch 449/1000, Training Loss: 9.664109125061469e-05, Test Loss: 0.00010087103587809164\n",
            "Epoch 450/1000, Training Loss: 9.542514714763354e-05, Test Loss: 9.966774774866913e-05\n",
            "Epoch 451/1000, Training Loss: 9.42362213585892e-05, Test Loss: 9.848956786909535e-05\n",
            "Epoch 452/1000, Training Loss: 9.307361860870099e-05, Test Loss: 9.733588417167793e-05\n",
            "Epoch 453/1000, Training Loss: 9.193666301184338e-05, Test Loss: 9.620610097937438e-05\n",
            "Epoch 454/1000, Training Loss: 9.082469750368319e-05, Test Loss: 9.509963853785778e-05\n",
            "Epoch 455/1000, Training Loss: 8.97370832913898e-05, Test Loss: 9.40159325612471e-05\n",
            "Epoch 456/1000, Training Loss: 8.867319931946959e-05, Test Loss: 9.295443379113285e-05\n",
            "Epoch 457/1000, Training Loss: 8.763244175130636e-05, Test Loss: 9.19146075685197e-05\n",
            "Epoch 458/1000, Training Loss: 8.661422346599844e-05, Test Loss: 9.089593341835386e-05\n",
            "Epoch 459/1000, Training Loss: 8.561797357005148e-05, Test Loss: 8.989790464624383e-05\n",
            "Epoch 460/1000, Training Loss: 8.464313692354429e-05, Test Loss: 8.892002794706664e-05\n",
            "Epoch 461/1000, Training Loss: 8.368917368035464e-05, Test Loss: 8.796182302509108e-05\n",
            "Epoch 462/1000, Training Loss: 8.275555884205835e-05, Test Loss: 8.702282222530624e-05\n",
            "Epoch 463/1000, Training Loss: 8.184178182511634e-05, Test Loss: 8.61025701756302e-05\n",
            "Epoch 464/1000, Training Loss: 8.094734604096889e-05, Test Loss: 8.520062343965817e-05\n",
            "Epoch 465/1000, Training Loss: 8.007176848867778e-05, Test Loss: 8.431655017970787e-05\n",
            "Epoch 466/1000, Training Loss: 7.921457935975129e-05, Test Loss: 8.344992982977236e-05\n",
            "Epoch 467/1000, Training Loss: 7.837532165479631e-05, Test Loss: 8.260035277816484e-05\n",
            "Epoch 468/1000, Training Loss: 7.755355081166332e-05, Test Loss: 8.17674200595276e-05\n",
            "Epoch 469/1000, Training Loss: 7.674883434473939e-05, Test Loss: 8.095074305594856e-05\n",
            "Epoch 470/1000, Training Loss: 7.596075149507293e-05, Test Loss: 8.0149943206904e-05\n",
            "Epoch 471/1000, Training Loss: 7.518889289099168e-05, Test Loss: 7.936465172776173e-05\n",
            "Epoch 472/1000, Training Loss: 7.44328602189338e-05, Test Loss: 7.859450933661035e-05\n",
            "Epoch 473/1000, Training Loss: 7.369226590415645e-05, Test Loss: 7.783916598914109e-05\n",
            "Epoch 474/1000, Training Loss: 7.296673280104783e-05, Test Loss: 7.709828062135265e-05\n",
            "Epoch 475/1000, Training Loss: 7.225589389274883e-05, Test Loss: 7.637152089984342e-05\n",
            "Epoch 476/1000, Training Loss: 7.155939199980411e-05, Test Loss: 7.565856297946166e-05\n",
            "Epoch 477/1000, Training Loss: 7.087687949757319e-05, Test Loss: 7.49590912680897e-05\n",
            "Epoch 478/1000, Training Loss: 7.020801804213345e-05, Test Loss: 7.427279819835577e-05\n",
            "Epoch 479/1000, Training Loss: 6.955247830442324e-05, Test Loss: 7.359938400604785e-05\n",
            "Epoch 480/1000, Training Loss: 6.890993971237152e-05, Test Loss: 7.293855651504292e-05\n",
            "Epoch 481/1000, Training Loss: 6.828009020076911e-05, Test Loss: 7.229003092854601e-05\n",
            "Epoch 482/1000, Training Loss: 6.766262596865354e-05, Test Loss: 7.165352962644711e-05\n",
            "Epoch 483/1000, Training Loss: 6.70572512439673e-05, Test Loss: 7.10287819686139e-05\n",
            "Epoch 484/1000, Training Loss: 6.646367805528219e-05, Test Loss: 7.041552410394184e-05\n",
            "Epoch 485/1000, Training Loss: 6.588162601035499e-05, Test Loss: 6.98134987849785e-05\n",
            "Epoch 486/1000, Training Loss: 6.53108220813209e-05, Test Loss: 6.922245518795789e-05\n",
            "Epoch 487/1000, Training Loss: 6.47510003963141e-05, Test Loss: 6.864214873808422e-05\n",
            "Epoch 488/1000, Training Loss: 6.420190203732066e-05, Test Loss: 6.807234093990353e-05\n",
            "Epoch 489/1000, Training Loss: 6.366327484406622e-05, Test Loss: 6.751279921259607e-05\n",
            "Epoch 490/1000, Training Loss: 6.313487322376094e-05, Test Loss: 6.696329673006123e-05\n",
            "Epoch 491/1000, Training Loss: 6.261645796652084e-05, Test Loss: 6.642361226563504e-05\n",
            "Epoch 492/1000, Training Loss: 6.210779606628042e-05, Test Loss: 6.58935300413014e-05\n",
            "Epoch 493/1000, Training Loss: 6.160866054704008e-05, Test Loss: 6.537283958125978e-05\n",
            "Epoch 494/1000, Training Loss: 6.11188302942815e-05, Test Loss: 6.486133556973171e-05\n",
            "Epoch 495/1000, Training Loss: 6.063808989138022e-05, Test Loss: 6.435881771284653e-05\n",
            "Epoch 496/1000, Training Loss: 6.0166229460878846e-05, Test Loss: 6.386509060451671e-05\n",
            "Epoch 497/1000, Training Loss: 5.9703044510460185e-05, Test Loss: 6.33799635961606e-05\n",
            "Epoch 498/1000, Training Loss: 5.924833578347211e-05, Test Loss: 6.290325067015735e-05\n",
            "Epoch 499/1000, Training Loss: 5.880190911387757e-05, Test Loss: 6.243477031693268e-05\n",
            "Epoch 500/1000, Training Loss: 5.8363575285482226e-05, Test Loss: 6.197434541556e-05\n",
            "Epoch 501/1000, Training Loss: 5.7933149895306456e-05, Test Loss: 6.152180311775383e-05\n",
            "Epoch 502/1000, Training Loss: 5.751045322098608e-05, Test Loss: 6.107697473518654e-05\n",
            "Epoch 503/1000, Training Loss: 5.709531009206534e-05, Test Loss: 6.063969562999875e-05\n",
            "Epoch 504/1000, Training Loss: 5.668754976506909e-05, Test Loss: 6.020980510842405e-05\n",
            "Epoch 505/1000, Training Loss: 5.628700580223427e-05, Test Loss: 5.978714631741859e-05\n",
            "Epoch 506/1000, Training Loss: 5.5893515953794746e-05, Test Loss: 5.937156614422851e-05\n",
            "Epoch 507/1000, Training Loss: 5.550692204370031e-05, Test Loss: 5.8962915118786795e-05\n",
            "Epoch 508/1000, Training Loss: 5.512706985867071e-05, Test Loss: 5.8561047318852305e-05\n",
            "Epoch 509/1000, Training Loss: 5.475380904048574e-05, Test Loss: 5.816582027783382e-05\n",
            "Epoch 510/1000, Training Loss: 5.43869929814043e-05, Test Loss: 5.7777094895191625e-05\n",
            "Epoch 511/1000, Training Loss: 5.4026478722620147e-05, Test Loss: 5.739473534935109e-05\n",
            "Epoch 512/1000, Training Loss: 5.367212685566235e-05, Test Loss: 5.701860901305823e-05\n",
            "Epoch 513/1000, Training Loss: 5.332380142665001e-05, Test Loss: 5.664858637109096e-05\n",
            "Epoch 514/1000, Training Loss: 5.298136984331375e-05, Test Loss: 5.628454094026868e-05\n",
            "Epoch 515/1000, Training Loss: 5.2644702784698756e-05, Test Loss: 5.592634919168735e-05\n",
            "Epoch 516/1000, Training Loss: 5.2313674113467215e-05, Test Loss: 5.557389047510812e-05\n",
            "Epoch 517/1000, Training Loss: 5.198816079072594e-05, Test Loss: 5.522704694544658e-05\n",
            "Epoch 518/1000, Training Loss: 5.1668042793293466e-05, Test Loss: 5.4885703491290226e-05\n",
            "Epoch 519/1000, Training Loss: 5.135320303334091e-05, Test Loss: 5.4549747665386765e-05\n",
            "Epoch 520/1000, Training Loss: 5.1043527280331037e-05, Test Loss: 5.421906961705747e-05\n",
            "Epoch 521/1000, Training Loss: 5.0738904085186036e-05, Test Loss: 5.389356202645818e-05\n",
            "Epoch 522/1000, Training Loss: 5.043922470661421e-05, Test Loss: 5.357312004064368e-05\n",
            "Epoch 523/1000, Training Loss: 5.014438303953654e-05, Test Loss: 5.325764121138744e-05\n",
            "Epoch 524/1000, Training Loss: 4.985427554554162e-05, Test Loss: 5.2947025434697344e-05\n",
            "Epoch 525/1000, Training Loss: 4.956880118531774e-05, Test Loss: 5.264117489198197e-05\n",
            "Epoch 526/1000, Training Loss: 4.9287861352989854e-05, Test Loss: 5.233999399281639e-05\n",
            "Epoch 527/1000, Training Loss: 4.901135981231821e-05, Test Loss: 5.20433893192626e-05\n",
            "Epoch 528/1000, Training Loss: 4.873920263468906e-05, Test Loss: 5.175126957169482e-05\n",
            "Epoch 529/1000, Training Loss: 4.8471298138856245e-05, Test Loss: 5.146354551610058e-05\n",
            "Epoch 530/1000, Training Loss: 4.820755683237279e-05, Test Loss: 5.1180129932792906e-05\n",
            "Epoch 531/1000, Training Loss: 4.794789135466281e-05, Test Loss: 5.090093756650744e-05\n",
            "Epoch 532/1000, Training Loss: 4.7692216421690665e-05, Test Loss: 5.062588507784381e-05\n",
            "Epoch 533/1000, Training Loss: 4.744044877217162e-05, Test Loss: 5.0354890996005105e-05\n",
            "Epoch 534/1000, Training Loss: 4.719250711528397e-05, Test Loss: 5.0087875672804664e-05\n",
            "Epoch 535/1000, Training Loss: 4.6948312079837624e-05, Test Loss: 4.9824761237902e-05\n",
            "Epoch 536/1000, Training Loss: 4.670778616485103e-05, Test Loss: 4.9565471555228375e-05\n",
            "Epoch 537/1000, Training Loss: 4.6470853691501016e-05, Test Loss: 4.9309932180574625e-05\n",
            "Epoch 538/1000, Training Loss: 4.6237440756401115e-05, Test Loss: 4.905807032030447e-05\n",
            "Epoch 539/1000, Training Loss: 4.6007475186169014e-05, Test Loss: 4.880981479116016e-05\n",
            "Epoch 540/1000, Training Loss: 4.5780886493248484e-05, Test Loss: 4.856509598112951e-05\n",
            "Epoch 541/1000, Training Loss: 4.555760583294561e-05, Test Loss: 4.832384581134939e-05\n",
            "Epoch 542/1000, Training Loss: 4.5337565961643744e-05, Test Loss: 4.808599769900853e-05\n",
            "Epoch 543/1000, Training Loss: 4.5120701196164813e-05, Test Loss: 4.785148652122635e-05\n",
            "Epoch 544/1000, Training Loss: 4.4906947374241034e-05, Test Loss: 4.762024857987372e-05\n",
            "Epoch 545/1000, Training Loss: 4.4696241816066845e-05, Test Loss: 4.739222156732323e-05\n",
            "Epoch 546/1000, Training Loss: 4.448852328689811e-05, Test Loss: 4.7167344533086886e-05\n",
            "Epoch 547/1000, Training Loss: 4.428373196066636e-05, Test Loss: 4.6945557851322263e-05\n",
            "Epoch 548/1000, Training Loss: 4.408180938458507e-05, Test Loss: 4.672680318918855e-05\n",
            "Epoch 549/1000, Training Loss: 4.38826984447084e-05, Test Loss: 4.651102347601727e-05\n",
            "Epoch 550/1000, Training Loss: 4.3686343332426096e-05, Test Loss: 4.629816287328574e-05\n",
            "Epoch 551/1000, Training Loss: 4.349268951186041e-05, Test Loss: 4.608816674536273e-05\n",
            "Epoch 552/1000, Training Loss: 4.3301683688140265e-05, Test Loss: 4.5880981631012425e-05\n",
            "Epoch 553/1000, Training Loss: 4.311327377653072e-05, Test Loss: 4.5676555215627476e-05\n",
            "Epoch 554/1000, Training Loss: 4.2927408872387056e-05, Test Loss: 4.547483630417466e-05\n",
            "Epoch 555/1000, Training Loss: 4.274403922191567e-05, Test Loss: 4.527577479483785e-05\n",
            "Epoch 556/1000, Training Loss: 4.256311619371526e-05, Test Loss: 4.5079321653330944e-05\n",
            "Epoch 557/1000, Training Loss: 4.2384592251076875e-05, Test Loss: 4.488542888786731e-05\n",
            "Epoch 558/1000, Training Loss: 4.2208420925021834e-05, Test Loss: 4.469404952476437e-05\n",
            "Epoch 559/1000, Training Loss: 4.203455678805465e-05, Test Loss: 4.450513758467244e-05\n",
            "Epoch 560/1000, Training Loss: 4.1862955428613205e-05, Test Loss: 4.431864805940481e-05\n",
            "Epoch 561/1000, Training Loss: 4.169357342619459e-05, Test Loss: 4.4134536889350247e-05\n",
            "Epoch 562/1000, Training Loss: 4.15263683271359e-05, Test Loss: 4.395276094145716e-05\n",
            "Epoch 563/1000, Training Loss: 4.136129862103603e-05, Test Loss: 4.377327798777737e-05\n",
            "Epoch 564/1000, Training Loss: 4.119832371779656e-05, Test Loss: 4.359604668454528e-05\n",
            "Epoch 565/1000, Training Loss: 4.103740392526541e-05, Test Loss: 4.342102655177798e-05\n",
            "Epoch 566/1000, Training Loss: 4.087850042746614e-05, Test Loss: 4.324817795339814e-05\n",
            "Epoch 567/1000, Training Loss: 4.0721575263395207e-05, Test Loss: 4.307746207783729e-05\n",
            "Epoch 568/1000, Training Loss: 4.056659130637722e-05, Test Loss: 4.290884091914001e-05\n",
            "Epoch 569/1000, Training Loss: 4.041351224395237e-05, Test Loss: 4.2742277258525924e-05\n",
            "Epoch 570/1000, Training Loss: 4.02623025582901e-05, Test Loss: 4.257773464640992e-05\n",
            "Epoch 571/1000, Training Loss: 4.0112927507110196e-05, Test Loss: 4.24151773848817e-05\n",
            "Epoch 572/1000, Training Loss: 3.996535310509796e-05, Test Loss: 4.225457051059964e-05\n",
            "Epoch 573/1000, Training Loss: 3.981954610579963e-05, Test Loss: 4.20958797781214e-05\n",
            "Epoch 574/1000, Training Loss: 3.967547398398592e-05, Test Loss: 4.19390716436368e-05\n",
            "Epoch 575/1000, Training Loss: 3.9533104918467505e-05, Test Loss: 4.178411324910549e-05\n",
            "Epoch 576/1000, Training Loss: 3.939240777535484e-05, Test Loss: 4.163097240678319e-05\n",
            "Epoch 577/1000, Training Loss: 3.9253352091744525e-05, Test Loss: 4.147961758412424e-05\n",
            "Epoch 578/1000, Training Loss: 3.9115908059826735e-05, Test Loss: 4.133001788905356e-05\n",
            "Epoch 579/1000, Training Loss: 3.898004651139414e-05, Test Loss: 4.118214305560258e-05\n",
            "Epoch 580/1000, Training Loss: 3.884573890275148e-05, Test Loss: 4.1035963429887974e-05\n",
            "Epoch 581/1000, Training Loss: 3.8712957300004354e-05, Test Loss: 4.0891449956435676e-05\n",
            "Epoch 582/1000, Training Loss: 3.8581674364724505e-05, Test Loss: 4.074857416483444e-05\n",
            "Epoch 583/1000, Training Loss: 3.845186333997828e-05, Test Loss: 4.0607308156715314e-05\n",
            "Epoch 584/1000, Training Loss: 3.832349803670744e-05, Test Loss: 4.046762459304019e-05\n",
            "Epoch 585/1000, Training Loss: 3.819655282045489e-05, Test Loss: 4.032949668170085e-05\n",
            "Epoch 586/1000, Training Loss: 3.807100259842647e-05, Test Loss: 4.0192898165417126e-05\n",
            "Epoch 587/1000, Training Loss: 3.7946822806875435e-05, Test Loss: 4.0057803309920644e-05\n",
            "Epoch 588/1000, Training Loss: 3.7823989398806096e-05, Test Loss: 3.992418689242794e-05\n",
            "Epoch 589/1000, Training Loss: 3.7702478831985244e-05, Test Loss: 3.9792024190383876e-05\n",
            "Epoch 590/1000, Training Loss: 3.758226805725328e-05, Test Loss: 3.9661290970477176e-05\n",
            "Epoch 591/1000, Training Loss: 3.7463334507127466e-05, Test Loss: 3.9531963477916174e-05\n",
            "Epoch 592/1000, Training Loss: 3.734565608468875e-05, Test Loss: 3.9404018425962e-05\n",
            "Epoch 593/1000, Training Loss: 3.722921115274725e-05, Test Loss: 3.927743298570441e-05\n",
            "Epoch 594/1000, Training Loss: 3.7113978523273594e-05, Test Loss: 3.91521847760846e-05\n",
            "Epoch 595/1000, Training Loss: 3.6999937447095715e-05, Test Loss: 3.902825185415525e-05\n",
            "Epoch 596/1000, Training Loss: 3.688706760384774e-05, Test Loss: 3.890561270556585e-05\n",
            "Epoch 597/1000, Training Loss: 3.6775349092168664e-05, Test Loss: 3.878424623527449e-05\n",
            "Epoch 598/1000, Training Loss: 3.666476242014354e-05, Test Loss: 3.86641317584765e-05\n",
            "Epoch 599/1000, Training Loss: 3.655528849597759e-05, Test Loss: 3.854524899175094e-05\n",
            "Epoch 600/1000, Training Loss: 3.6446908618901306e-05, Test Loss: 3.842757804440638e-05\n",
            "Epoch 601/1000, Training Loss: 3.633960447029746e-05, Test Loss: 3.831109941003508e-05\n",
            "Epoch 602/1000, Training Loss: 3.62333581050471e-05, Test Loss: 3.8195793958263944e-05\n",
            "Epoch 603/1000, Training Loss: 3.612815194308446e-05, Test Loss: 3.8081642926693793e-05\n",
            "Epoch 604/1000, Training Loss: 3.602396876115955e-05, Test Loss: 3.796862791302756e-05\n",
            "Epoch 605/1000, Training Loss: 3.5920791684801085e-05, Test Loss: 3.7856730867387916e-05\n",
            "Epoch 606/1000, Training Loss: 3.581860418047417e-05, Test Loss: 3.774593408479568e-05\n",
            "Epoch 607/1000, Training Loss: 3.5717390047929275e-05, Test Loss: 3.7636220197844025e-05\n",
            "Epoch 608/1000, Training Loss: 3.5617133412735233e-05, Test Loss: 3.7527572169526956e-05\n",
            "Epoch 609/1000, Training Loss: 3.551781871899294e-05, Test Loss: 3.741997328623395e-05\n",
            "Epoch 610/1000, Training Loss: 3.541943072222645e-05, Test Loss: 3.731340715091358e-05\n",
            "Epoch 611/1000, Training Loss: 3.532195448244311e-05, Test Loss: 3.72078576763867e-05\n",
            "Epoch 612/1000, Training Loss: 3.522537535736156e-05, Test Loss: 3.710330907881416e-05\n",
            "Epoch 613/1000, Training Loss: 3.5129678995803574e-05, Test Loss: 3.6999745871317443e-05\n",
            "Epoch 614/1000, Training Loss: 3.503485133124072e-05, Test Loss: 3.689715285774083e-05\n",
            "Epoch 615/1000, Training Loss: 3.4940878575499645e-05, Test Loss: 3.679551512655755e-05\n",
            "Epoch 616/1000, Training Loss: 3.484774721261425e-05, Test Loss: 3.669481804491447e-05\n",
            "Epoch 617/1000, Training Loss: 3.47554439928265e-05, Test Loss: 3.659504725281181e-05\n",
            "Epoch 618/1000, Training Loss: 3.466395592672885e-05, Test Loss: 3.649618865741458e-05\n",
            "Epoch 619/1000, Training Loss: 3.457327027954653e-05, Test Loss: 3.639822842749429e-05\n",
            "Epoch 620/1000, Training Loss: 3.4483374565554844e-05, Test Loss: 3.6301152987993154e-05\n",
            "Epoch 621/1000, Training Loss: 3.439425654262902e-05, Test Loss: 3.620494901471333e-05\n",
            "Epoch 622/1000, Training Loss: 3.430590420692221e-05, Test Loss: 3.610960342912416e-05\n",
            "Epoch 623/1000, Training Loss: 3.4218305787669854e-05, Test Loss: 3.601510339328897e-05\n",
            "Epoch 624/1000, Training Loss: 3.4131449742116055e-05, Test Loss: 3.5921436304899785e-05\n",
            "Epoch 625/1000, Training Loss: 3.404532475055944e-05, Test Loss: 3.582858979243158e-05\n",
            "Epoch 626/1000, Training Loss: 3.395991971151484e-05, Test Loss: 3.5736551710394903e-05\n",
            "Epoch 627/1000, Training Loss: 3.387522373698843e-05, Test Loss: 3.564531013470639e-05\n",
            "Epoch 628/1000, Training Loss: 3.3791226147864055e-05, Test Loss: 3.5554853358149314e-05\n",
            "Epoch 629/1000, Training Loss: 3.370791646939634e-05, Test Loss: 3.5465169885943685e-05\n",
            "Epoch 630/1000, Training Loss: 3.3625284426809264e-05, Test Loss: 3.5376248431414636e-05\n",
            "Epoch 631/1000, Training Loss: 3.354331994099807e-05, Test Loss: 3.5288077911751335e-05\n",
            "Epoch 632/1000, Training Loss: 3.346201312432898e-05, Test Loss: 3.5200647443867475e-05\n",
            "Epoch 633/1000, Training Loss: 3.338135427653877e-05, Test Loss: 3.511394634034293e-05\n",
            "Epoch 634/1000, Training Loss: 3.330133388072741e-05, Test Loss: 3.5027964105464966e-05\n",
            "Epoch 635/1000, Training Loss: 3.322194259944454e-05, Test Loss: 3.494269043135308e-05\n",
            "Epoch 636/1000, Training Loss: 3.3143171270865225e-05, Test Loss: 3.4858115194163166e-05\n",
            "Epoch 637/1000, Training Loss: 3.306501090505495e-05, Test Loss: 3.4774228450384846e-05\n",
            "Epoch 638/1000, Training Loss: 3.298745268031957e-05, Test Loss: 3.4691020433213556e-05\n",
            "Epoch 639/1000, Training Loss: 3.291048793964073e-05, Test Loss: 3.4608481549004846e-05\n",
            "Epoch 640/1000, Training Loss: 3.283410818719044e-05, Test Loss: 3.452660237380091e-05\n",
            "Epoch 641/1000, Training Loss: 3.275830508492774e-05, Test Loss: 3.444537364994109e-05\n",
            "Epoch 642/1000, Training Loss: 3.268307044927211e-05, Test Loss: 3.436478628274009e-05\n",
            "Epoch 643/1000, Training Loss: 3.26083962478529e-05, Test Loss: 3.428483133723783e-05\n",
            "Epoch 644/1000, Training Loss: 3.253427459633333e-05, Test Loss: 3.420550003502601e-05\n",
            "Epoch 645/1000, Training Loss: 3.246069775530597e-05, Test Loss: 3.412678375113694e-05\n",
            "Epoch 646/1000, Training Loss: 3.238765812725887e-05, Test Loss: 3.4048674011000356e-05\n",
            "Epoch 647/1000, Training Loss: 3.231514825361186e-05, Test Loss: 3.397116248747092e-05\n",
            "Epoch 648/1000, Training Loss: 3.224316081181751e-05, Test Loss: 3.3894240997914e-05\n",
            "Epoch 649/1000, Training Loss: 3.2171688612530286e-05, Test Loss: 3.3817901501359347e-05\n",
            "Epoch 650/1000, Training Loss: 3.210072459683675e-05, Test Loss: 3.374213609570833e-05\n",
            "Epoch 651/1000, Training Loss: 3.203026183355131e-05, Test Loss: 3.3666937015010714e-05\n",
            "Epoch 652/1000, Training Loss: 3.1960293516569745e-05, Test Loss: 3.359229662679168e-05\n",
            "Epoch 653/1000, Training Loss: 3.189081296228488e-05, Test Loss: 3.351820742944045e-05\n",
            "Epoch 654/1000, Training Loss: 3.18218136070581e-05, Test Loss: 3.3444662049653096e-05\n",
            "Epoch 655/1000, Training Loss: 3.1753289004748836e-05, Test Loss: 3.337165323993056e-05\n",
            "Epoch 656/1000, Training Loss: 3.168523282429892e-05, Test Loss: 3.329917387612856e-05\n",
            "Epoch 657/1000, Training Loss: 3.161763884736987e-05, Test Loss: 3.322721695506018e-05\n",
            "Epoch 658/1000, Training Loss: 3.1550500966035136e-05, Test Loss: 3.315577559215078e-05\n",
            "Epoch 659/1000, Training Loss: 3.148381318052155e-05, Test Loss: 3.3084843019140176e-05\n",
            "Epoch 660/1000, Training Loss: 3.1417569597001483e-05, Test Loss: 3.301441258183361e-05\n",
            "Epoch 661/1000, Training Loss: 3.1351764425435486e-05, Test Loss: 3.2944477737904895e-05\n",
            "Epoch 662/1000, Training Loss: 3.128639197745971e-05, Test Loss: 3.28750320547377e-05\n",
            "Epoch 663/1000, Training Loss: 3.122144666432346e-05, Test Loss: 3.280606920731928e-05\n",
            "Epoch 664/1000, Training Loss: 3.115692299486972e-05, Test Loss: 3.2737582976173814e-05\n",
            "Epoch 665/1000, Training Loss: 3.109281557356106e-05, Test Loss: 3.26695672453416e-05\n",
            "Epoch 666/1000, Training Loss: 3.102911909855013e-05, Test Loss: 3.260201600040158e-05\n",
            "Epoch 667/1000, Training Loss: 3.096582835979102e-05, Test Loss: 3.253492332653215e-05\n",
            "Epoch 668/1000, Training Loss: 3.090293823719245e-05, Test Loss: 3.2468283406614327e-05\n",
            "Epoch 669/1000, Training Loss: 3.084044369881153e-05, Test Loss: 3.24020905193727e-05\n",
            "Epoch 670/1000, Training Loss: 3.077833979908802e-05, Test Loss: 3.233633903755915e-05\n",
            "Epoch 671/1000, Training Loss: 3.071662167711512e-05, Test Loss: 3.22710234261702e-05\n",
            "Epoch 672/1000, Training Loss: 3.065528455495027e-05, Test Loss: 3.220613824070175e-05\n",
            "Epoch 673/1000, Training Loss: 3.05943237359612e-05, Test Loss: 3.214167812544327e-05\n",
            "Epoch 674/1000, Training Loss: 3.053373460320841e-05, Test Loss: 3.20776378118023e-05\n",
            "Epoch 675/1000, Training Loss: 3.04735126178623e-05, Test Loss: 3.2014012116667054e-05\n",
            "Epoch 676/1000, Training Loss: 3.0413653317656882e-05, Test Loss: 3.1950795940804596e-05\n",
            "Epoch 677/1000, Training Loss: 3.0354152315373267e-05, Test Loss: 3.1887984267285014e-05\n",
            "Epoch 678/1000, Training Loss: 3.0295005297359584e-05, Test Loss: 3.1825572159947257e-05\n",
            "Epoch 679/1000, Training Loss: 3.0236208022079664e-05, Test Loss: 3.176355476188553e-05\n",
            "Epoch 680/1000, Training Loss: 3.0177756318695764e-05, Test Loss: 3.170192729397868e-05\n",
            "Epoch 681/1000, Training Loss: 3.0119646085680596e-05, Test Loss: 3.164068505344003e-05\n",
            "Epoch 682/1000, Training Loss: 3.0061873289458236e-05, Test Loss: 3.157982341240133e-05\n",
            "Epoch 683/1000, Training Loss: 3.0004433963076326e-05, Test Loss: 3.151933781652601e-05\n",
            "Epoch 684/1000, Training Loss: 2.994732420490502e-05, Test Loss: 3.145922378364747e-05\n",
            "Epoch 685/1000, Training Loss: 2.9890540177364404e-05, Test Loss: 3.139947690243896e-05\n",
            "Epoch 686/1000, Training Loss: 2.9834078105679547e-05, Test Loss: 3.1340092831108416e-05\n",
            "Epoch 687/1000, Training Loss: 2.9777934276660972e-05, Test Loss: 3.128106729611832e-05\n",
            "Epoch 688/1000, Training Loss: 2.9722105037512136e-05, Test Loss: 3.122239609093387e-05\n",
            "Epoch 689/1000, Training Loss: 2.966658679466226e-05, Test Loss: 3.1164075074797245e-05\n",
            "Epoch 690/1000, Training Loss: 2.961137601262289e-05, Test Loss: 3.11061001715217e-05\n",
            "Epoch 691/1000, Training Loss: 2.9556469212870685e-05, Test Loss: 3.104846736831845e-05\n",
            "Epoch 692/1000, Training Loss: 2.9501862972751494e-05, Test Loss: 3.099117271463608e-05\n",
            "Epoch 693/1000, Training Loss: 2.9447553924409072e-05, Test Loss: 3.093421232103338e-05\n",
            "Epoch 694/1000, Training Loss: 2.9393538753736916e-05, Test Loss: 3.087758235806965e-05\n",
            "Epoch 695/1000, Training Loss: 2.933981419935074e-05, Test Loss: 3.0821279055218097e-05\n",
            "Epoch 696/1000, Training Loss: 2.928637705158335e-05, Test Loss: 3.0765298699800755e-05\n",
            "Epoch 697/1000, Training Loss: 2.9233224151500644e-05, Test Loss: 3.070963763594394e-05\n",
            "Epoch 698/1000, Training Loss: 2.9180352389938303e-05, Test Loss: 3.0654292263559005e-05\n",
            "Epoch 699/1000, Training Loss: 2.912775870655887e-05, Test Loss: 3.059925903733804e-05\n",
            "Epoch 700/1000, Training Loss: 2.90754400889284e-05, Test Loss: 3.054453446576976e-05\n",
            "Epoch 701/1000, Training Loss: 2.9023393571611682e-05, Test Loss: 3.0490115110180815e-05\n",
            "Epoch 702/1000, Training Loss: 2.8971616235286914e-05, Test Loss: 3.04359975837862e-05\n",
            "Epoch 703/1000, Training Loss: 2.892010520588034e-05, Test Loss: 3.0382178550771256e-05\n",
            "Epoch 704/1000, Training Loss: 2.8868857653716278e-05, Test Loss: 3.0328654725379677e-05\n",
            "Epoch 705/1000, Training Loss: 2.881787079268598e-05, Test Loss: 3.027542287102596e-05\n",
            "Epoch 706/1000, Training Loss: 2.876714187943474e-05, Test Loss: 3.022247979942377e-05\n",
            "Epoch 707/1000, Training Loss: 2.8716668212564055e-05, Test Loss: 3.0169822369733914e-05\n",
            "Epoch 708/1000, Training Loss: 2.8666447131851712e-05, Test Loss: 3.011744748772339e-05\n",
            "Epoch 709/1000, Training Loss: 2.8616476017486996e-05, Test Loss: 3.0065352104947064e-05\n",
            "Epoch 710/1000, Training Loss: 2.856675228932235e-05, Test Loss: 3.0013533217940876e-05\n",
            "Epoch 711/1000, Training Loss: 2.851727340614024e-05, Test Loss: 2.9961987867434064e-05\n",
            "Epoch 712/1000, Training Loss: 2.8468036864935383e-05, Test Loss: 2.9910713137575163e-05\n",
            "Epoch 713/1000, Training Loss: 2.8419040200210372e-05, Test Loss: 2.98597061551716e-05\n",
            "Epoch 714/1000, Training Loss: 2.8370280983288175e-05, Test Loss: 2.9808964088948387e-05\n",
            "Epoch 715/1000, Training Loss: 2.8321756821636663e-05, Test Loss: 2.9758484148816343e-05\n",
            "Epoch 716/1000, Training Loss: 2.827346535820767e-05, Test Loss: 2.9708263585157313e-05\n",
            "Epoch 717/1000, Training Loss: 2.822540427078941e-05, Test Loss: 2.9658299688124753e-05\n",
            "Epoch 718/1000, Training Loss: 2.8177571271372683e-05, Test Loss: 2.960858978695237e-05\n",
            "Epoch 719/1000, Training Loss: 2.8129964105529197e-05, Test Loss: 2.9559131249282407e-05\n",
            "Epoch 720/1000, Training Loss: 2.8082580551802676e-05, Test Loss: 2.9509921480501568e-05\n",
            "Epoch 721/1000, Training Loss: 2.8035418421112918e-05, Test Loss: 2.946095792309482e-05\n",
            "Epoch 722/1000, Training Loss: 2.7988475556171418e-05, Test Loss: 2.9412238056006357e-05\n",
            "Epoch 723/1000, Training Loss: 2.794174983090912e-05, Test Loss: 2.9363759394018334e-05\n",
            "Epoch 724/1000, Training Loss: 2.789523914991439e-05, Test Loss: 2.931551948713453e-05\n",
            "Epoch 725/1000, Training Loss: 2.784894144788523e-05, Test Loss: 2.926751591998431e-05\n",
            "Epoch 726/1000, Training Loss: 2.7802854689089955e-05, Test Loss: 2.9219746311232997e-05\n",
            "Epoch 727/1000, Training Loss: 2.775697686683917e-05, Test Loss: 2.9172208312999044e-05\n",
            "Epoch 728/1000, Training Loss: 2.7711306002969277e-05, Test Loss: 2.9124899610292545e-05\n",
            "Epoch 729/1000, Training Loss: 2.7665840147335995e-05, Test Loss: 2.9077817920459206e-05\n",
            "Epoch 730/1000, Training Loss: 2.762057737731717e-05, Test Loss: 2.9030960992631464e-05\n",
            "Epoch 731/1000, Training Loss: 2.757551579732643e-05, Test Loss: 2.8984326607195776e-05\n",
            "Epoch 732/1000, Training Loss: 2.7530653538336884e-05, Test Loss: 2.8937912575266305e-05\n",
            "Epoch 733/1000, Training Loss: 2.7485988757413315e-05, Test Loss: 2.8891716738173235e-05\n",
            "Epoch 734/1000, Training Loss: 2.7441519637253688e-05, Test Loss: 2.8845736966952374e-05\n",
            "Epoch 735/1000, Training Loss: 2.739724438574106e-05, Test Loss: 2.8799971161853108e-05\n",
            "Epoch 736/1000, Training Loss: 2.73531612355031e-05, Test Loss: 2.8754417251849784e-05\n",
            "Epoch 737/1000, Training Loss: 2.7309268443480937e-05, Test Loss: 2.870907319416791e-05\n",
            "Epoch 738/1000, Training Loss: 2.726556429050592e-05, Test Loss: 2.8663936973809548e-05\n",
            "Epoch 739/1000, Training Loss: 2.7222047080885978e-05, Test Loss: 2.8619006603099257e-05\n",
            "Epoch 740/1000, Training Loss: 2.7178715141998635e-05, Test Loss: 2.8574280121231432e-05\n",
            "Epoch 741/1000, Training Loss: 2.7135566823893222e-05, Test Loss: 2.8529755593826653e-05\n",
            "Epoch 742/1000, Training Loss: 2.7092600498900477e-05, Test Loss: 2.8485431112498537e-05\n",
            "Epoch 743/1000, Training Loss: 2.7049814561249274e-05, Test Loss: 2.844130479442743e-05\n",
            "Epoch 744/1000, Training Loss: 2.700720742669207e-05, Test Loss: 2.839737478194092e-05\n",
            "Epoch 745/1000, Training Loss: 2.6964777532136157e-05, Test Loss: 2.8353639242102788e-05\n",
            "Epoch 746/1000, Training Loss: 2.692252333528399e-05, Test Loss: 2.8310096366313727e-05\n",
            "Epoch 747/1000, Training Loss: 2.6880443314278733e-05, Test Loss: 2.8266744369911203e-05\n",
            "Epoch 748/1000, Training Loss: 2.6838535967357623e-05, Test Loss: 2.822358149178407e-05\n",
            "Epoch 749/1000, Training Loss: 2.67967998125122e-05, Test Loss: 2.8180605993990158e-05\n",
            "Epoch 750/1000, Training Loss: 2.6755233387154716e-05, Test Loss: 2.813781616138347e-05\n",
            "Epoch 751/1000, Training Loss: 2.671383524779075e-05, Test Loss: 2.8095210301245396e-05\n",
            "Epoch 752/1000, Training Loss: 2.6672603969699385e-05, Test Loss: 2.8052786742924512e-05\n",
            "Epoch 753/1000, Training Loss: 2.6631538146618247e-05, Test Loss: 2.8010543837484612e-05\n",
            "Epoch 754/1000, Training Loss: 2.6590636390434664e-05, Test Loss: 2.79684799573538e-05\n",
            "Epoch 755/1000, Training Loss: 2.6549897330884168e-05, Test Loss: 2.7926593495987886e-05\n",
            "Epoch 756/1000, Training Loss: 2.6509319615253558e-05, Test Loss: 2.7884882867530923e-05\n",
            "Epoch 757/1000, Training Loss: 2.6468901908089572e-05, Test Loss: 2.7843346506488722e-05\n",
            "Epoch 758/1000, Training Loss: 2.642864289091365e-05, Test Loss: 2.780198286740757e-05\n",
            "Epoch 759/1000, Training Loss: 2.6388541261942522e-05, Test Loss: 2.776079042455333e-05\n",
            "Epoch 760/1000, Training Loss: 2.6348595735812978e-05, Test Loss: 2.7719767671603362e-05\n",
            "Epoch 761/1000, Training Loss: 2.6308805043313123e-05, Test Loss: 2.767891312134044e-05\n",
            "Epoch 762/1000, Training Loss: 2.6269167931117836e-05, Test Loss: 2.763822530535179e-05\n",
            "Epoch 763/1000, Training Loss: 2.6229683161529682e-05, Test Loss: 2.7597702773734335e-05\n",
            "Epoch 764/1000, Training Loss: 2.6190349512224824e-05, Test Loss: 2.7557344094806356e-05\n",
            "Epoch 765/1000, Training Loss: 2.6151165776003284e-05, Test Loss: 2.751714785482245e-05\n",
            "Epoch 766/1000, Training Loss: 2.6112130760544282e-05, Test Loss: 2.7477112657694776e-05\n",
            "Epoch 767/1000, Training Loss: 2.607324328816646e-05, Test Loss: 2.74372371247179e-05\n",
            "Epoch 768/1000, Training Loss: 2.6034502195591774e-05, Test Loss: 2.739751989430172e-05\n",
            "Epoch 769/1000, Training Loss: 2.5995906333714714e-05, Test Loss: 2.7357959621704107e-05\n",
            "Epoch 770/1000, Training Loss: 2.5957454567375734e-05, Test Loss: 2.731855497877477e-05\n",
            "Epoch 771/1000, Training Loss: 2.591914577513801e-05, Test Loss: 2.7279304653695762e-05\n",
            "Epoch 772/1000, Training Loss: 2.5880978849069846e-05, Test Loss: 2.7240207350735604e-05\n",
            "Epoch 773/1000, Training Loss: 2.5842952694529678e-05, Test Loss: 2.7201261789999488e-05\n",
            "Epoch 774/1000, Training Loss: 2.5805066229955864e-05, Test Loss: 2.7162466707189295e-05\n",
            "Epoch 775/1000, Training Loss: 2.576731838666085e-05, Test Loss: 2.7123820853364208e-05\n",
            "Epoch 776/1000, Training Loss: 2.572970810862797e-05, Test Loss: 2.708532299471035e-05\n",
            "Epoch 777/1000, Training Loss: 2.56922343523129e-05, Test Loss: 2.704697191230862e-05\n",
            "Epoch 778/1000, Training Loss: 2.56548960864485e-05, Test Loss: 2.7008766401910278e-05\n",
            "Epoch 779/1000, Training Loss: 2.5617692291853383e-05, Test Loss: 2.6970705273717443e-05\n",
            "Epoch 780/1000, Training Loss: 2.5580621961243866e-05, Test Loss: 2.693278735216237e-05\n",
            "Epoch 781/1000, Training Loss: 2.5543684099049397e-05, Test Loss: 2.6895011475696376e-05\n",
            "Epoch 782/1000, Training Loss: 2.5506877721231297e-05, Test Loss: 2.685737649657895e-05\n",
            "Epoch 783/1000, Training Loss: 2.5470201855105193e-05, Test Loss: 2.68198812806716e-05\n",
            "Epoch 784/1000, Training Loss: 2.5433655539166504e-05, Test Loss: 2.678252470723601e-05\n",
            "Epoch 785/1000, Training Loss: 2.539723782291892e-05, Test Loss: 2.674530566873289e-05\n",
            "Epoch 786/1000, Training Loss: 2.5360947766705735e-05, Test Loss: 2.6708223070626267e-05\n",
            "Epoch 787/1000, Training Loss: 2.5324784441545734e-05, Test Loss: 2.6671275831194368e-05\n",
            "Epoch 788/1000, Training Loss: 2.5288746928969764e-05, Test Loss: 2.6634462881337673e-05\n",
            "Epoch 789/1000, Training Loss: 2.525283432086295e-05, Test Loss: 2.6597783164394338e-05\n",
            "Epoch 790/1000, Training Loss: 2.5217045719306825e-05, Test Loss: 2.6561235635955266e-05\n",
            "Epoch 791/1000, Training Loss: 2.5181380236426724e-05, Test Loss: 2.6524819263687586e-05\n",
            "Epoch 792/1000, Training Loss: 2.5145836994240542e-05, Test Loss: 2.6488533027156882e-05\n",
            "Epoch 793/1000, Training Loss: 2.5110415124511635e-05, Test Loss: 2.6452375917654647e-05\n",
            "Epoch 794/1000, Training Loss: 2.5075113768601764e-05, Test Loss: 2.6416346938024658e-05\n",
            "Epoch 795/1000, Training Loss: 2.503993207733052e-05, Test Loss: 2.638044510250232e-05\n",
            "Epoch 796/1000, Training Loss: 2.5004869210832457e-05, Test Loss: 2.6344669436540632e-05\n",
            "Epoch 797/1000, Training Loss: 2.4969924338421963e-05, Test Loss: 2.630901897665656e-05\n",
            "Epoch 798/1000, Training Loss: 2.4935096638456437e-05, Test Loss: 2.627349277026757e-05\n",
            "Epoch 799/1000, Training Loss: 2.490038529820365e-05, Test Loss: 2.6238089875534192e-05\n",
            "Epoch 800/1000, Training Loss: 2.4865789513711518e-05, Test Loss: 2.6202809361210692e-05\n",
            "Epoch 801/1000, Training Loss: 2.4831308489679707e-05, Test Loss: 2.616765030648964e-05\n",
            "Epoch 802/1000, Training Loss: 2.4796941439334213e-05, Test Loss: 2.6132611800854603e-05\n",
            "Epoch 803/1000, Training Loss: 2.476268758430287e-05, Test Loss: 2.6097692943933968e-05\n",
            "Epoch 804/1000, Training Loss: 2.472854615449421e-05, Test Loss: 2.606289284535763e-05\n",
            "Epoch 805/1000, Training Loss: 2.469451638797829e-05, Test Loss: 2.6028210624613927e-05\n",
            "Epoch 806/1000, Training Loss: 2.4660597530868912e-05, Test Loss: 2.5993645410913167e-05\n",
            "Epoch 807/1000, Training Loss: 2.462678883720885e-05, Test Loss: 2.595919634304803e-05\n",
            "Epoch 808/1000, Training Loss: 2.4593089568856353e-05, Test Loss: 2.592486256926267e-05\n",
            "Epoch 809/1000, Training Loss: 2.4559498995373985e-05, Test Loss: 2.589064324711548e-05\n",
            "Epoch 810/1000, Training Loss: 2.4526016393919288e-05, Test Loss: 2.5856537543353638e-05\n",
            "Epoch 811/1000, Training Loss: 2.449264104913714e-05, Test Loss: 2.582254463378318e-05\n",
            "Epoch 812/1000, Training Loss: 2.4459372253054405e-05, Test Loss: 2.5788663703142607e-05\n",
            "Epoch 813/1000, Training Loss: 2.4426209304976477e-05, Test Loss: 2.5754893944981266e-05\n",
            "Epoch 814/1000, Training Loss: 2.4393151511384786e-05, Test Loss: 2.572123456153499e-05\n",
            "Epoch 815/1000, Training Loss: 2.436019818583643e-05, Test Loss: 2.5687684763608202e-05\n",
            "Epoch 816/1000, Training Loss: 2.4327348648866856e-05, Test Loss: 2.5654243770456493e-05\n",
            "Epoch 817/1000, Training Loss: 2.429460222789186e-05, Test Loss: 2.5620910809669512e-05\n",
            "Epoch 818/1000, Training Loss: 2.4261958257112763e-05, Test Loss: 2.5587685117056793e-05\n",
            "Epoch 819/1000, Training Loss: 2.4229416077423256e-05, Test Loss: 2.5554565936537223e-05\n",
            "Epoch 820/1000, Training Loss: 2.419697503631685e-05, Test Loss: 2.5521552520026586e-05\n",
            "Epoch 821/1000, Training Loss: 2.4164634487797335e-05, Test Loss: 2.548864412733136e-05\n",
            "Epoch 822/1000, Training Loss: 2.413239379228927e-05, Test Loss: 2.545584002604162e-05\n",
            "Epoch 823/1000, Training Loss: 2.4100252316550486e-05, Test Loss: 2.5423139491423092e-05\n",
            "Epoch 824/1000, Training Loss: 2.406820943358711e-05, Test Loss: 2.5390541806317055e-05\n",
            "Epoch 825/1000, Training Loss: 2.4036264522568168e-05, Test Loss: 2.5358046261038217e-05\n",
            "Epoch 826/1000, Training Loss: 2.400441696874366e-05, Test Loss: 2.5325652153272905e-05\n",
            "Epoch 827/1000, Training Loss: 2.397266616336166e-05, Test Loss: 2.5293358787982285e-05\n",
            "Epoch 828/1000, Training Loss: 2.3941011503588917e-05, Test Loss: 2.526116547730494e-05\n",
            "Epoch 829/1000, Training Loss: 2.3909452392431785e-05, Test Loss: 2.5229071540460095e-05\n",
            "Epoch 830/1000, Training Loss: 2.3877988238658764e-05, Test Loss: 2.5197076303656237e-05\n",
            "Epoch 831/1000, Training Loss: 2.384661845672371e-05, Test Loss: 2.5165179099998243e-05\n",
            "Epoch 832/1000, Training Loss: 2.381534246669109e-05, Test Loss: 2.513337926939482e-05\n",
            "Epoch 833/1000, Training Loss: 2.378415969416193e-05, Test Loss: 2.510167615846879e-05\n",
            "Epoch 834/1000, Training Loss: 2.3753069570202112e-05, Test Loss: 2.5070069120473097e-05\n",
            "Epoch 835/1000, Training Loss: 2.3722071531269597e-05, Test Loss: 2.5038557515198456e-05\n",
            "Epoch 836/1000, Training Loss: 2.36911650191453e-05, Test Loss: 2.500714070889389e-05\n",
            "Epoch 837/1000, Training Loss: 2.366034948086323e-05, Test Loss: 2.4975818074176747e-05\n",
            "Epoch 838/1000, Training Loss: 2.3629624368643146e-05, Test Loss: 2.4944588989954367e-05\n",
            "Epoch 839/1000, Training Loss: 2.3598989139823136e-05, Test Loss: 2.4913452841341536e-05\n",
            "Epoch 840/1000, Training Loss: 2.3568443256794484e-05, Test Loss: 2.4882409019579972e-05\n",
            "Epoch 841/1000, Training Loss: 2.35379861869363e-05, Test Loss: 2.4851456921958114e-05\n",
            "Epoch 842/1000, Training Loss: 2.3507617402552684e-05, Test Loss: 2.482059595173876e-05\n",
            "Epoch 843/1000, Training Loss: 2.347733638080907e-05, Test Loss: 2.4789825518074525e-05\n",
            "Epoch 844/1000, Training Loss: 2.3447142603671033e-05, Test Loss: 2.4759145035939115e-05\n",
            "Epoch 845/1000, Training Loss: 2.3417035557844276e-05, Test Loss: 2.4728553926050707e-05\n",
            "Epoch 846/1000, Training Loss: 2.3387014734714074e-05, Test Loss: 2.469805161479844e-05\n",
            "Epoch 847/1000, Training Loss: 2.3357079630286943e-05, Test Loss: 2.4667637534172833e-05\n",
            "Epoch 848/1000, Training Loss: 2.3327229745132714e-05, Test Loss: 2.46373111216914e-05\n",
            "Epoch 849/1000, Training Loss: 2.3297464584327884e-05, Test Loss: 2.4607071820332047e-05\n",
            "Epoch 850/1000, Training Loss: 2.3267783657399338e-05, Test Loss: 2.4576919078463986e-05\n",
            "Epoch 851/1000, Training Loss: 2.3238186478269614e-05, Test Loss: 2.45468523497785e-05\n",
            "Epoch 852/1000, Training Loss: 2.32086725652027e-05, Test Loss: 2.45168710932247e-05\n",
            "Epoch 853/1000, Training Loss: 2.3179241440749984e-05, Test Loss: 2.448697477294144e-05\n",
            "Epoch 854/1000, Training Loss: 2.314989263169841e-05, Test Loss: 2.4457162858193443e-05\n",
            "Epoch 855/1000, Training Loss: 2.3120625669018692e-05, Test Loss: 2.4427434823310836e-05\n",
            "Epoch 856/1000, Training Loss: 2.3091440087814155e-05, Test Loss: 2.4397790147621124e-05\n",
            "Epoch 857/1000, Training Loss: 2.306233542727069e-05, Test Loss: 2.4368228315391246e-05\n",
            "Epoch 858/1000, Training Loss: 2.3033311230607842e-05, Test Loss: 2.433874881576677e-05\n",
            "Epoch 859/1000, Training Loss: 2.3004367045029955e-05, Test Loss: 2.4309351142710687e-05\n",
            "Epoch 860/1000, Training Loss: 2.297550242167757e-05, Test Loss: 2.428003479494383e-05\n",
            "Epoch 861/1000, Training Loss: 2.2946716915581527e-05, Test Loss: 2.4250799275888042e-05\n",
            "Epoch 862/1000, Training Loss: 2.2918010085616024e-05, Test Loss: 2.422164409360943e-05\n",
            "Epoch 863/1000, Training Loss: 2.2889381494452972e-05, Test Loss: 2.4192568760760302e-05\n",
            "Epoch 864/1000, Training Loss: 2.2860830708517038e-05, Test Loss: 2.4163572794523722e-05\n",
            "Epoch 865/1000, Training Loss: 2.2832357297941597e-05, Test Loss: 2.413465571656133e-05\n",
            "Epoch 866/1000, Training Loss: 2.2803960836524956e-05, Test Loss: 2.4105817052955235e-05\n",
            "Epoch 867/1000, Training Loss: 2.277564090168733e-05, Test Loss: 2.407705633416001e-05\n",
            "Epoch 868/1000, Training Loss: 2.2747397074428755e-05, Test Loss: 2.4048373094944724e-05\n",
            "Epoch 869/1000, Training Loss: 2.2719228939286786e-05, Test Loss: 2.401976687434518e-05\n",
            "Epoch 870/1000, Training Loss: 2.2691136084296328e-05, Test Loss: 2.3991237215610464e-05\n",
            "Epoch 871/1000, Training Loss: 2.2663118100949016e-05, Test Loss: 2.396278366615611e-05\n",
            "Epoch 872/1000, Training Loss: 2.2635174584152662e-05, Test Loss: 2.3934405777512436e-05\n",
            "Epoch 873/1000, Training Loss: 2.260730513219327e-05, Test Loss: 2.3906103105275394e-05\n",
            "Epoch 874/1000, Training Loss: 2.2579509346695082e-05, Test Loss: 2.3877875209060564e-05\n",
            "Epoch 875/1000, Training Loss: 2.255178683258334e-05, Test Loss: 2.3849721652452214e-05\n",
            "Epoch 876/1000, Training Loss: 2.2524137198046482e-05, Test Loss: 2.3821642002961455e-05\n",
            "Epoch 877/1000, Training Loss: 2.2496560054499585e-05, Test Loss: 2.379363583197783e-05\n",
            "Epoch 878/1000, Training Loss: 2.2469055016546983e-05, Test Loss: 2.3765702714722258e-05\n",
            "Epoch 879/1000, Training Loss: 2.244162170194758e-05, Test Loss: 2.3737842230206963e-05\n",
            "Epoch 880/1000, Training Loss: 2.241425973157833e-05, Test Loss: 2.3710053961185975e-05\n",
            "Epoch 881/1000, Training Loss: 2.2386968729400423e-05, Test Loss: 2.3682337494117005e-05\n",
            "Epoch 882/1000, Training Loss: 2.235974832242425e-05, Test Loss: 2.3654692419114247e-05\n",
            "Epoch 883/1000, Training Loss: 2.2332598140675925e-05, Test Loss: 2.362711832990929e-05\n",
            "Epoch 884/1000, Training Loss: 2.230551781716358e-05, Test Loss: 2.3599614823808092e-05\n",
            "Epoch 885/1000, Training Loss: 2.2278506987844782e-05, Test Loss: 2.3572181501649536e-05\n",
            "Epoch 886/1000, Training Loss: 2.2251565291594046e-05, Test Loss: 2.3544817967767294e-05\n",
            "Epoch 887/1000, Training Loss: 2.2224692370170842e-05, Test Loss: 2.3517523829947096e-05\n",
            "Epoch 888/1000, Training Loss: 2.2197887868187806e-05, Test Loss: 2.3490298699389272e-05\n",
            "Epoch 889/1000, Training Loss: 2.217115143308054e-05, Test Loss: 2.346314219066905e-05\n",
            "Epoch 890/1000, Training Loss: 2.2144482715075994e-05, Test Loss: 2.3436053921698982e-05\n",
            "Epoch 891/1000, Training Loss: 2.2117881367163428e-05, Test Loss: 2.3409033513691885e-05\n",
            "Epoch 892/1000, Training Loss: 2.2091347045063117e-05, Test Loss: 2.338208059112135e-05\n",
            "Epoch 893/1000, Training Loss: 2.2064879407198572e-05, Test Loss: 2.33551947816881e-05\n",
            "Epoch 894/1000, Training Loss: 2.2038478114666657e-05, Test Loss: 2.332837571628157e-05\n",
            "Epoch 895/1000, Training Loss: 2.201214283120978e-05, Test Loss: 2.330162302894469e-05\n",
            "Epoch 896/1000, Training Loss: 2.1985873223186814e-05, Test Loss: 2.3274936356839373e-05\n",
            "Epoch 897/1000, Training Loss: 2.195966895954624e-05, Test Loss: 2.324831534021154e-05\n",
            "Epoch 898/1000, Training Loss: 2.1933529711798454e-05, Test Loss: 2.3221759622357644e-05\n",
            "Epoch 899/1000, Training Loss: 2.190745515398897e-05, Test Loss: 2.3195268849589117e-05\n",
            "Epoch 900/1000, Training Loss: 2.1881444962671322e-05, Test Loss: 2.3168842671200345e-05\n",
            "Epoch 901/1000, Training Loss: 2.185549881688151e-05, Test Loss: 2.3142480739436398e-05\n",
            "Epoch 902/1000, Training Loss: 2.1829616398111674e-05, Test Loss: 2.311618270945928e-05\n",
            "Epoch 903/1000, Training Loss: 2.18037973902847e-05, Test Loss: 2.308994823931626e-05\n",
            "Epoch 904/1000, Training Loss: 2.1778041479729093e-05, Test Loss: 2.306377698990934e-05\n",
            "Epoch 905/1000, Training Loss: 2.1752348355154186e-05, Test Loss: 2.303766862496344e-05\n",
            "Epoch 906/1000, Training Loss: 2.1726717707625432e-05, Test Loss: 2.301162281099568e-05\n",
            "Epoch 907/1000, Training Loss: 2.1701149230540385e-05, Test Loss: 2.2985639217283892e-05\n",
            "Epoch 908/1000, Training Loss: 2.1675642619604897e-05, Test Loss: 2.2959717515841076e-05\n",
            "Epoch 909/1000, Training Loss: 2.1650197572809453e-05, Test Loss: 2.293385738138102e-05\n",
            "Epoch 910/1000, Training Loss: 2.1624813790405985e-05, Test Loss: 2.290805849129026e-05\n",
            "Epoch 911/1000, Training Loss: 2.1599490974885237e-05, Test Loss: 2.2882320525602224e-05\n",
            "Epoch 912/1000, Training Loss: 2.15742288309537e-05, Test Loss: 2.2856643166965347e-05\n",
            "Epoch 913/1000, Training Loss: 2.1549027065511704e-05, Test Loss: 2.2831026100618793e-05\n",
            "Epoch 914/1000, Training Loss: 2.152388538763115e-05, Test Loss: 2.28054690143602e-05\n",
            "Epoch 915/1000, Training Loss: 2.149880350853381e-05, Test Loss: 2.2779971598522296e-05\n",
            "Epoch 916/1000, Training Loss: 2.147378114157043e-05, Test Loss: 2.2754533545945546e-05\n",
            "Epoch 917/1000, Training Loss: 2.1448818002198524e-05, Test Loss: 2.272915455194949e-05\n",
            "Epoch 918/1000, Training Loss: 2.142391380796264e-05, Test Loss: 2.2703834314309343e-05\n",
            "Epoch 919/1000, Training Loss: 2.1399068278472997e-05, Test Loss: 2.267857253322849e-05\n",
            "Epoch 920/1000, Training Loss: 2.1374281135385407e-05, Test Loss: 2.2653368911312063e-05\n",
            "Epoch 921/1000, Training Loss: 2.134955210238111e-05, Test Loss: 2.2628223153545055e-05\n",
            "Epoch 922/1000, Training Loss: 2.1324880905146802e-05, Test Loss: 2.260313496726459e-05\n",
            "Epoch 923/1000, Training Loss: 2.1300267271355658e-05, Test Loss: 2.2578104062136312e-05\n",
            "Epoch 924/1000, Training Loss: 2.1275710930647565e-05, Test Loss: 2.2553130150130547e-05\n",
            "Epoch 925/1000, Training Loss: 2.1251211614609886e-05, Test Loss: 2.252821294549793e-05\n",
            "Epoch 926/1000, Training Loss: 2.122676905675875e-05, Test Loss: 2.2503352164745158e-05\n",
            "Epoch 927/1000, Training Loss: 2.1202382992521023e-05, Test Loss: 2.2478547526613997e-05\n",
            "Epoch 928/1000, Training Loss: 2.1178053159215114e-05, Test Loss: 2.2453798752055505e-05\n",
            "Epoch 929/1000, Training Loss: 2.1153779296033465e-05, Test Loss: 2.2429105564209407e-05\n",
            "Epoch 930/1000, Training Loss: 2.1129561144024412e-05, Test Loss: 2.2404467688380242e-05\n",
            "Epoch 931/1000, Training Loss: 2.1105398446074256e-05, Test Loss: 2.2379884852017287e-05\n",
            "Epoch 932/1000, Training Loss: 2.108129094689039e-05, Test Loss: 2.235535678468943e-05\n",
            "Epoch 933/1000, Training Loss: 2.1057238392983565e-05, Test Loss: 2.2330883218067413e-05\n",
            "Epoch 934/1000, Training Loss: 2.1033240532650973e-05, Test Loss: 2.230646388589912e-05\n",
            "Epoch 935/1000, Training Loss: 2.1009297115959963e-05, Test Loss: 2.228209852399096e-05\n",
            "Epoch 936/1000, Training Loss: 2.098540789473042e-05, Test Loss: 2.2257786870186563e-05\n",
            "Epoch 937/1000, Training Loss: 2.0961572622519332e-05, Test Loss: 2.2233528664344525e-05\n",
            "Epoch 938/1000, Training Loss: 2.0937791054603908e-05, Test Loss: 2.220932364832018e-05\n",
            "Epoch 939/1000, Training Loss: 2.0914062947966172e-05, Test Loss: 2.218517156594536e-05\n",
            "Epoch 940/1000, Training Loss: 2.0890388061276494e-05, Test Loss: 2.2161072163008083e-05\n",
            "Epoch 941/1000, Training Loss: 2.0866766154878557e-05, Test Loss: 2.2137025187232516e-05\n",
            "Epoch 942/1000, Training Loss: 2.084319699077382e-05, Test Loss: 2.2113030388261694e-05\n",
            "Epoch 943/1000, Training Loss: 2.0819680332605615e-05, Test Loss: 2.2089087517635542e-05\n",
            "Epoch 944/1000, Training Loss: 2.0796215945644977e-05, Test Loss: 2.206519632877457e-05\n",
            "Epoch 945/1000, Training Loss: 2.0772803596775484e-05, Test Loss: 2.2041356576960644e-05\n",
            "Epoch 946/1000, Training Loss: 2.0749443054478306e-05, Test Loss: 2.201756801931732e-05\n",
            "Epoch 947/1000, Training Loss: 2.0726134088817914e-05, Test Loss: 2.199383041479417e-05\n",
            "Epoch 948/1000, Training Loss: 2.0702876471427583e-05, Test Loss: 2.1970143524147006e-05\n",
            "Epoch 949/1000, Training Loss: 2.067966997549532e-05, Test Loss: 2.194650710991978e-05\n",
            "Epoch 950/1000, Training Loss: 2.065651437575004e-05, Test Loss: 2.192292093642898e-05\n",
            "Epoch 951/1000, Training Loss: 2.0633409448447102e-05, Test Loss: 2.1899384769746804e-05\n",
            "Epoch 952/1000, Training Loss: 2.0610354971355252e-05, Test Loss: 2.1875898377678814e-05\n",
            "Epoch 953/1000, Training Loss: 2.0587350723742652e-05, Test Loss: 2.185246152975454e-05\n",
            "Epoch 954/1000, Training Loss: 2.0564396486363877e-05, Test Loss: 2.1829073997205723e-05\n",
            "Epoch 955/1000, Training Loss: 2.0541492041446557e-05, Test Loss: 2.180573555295223e-05\n",
            "Epoch 956/1000, Training Loss: 2.0518637172678053e-05, Test Loss: 2.1782445971584236e-05\n",
            "Epoch 957/1000, Training Loss: 2.049583166519285e-05, Test Loss: 2.175920502934719e-05\n",
            "Epoch 958/1000, Training Loss: 2.0473075305559875e-05, Test Loss: 2.1736012504125725e-05\n",
            "Epoch 959/1000, Training Loss: 2.0450367881769473e-05, Test Loss: 2.1712868175427668e-05\n",
            "Epoch 960/1000, Training Loss: 2.042770918322138e-05, Test Loss: 2.1689771824369782e-05\n",
            "Epoch 961/1000, Training Loss: 2.040509900071208e-05, Test Loss: 2.166672323366014e-05\n",
            "Epoch 962/1000, Training Loss: 2.0382537126422505e-05, Test Loss: 2.1643722187584947e-05\n",
            "Epoch 963/1000, Training Loss: 2.0360023353906565e-05, Test Loss: 2.1620768471993156e-05\n",
            "Epoch 964/1000, Training Loss: 2.033755747807857e-05, Test Loss: 2.1597861874282006e-05\n",
            "Epoch 965/1000, Training Loss: 2.031513929520187e-05, Test Loss: 2.1575002183380093e-05\n",
            "Epoch 966/1000, Training Loss: 2.0292768602877092e-05, Test Loss: 2.155218918973673e-05\n",
            "Epoch 967/1000, Training Loss: 2.0270445200030325e-05, Test Loss: 2.152942268530424e-05\n",
            "Epoch 968/1000, Training Loss: 2.0248168886902327e-05, Test Loss: 2.1506702463526174e-05\n",
            "Epoch 969/1000, Training Loss: 2.0225939465036754e-05, Test Loss: 2.1484028319322567e-05\n",
            "Epoch 970/1000, Training Loss: 2.0203756737269415e-05, Test Loss: 2.1461400049075663e-05\n",
            "Epoch 971/1000, Training Loss: 2.018162050771671e-05, Test Loss: 2.143881745061633e-05\n",
            "Epoch 972/1000, Training Loss: 2.0159530581765316e-05, Test Loss: 2.14162803232124e-05\n",
            "Epoch 973/1000, Training Loss: 2.0137486766060836e-05, Test Loss: 2.1393788467551747e-05\n",
            "Epoch 974/1000, Training Loss: 2.0115488868497854e-05, Test Loss: 2.1371341685734288e-05\n",
            "Epoch 975/1000, Training Loss: 2.0093536698208752e-05, Test Loss: 2.1348939781254003e-05\n",
            "Epoch 976/1000, Training Loss: 2.0071630065553372e-05, Test Loss: 2.1326582558987953e-05\n",
            "Epoch 977/1000, Training Loss: 2.00497687821094e-05, Test Loss: 2.1304269825185085e-05\n",
            "Epoch 978/1000, Training Loss: 2.0027952660661112e-05, Test Loss: 2.1282001387451288e-05\n",
            "Epoch 979/1000, Training Loss: 2.000618151519015e-05, Test Loss: 2.1259777054737813e-05\n",
            "Epoch 980/1000, Training Loss: 1.998445516086471e-05, Test Loss: 2.1237596637329955e-05\n",
            "Epoch 981/1000, Training Loss: 1.996277341403065e-05, Test Loss: 2.1215459946833772e-05\n",
            "Epoch 982/1000, Training Loss: 1.9941136092201088e-05, Test Loss: 2.119336679616477e-05\n",
            "Epoch 983/1000, Training Loss: 1.9919543014046665e-05, Test Loss: 2.117131699953485e-05\n",
            "Epoch 984/1000, Training Loss: 1.9897993999386374e-05, Test Loss: 2.1149310372441415e-05\n",
            "Epoch 985/1000, Training Loss: 1.9876488869177918e-05, Test Loss: 2.1127346731657644e-05\n",
            "Epoch 986/1000, Training Loss: 1.9855027445508125e-05, Test Loss: 2.1105425895216475e-05\n",
            "Epoch 987/1000, Training Loss: 1.9833609551583915e-05, Test Loss: 2.1083547682402754e-05\n",
            "Epoch 988/1000, Training Loss: 1.981223501172352e-05, Test Loss: 2.106171191374196e-05\n",
            "Epoch 989/1000, Training Loss: 1.9790903651346475e-05, Test Loss: 2.103991841098735e-05\n",
            "Epoch 990/1000, Training Loss: 1.9769615296965695e-05, Test Loss: 2.1018166997111036e-05\n",
            "Epoch 991/1000, Training Loss: 1.9748369776177673e-05, Test Loss: 2.0996457496290116e-05\n",
            "Epoch 992/1000, Training Loss: 1.9727166917654394e-05, Test Loss: 2.0974789733900013e-05\n",
            "Epoch 993/1000, Training Loss: 1.97060065511341e-05, Test Loss: 2.095316353650016e-05\n",
            "Epoch 994/1000, Training Loss: 1.968488850741308e-05, Test Loss: 2.0931578731825344e-05\n",
            "Epoch 995/1000, Training Loss: 1.9663812618337185e-05, Test Loss: 2.091003514877558e-05\n",
            "Epoch 996/1000, Training Loss: 1.964277871679294e-05, Test Loss: 2.0888532617404225e-05\n",
            "Epoch 997/1000, Training Loss: 1.962178663669942e-05, Test Loss: 2.0867070968909055e-05\n",
            "Epoch 998/1000, Training Loss: 1.960083621300053e-05, Test Loss: 2.0845650035622614e-05\n",
            "Epoch 999/1000, Training Loss: 1.9579927281655707e-05, Test Loss: 2.0824269651001305e-05\n",
            "Epoch 1000/1000, Training Loss: 1.9559059679632967e-05, Test Loss: 2.0802929649614836e-05\n",
            "Epoch 1/1000, Training Loss: 0.003394803903143811, Test Loss: 0.0029629270180374506\n",
            "Epoch 2/1000, Training Loss: 0.0033214278107046983, Test Loss: 0.0028997617592112696\n",
            "Epoch 3/1000, Training Loss: 0.003310877749515794, Test Loss: 0.002891950764775109\n",
            "Epoch 4/1000, Training Loss: 0.0033074010960201495, Test Loss: 0.002889334913044088\n",
            "Epoch 5/1000, Training Loss: 0.0033053881723562415, Test Loss: 0.0028876132081855875\n",
            "Epoch 6/1000, Training Loss: 0.0033038274276010636, Test Loss: 0.00288619195617024\n",
            "Epoch 7/1000, Training Loss: 0.0033024495777515535, Test Loss: 0.0028849416732003533\n",
            "Epoch 8/1000, Training Loss: 0.003301156285737788, Test Loss: 0.0028838046341902747\n",
            "Epoch 9/1000, Training Loss: 0.0032999033610751983, Test Loss: 0.00288274174510049\n",
            "Epoch 10/1000, Training Loss: 0.0032986684575217573, Test Loss: 0.0028817244153198384\n",
            "Epoch 11/1000, Training Loss: 0.003297439376848009, Test Loss: 0.0028807319245299345\n",
            "Epoch 12/1000, Training Loss: 0.003296209005428012, Test Loss: 0.0028797494701360655\n",
            "Epoch 13/1000, Training Loss: 0.003294972901192524, Test Loss: 0.002878766564913638\n",
            "Epoch 14/1000, Training Loss: 0.003293728082020107, Test Loss: 0.0028777758060610074\n",
            "Epoch 15/1000, Training Loss: 0.0032924723957270773, Test Loss: 0.0028767719753167264\n",
            "Epoch 16/1000, Training Loss: 0.0032912041824193507, Test Loss: 0.0028757513973300336\n",
            "Epoch 17/1000, Training Loss: 0.003289922087843903, Test Loss: 0.0028747114863554384\n",
            "Epoch 18/1000, Training Loss: 0.003288624956616649, Test Loss: 0.0028736504261274946\n",
            "Epoch 19/1000, Training Loss: 0.0032873117687344843, Test Loss: 0.0028725669429127947\n",
            "Epoch 20/1000, Training Loss: 0.003285981600165585, Test Loss: 0.00287146014375699\n",
            "Epoch 21/1000, Training Loss: 0.003284633597238494, Test Loss: 0.0028703294006035013\n",
            "Epoch 22/1000, Training Loss: 0.003283266959214409, Test Loss: 0.0028691742669427283\n",
            "Epoch 23/1000, Training Loss: 0.003281880925904872, Test Loss: 0.0028679944177206584\n",
            "Epoch 24/1000, Training Loss: 0.0032804747685354053, Test Loss: 0.002866789606002247\n",
            "Epoch 25/1000, Training Loss: 0.0032790477827914365, Test Loss: 0.002865559631778197\n",
            "Epoch 26/1000, Training Loss: 0.0032775992833953423, Test Loss: 0.0028643043196137645\n",
            "Epoch 27/1000, Training Loss: 0.0032761285998000713, Test Loss: 0.002863023502755504\n",
            "Epoch 28/1000, Training Loss: 0.0032746350727242786, Test Loss: 0.002861717011962199\n",
            "Epoch 29/1000, Training Loss: 0.0032731180513387343, Test Loss: 0.002860384667792047\n",
            "Epoch 30/1000, Training Loss: 0.003271576890967357, Test Loss: 0.002859026275415082\n",
            "Epoch 31/1000, Training Loss: 0.0032700109512013827, Test Loss: 0.0028576416212652274\n",
            "Epoch 32/1000, Training Loss: 0.0032684195943491912, Test Loss: 0.002856230471026259\n",
            "Epoch 33/1000, Training Loss: 0.0032668021841614405, Test Loss: 0.002854792568578366\n",
            "Epoch 34/1000, Training Loss: 0.003265158084783652, Test Loss: 0.0028533276356298193\n",
            "Epoch 35/1000, Training Loss: 0.003263486659897943, Test Loss: 0.0028518353718307065\n",
            "Epoch 36/1000, Training Loss: 0.0032617872720229163, Test Loss: 0.0028503154552193925\n",
            "Epoch 37/1000, Training Loss: 0.0032600592819465504, Test Loss: 0.0028487675428922805\n",
            "Epoch 38/1000, Training Loss: 0.0032583020482715224, Test Loss: 0.0028471912718170526\n",
            "Epoch 39/1000, Training Loss: 0.0032565149270561734, Test Loss: 0.002845586259731582\n",
            "Epoch 40/1000, Training Loss: 0.0032546972715373256, Test Loss: 0.0028439521060870123\n",
            "Epoch 41/1000, Training Loss: 0.0032528484319236627, Test Loss: 0.0028422883930056602\n",
            "Epoch 42/1000, Training Loss: 0.003250967755250409, Test Loss: 0.002840594686233249\n",
            "Epoch 43/1000, Training Loss: 0.0032490545852877028, Test Loss: 0.0028388705360716847\n",
            "Epoch 44/1000, Training Loss: 0.0032471082624964814, Test Loss: 0.002837115478283369\n",
            "Epoch 45/1000, Training Loss: 0.003245128124026771, Test Loss: 0.0028353290349616062\n",
            "Epoch 46/1000, Training Loss: 0.003243113503754276, Test Loss: 0.0028335107153642613\n",
            "Epoch 47/1000, Training Loss: 0.003241063732351908, Test Loss: 0.002831660016709622\n",
            "Epoch 48/1000, Training Loss: 0.0032389781373935588, Test Loss: 0.002829776424934793\n",
            "Epoch 49/1000, Training Loss: 0.003236856043487937, Test Loss: 0.002827859415417795\n",
            "Epoch 50/1000, Training Loss: 0.0032346967724407646, Test Loss: 0.002825908453665231\n",
            "Epoch 51/1000, Training Loss: 0.003232499643443968, Test Loss: 0.002823922995967715\n",
            "Epoch 52/1000, Training Loss: 0.0032302639732908257, Test Loss: 0.002821902490025545\n",
            "Epoch 53/1000, Training Loss: 0.0032279890766162707, Test Loss: 0.0028198463755472213\n",
            "Epoch 54/1000, Training Loss: 0.0032256742661617636, Test Loss: 0.0028177540848234126\n",
            "Epoch 55/1000, Training Loss: 0.003223318853064334, Test Loss: 0.002815625043279029\n",
            "Epoch 56/1000, Training Loss: 0.0032209221471694955, Test Loss: 0.002813458670005917\n",
            "Epoch 57/1000, Training Loss: 0.0032184834573678856, Test Loss: 0.0028112543782786736\n",
            "Epoch 58/1000, Training Loss: 0.0032160020919555716, Test Loss: 0.002809011576055916\n",
            "Epoch 59/1000, Training Loss: 0.003213477359017992, Test Loss: 0.0028067296664692643\n",
            "Epoch 60/1000, Training Loss: 0.0032109085668376182, Test Loss: 0.00280440804830214\n",
            "Epoch 61/1000, Training Loss: 0.0032082950243254127, Test Loss: 0.002802046116460362\n",
            "Epoch 62/1000, Training Loss: 0.003205636041476201, Test Loss: 0.0027996432624364065\n",
            "Epoch 63/1000, Training Loss: 0.0032029309298480914, Test Loss: 0.0027971988747690236\n",
            "Epoch 64/1000, Training Loss: 0.003200179003066077, Test Loss: 0.002794712339499856\n",
            "Epoch 65/1000, Training Loss: 0.0031973795773499285, Test Loss: 0.002792183040628469\n",
            "Epoch 66/1000, Training Loss: 0.0031945319720664872, Test Loss: 0.002789610360567191\n",
            "Epoch 67/1000, Training Loss: 0.0031916355103064355, Test Loss: 0.002786993680596961\n",
            "Epoch 68/1000, Training Loss: 0.0031886895194855698, Test Loss: 0.0027843323813253034\n",
            "Epoch 69/1000, Training Loss: 0.0031856933319705655, Test Loss: 0.0027816258431474367\n",
            "Epoch 70/1000, Training Loss: 0.0031826462857291685, Test Loss: 0.0027788734467113787\n",
            "Epoch 71/1000, Training Loss: 0.0031795477250046657, Test Loss: 0.002776074573387834\n",
            "Epoch 72/1000, Training Loss: 0.003176397001014424, Test Loss: 0.002773228605745526\n",
            "Epoch 73/1000, Training Loss: 0.003173193472672214, Test Loss: 0.0027703349280325173\n",
            "Epoch 74/1000, Training Loss: 0.0031699365073339014, Test Loss: 0.0027673929266639596\n",
            "Epoch 75/1000, Training Loss: 0.003166625481566028, Test Loss: 0.002764401990716615\n",
            "Epoch 76/1000, Training Loss: 0.0031632597819366555, Test Loss: 0.002761361512430347\n",
            "Epoch 77/1000, Training Loss: 0.003159838805827746, Test Loss: 0.0027582708877167115\n",
            "Epoch 78/1000, Training Loss: 0.0031563619622681995, Test Loss: 0.0027551295166746083\n",
            "Epoch 79/1000, Training Loss: 0.003152828672786534, Test Loss: 0.0027519368041128943\n",
            "Epoch 80/1000, Training Loss: 0.003149238372282033, Test Loss: 0.002748692160079694\n",
            "Epoch 81/1000, Training Loss: 0.003145590509913013, Test Loss: 0.002745395000398026\n",
            "Epoch 82/1000, Training Loss: 0.0031418845500007187, Test Loss: 0.002742044747207279\n",
            "Epoch 83/1000, Training Loss: 0.003138119972947101, Test Loss: 0.0027386408295098858\n",
            "Epoch 84/1000, Training Loss: 0.003134296276164618, Test Loss: 0.002735182683722429\n",
            "Epoch 85/1000, Training Loss: 0.003130412975015924, Test Loss: 0.0027316697542303153\n",
            "Epoch 86/1000, Training Loss: 0.0031264696037611294, Test Loss: 0.002728101493944935\n",
            "Epoch 87/1000, Training Loss: 0.0031224657165100887, Test Loss: 0.0027244773648621786\n",
            "Epoch 88/1000, Training Loss: 0.003118400888176921, Test Loss: 0.0027207968386209367\n",
            "Epoch 89/1000, Training Loss: 0.003114274715433749, Test Loss: 0.0027170593970601412\n",
            "Epoch 90/1000, Training Loss: 0.0031100868176603875, Test Loss: 0.0027132645327727135\n",
            "Epoch 91/1000, Training Loss: 0.003105836837886442, Test Loss: 0.0027094117496546293\n",
            "Epoch 92/1000, Training Loss: 0.00310152444372206, Test Loss: 0.0027055005634471874\n",
            "Epoch 93/1000, Training Loss: 0.0030971493282732744, Test Loss: 0.0027015305022703804\n",
            "Epoch 94/1000, Training Loss: 0.003092711211037654, Test Loss: 0.0026975011071451362\n",
            "Epoch 95/1000, Training Loss: 0.0030882098387756945, Test Loss: 0.002693411932502018\n",
            "Epoch 96/1000, Training Loss: 0.003083644986353126, Test Loss: 0.002689262546673873\n",
            "Epoch 97/1000, Training Loss: 0.0030790164575490763, Test Loss: 0.002685052532369685\n",
            "Epoch 98/1000, Training Loss: 0.003074324085824771, Test Loss: 0.0026807814871268477\n",
            "Epoch 99/1000, Training Loss: 0.0030695677350472115, Test Loss: 0.002676449023738848\n",
            "Epoch 100/1000, Training Loss: 0.003064747300162047, Test Loss: 0.002672054770655269\n",
            "Epoch 101/1000, Training Loss: 0.0030598627078096702, Test Loss: 0.002667598372350892\n",
            "Epoch 102/1000, Training Loss: 0.0030549139168783274, Test Loss: 0.002663079489660552\n",
            "Epoch 103/1000, Training Loss: 0.003049900918987908, Test Loss: 0.0026584978000763143\n",
            "Epoch 104/1000, Training Loss: 0.003044823738897895, Test Loss: 0.0026538529980034537\n",
            "Epoch 105/1000, Training Loss: 0.003039682434832841, Test Loss: 0.002649144794971628\n",
            "Epoch 106/1000, Training Loss: 0.003034477098718664, Test Loss: 0.0026443729197976255\n",
            "Epoch 107/1000, Training Loss: 0.0030292078563229513, Test Loss: 0.002639537118695967\n",
            "Epoch 108/1000, Training Loss: 0.0030238748672924646, Test Loss: 0.0026346371553336943\n",
            "Epoch 109/1000, Training Loss: 0.003018478325081045, Test Loss: 0.0026296728108256245\n",
            "Epoch 110/1000, Training Loss: 0.003013018456761154, Test Loss: 0.0026246438836664194\n",
            "Epoch 111/1000, Training Loss: 0.003007495522712378, Test Loss: 0.002619550189595845\n",
            "Epoch 112/1000, Training Loss: 0.0030019098161803907, Test Loss: 0.0026143915613936917\n",
            "Epoch 113/1000, Training Loss: 0.0029962616627000078, Test Loss: 0.00260916784860092\n",
            "Epoch 114/1000, Training Loss: 0.0029905514193762327, Test Loss: 0.00260387891716372\n",
            "Epoch 115/1000, Training Loss: 0.002984779474017456, Test Loss: 0.0025985246489973746\n",
            "Epoch 116/1000, Training Loss: 0.002978946244115306, Test Loss: 0.002593104941466941\n",
            "Epoch 117/1000, Training Loss: 0.002973052175666021, Test Loss: 0.0025876197067820294\n",
            "Epoch 118/1000, Training Loss: 0.0029670977418286425, Test Loss: 0.0025820688713031996\n",
            "Epoch 119/1000, Training Loss: 0.0029610834414158004, Test Loss: 0.002576452374757754\n",
            "Epoch 120/1000, Training Loss: 0.0029550097972133593, Test Loss: 0.002570770169362984\n",
            "Epoch 121/1000, Training Loss: 0.0029488773541258007, Test Loss: 0.002565022218855343\n",
            "Epoch 122/1000, Training Loss: 0.0029426866771447347, Test Loss: 0.002559208497424238\n",
            "Epoch 123/1000, Training Loss: 0.0029364383491386377, Test Loss: 0.002553328988549621\n",
            "Epoch 124/1000, Training Loss: 0.0029301329684625166, Test Loss: 0.0025473836837428938\n",
            "Epoch 125/1000, Training Loss: 0.0029237711463869004, Test Loss: 0.0025413725811910463\n",
            "Epoch 126/1000, Training Loss: 0.002917353504346255, Test Loss: 0.002535295684304423\n",
            "Epoch 127/1000, Training Loss: 0.002910880671007586, Test Loss: 0.0025291530001688694\n",
            "Epoch 128/1000, Training Loss: 0.002904353279160753, Test Loss: 0.0025229445379035516\n",
            "Epoch 129/1000, Training Loss: 0.0028977719624326353, Test Loss: 0.002516670306926069\n",
            "Epoch 130/1000, Training Loss: 0.0028911373518280296, Test Loss: 0.002510330315127042\n",
            "Epoch 131/1000, Training Loss: 0.0028844500721007613, Test Loss: 0.002503924566956711\n",
            "Epoch 132/1000, Training Loss: 0.002877710737959106, Test Loss: 0.002497453061426548\n",
            "Epoch 133/1000, Training Loss: 0.0028709199501101845, Test Loss: 0.002490915790029298\n",
            "Epoch 134/1000, Training Loss: 0.0028640782911484998, Test Loss: 0.0024843127345812554\n",
            "Epoch 135/1000, Training Loss: 0.0028571863212941656, Test Loss: 0.0024776438649909364\n",
            "Epoch 136/1000, Training Loss: 0.0028502445739867956, Test Loss: 0.0024709091369586825\n",
            "Epoch 137/1000, Training Loss: 0.0028432535513412283, Test Loss: 0.0024641084896120293\n",
            "Epoch 138/1000, Training Loss: 0.0028362137194714435, Test Loss: 0.002457241843081922\n",
            "Epoch 139/1000, Training Loss: 0.002829125503689068, Test Loss: 0.0024503090960251283\n",
            "Epoch 140/1000, Training Loss: 0.002821989283582822, Test Loss: 0.0024433101230983863\n",
            "Epoch 141/1000, Training Loss: 0.0028148053879850387, Test Loss: 0.0024362447723899126\n",
            "Epoch 142/1000, Training Loss: 0.0028075740898310803, Test Loss: 0.002429112862814074\n",
            "Epoch 143/1000, Training Loss: 0.0028002956009170322, Test Loss: 0.0024219141814749807\n",
            "Epoch 144/1000, Training Loss: 0.002792970066560414, Test Loss: 0.0024146484810048194\n",
            "Epoch 145/1000, Training Loss: 0.002785597560167954, Test Loss: 0.0024073154768826208\n",
            "Epoch 146/1000, Training Loss: 0.0027781780777135245, Test Loss: 0.002399914844739082\n",
            "Epoch 147/1000, Training Loss: 0.0027707115321283404, Test Loss: 0.002392446217652856\n",
            "Epoch 148/1000, Training Loss: 0.0027631977476043363, Test Loss: 0.002384909183443543\n",
            "Epoch 149/1000, Training Loss: 0.0027556364538102664, Test Loss: 0.002377303281966317\n",
            "Epoch 150/1000, Training Loss: 0.0027480272800186655, Test Loss: 0.0023696280024128155\n",
            "Epoch 151/1000, Training Loss: 0.0027403697491401747, Test Loss: 0.0023618827806226083\n",
            "Epoch 152/1000, Training Loss: 0.0027326632716600263, Test Loss: 0.002354066996409118\n",
            "Epoch 153/1000, Training Loss: 0.002724907139469684, Test Loss: 0.0023461799709035623\n",
            "Epoch 154/1000, Training Loss: 0.002717100519584626, Test Loss: 0.002338220963919932\n",
            "Epoch 155/1000, Training Loss: 0.00270924244773735, Test Loss: 0.0023301891713437464\n",
            "Epoch 156/1000, Training Loss: 0.002701331821832525, Test Loss: 0.0023220837225468084\n",
            "Epoch 157/1000, Training Loss: 0.0026933673952491246, Test Loss: 0.0023139036778298196\n",
            "Epoch 158/1000, Training Loss: 0.00268534776997224, Test Loss: 0.0023056480258943895\n",
            "Epoch 159/1000, Training Loss: 0.0026772713895351187, Test Loss: 0.0022973156813456174\n",
            "Epoch 160/1000, Training Loss: 0.002669136531749907, Test Loss: 0.0022889054822262753\n",
            "Epoch 161/1000, Training Loss: 0.0026609413012035257, Test Loss: 0.002280416187583388\n",
            "Epoch 162/1000, Training Loss: 0.002652683621493226, Test Loss: 0.002271846475068138\n",
            "Epoch 163/1000, Training Loss: 0.002644361227174609, Test Loss: 0.0022631949385700455\n",
            "Epoch 164/1000, Training Loss: 0.002635971655393392, Test Loss: 0.0022544600858868348\n",
            "Epoch 165/1000, Training Loss: 0.0026275122371708912, Test Loss: 0.0022456403364318725\n",
            "Epoch 166/1000, Training Loss: 0.0026189800883122875, Test Loss: 0.0022367340189819143\n",
            "Epoch 167/1000, Training Loss: 0.0026103720999062297, Test Loss: 0.0022277393694690997\n",
            "Epoch 168/1000, Training Loss: 0.002601684928384225, Test Loss: 0.0022186545288225023\n",
            "Epoch 169/1000, Training Loss: 0.002592914985108877, Test Loss: 0.0022094775408665725\n",
            "Epoch 170/1000, Training Loss: 0.0025840584254611244, Test Loss: 0.0022002063502860806\n",
            "Epoch 171/1000, Training Loss: 0.0025751111373986833, Test Loss: 0.0021908388006700707\n",
            "Epoch 172/1000, Training Loss: 0.0025660687294606966, Test Loss: 0.0021813726326508167\n",
            "Epoch 173/1000, Training Loss: 0.002556926518197451, Test Loss: 0.0021718054821578178\n",
            "Epoch 174/1000, Training Loss: 0.002547679515009024, Test Loss: 0.002162134878811633\n",
            "Epoch 175/1000, Training Loss: 0.0025383224123829845, Test Loss: 0.002152358244487858\n",
            "Epoch 176/1000, Training Loss: 0.0025288495695289393, Test Loss: 0.0021424728920877768\n",
            "Epoch 177/1000, Training Loss: 0.0025192549974169666, Test Loss: 0.002132476024559235\n",
            "Epoch 178/1000, Training Loss: 0.0025095323432379277, Test Loss: 0.0021223647342190417\n",
            "Epoch 179/1000, Training Loss: 0.002499674874316472, Test Loss: 0.0021121360024367215\n",
            "Epoch 180/1000, Training Loss: 0.002489675461522363, Test Loss: 0.0021017866997485803\n",
            "Epoch 181/1000, Training Loss: 0.0024795265622427664, Test Loss: 0.0020913135864806622\n",
            "Epoch 182/1000, Training Loss: 0.0024692202029974446, Test Loss: 0.002080713313969169\n",
            "Epoch 183/1000, Training Loss: 0.002458747961800549, Test Loss: 0.0020699824264769247\n",
            "Epoch 184/1000, Training Loss: 0.0024481009503971116, Test Loss: 0.002059117363914124\n",
            "Epoch 185/1000, Training Loss: 0.002437269796529477, Test Loss: 0.0020481144654805653\n",
            "Epoch 186/1000, Training Loss: 0.002426244626419091, Test Loss: 0.0020369699743540896\n",
            "Epoch 187/1000, Training Loss: 0.0024150150476824127, Test Loss: 0.002025680043555454\n",
            "Epoch 188/1000, Training Loss: 0.002403570132936679, Test Loss: 0.0020142407431224113\n",
            "Epoch 189/1000, Training Loss: 0.0023918984043921647, Test Loss: 0.002002648068724439\n",
            "Epoch 190/1000, Training Loss: 0.002379987819773182, Test Loss: 0.0019908979518430724\n",
            "Epoch 191/1000, Training Loss: 0.002367825759961117, Test Loss: 0.001978986271630051\n",
            "Epoch 192/1000, Training Loss: 0.0023553990188105553, Test Loss: 0.001966908868534827\n",
            "Epoch 193/1000, Training Loss: 0.0023426937956556003, Test Loss: 0.001954661559762986\n",
            "Epoch 194/1000, Training Loss: 0.0023296956910999374, Test Loss: 0.0019422401565862514\n",
            "Epoch 195/1000, Training Loss: 0.0023163897067738147, Test Loss: 0.0019296404834711553\n",
            "Epoch 196/1000, Training Loss: 0.002302760249847291, Test Loss: 0.0019168583989257726\n",
            "Epoch 197/1000, Training Loss: 0.002288791143216089, Test Loss: 0.0019038898178808153\n",
            "Epoch 198/1000, Training Loss: 0.002274465642428945, Test Loss: 0.0018907307353215317\n",
            "Epoch 199/1000, Training Loss: 0.002259766460609219, Test Loss: 0.0018773772507705395\n",
            "Epoch 200/1000, Training Loss: 0.0022446758028441265, Test Loss: 0.001863825593088634\n",
            "Epoch 201/1000, Training Loss: 0.0022291754117780152, Test Loss: 0.0018500721449134947\n",
            "Epoch 202/1000, Training Loss: 0.002213246626455646, Test Loss: 0.0018361134658983448\n",
            "Epoch 203/1000, Training Loss: 0.00219687045681929, Test Loss: 0.001821946313751356\n",
            "Epoch 204/1000, Training Loss: 0.0021800276766663286, Test Loss: 0.001807567661922063\n",
            "Epoch 205/1000, Training Loss: 0.0021626989383120784, Test Loss: 0.001792974712649661\n",
            "Epoch 206/1000, Training Loss: 0.0021448649126545494, Test Loss: 0.0017781649040021644\n",
            "Epoch 207/1000, Training Loss: 0.0021265064587680364, Test Loss: 0.0017631359095261158\n",
            "Epoch 208/1000, Training Loss: 0.0021076048275055527, Test Loss: 0.0017478856292346533\n",
            "Epoch 209/1000, Training Loss: 0.0020881419037858968, Test Loss: 0.0017324121709376705\n",
            "Epoch 210/1000, Training Loss: 0.002068100492171896, Test Loss: 0.0017167138214212096\n",
            "Epoch 211/1000, Training Loss: 0.0020474646498734855, Test Loss: 0.0017007890077771848\n",
            "Epoch 212/1000, Training Loss: 0.0020262200702685206, Test Loss: 0.0016846362503275577\n",
            "Epoch 213/1000, Training Loss: 0.0020043545182436714, Test Loss: 0.0016682541101158074\n",
            "Epoch 214/1000, Training Loss: 0.0019818583159412276, Test Loss: 0.0016516411358464508\n",
            "Epoch 215/1000, Training Loss: 0.001958724873716379, Test Loss: 0.0016347958173582747\n",
            "Epoch 216/1000, Training Loss: 0.001934951256212475, Test Loss: 0.0016177165550302235\n",
            "Epoch 217/1000, Training Loss: 0.0019105387675433954, Test Loss: 0.0016004016566147552\n",
            "Epoch 218/1000, Training Loss: 0.0018854935329375407, Test Loss: 0.0015828493744016441\n",
            "Epoch 219/1000, Training Loss: 0.0018598270474118415, Test Loss: 0.0015650579957456007\n",
            "Epoch 220/1000, Training Loss: 0.0018335566559505145, Test Loss: 0.0015470259982162935\n",
            "Epoch 221/1000, Training Loss: 0.001806705925342314, Test Loss: 0.001528752276427641\n",
            "Epoch 222/1000, Training Loss: 0.001779304866479001, Test Loss: 0.001510236440754638\n",
            "Epoch 223/1000, Training Loss: 0.0017513899686456174, Test Loss: 0.0014914791789401916\n",
            "Epoch 224/1000, Training Loss: 0.0017230040148927392, Test Loss: 0.0014724826609798572\n",
            "Epoch 225/1000, Training Loss: 0.00169419566009204, Test Loss: 0.0014532509572609668\n",
            "Epoch 226/1000, Training Loss: 0.0016650187700159875, Test Loss: 0.001433790431791232\n",
            "Epoch 227/1000, Training Loss: 0.0016355315391298646, Test Loss: 0.001414110068573684\n",
            "Epoch 228/1000, Training Loss: 0.001605795424379855, Test Loss: 0.0013942216913222256\n",
            "Epoch 229/1000, Training Loss: 0.001575873949379255, Test Loss: 0.0013741400452390612\n",
            "Epoch 230/1000, Training Loss: 0.0015458314454691135, Test Loss: 0.0013538827235835696\n",
            "Epoch 231/1000, Training Loss: 0.0015157318012942766, Test Loss: 0.0013334699390407073\n",
            "Epoch 232/1000, Training Loss: 0.0014856372900381055, Test Loss: 0.00131292415741383\n",
            "Epoch 233/1000, Training Loss: 0.001455607533822405, Test Loss: 0.0012922696258217989\n",
            "Epoch 234/1000, Training Loss: 0.0014256986496626741, Test Loss: 0.0012715318369741493\n",
            "Epoch 235/1000, Training Loss: 0.0013959626031826106, Test Loss: 0.0012507369740747494\n",
            "Epoch 236/1000, Training Loss: 0.0013664467776687622, Test Loss: 0.00122991137769985\n",
            "Epoch 237/1000, Training Loss: 0.0013371937493342125, Test Loss: 0.001209081068001251\n",
            "Epoch 238/1000, Training Loss: 0.0013082412465464375, Test Loss: 0.0011882713448446726\n",
            "Epoch 239/1000, Training Loss: 0.001279622262113571, Test Loss: 0.00116750647713929\n",
            "Epoch 240/1000, Training Loss: 0.0012513652835710895, Test Loss: 0.0011468094824088745\n",
            "Epoch 241/1000, Training Loss: 0.0012234946061996683, Test Loss: 0.0011262019897296672\n",
            "Epoch 242/1000, Training Loss: 0.001196030696295084, Test Loss: 0.001105704173975472\n",
            "Epoch 243/1000, Training Loss: 0.0011689905769366711, Test Loss: 0.0010853347467681027\n",
            "Epoch 244/1000, Training Loss: 0.0011423882141636487, Test Loss: 0.0010651109891682482\n",
            "Epoch 245/1000, Training Loss: 0.001116234887259637, Test Loss: 0.0010450488123305596\n",
            "Epoch 246/1000, Training Loss: 0.0010905395321930884, Test Loss: 0.0010251628344548137\n",
            "Epoch 247/1000, Training Loss: 0.0010653090518265844, Test Loss: 0.001005466464856209\n",
            "Epoch 248/1000, Training Loss: 0.0010405485901470082, Test Loss: 0.0009859719884584737\n",
            "Epoch 249/1000, Training Loss: 0.0010162617704790162, Test Loss: 0.0009666906462330548\n",
            "Epoch 250/1000, Training Loss: 0.000992450899515198, Test Loss: 0.0009476327089407676\n",
            "Epoch 251/1000, Training Loss: 0.0009691171401630994, Test Loss: 0.0009288075429434554\n",
            "Epoch 252/1000, Training Loss: 0.000946260656821804, Test Loss: 0.0009102236678661781\n",
            "Epoch 253/1000, Training Loss: 0.0009238807369032029, Test Loss: 0.0008918888065620603\n",
            "Epoch 254/1000, Training Loss: 0.0009019758923304752, Test Loss: 0.0008738099282297049\n",
            "Epoch 255/1000, Training Loss: 0.0008805439444816523, Test Loss: 0.0008559932857258204\n",
            "Epoch 256/1000, Training Loss: 0.0008595820956783404, Test Loss: 0.0008384444481632593\n",
            "Epoch 257/1000, Training Loss: 0.00083908698990661, Test Loss: 0.0008211683298366679\n",
            "Epoch 258/1000, Training Loss: 0.0008190547650391788, Test Loss: 0.0008041692164131274\n",
            "Epoch 259/1000, Training Loss: 0.0007994810984307181, Test Loss: 0.000787450789191094\n",
            "Epoch 260/1000, Training Loss: 0.0007803612473973163, Test Loss: 0.0007710161480879592\n",
            "Epoch 261/1000, Training Loss: 0.0007616900857738041, Test Loss: 0.0007548678338767443\n",
            "Epoch 262/1000, Training Loss: 0.0007434621374713368, Test Loss: 0.000739007850064067\n",
            "Epoch 263/1000, Training Loss: 0.0007256716077311507, Test Loss: 0.0007234376846885601\n",
            "Epoch 264/1000, Training Loss: 0.0007083124125851209, Test Loss: 0.0007081583322228074\n",
            "Epoch 265/1000, Training Loss: 0.0006913782068854377, Test Loss: 0.0006931703156827062\n",
            "Epoch 266/1000, Training Loss: 0.0006748624111497209, Test Loss: 0.0006784737089850492\n",
            "Epoch 267/1000, Training Loss: 0.0006587582373786787, Test Loss: 0.000664068159545059\n",
            "Epoch 268/1000, Training Loss: 0.0006430587139373401, Test Loss: 0.0006499529110694072\n",
            "Epoch 269/1000, Training Loss: 0.0006277567095428169, Test Loss: 0.0006361268264742463\n",
            "Epoch 270/1000, Training Loss: 0.0006128449563687296, Test Loss: 0.0006225884108411338\n",
            "Epoch 271/1000, Training Loss: 0.000598316072254794, Test Loss: 0.0006093358343137027\n",
            "Epoch 272/1000, Training Loss: 0.0005841625819979916, Test Loss: 0.0005963669548342715\n",
            "Epoch 273/1000, Training Loss: 0.0005703769376958044, Test Loss: 0.0005836793406197832\n",
            "Epoch 274/1000, Training Loss: 0.0005569515381115926, Test Loss: 0.0005712702922805009\n",
            "Epoch 275/1000, Training Loss: 0.0005438787470348531, Test Loss: 0.0005591368644911122\n",
            "Epoch 276/1000, Training Loss: 0.0005311509106143946, Test Loss: 0.0005472758871319589\n",
            "Epoch 277/1000, Training Loss: 0.0005187603736489594, Test Loss: 0.0005356839858271551\n",
            "Epoch 278/1000, Training Loss: 0.0005066994948272287, Test Loss: 0.000524357601816123\n",
            "Epoch 279/1000, Training Loss: 0.0004949606609166632, Test Loss: 0.0005132930111049409\n",
            "Epoch 280/1000, Training Loss: 0.00048353629990797146, Test Loss: 0.0005024863428536457\n",
            "Epoch 281/1000, Training Loss: 0.00047241889312904756, Test Loss: 0.0004919335969652706\n",
            "Epoch 282/1000, Training Loss: 0.00046160098634839304, Test Loss: 0.00048163066085117644\n",
            "Epoch 283/1000, Training Loss: 0.00045107519989379634, Test Loss: 0.00047157332535577767\n",
            "Epoch 284/1000, Training Loss: 0.0004408342378167486, Test Loss: 0.0004617572998313543\n",
            "Epoch 285/1000, Training Loss: 0.00043087089613725997, Test Loss: 0.00045217822636070494\n",
            "Epoch 286/1000, Training Loss: 0.0004211780702068605, Test Loss: 0.00044283169313153993\n",
            "Epoch 287/1000, Training Loss: 0.0004117487612302332, Test Loss: 0.0004337132469720487\n",
            "Epoch 288/1000, Training Loss: 0.00040257608198772165, Test Loss: 0.0004248184050618859\n",
            "Epoch 289/1000, Training Loss: 0.0003936532618021921, Test Loss: 0.0004161426658368579\n",
            "Epoch 290/1000, Training Loss: 0.00038497365079440875, Test Loss: 0.00040768151910909866\n",
            "Epoch 291/1000, Training Loss: 0.0003765307234712344, Test Loss: 0.0003994304554273189\n",
            "Epoch 292/1000, Training Loss: 0.0003683180816907075, Test Loss: 0.0003913849747040277\n",
            "Epoch 293/1000, Training Loss: 0.00036032945704744487, Test Loss: 0.0003835405941384308\n",
            "Epoch 294/1000, Training Loss: 0.00035255871272084555, Test Loss: 0.0003758928554650031\n",
            "Epoch 295/1000, Training Loss: 0.0003449998448273796, Test Loss: 0.0003684373315586574\n",
            "Epoch 296/1000, Training Loss: 0.00033764698331687285, Test Loss: 0.0003611696324280176\n",
            "Epoch 297/1000, Training Loss: 0.0003304943924510909, Test Loss: 0.00035408541062848604\n",
            "Epoch 298/1000, Training Loss: 0.000323536470901303, Test Loss: 0.00034718036612682503\n",
            "Epoch 299/1000, Training Loss: 0.000316767751499656, Test Loss: 0.00034045025064860637\n",
            "Epoch 300/1000, Training Loss: 0.00031018290067745107, Test Loss: 0.0003338908715394856\n",
            "Epoch 301/1000, Training Loss: 0.0003037767176215122, Test Loss: 0.00032749809517053884\n",
            "Epoch 302/1000, Training Loss: 0.0002975441331779935, Test Loss: 0.00032126784991714044\n",
            "Epoch 303/1000, Training Loss: 0.0002914802085311445, Test Loss: 0.0003151961287399557\n",
            "Epoch 304/1000, Training Loss: 0.0002855801336827308, Test Loss: 0.00030927899139559964\n",
            "Epoch 305/1000, Training Loss: 0.00027983922575603164, Test Loss: 0.00030351256630347496\n",
            "Epoch 306/1000, Training Loss: 0.00027425292714663746, Test Loss: 0.0002978930520941635\n",
            "Epoch 307/1000, Training Loss: 0.00026881680354062207, Test Loss: 0.00029241671886363104\n",
            "Epoch 308/1000, Training Loss: 0.00026352654181906063, Test Loss: 0.0002870799091562944\n",
            "Epoch 309/1000, Training Loss: 0.0002583779478663518, Test Loss: 0.0002818790386988621\n",
            "Epoch 310/1000, Training Loss: 0.000253366944298399, Test Loss: 0.0002768105969056917\n",
            "Epoch 311/1000, Training Loss: 0.0002484895681252835, Test Loss: 0.0002718711471752016\n",
            "Epoch 312/1000, Training Loss: 0.00024374196836185038, Test Loss: 0.000267057326995823\n",
            "Epoch 313/1000, Training Loss: 0.00023912040359834352, Test Loss: 0.0002623658478787794\n",
            "Epoch 314/1000, Training Loss: 0.00023462123954215376, Test Loss: 0.0002577934951339731\n",
            "Epoch 315/1000, Training Loss: 0.00023024094654066477, Test Loss: 0.00025333712750416767\n",
            "Epoch 316/1000, Training Loss: 0.00022597609709416498, Test Loss: 0.000248993676671678\n",
            "Epoch 317/1000, Training Loss: 0.00022182336336693865, Test Loss: 0.00024476014665081346\n",
            "Epoch 318/1000, Training Loss: 0.0002177795147037267, Test Loss: 0.0002406336130783641\n",
            "Epoch 319/1000, Training Loss: 0.0002138414151580284, Test Loss: 0.0002366112224136099\n",
            "Epoch 320/1000, Training Loss: 0.00021000602103792246, Test Loss: 0.00023269019105840412\n",
            "Epoch 321/1000, Training Loss: 0.00020627037847446885, Test Loss: 0.0002288678044071705\n",
            "Epoch 322/1000, Training Loss: 0.00020263162101710974, Test Loss: 0.00022514141583585706\n",
            "Epoch 323/1000, Training Loss: 0.00019908696725992687, Test Loss: 0.0002215084456381956\n",
            "Epoch 324/1000, Training Loss: 0.00019563371850211074, Test Loss: 0.00021796637991696596\n",
            "Epoch 325/1000, Training Loss: 0.0001922692564454948, Test Loss: 0.00021451276943726342\n",
            "Epoch 326/1000, Training Loss: 0.0001889910409315996, Test Loss: 0.000211145228448299\n",
            "Epoch 327/1000, Training Loss: 0.00018579660772026125, Test Loss: 0.0002078614334796111\n",
            "Epoch 328/1000, Training Loss: 0.00018268356631150743, Test Loss: 0.00020465912211708459\n",
            "Epoch 329/1000, Training Loss: 0.00017964959781206996, Test Loss: 0.00020153609176371248\n",
            "Epoch 330/1000, Training Loss: 0.00017669245284762747, Test Loss: 0.00019849019838954343\n",
            "Epoch 331/1000, Training Loss: 0.00017380994952159002, Test Loss: 0.00019551935527490851\n",
            "Epoch 332/1000, Training Loss: 0.000170999971421018, Test Loss: 0.00019262153175054052\n",
            "Epoch 333/1000, Training Loss: 0.00016826046567005578, Test Loss: 0.00018979475193793221\n",
            "Epoch 334/1000, Training Loss: 0.00016558944103107977, Test Loss: 0.00018703709349289057\n",
            "Epoch 335/1000, Training Loss: 0.00016298496605357752, Test Loss: 0.00018434668635496597\n",
            "Epoch 336/1000, Training Loss: 0.00016044516727063487, Test Loss: 0.00018172171150509912\n",
            "Epoch 337/1000, Training Loss: 0.00015796822744278385, Test Loss: 0.0001791603997336578\n",
            "Epoch 338/1000, Training Loss: 0.0001555523838488507, Test Loss: 0.0001766610304207093\n",
            "Epoch 339/1000, Training Loss: 0.0001531959266233119, Test Loss: 0.00017422193033019196\n",
            "Epoch 340/1000, Training Loss: 0.0001508971971396227, Test Loss: 0.00017184147241941928\n",
            "Epoch 341/1000, Training Loss: 0.00014865458643889442, Test Loss: 0.00016951807466522237\n",
            "Epoch 342/1000, Training Loss: 0.0001464665337031865, Test Loss: 0.00016725019890777778\n",
            "Epoch 343/1000, Training Loss: 0.00014433152477271734, Test Loss: 0.00016503634971308935\n",
            "Epoch 344/1000, Training Loss: 0.00014224809070614364, Test Loss: 0.00016287507325489823\n",
            "Epoch 345/1000, Training Loss: 0.0001402148063831014, Test Loss: 0.0001607649562166867\n",
            "Epoch 346/1000, Training Loss: 0.0001382302891481476, Test Loss: 0.00015870462471432586\n",
            "Epoch 347/1000, Training Loss: 0.00013629319749518923, Test Loss: 0.0001566927432398007\n",
            "Epoch 348/1000, Training Loss: 0.00013440222979150269, Test Loss: 0.0001547280136263377\n",
            "Epoch 349/1000, Training Loss: 0.00013255612304042391, Test Loss: 0.0001528091740352041\n",
            "Epoch 350/1000, Training Loss: 0.00013075365168175917, Test Loss: 0.00015093499796433067\n",
            "Epoch 351/1000, Training Loss: 0.00012899362642899097, Test Loss: 0.00014910429327885587\n",
            "Epoch 352/1000, Training Loss: 0.00012727489314235096, Test Loss: 0.00014731590126363986\n",
            "Epoch 353/1000, Training Loss: 0.00012559633173678356, Test Loss: 0.00014556869569770653\n",
            "Epoch 354/1000, Training Loss: 0.00012395685512390145, Test Loss: 0.00014386158195052776\n",
            "Epoch 355/1000, Training Loss: 0.00012235540818699545, Test Loss: 0.0001421934961000482\n",
            "Epoch 356/1000, Training Loss: 0.0001207909667881741, Test Loss: 0.000140563404072271\n",
            "Epoch 357/1000, Training Loss: 0.00011926253680673463, Test Loss: 0.00013897030080219962\n",
            "Epoch 358/1000, Training Loss: 0.00011776915320786252, Test Loss: 0.00013741320941591147\n",
            "Epoch 359/1000, Training Loss: 0.00011630987914078303, Test Loss: 0.00013589118043349108\n",
            "Epoch 360/1000, Training Loss: 0.00011488380506550226, Test Loss: 0.0001344032909925462\n",
            "Epoch 361/1000, Training Loss: 0.00011349004790728134, Test Loss: 0.00013294864409199288\n",
            "Epoch 362/1000, Training Loss: 0.00011212775023801072, Test Loss: 0.00013152636785578275\n",
            "Epoch 363/1000, Training Loss: 0.00011079607948367969, Test Loss: 0.00013013561481624802\n",
            "Epoch 364/1000, Training Loss: 0.00010949422715711998, Test Loss: 0.00012877556121667873\n",
            "Epoch 365/1000, Training Loss: 0.00010822140811526904, Test Loss: 0.00012744540633279931\n",
            "Epoch 366/1000, Training Loss: 0.00010697685984018469, Test Loss: 0.00012614437181276766\n",
            "Epoch 367/1000, Training Loss: 0.00010575984174305315, Test Loss: 0.0001248717010352953\n",
            "Epoch 368/1000, Training Loss: 0.00010456963449049012, Test Loss: 0.00012362665848553137\n",
            "Epoch 369/1000, Training Loss: 0.00010340553935241488, Test Loss: 0.00012240852914831182\n",
            "Epoch 370/1000, Training Loss: 0.00010226687757081491, Test Loss: 0.00012121661791838096\n",
            "Epoch 371/1000, Training Loss: 0.00010115298974874316, Test Loss: 0.00012005024902721013\n",
            "Epoch 372/1000, Training Loss: 0.00010006323525888039, Test Loss: 0.00011890876548600295\n",
            "Epoch 373/1000, Training Loss: 9.899699167106125e-05, Test Loss: 0.00011779152854452138\n",
            "Epoch 374/1000, Training Loss: 9.79536541981184e-05, Test Loss: 0.00011669791716533243\n",
            "Epoch 375/1000, Training Loss: 9.693263515948315e-05, Test Loss: 0.00011562732751309107\n",
            "Epoch 376/1000, Training Loss: 9.593336346193869e-05, Test Loss: 0.00011457917245849\n",
            "Epoch 377/1000, Training Loss: 9.495528409698967e-05, Test Loss: 0.00011355288109649801\n",
            "Epoch 378/1000, Training Loss: 9.399785765429967e-05, Test Loss: 0.00011254789827850872\n",
            "Epoch 379/1000, Training Loss: 9.306055985065392e-05, Test Loss: 0.00011156368415803555\n",
            "Epoch 380/1000, Training Loss: 9.214288107397028e-05, Test Loss: 0.00011059971374961151\n",
            "Epoch 381/1000, Training Loss: 9.124432594183627e-05, Test Loss: 0.00010965547650050669\n",
            "Epoch 382/1000, Training Loss: 9.036441287411802e-05, Test Loss: 0.00010873047587495209\n",
            "Epoch 383/1000, Training Loss: 8.95026736791624e-05, Test Loss: 0.00010782422895049474\n",
            "Epoch 384/1000, Training Loss: 8.865865315314584e-05, Test Loss: 0.0001069362660261706\n",
            "Epoch 385/1000, Training Loss: 8.783190869215523e-05, Test Loss: 0.00010606613024216252\n",
            "Epoch 386/1000, Training Loss: 8.70220099165576e-05, Test Loss: 0.00010521337721061471\n",
            "Epoch 387/1000, Training Loss: 8.622853830726869e-05, Test Loss: 0.00010437757465729664\n",
            "Epoch 388/1000, Training Loss: 8.545108685352577e-05, Test Loss: 0.00010355830207380687\n",
            "Epoch 389/1000, Training Loss: 8.468925971178303e-05, Test Loss: 0.00010275515038001421\n",
            "Epoch 390/1000, Training Loss: 8.394267187536145e-05, Test Loss: 0.00010196772159644486\n",
            "Epoch 391/1000, Training Loss: 8.321094885450301e-05, Test Loss: 0.00010119562852632606\n",
            "Epoch 392/1000, Training Loss: 8.249372636648634e-05, Test Loss: 0.00010043849444701098\n",
            "Epoch 393/1000, Training Loss: 8.179065003546753e-05, Test Loss: 9.96959528105084e-05\n",
            "Epoch 394/1000, Training Loss: 8.110137510173524e-05, Test Loss: 9.896764695285502e-05\n",
            "Epoch 395/1000, Training Loss: 8.042556614005074e-05, Test Loss: 9.825322981206389e-05\n",
            "Epoch 396/1000, Training Loss: 7.976289678680195e-05, Test Loss: 9.755236365440907e-05\n",
            "Epoch 397/1000, Training Loss: 7.911304947566209e-05, Test Loss: 9.68647198087892e-05\n",
            "Epoch 398/1000, Training Loss: 7.847571518147276e-05, Test Loss: 9.618997840893854e-05\n",
            "Epoch 399/1000, Training Loss: 7.785059317210225e-05, Test Loss: 9.552782814325075e-05\n",
            "Epoch 400/1000, Training Loss: 7.723739076798414e-05, Test Loss: 9.48779660119861e-05\n",
            "Epoch 401/1000, Training Loss: 7.663582310912359e-05, Test Loss: 9.424009709164908e-05\n",
            "Epoch 402/1000, Training Loss: 7.604561292928908e-05, Test Loss: 9.361393430631282e-05\n",
            "Epoch 403/1000, Training Loss: 7.546649033718603e-05, Test Loss: 9.299919820569805e-05\n",
            "Epoch 404/1000, Training Loss: 7.489819260436638e-05, Test Loss: 9.239561674978789e-05\n",
            "Epoch 405/1000, Training Loss: 7.434046395965753e-05, Test Loss: 9.180292509978691e-05\n",
            "Epoch 406/1000, Training Loss: 7.379305538990339e-05, Test Loss: 9.12208654152458e-05\n",
            "Epoch 407/1000, Training Loss: 7.325572444680952e-05, Test Loss: 9.06491866571534e-05\n",
            "Epoch 408/1000, Training Loss: 7.272823505969118e-05, Test Loss: 9.008764439681678e-05\n",
            "Epoch 409/1000, Training Loss: 7.221035735393652e-05, Test Loss: 8.953600063036499e-05\n",
            "Epoch 410/1000, Training Loss: 7.17018674750047e-05, Test Loss: 8.899402359870079e-05\n",
            "Epoch 411/1000, Training Loss: 7.120254741776268e-05, Test Loss: 8.846148761272932e-05\n",
            "Epoch 412/1000, Training Loss: 7.071218486101107e-05, Test Loss: 8.79381728837226e-05\n",
            "Epoch 413/1000, Training Loss: 7.023057300701739e-05, Test Loss: 8.74238653586488e-05\n",
            "Epoch 414/1000, Training Loss: 6.97575104258904e-05, Test Loss: 8.691835656031542e-05\n",
            "Epoch 415/1000, Training Loss: 6.929280090466739e-05, Test Loss: 8.642144343220064e-05\n",
            "Epoch 416/1000, Training Loss: 6.883625330092463e-05, Test Loss: 8.593292818780654e-05\n",
            "Epoch 417/1000, Training Loss: 6.838768140079913e-05, Test Loss: 8.545261816441838e-05\n",
            "Epoch 418/1000, Training Loss: 6.794690378126221e-05, Test Loss: 8.498032568111989e-05\n",
            "Epoch 419/1000, Training Loss: 6.751374367651363e-05, Test Loss: 8.451586790094771e-05\n",
            "Epoch 420/1000, Training Loss: 6.708802884837191e-05, Test Loss: 8.405906669705882e-05\n",
            "Epoch 421/1000, Training Loss: 6.666959146051829e-05, Test Loss: 8.360974852277915e-05\n",
            "Epoch 422/1000, Training Loss: 6.625826795649215e-05, Test Loss: 8.31677442854339e-05\n",
            "Epoch 423/1000, Training Loss: 6.585389894130179e-05, Test Loss: 8.273288922383079e-05\n",
            "Epoch 424/1000, Training Loss: 6.545632906654356e-05, Test Loss: 8.230502278929322e-05\n",
            "Epoch 425/1000, Training Loss: 6.506540691892092e-05, Test Loss: 8.188398853013334e-05\n",
            "Epoch 426/1000, Training Loss: 6.46809849120495e-05, Test Loss: 8.146963397946337e-05\n",
            "Epoch 427/1000, Training Loss: 6.430291918145267e-05, Test Loss: 8.106181054624708e-05\n",
            "Epoch 428/1000, Training Loss: 6.393106948263798e-05, Test Loss: 8.066037340948086e-05\n",
            "Epoch 429/1000, Training Loss: 6.356529909217279e-05, Test Loss: 8.026518141543667e-05\n",
            "Epoch 430/1000, Training Loss: 6.320547471164382e-05, Test Loss: 7.987609697784423e-05\n",
            "Epoch 431/1000, Training Loss: 6.285146637443309e-05, Test Loss: 7.949298598094845e-05\n",
            "Epoch 432/1000, Training Loss: 6.250314735520628e-05, Test Loss: 7.911571768533793e-05\n",
            "Epoch 433/1000, Training Loss: 6.216039408203739e-05, Test Loss: 7.87441646364751e-05\n",
            "Epoch 434/1000, Training Loss: 6.182308605108296e-05, Test Loss: 7.837820257584405e-05\n",
            "Epoch 435/1000, Training Loss: 6.149110574373302e-05, Test Loss: 7.801771035462795e-05\n",
            "Epoch 436/1000, Training Loss: 6.116433854615973e-05, Test Loss: 7.766256984985353e-05\n",
            "Epoch 437/1000, Training Loss: 6.084267267118155e-05, Test Loss: 7.731266588292627e-05\n",
            "Epoch 438/1000, Training Loss: 6.052599908238551e-05, Test Loss: 7.696788614047295e-05\n",
            "Epoch 439/1000, Training Loss: 6.021421142042823e-05, Test Loss: 7.662812109744133e-05\n",
            "Epoch 440/1000, Training Loss: 5.990720593145595e-05, Test Loss: 7.6293263942376e-05\n",
            "Epoch 441/1000, Training Loss: 5.960488139756866e-05, Test Loss: 7.596321050481389e-05\n",
            "Epoch 442/1000, Training Loss: 5.930713906927616e-05, Test Loss: 7.563785918472611e-05\n",
            "Epoch 443/1000, Training Loss: 5.901388259988228e-05, Test Loss: 7.531711088396179e-05\n",
            "Epoch 444/1000, Training Loss: 5.8725017981734494e-05, Test Loss: 7.500086893961925e-05\n",
            "Epoch 445/1000, Training Loss: 5.844045348428708e-05, Test Loss: 7.468903905929309e-05\n",
            "Epoch 446/1000, Training Loss: 5.816009959392e-05, Test Loss: 7.438152925814483e-05\n",
            "Epoch 447/1000, Training Loss: 5.788386895546239e-05, Test Loss: 7.40782497977407e-05\n",
            "Epoch 448/1000, Training Loss: 5.761167631537049e-05, Test Loss: 7.3779113126604e-05\n",
            "Epoch 449/1000, Training Loss: 5.734343846650157e-05, Test Loss: 7.348403382242761e-05\n",
            "Epoch 450/1000, Training Loss: 5.707907419444928e-05, Test Loss: 7.319292853591001e-05\n",
            "Epoch 451/1000, Training Loss: 5.6818504225385034e-05, Test Loss: 7.290571593615248e-05\n",
            "Epoch 452/1000, Training Loss: 5.656165117535668e-05, Test Loss: 7.262231665758429e-05\n",
            "Epoch 453/1000, Training Loss: 5.6308439501013786e-05, Test Loss: 7.234265324835948e-05\n",
            "Epoch 454/1000, Training Loss: 5.6058795451701616e-05, Test Loss: 7.206665012019369e-05\n",
            "Epoch 455/1000, Training Loss: 5.5812647022899194e-05, Test Loss: 7.179423349958925e-05\n",
            "Epoch 456/1000, Training Loss: 5.556992391094549e-05, Test Loss: 7.152533138041342e-05\n",
            "Epoch 457/1000, Training Loss: 5.5330557469026786e-05, Test Loss: 7.125987347779198e-05\n",
            "Epoch 458/1000, Training Loss: 5.509448066438493e-05, Test Loss: 7.099779118327367e-05\n",
            "Epoch 459/1000, Training Loss: 5.486162803670537e-05, Test Loss: 7.073901752123064e-05\n",
            "Epoch 460/1000, Training Loss: 5.463193565766028e-05, Test Loss: 7.048348710646687e-05\n",
            "Epoch 461/1000, Training Loss: 5.4405341091563555e-05, Test Loss: 7.023113610298497e-05\n",
            "Epoch 462/1000, Training Loss: 5.4181783357107136e-05, Test Loss: 6.998190218389106e-05\n",
            "Epoch 463/1000, Training Loss: 5.3961202890154513e-05, Test Loss: 6.973572449240203e-05\n",
            "Epoch 464/1000, Training Loss: 5.3743541507544544e-05, Test Loss: 6.949254360391189e-05\n",
            "Epoch 465/1000, Training Loss: 5.35287423718959e-05, Test Loss: 6.925230148910765e-05\n",
            "Epoch 466/1000, Training Loss: 5.331674995736601e-05, Test Loss: 6.901494147807944e-05\n",
            "Epoch 467/1000, Training Loss: 5.3107510016346604e-05, Test Loss: 6.878040822542342e-05\n",
            "Epoch 468/1000, Training Loss: 5.29009695470665e-05, Test Loss: 6.85486476762887e-05\n",
            "Epoch 469/1000, Training Loss: 5.269707676206925e-05, Test Loss: 6.831960703334324e-05\n",
            "Epoch 470/1000, Training Loss: 5.249578105755456e-05, Test Loss: 6.809323472464285e-05\n",
            "Epoch 471/1000, Training Loss: 5.229703298354071e-05, Test Loss: 6.786948037237684e-05\n",
            "Epoch 472/1000, Training Loss: 5.2100784214839326e-05, Test Loss: 6.764829476244473e-05\n",
            "Epoch 473/1000, Training Loss: 5.190698752280983e-05, Test Loss: 6.742962981486992e-05\n",
            "Epoch 474/1000, Training Loss: 5.171559674787551e-05, Test Loss: 6.721343855500432e-05\n",
            "Epoch 475/1000, Training Loss: 5.1526566772779074e-05, Test Loss: 6.699967508551551e-05\n",
            "Epoch 476/1000, Training Loss: 5.133985349655484e-05, Test Loss: 6.678829455911972e-05\n",
            "Epoch 477/1000, Training Loss: 5.11554138091976e-05, Test Loss: 6.657925315205981e-05\n",
            "Epoch 478/1000, Training Loss: 5.0973205567009224e-05, Test Loss: 6.637250803828175e-05\n",
            "Epoch 479/1000, Training Loss: 5.079318756860663e-05, Test Loss: 6.616801736431345e-05\n",
            "Epoch 480/1000, Training Loss: 5.061531953156039e-05, Test Loss: 6.596574022481486e-05\n",
            "Epoch 481/1000, Training Loss: 5.0439562069664896e-05, Test Loss: 6.576563663877613e-05\n",
            "Epoch 482/1000, Training Loss: 5.026587667080389e-05, Test Loss: 6.556766752635761e-05\n",
            "Epoch 483/1000, Training Loss: 5.009422567540876e-05, Test Loss: 6.537179468634296e-05\n",
            "Epoch 484/1000, Training Loss: 4.992457225548304e-05, Test Loss: 6.517798077419371e-05\n",
            "Epoch 485/1000, Training Loss: 4.975688039418164e-05, Test Loss: 6.498618928069059e-05\n",
            "Epoch 486/1000, Training Loss: 4.959111486592994e-05, Test Loss: 6.479638451113708e-05\n",
            "Epoch 487/1000, Training Loss: 4.942724121706451e-05, Test Loss: 6.46085315651131e-05\n",
            "Epoch 488/1000, Training Loss: 4.926522574698489e-05, Test Loss: 6.442259631677432e-05\n",
            "Epoch 489/1000, Training Loss: 4.91050354897969e-05, Test Loss: 6.423854539565703e-05\n",
            "Epoch 490/1000, Training Loss: 4.8946638196440006e-05, Test Loss: 6.405634616800532e-05\n",
            "Epoch 491/1000, Training Loss: 4.879000231727857e-05, Test Loss: 6.387596671857949e-05\n",
            "Epoch 492/1000, Training Loss: 4.863509698515003e-05, Test Loss: 6.36973758329455e-05\n",
            "Epoch 493/1000, Training Loss: 4.84818919988559e-05, Test Loss: 6.352054298023777e-05\n",
            "Epoch 494/1000, Training Loss: 4.83303578070781e-05, Test Loss: 6.334543829636272e-05\n",
            "Epoch 495/1000, Training Loss: 4.818046549271652e-05, Test Loss: 6.317203256764938e-05\n",
            "Epoch 496/1000, Training Loss: 4.803218675763254e-05, Test Loss: 6.300029721492657e-05\n",
            "Epoch 497/1000, Training Loss: 4.788549390778604e-05, Test Loss: 6.283020427801843e-05\n",
            "Epoch 498/1000, Training Loss: 4.77403598387573e-05, Test Loss: 6.266172640063844e-05\n",
            "Epoch 499/1000, Training Loss: 4.759675802164252e-05, Test Loss: 6.249483681568412e-05\n",
            "Epoch 500/1000, Training Loss: 4.7454662489313005e-05, Test Loss: 6.232950933091442e-05\n",
            "Epoch 501/1000, Training Loss: 4.731404782302728e-05, Test Loss: 6.216571831498909e-05\n",
            "Epoch 502/1000, Training Loss: 4.7174889139386994e-05, Test Loss: 6.200343868388588e-05\n",
            "Epoch 503/1000, Training Loss: 4.7037162077628e-05, Test Loss: 6.184264588765884e-05\n",
            "Epoch 504/1000, Training Loss: 4.690084278723417e-05, Test Loss: 6.168331589754106e-05\n",
            "Epoch 505/1000, Training Loss: 4.676590791587291e-05, Test Loss: 6.152542519338672e-05\n",
            "Epoch 506/1000, Training Loss: 4.66323345976322e-05, Test Loss: 6.136895075142975e-05\n",
            "Epoch 507/1000, Training Loss: 4.650010044156195e-05, Test Loss: 6.121387003236078e-05\n",
            "Epoch 508/1000, Training Loss: 4.6369183520504236e-05, Test Loss: 6.106016096971447e-05\n",
            "Epoch 509/1000, Training Loss: 4.623956236020859e-05, Test Loss: 6.090780195855299e-05\n",
            "Epoch 510/1000, Training Loss: 4.6111215928722195e-05, Test Loss: 6.0756771844435126e-05\n",
            "Epoch 511/1000, Training Loss: 4.598412362604864e-05, Test Loss: 6.060704991267686e-05\n",
            "Epoch 512/1000, Training Loss: 4.58582652740689e-05, Test Loss: 6.045861587788139e-05\n",
            "Epoch 513/1000, Training Loss: 4.573362110671643e-05, Test Loss: 6.0311449873742106e-05\n",
            "Epoch 514/1000, Training Loss: 4.561017176039791e-05, Test Loss: 6.016553244309428e-05\n",
            "Epoch 515/1000, Training Loss: 4.548789826465765e-05, Test Loss: 6.002084452823562e-05\n",
            "Epoch 516/1000, Training Loss: 4.5366782033074446e-05, Test Loss: 5.987736746148237e-05\n",
            "Epoch 517/1000, Training Loss: 4.524680485438678e-05, Test Loss: 5.9735082955971577e-05\n",
            "Epoch 518/1000, Training Loss: 4.512794888384165e-05, Test Loss: 5.959397309669344e-05\n",
            "Epoch 519/1000, Training Loss: 4.501019663475703e-05, Test Loss: 5.945402033174928e-05\n",
            "Epoch 520/1000, Training Loss: 4.4893530970297995e-05, Test Loss: 5.9315207463839874e-05\n",
            "Epoch 521/1000, Training Loss: 4.477793509545463e-05, Test Loss: 5.917751764196015e-05\n",
            "Epoch 522/1000, Training Loss: 4.4663392549221904e-05, Test Loss: 5.9040934353307063e-05\n",
            "Epoch 523/1000, Training Loss: 4.454988719697198e-05, Test Loss: 5.890544141538672e-05\n",
            "Epoch 524/1000, Training Loss: 4.443740322301749e-05, Test Loss: 5.877102296833467e-05\n",
            "Epoch 525/1000, Training Loss: 4.432592512335613e-05, Test Loss: 5.863766346741084e-05\n",
            "Epoch 526/1000, Training Loss: 4.4215437698597055e-05, Test Loss: 5.850534767569334e-05\n",
            "Epoch 527/1000, Training Loss: 4.410592604706196e-05, Test Loss: 5.837406065695912e-05\n",
            "Epoch 528/1000, Training Loss: 4.399737555805332e-05, Test Loss: 5.824378776873046e-05\n",
            "Epoch 529/1000, Training Loss: 4.3889771905290974e-05, Test Loss: 5.811451465550551e-05\n",
            "Epoch 530/1000, Training Loss: 4.3783101040510056e-05, Test Loss: 5.798622724216229e-05\n",
            "Epoch 531/1000, Training Loss: 4.36773491872125e-05, Test Loss: 5.7858911727504854e-05\n",
            "Epoch 532/1000, Training Loss: 4.357250283457684e-05, Test Loss: 5.773255457799855e-05\n",
            "Epoch 533/1000, Training Loss: 4.346854873151191e-05, Test Loss: 5.76071425216464e-05\n",
            "Epoch 534/1000, Training Loss: 4.3365473880858154e-05, Test Loss: 5.748266254201229e-05\n",
            "Epoch 535/1000, Training Loss: 4.326326553372981e-05, Test Loss: 5.735910187240758e-05\n",
            "Epoch 536/1000, Training Loss: 4.316191118399504e-05, Test Loss: 5.723644799020704e-05\n",
            "Epoch 537/1000, Training Loss: 4.3061398562890624e-05, Test Loss: 5.7114688611320725e-05\n",
            "Epoch 538/1000, Training Loss: 4.296171563376524e-05, Test Loss: 5.6993811684785446e-05\n",
            "Epoch 539/1000, Training Loss: 4.286285058695192e-05, Test Loss: 5.6873805387501356e-05\n",
            "Epoch 540/1000, Training Loss: 4.276479183476407e-05, Test Loss: 5.675465811909813e-05\n",
            "Epoch 541/1000, Training Loss: 4.266752800661163e-05, Test Loss: 5.663635849691844e-05\n",
            "Epoch 542/1000, Training Loss: 4.257104794423658e-05, Test Loss: 5.651889535113637e-05\n",
            "Epoch 543/1000, Training Loss: 4.2475340697060505e-05, Test Loss: 5.6402257719985624e-05\n",
            "Epoch 544/1000, Training Loss: 4.238039551764734e-05, Test Loss: 5.628643484511234e-05\n",
            "Epoch 545/1000, Training Loss: 4.2286201857270524e-05, Test Loss: 5.617141616703502e-05\n",
            "Epoch 546/1000, Training Loss: 4.2192749361590993e-05, Test Loss: 5.605719132072468e-05\n",
            "Epoch 547/1000, Training Loss: 4.2100027866435965e-05, Test Loss: 5.594375013127872e-05\n",
            "Epoch 548/1000, Training Loss: 4.200802739367791e-05, Test Loss: 5.583108260971292e-05\n",
            "Epoch 549/1000, Training Loss: 4.191673814721344e-05, Test Loss: 5.571917894885252e-05\n",
            "Epoch 550/1000, Training Loss: 4.182615050903804e-05, Test Loss: 5.560802951932304e-05\n",
            "Epoch 551/1000, Training Loss: 4.1736255035412705e-05, Test Loss: 5.5497624865626684e-05\n",
            "Epoch 552/1000, Training Loss: 4.16470424531207e-05, Test Loss: 5.5387955702339556e-05\n",
            "Epoch 553/1000, Training Loss: 4.1558503655817076e-05, Test Loss: 5.5279012910375904e-05\n",
            "Epoch 554/1000, Training Loss: 4.1470629700459135e-05, Test Loss: 5.517078753335714e-05\n",
            "Epoch 555/1000, Training Loss: 4.1383411803825774e-05, Test Loss: 5.506327077405949e-05\n",
            "Epoch 556/1000, Training Loss: 4.129684133911619e-05, Test Loss: 5.4956453990953596e-05\n",
            "Epoch 557/1000, Training Loss: 4.121090983262951e-05, Test Loss: 5.485032869482949e-05\n",
            "Epoch 558/1000, Training Loss: 4.1125608960521715e-05, Test Loss: 5.474488654549294e-05\n",
            "Epoch 559/1000, Training Loss: 4.10409305456408e-05, Test Loss: 5.4640119348546994e-05\n",
            "Epoch 560/1000, Training Loss: 4.095686655443331e-05, Test Loss: 5.4536019052255705e-05\n",
            "Epoch 561/1000, Training Loss: 4.0873409093924304e-05, Test Loss: 5.443257774446973e-05\n",
            "Epoch 562/1000, Training Loss: 4.079055040876819e-05, Test Loss: 5.432978764963967e-05\n",
            "Epoch 563/1000, Training Loss: 4.070828287836904e-05, Test Loss: 5.422764112589131e-05\n",
            "Epoch 564/1000, Training Loss: 4.062659901406528e-05, Test Loss: 5.4126130662172396e-05\n",
            "Epoch 565/1000, Training Loss: 4.0545491456382106e-05, Test Loss: 5.402524887546775e-05\n",
            "Epoch 566/1000, Training Loss: 4.046495297234747e-05, Test Loss: 5.3924988508085575e-05\n",
            "Epoch 567/1000, Training Loss: 4.038497645286878e-05, Test Loss: 5.382534242499652e-05\n",
            "Epoch 568/1000, Training Loss: 4.030555491017133e-05, Test Loss: 5.372630361124804e-05\n",
            "Epoch 569/1000, Training Loss: 4.022668147529618e-05, Test Loss: 5.362786516943289e-05\n",
            "Epoch 570/1000, Training Loss: 4.014834939565475e-05, Test Loss: 5.353002031721788e-05\n",
            "Epoch 571/1000, Training Loss: 4.007055203263962e-05, Test Loss: 5.3432762384931227e-05\n",
            "Epoch 572/1000, Training Loss: 3.99932828592914e-05, Test Loss: 5.3336084813209894e-05\n",
            "Epoch 573/1000, Training Loss: 3.991653545801745e-05, Test Loss: 5.323998115069919e-05\n",
            "Epoch 574/1000, Training Loss: 3.984030351836463e-05, Test Loss: 5.314444505180284e-05\n",
            "Epoch 575/1000, Training Loss: 3.97645808348409e-05, Test Loss: 5.3049470274495256e-05\n",
            "Epoch 576/1000, Training Loss: 3.9689361304788265e-05, Test Loss: 5.295505067817591e-05\n",
            "Epoch 577/1000, Training Loss: 3.9614638926303016e-05, Test Loss: 5.2861180221580444e-05\n",
            "Epoch 578/1000, Training Loss: 3.954040779620502e-05, Test Loss: 5.276785296073047e-05\n",
            "Epoch 579/1000, Training Loss: 3.946666210804986e-05, Test Loss: 5.2675063046943954e-05\n",
            "Epoch 580/1000, Training Loss: 3.9393396150189394e-05, Test Loss: 5.2582804724887196e-05\n",
            "Epoch 581/1000, Training Loss: 3.932060430387391e-05, Test Loss: 5.249107233066515e-05\n",
            "Epoch 582/1000, Training Loss: 3.92482810413984e-05, Test Loss: 5.239986028996316e-05\n",
            "Epoch 583/1000, Training Loss: 3.9176420924290254e-05, Test Loss: 5.230916311623388e-05\n",
            "Epoch 584/1000, Training Loss: 3.9105018601536455e-05, Test Loss: 5.221897540891456e-05\n",
            "Epoch 585/1000, Training Loss: 3.903406880785311e-05, Test Loss: 5.212929185169977e-05\n",
            "Epoch 586/1000, Training Loss: 3.896356636199135e-05, Test Loss: 5.204010721084276e-05\n",
            "Epoch 587/1000, Training Loss: 3.889350616508261e-05, Test Loss: 5.195141633350111e-05\n",
            "Epoch 588/1000, Training Loss: 3.8823883199020114e-05, Test Loss: 5.186321414612258e-05\n",
            "Epoch 589/1000, Training Loss: 3.875469252487661e-05, Test Loss: 5.177549565285816e-05\n",
            "Epoch 590/1000, Training Loss: 3.8685929281357885e-05, Test Loss: 5.168825593402425e-05\n",
            "Epoch 591/1000, Training Loss: 3.861758868328954e-05, Test Loss: 5.1601490144588554e-05\n",
            "Epoch 592/1000, Training Loss: 3.854966602013857e-05, Test Loss: 5.151519351269965e-05\n",
            "Epoch 593/1000, Training Loss: 3.848215665456587e-05, Test Loss: 5.142936133824203e-05\n",
            "Epoch 594/1000, Training Loss: 3.841505602101312e-05, Test Loss: 5.1343988991428425e-05\n",
            "Epoch 595/1000, Training Loss: 3.8348359624318754e-05, Test Loss: 5.1259071911424256e-05\n",
            "Epoch 596/1000, Training Loss: 3.8282063038365405e-05, Test Loss: 5.117460560500436e-05\n",
            "Epoch 597/1000, Training Loss: 3.8216161904757065e-05, Test Loss: 5.10905856452317e-05\n",
            "Epoch 598/1000, Training Loss: 3.8150651931524206e-05, Test Loss: 5.100700767017908e-05\n",
            "Epoch 599/1000, Training Loss: 3.808552889185865e-05, Test Loss: 5.0923867381665435e-05\n",
            "Epoch 600/1000, Training Loss: 3.8020788622875714e-05, Test Loss: 5.084116054403237e-05\n",
            "Epoch 601/1000, Training Loss: 3.795642702440279e-05, Test Loss: 5.075888298294453e-05\n",
            "Epoch 602/1000, Training Loss: 3.789244005779417e-05, Test Loss: 5.0677030584209295e-05\n",
            "Epoch 603/1000, Training Loss: 3.782882374477317e-05, Test Loss: 5.0595599292636674e-05\n",
            "Epoch 604/1000, Training Loss: 3.776557416629614e-05, Test Loss: 5.051458511091237e-05\n",
            "Epoch 605/1000, Training Loss: 3.7702687461445245e-05, Test Loss: 5.043398409850073e-05\n",
            "Epoch 606/1000, Training Loss: 3.7640159826342006e-05, Test Loss: 5.0353792370572365e-05\n",
            "Epoch 607/1000, Training Loss: 3.75779875130858e-05, Test Loss: 5.027400609695755e-05\n",
            "Epoch 608/1000, Training Loss: 3.751616682871313e-05, Test Loss: 5.01946215011167e-05\n",
            "Epoch 609/1000, Training Loss: 3.7454694134182924e-05, Test Loss: 5.0115634859139926e-05\n",
            "Epoch 610/1000, Training Loss: 3.7393565843379706e-05, Test Loss: 5.0037042498763835e-05\n",
            "Epoch 611/1000, Training Loss: 3.733277842213991e-05, Test Loss: 4.995884079841656e-05\n",
            "Epoch 612/1000, Training Loss: 3.7272328387300444e-05, Test Loss: 4.9881026186269853e-05\n",
            "Epoch 613/1000, Training Loss: 3.7212212305763704e-05, Test Loss: 4.980359513933042e-05\n",
            "Epoch 614/1000, Training Loss: 3.7152426793585715e-05, Test Loss: 4.972654418253899e-05\n",
            "Epoch 615/1000, Training Loss: 3.7092968515083064e-05, Test Loss: 4.964986988789127e-05\n",
            "Epoch 616/1000, Training Loss: 3.703383418195668e-05, Test Loss: 4.957356887357822e-05\n",
            "Epoch 617/1000, Training Loss: 3.6975020552436974e-05, Test Loss: 4.949763780314879e-05\n",
            "Epoch 618/1000, Training Loss: 3.691652443044455e-05, Test Loss: 4.942207338468134e-05\n",
            "Epoch 619/1000, Training Loss: 3.685834266477099e-05, Test Loss: 4.934687236998806e-05\n",
            "Epoch 620/1000, Training Loss: 3.680047214827413e-05, Test Loss: 4.927203155382141e-05\n",
            "Epoch 621/1000, Training Loss: 3.674290981709247e-05, Test Loss: 4.919754777310343e-05\n",
            "Epoch 622/1000, Training Loss: 3.668565264987425e-05, Test Loss: 4.9123417906177244e-05\n",
            "Epoch 623/1000, Training Loss: 3.6628697667025004e-05, Test Loss: 4.904963887206448e-05\n",
            "Epoch 624/1000, Training Loss: 3.657204192996656e-05, Test Loss: 4.8976207629746695e-05\n",
            "Epoch 625/1000, Training Loss: 3.651568254041679e-05, Test Loss: 4.8903121177459074e-05\n",
            "Epoch 626/1000, Training Loss: 3.645961663968032e-05, Test Loss: 4.883037655199648e-05\n",
            "Epoch 627/1000, Training Loss: 3.6403841407954836e-05, Test Loss: 4.875797082804192e-05\n",
            "Epoch 628/1000, Training Loss: 3.634835406365225e-05, Test Loss: 4.868590111749787e-05\n",
            "Epoch 629/1000, Training Loss: 3.62931518627338e-05, Test Loss: 4.861416456884271e-05\n",
            "Epoch 630/1000, Training Loss: 3.623823209805792e-05, Test Loss: 4.854275836649541e-05\n",
            "Epoch 631/1000, Training Loss: 3.618359209874366e-05, Test Loss: 4.8471679730195894e-05\n",
            "Epoch 632/1000, Training Loss: 3.612922922954392e-05, Test Loss: 4.840092591439329e-05\n",
            "Epoch 633/1000, Training Loss: 3.607514089023461e-05, Test Loss: 4.833049420765414e-05\n",
            "Epoch 634/1000, Training Loss: 3.6021324515012816e-05, Test Loss: 4.826038193207715e-05\n",
            "Epoch 635/1000, Training Loss: 3.596777757191079e-05, Test Loss: 4.819058644272379e-05\n",
            "Epoch 636/1000, Training Loss: 3.5914497562220024e-05, Test Loss: 4.812110512705955e-05\n",
            "Epoch 637/1000, Training Loss: 3.586148201992637e-05, Test Loss: 4.805193540440845e-05\n",
            "Epoch 638/1000, Training Loss: 3.580872851115759e-05, Test Loss: 4.7983074725411756e-05\n",
            "Epoch 639/1000, Training Loss: 3.575623463364196e-05, Test Loss: 4.791452057150948e-05\n",
            "Epoch 640/1000, Training Loss: 3.570399801617749e-05, Test Loss: 4.7846270454423474e-05\n",
            "Epoch 641/1000, Training Loss: 3.5652016318111586e-05, Test Loss: 4.777832191565388e-05\n",
            "Epoch 642/1000, Training Loss: 3.560028722883103e-05, Test Loss: 4.771067252598891e-05\n",
            "Epoch 643/1000, Training Loss: 3.5548808467262747e-05, Test Loss: 4.764331988501779e-05\n",
            "Epoch 644/1000, Training Loss: 3.549757778138467e-05, Test Loss: 4.7576261620659916e-05\n",
            "Epoch 645/1000, Training Loss: 3.544659294774476e-05, Test Loss: 4.750949538870288e-05\n",
            "Epoch 646/1000, Training Loss: 3.539585177099132e-05, Test Loss: 4.744301887234627e-05\n",
            "Epoch 647/1000, Training Loss: 3.534535208341103e-05, Test Loss: 4.737682978175666e-05\n",
            "Epoch 648/1000, Training Loss: 3.52950917444777e-05, Test Loss: 4.731092585363289e-05\n",
            "Epoch 649/1000, Training Loss: 3.5245068640409246e-05, Test Loss: 4.7245304850781845e-05\n",
            "Epoch 650/1000, Training Loss: 3.5195280683731694e-05, Test Loss: 4.717996456169212e-05\n",
            "Epoch 651/1000, Training Loss: 3.514572581285522e-05, Test Loss: 4.711490280013346e-05\n",
            "Epoch 652/1000, Training Loss: 3.509640199165462e-05, Test Loss: 4.705011740474975e-05\n",
            "Epoch 653/1000, Training Loss: 3.504730720906052e-05, Test Loss: 4.6985606238662256e-05\n",
            "Epoch 654/1000, Training Loss: 3.4998439478658305e-05, Test Loss: 4.6921367189091965e-05\n",
            "Epoch 655/1000, Training Loss: 3.4949796838293056e-05, Test Loss: 4.685739816697643e-05\n",
            "Epoch 656/1000, Training Loss: 3.4901377349684204e-05, Test Loss: 4.679369710659482e-05\n",
            "Epoch 657/1000, Training Loss: 3.4853179098046086e-05, Test Loss: 4.67302619652118e-05\n",
            "Epoch 658/1000, Training Loss: 3.4805200191716905e-05, Test Loss: 4.6667090722718704e-05\n",
            "Epoch 659/1000, Training Loss: 3.475743876179411e-05, Test Loss: 4.660418138128189e-05\n",
            "Epoch 660/1000, Training Loss: 3.4709892961776654e-05, Test Loss: 4.654153196500407e-05\n",
            "Epoch 661/1000, Training Loss: 3.466256096721521e-05, Test Loss: 4.647914051958474e-05\n",
            "Epoch 662/1000, Training Loss: 3.46154409753671e-05, Test Loss: 4.6417005111993754e-05\n",
            "Epoch 663/1000, Training Loss: 3.456853120485918e-05, Test Loss: 4.635512383014597e-05\n",
            "Epoch 664/1000, Training Loss: 3.452182989535729e-05, Test Loss: 4.629349478258525e-05\n",
            "Epoch 665/1000, Training Loss: 3.4475335307240914e-05, Test Loss: 4.623211609817736e-05\n",
            "Epoch 666/1000, Training Loss: 3.442904572128407e-05, Test Loss: 4.617098592579941e-05\n",
            "Epoch 667/1000, Training Loss: 3.438295943834371e-05, Test Loss: 4.61101024340446e-05\n",
            "Epoch 668/1000, Training Loss: 3.4337074779051915e-05, Test Loss: 4.60494638109306e-05\n",
            "Epoch 669/1000, Training Loss: 3.429139008351485e-05, Test Loss: 4.5989068263608406e-05\n",
            "Epoch 670/1000, Training Loss: 3.424590371101769e-05, Test Loss: 4.592891401808305e-05\n",
            "Epoch 671/1000, Training Loss: 3.4200614039733635e-05, Test Loss: 4.5868999318934374e-05\n",
            "Epoch 672/1000, Training Loss: 3.415551946644032e-05, Test Loss: 4.580932242904964e-05\n",
            "Epoch 673/1000, Training Loss: 3.4110618406239564e-05, Test Loss: 4.574988162935589e-05\n",
            "Epoch 674/1000, Training Loss: 3.40659092922833e-05, Test Loss: 4.569067521855343e-05\n",
            "Epoch 675/1000, Training Loss: 3.402139057550449e-05, Test Loss: 4.5631701512866145e-05\n",
            "Epoch 676/1000, Training Loss: 3.3977060724352416e-05, Test Loss: 4.557295884578814e-05\n",
            "Epoch 677/1000, Training Loss: 3.393291822453373e-05, Test Loss: 4.5514445567833373e-05\n",
            "Epoch 678/1000, Training Loss: 3.3888961578756966e-05, Test Loss: 4.545616004630049e-05\n",
            "Epoch 679/1000, Training Loss: 3.384518930648288e-05, Test Loss: 4.53981006650254e-05\n",
            "Epoch 680/1000, Training Loss: 3.380159994367888e-05, Test Loss: 4.5340265824154656e-05\n",
            "Epoch 681/1000, Training Loss: 3.37581920425775e-05, Test Loss: 4.528265393991612e-05\n",
            "Epoch 682/1000, Training Loss: 3.371496417144016e-05, Test Loss: 4.522526344438938e-05\n",
            "Epoch 683/1000, Training Loss: 3.367191491432332e-05, Test Loss: 4.51680927852896e-05\n",
            "Epoch 684/1000, Training Loss: 3.362904287085126e-05, Test Loss: 4.511114042574518e-05\n",
            "Epoch 685/1000, Training Loss: 3.358634665599147e-05, Test Loss: 4.505440484409337e-05\n",
            "Epoch 686/1000, Training Loss: 3.354382489983402e-05, Test Loss: 4.499788453366134e-05\n",
            "Epoch 687/1000, Training Loss: 3.350147624737609e-05, Test Loss: 4.494157800256962e-05\n",
            "Epoch 688/1000, Training Loss: 3.345929935830735e-05, Test Loss: 4.488548377352338e-05\n",
            "Epoch 689/1000, Training Loss: 3.3417292906803056e-05, Test Loss: 4.4829600383616324e-05\n",
            "Epoch 690/1000, Training Loss: 3.3375455581317556e-05, Test Loss: 4.477392638414e-05\n",
            "Epoch 691/1000, Training Loss: 3.333378608438406e-05, Test Loss: 4.471846034038753e-05\n",
            "Epoch 692/1000, Training Loss: 3.329228313241516e-05, Test Loss: 4.4663200831469885e-05\n",
            "Epoch 693/1000, Training Loss: 3.325094545550951e-05, Test Loss: 4.460814645012828e-05\n",
            "Epoch 694/1000, Training Loss: 3.3209771797259945e-05, Test Loss: 4.455329580255653e-05\n",
            "Epoch 695/1000, Training Loss: 3.316876091456503e-05, Test Loss: 4.449864750821873e-05\n",
            "Epoch 696/1000, Training Loss: 3.3127911577445727e-05, Test Loss: 4.444420019967641e-05\n",
            "Epoch 697/1000, Training Loss: 3.3087222568863e-05, Test Loss: 4.43899525224216e-05\n",
            "Epoch 698/1000, Training Loss: 3.3046692684540133e-05, Test Loss: 4.433590313469932e-05\n",
            "Epoch 699/1000, Training Loss: 3.300632073278643e-05, Test Loss: 4.428205070734435e-05\n",
            "Epoch 700/1000, Training Loss: 3.2966105534326465e-05, Test Loss: 4.4228393923624335e-05\n",
            "Epoch 701/1000, Training Loss: 3.2926045922129404e-05, Test Loss: 4.417493147907208e-05\n",
            "Epoch 702/1000, Training Loss: 3.2886140741243214e-05, Test Loss: 4.4121662081331674e-05\n",
            "Epoch 703/1000, Training Loss: 3.284638884863066e-05, Test Loss: 4.406858444999991e-05\n",
            "Epoch 704/1000, Training Loss: 3.280678911300886e-05, Test Loss: 4.401569731648257e-05\n",
            "Epoch 705/1000, Training Loss: 3.2767340414690516e-05, Test Loss: 4.396299942383286e-05\n",
            "Epoch 706/1000, Training Loss: 3.2728041645428794e-05, Test Loss: 4.391048952661277e-05\n",
            "Epoch 707/1000, Training Loss: 3.268889170826482e-05, Test Loss: 4.3858166390749126e-05\n",
            "Epoch 708/1000, Training Loss: 3.264988951737673e-05, Test Loss: 4.380602879338505e-05\n",
            "Epoch 709/1000, Training Loss: 3.2611033997931704e-05, Test Loss: 4.37540755227435e-05\n",
            "Epoch 710/1000, Training Loss: 3.257232408594131e-05, Test Loss: 4.3702305377992576e-05\n",
            "Epoch 711/1000, Training Loss: 3.253375872811767e-05, Test Loss: 4.365071716910347e-05\n",
            "Epoch 712/1000, Training Loss: 3.2495336881733114e-05, Test Loss: 4.359930971672236e-05\n",
            "Epoch 713/1000, Training Loss: 3.2457057514482425e-05, Test Loss: 4.35480818520401e-05\n",
            "Epoch 714/1000, Training Loss: 3.241891960434587e-05, Test Loss: 4.349703241666148e-05\n",
            "Epoch 715/1000, Training Loss: 3.2380922139456386e-05, Test Loss: 4.3446160262480306e-05\n",
            "Epoch 716/1000, Training Loss: 3.234306411796716e-05, Test Loss: 4.3395464251550594e-05\n",
            "Epoch 717/1000, Training Loss: 3.230534454792168e-05, Test Loss: 4.334494325596904e-05\n",
            "Epoch 718/1000, Training Loss: 3.226776244712763e-05, Test Loss: 4.3294596157751104e-05\n",
            "Epoch 719/1000, Training Loss: 3.223031684303073e-05, Test Loss: 4.324442184871732e-05\n",
            "Epoch 720/1000, Training Loss: 3.2193006772591394e-05, Test Loss: 4.3194419230367794e-05\n",
            "Epoch 721/1000, Training Loss: 3.21558312821637e-05, Test Loss: 4.31445872137757e-05\n",
            "Epoch 722/1000, Training Loss: 3.211878942737576e-05, Test Loss: 4.3094924719470604e-05\n",
            "Epoch 723/1000, Training Loss: 3.208188027301216e-05, Test Loss: 4.3045430677328516e-05\n",
            "Epoch 724/1000, Training Loss: 3.204510289289842e-05, Test Loss: 4.2996104026461564e-05\n",
            "Epoch 725/1000, Training Loss: 3.2008456369787165e-05, Test Loss: 4.294694371511025e-05\n",
            "Epoch 726/1000, Training Loss: 3.197193979524583e-05, Test Loss: 4.2897948700532554e-05\n",
            "Epoch 727/1000, Training Loss: 3.193555226954693e-05, Test Loss: 4.284911794891197e-05\n",
            "Epoch 728/1000, Training Loss: 3.189929290155941e-05, Test Loss: 4.280045043524093e-05\n",
            "Epoch 729/1000, Training Loss: 3.1863160808641335e-05, Test Loss: 4.275194514323057e-05\n",
            "Epoch 730/1000, Training Loss: 3.1827155116535256e-05, Test Loss: 4.2703601065200644e-05\n",
            "Epoch 731/1000, Training Loss: 3.179127495926448e-05, Test Loss: 4.265541720199078e-05\n",
            "Epoch 732/1000, Training Loss: 3.1755519479031274e-05, Test Loss: 4.260739256285936e-05\n",
            "Epoch 733/1000, Training Loss: 3.171988782611671e-05, Test Loss: 4.2559526165387976e-05\n",
            "Epoch 734/1000, Training Loss: 3.168437915878096e-05, Test Loss: 4.2511817035388164e-05\n",
            "Epoch 735/1000, Training Loss: 3.164899264316719e-05, Test Loss: 4.246426420681059e-05\n",
            "Epoch 736/1000, Training Loss: 3.1613727453205384e-05, Test Loss: 4.2416866721651875e-05\n",
            "Epoch 737/1000, Training Loss: 3.157858277051757e-05, Test Loss: 4.2369623629867304e-05\n",
            "Epoch 738/1000, Training Loss: 3.1543557784325184e-05, Test Loss: 4.2322533989280384e-05\n",
            "Epoch 739/1000, Training Loss: 3.150865169135814e-05, Test Loss: 4.227559686549784e-05\n",
            "Epoch 740/1000, Training Loss: 3.147386369576401e-05, Test Loss: 4.222881133182228e-05\n",
            "Epoch 741/1000, Training Loss: 3.143919300901983e-05, Test Loss: 4.218217646916861e-05\n",
            "Epoch 742/1000, Training Loss: 3.140463884984425e-05, Test Loss: 4.213569136598117e-05\n",
            "Epoch 743/1000, Training Loss: 3.1370200444111924e-05, Test Loss: 4.2089355118153325e-05\n",
            "Epoch 744/1000, Training Loss: 3.133587702476767e-05, Test Loss: 4.204316682894142e-05\n",
            "Epoch 745/1000, Training Loss: 3.1301667831744506e-05, Test Loss: 4.199712560889174e-05\n",
            "Epoch 746/1000, Training Loss: 3.126757211188029e-05, Test Loss: 4.195123057575785e-05\n",
            "Epoch 747/1000, Training Loss: 3.1233589118836834e-05, Test Loss: 4.1905480854423925e-05\n",
            "Epoch 748/1000, Training Loss: 3.1199718113020136e-05, Test Loss: 4.1859875576830054e-05\n",
            "Epoch 749/1000, Training Loss: 3.1165958361501625e-05, Test Loss: 4.181441388189578e-05\n",
            "Epoch 750/1000, Training Loss: 3.1132309137940526e-05, Test Loss: 4.176909491544984e-05\n",
            "Epoch 751/1000, Training Loss: 3.1098769722507675e-05, Test Loss: 4.1723917830148526e-05\n",
            "Epoch 752/1000, Training Loss: 3.1065339401810044e-05, Test Loss: 4.167888178541162e-05\n",
            "Epoch 753/1000, Training Loss: 3.103201746881631e-05, Test Loss: 4.163398594735162e-05\n",
            "Epoch 754/1000, Training Loss: 3.099880322278423e-05, Test Loss: 4.158922948869802e-05\n",
            "Epoch 755/1000, Training Loss: 3.096569596918812e-05, Test Loss: 4.154461158873252e-05\n",
            "Epoch 756/1000, Training Loss: 3.093269501964818e-05, Test Loss: 4.150013143321977e-05\n",
            "Epoch 757/1000, Training Loss: 3.089979969185977e-05, Test Loss: 4.145578821434126e-05\n",
            "Epoch 758/1000, Training Loss: 3.0867009309525496e-05, Test Loss: 4.1411581130628604e-05\n",
            "Epoch 759/1000, Training Loss: 3.083432320228626e-05, Test Loss: 4.136750938689882e-05\n",
            "Epoch 760/1000, Training Loss: 3.080174070565458e-05, Test Loss: 4.1323572194186486e-05\n",
            "Epoch 761/1000, Training Loss: 3.076926116094825e-05, Test Loss: 4.127976876968835e-05\n",
            "Epoch 762/1000, Training Loss: 3.073688391522578e-05, Test Loss: 4.1236098336688886e-05\n",
            "Epoch 763/1000, Training Loss: 3.0704608321220954e-05, Test Loss: 4.119256012451058e-05\n",
            "Epoch 764/1000, Training Loss: 3.067243373728042e-05, Test Loss: 4.114915336844707e-05\n",
            "Epoch 765/1000, Training Loss: 3.0640359527301334e-05, Test Loss: 4.1105877309702695e-05\n",
            "Epoch 766/1000, Training Loss: 3.0608385060669266e-05, Test Loss: 4.106273119533411e-05\n",
            "Epoch 767/1000, Training Loss: 3.0576509712197146e-05, Test Loss: 4.1019714278190666e-05\n",
            "Epoch 768/1000, Training Loss: 3.054473286206623e-05, Test Loss: 4.097682581685985e-05\n",
            "Epoch 769/1000, Training Loss: 3.0513053895766536e-05, Test Loss: 4.0934065075607756e-05\n",
            "Epoch 770/1000, Training Loss: 3.0481472204038307e-05, Test Loss: 4.089143132432341e-05\n",
            "Epoch 771/1000, Training Loss: 3.0449987182815273e-05, Test Loss: 4.084892383846206e-05\n",
            "Epoch 772/1000, Training Loss: 3.0418598233167518e-05, Test Loss: 4.080654189899516e-05\n",
            "Epoch 773/1000, Training Loss: 3.0387304761245857e-05, Test Loss: 4.0764284792346896e-05\n",
            "Epoch 774/1000, Training Loss: 3.0356106178226058e-05, Test Loss: 4.072215181035292e-05\n",
            "Epoch 775/1000, Training Loss: 3.032500190025527e-05, Test Loss: 4.068014225019666e-05\n",
            "Epoch 776/1000, Training Loss: 3.029399134839782e-05, Test Loss: 4.06382554143652e-05\n",
            "Epoch 777/1000, Training Loss: 3.0263073948582543e-05, Test Loss: 4.05964906105941e-05\n",
            "Epoch 778/1000, Training Loss: 3.0232249131550694e-05, Test Loss: 4.055484715181563e-05\n",
            "Epoch 779/1000, Training Loss: 3.020151633280413e-05, Test Loss: 4.05133243561142e-05\n",
            "Epoch 780/1000, Training Loss: 3.0170874992554734e-05, Test Loss: 4.0471921546671424e-05\n",
            "Epoch 781/1000, Training Loss: 3.014032455567401e-05, Test Loss: 4.043063805171782e-05\n",
            "Epoch 782/1000, Training Loss: 3.010986447164393e-05, Test Loss: 4.03894732044869e-05\n",
            "Epoch 783/1000, Training Loss: 3.007949419450796e-05, Test Loss: 4.034842634317106e-05\n",
            "Epoch 784/1000, Training Loss: 3.004921318282285e-05, Test Loss: 4.0307496810866635e-05\n",
            "Epoch 785/1000, Training Loss: 3.001902089961101e-05, Test Loss: 4.026668395552865e-05\n",
            "Epoch 786/1000, Training Loss: 2.9988916812313918e-05, Test Loss: 4.0225987129933955e-05\n",
            "Epoch 787/1000, Training Loss: 2.9958900392745778e-05, Test Loss: 4.01854056916244e-05\n",
            "Epoch 788/1000, Training Loss: 2.9928971117047222e-05, Test Loss: 4.0144939002871424e-05\n",
            "Epoch 789/1000, Training Loss: 2.9899128465641035e-05, Test Loss: 4.010458643062303e-05\n",
            "Epoch 790/1000, Training Loss: 2.98693719231872e-05, Test Loss: 4.0064347346470415e-05\n",
            "Epoch 791/1000, Training Loss: 2.9839700978539053e-05, Test Loss: 4.002422112659737e-05\n",
            "Epoch 792/1000, Training Loss: 2.9810115124700033e-05, Test Loss: 3.998420715173762e-05\n",
            "Epoch 793/1000, Training Loss: 2.9780613858780904e-05, Test Loss: 3.99443048071401e-05\n",
            "Epoch 794/1000, Training Loss: 2.9751196681956664e-05, Test Loss: 3.990451348251595e-05\n",
            "Epoch 795/1000, Training Loss: 2.9721863099426197e-05, Test Loss: 3.986483257200946e-05\n",
            "Epoch 796/1000, Training Loss: 2.969261262037036e-05, Test Loss: 3.9825261474150145e-05\n",
            "Epoch 797/1000, Training Loss: 2.9663444757911656e-05, Test Loss: 3.9785799591812445e-05\n",
            "Epoch 798/1000, Training Loss: 2.963435902907297e-05, Test Loss: 3.974644633218145e-05\n",
            "Epoch 799/1000, Training Loss: 2.960535495473947e-05, Test Loss: 3.970720110670752e-05\n",
            "Epoch 800/1000, Training Loss: 2.9576432059618825e-05, Test Loss: 3.966806333107242e-05\n",
            "Epoch 801/1000, Training Loss: 2.954758987220231e-05, Test Loss: 3.962903242515086e-05\n",
            "Epoch 802/1000, Training Loss: 2.9518827924727258e-05, Test Loss: 3.9590107812965124e-05\n",
            "Epoch 803/1000, Training Loss: 2.949014575313863e-05, Test Loss: 3.955128892266198e-05\n",
            "Epoch 804/1000, Training Loss: 2.946154289705239e-05, Test Loss: 3.9512575186460415e-05\n",
            "Epoch 805/1000, Training Loss: 2.9433018899718332e-05, Test Loss: 3.9473966040625606e-05\n",
            "Epoch 806/1000, Training Loss: 2.9404573307984706e-05, Test Loss: 3.94354609254282e-05\n",
            "Epoch 807/1000, Training Loss: 2.9376205672261e-05, Test Loss: 3.93970592851113e-05\n",
            "Epoch 808/1000, Training Loss: 2.9347915546483857e-05, Test Loss: 3.9358760567853616e-05\n",
            "Epoch 809/1000, Training Loss: 2.9319702488081078e-05, Test Loss: 3.932056422573353e-05\n",
            "Epoch 810/1000, Training Loss: 2.929156605793814e-05, Test Loss: 3.9282469714698175e-05\n",
            "Epoch 811/1000, Training Loss: 2.926350582036282e-05, Test Loss: 3.924447649452485e-05\n",
            "Epoch 812/1000, Training Loss: 2.923552134305246e-05, Test Loss: 3.920658402879272e-05\n",
            "Epoch 813/1000, Training Loss: 2.9207612197060685e-05, Test Loss: 3.916879178484307e-05\n",
            "Epoch 814/1000, Training Loss: 2.917977795676389e-05, Test Loss: 3.913109923375245e-05\n",
            "Epoch 815/1000, Training Loss: 2.9152018199829747e-05, Test Loss: 3.9093505850296634e-05\n",
            "Epoch 816/1000, Training Loss: 2.9124332507184168e-05, Test Loss: 3.905601111291953e-05\n",
            "Epoch 817/1000, Training Loss: 2.9096720462979857e-05, Test Loss: 3.901861450370163e-05\n",
            "Epoch 818/1000, Training Loss: 2.9069181654565006e-05, Test Loss: 3.898131550832508e-05\n",
            "Epoch 819/1000, Training Loss: 2.9041715672452973e-05, Test Loss: 3.894411361604922e-05\n",
            "Epoch 820/1000, Training Loss: 2.9014322110290914e-05, Test Loss: 3.890700831967555e-05\n",
            "Epoch 821/1000, Training Loss: 2.898700056483003e-05, Test Loss: 3.886999911551455e-05\n",
            "Epoch 822/1000, Training Loss: 2.895975063589534e-05, Test Loss: 3.8833085503363056e-05\n",
            "Epoch 823/1000, Training Loss: 2.893257192635637e-05, Test Loss: 3.8796266986468096e-05\n",
            "Epoch 824/1000, Training Loss: 2.89054640420987e-05, Test Loss: 3.875954307149879e-05\n",
            "Epoch 825/1000, Training Loss: 2.8878426591993908e-05, Test Loss: 3.872291326851803e-05\n",
            "Epoch 826/1000, Training Loss: 2.8851459187871915e-05, Test Loss: 3.8686377090954696e-05\n",
            "Epoch 827/1000, Training Loss: 2.8824561444492443e-05, Test Loss: 3.8649934055573314e-05\n",
            "Epoch 828/1000, Training Loss: 2.8797732979518165e-05, Test Loss: 3.86135836824457e-05\n",
            "Epoch 829/1000, Training Loss: 2.8770973413485766e-05, Test Loss: 3.8577325494925867e-05\n",
            "Epoch 830/1000, Training Loss: 2.87442823697802e-05, Test Loss: 3.8541159019616535e-05\n",
            "Epoch 831/1000, Training Loss: 2.8717659474606925e-05, Test Loss: 3.850508378634989e-05\n",
            "Epoch 832/1000, Training Loss: 2.8691104356965706e-05, Test Loss: 3.846909932815266e-05\n",
            "Epoch 833/1000, Training Loss: 2.866461664862441e-05, Test Loss: 3.843320518122543e-05\n",
            "Epoch 834/1000, Training Loss: 2.863819598409296e-05, Test Loss: 3.8397400884911555e-05\n",
            "Epoch 835/1000, Training Loss: 2.8611842000597735e-05, Test Loss: 3.836168598167387e-05\n",
            "Epoch 836/1000, Training Loss: 2.8585554338056752e-05, Test Loss: 3.8326060017066836e-05\n",
            "Epoch 837/1000, Training Loss: 2.8559332639053772e-05, Test Loss: 3.8290522539712334e-05\n",
            "Epoch 838/1000, Training Loss: 2.853317654881442e-05, Test Loss: 3.8255073101274686e-05\n",
            "Epoch 839/1000, Training Loss: 2.8507085715180867e-05, Test Loss: 3.821971125643345e-05\n",
            "Epoch 840/1000, Training Loss: 2.848105978858845e-05, Test Loss: 3.818443656286095e-05\n",
            "Epoch 841/1000, Training Loss: 2.8455098422041158e-05, Test Loss: 3.814924858119467e-05\n",
            "Epoch 842/1000, Training Loss: 2.8429201271088156e-05, Test Loss: 3.81141468750158e-05\n",
            "Epoch 843/1000, Training Loss: 2.8403367993800497e-05, Test Loss: 3.807913101082345e-05\n",
            "Epoch 844/1000, Training Loss: 2.8377598250748673e-05, Test Loss: 3.804420055801128e-05\n",
            "Epoch 845/1000, Training Loss: 2.8351891704978343e-05, Test Loss: 3.8009355088843164e-05\n",
            "Epoch 846/1000, Training Loss: 2.832624802198848e-05, Test Loss: 3.797459417843077e-05\n",
            "Epoch 847/1000, Training Loss: 2.830066686970942e-05, Test Loss: 3.793991740471195e-05\n",
            "Epoch 848/1000, Training Loss: 2.8275147918479975e-05, Test Loss: 3.790532434842278e-05\n",
            "Epoch 849/1000, Training Loss: 2.8249690841026312e-05, Test Loss: 3.78708145930786e-05\n",
            "Epoch 850/1000, Training Loss: 2.822429531243992e-05, Test Loss: 3.7836387724952616e-05\n",
            "Epoch 851/1000, Training Loss: 2.8198961010156537e-05, Test Loss: 3.780204333305177e-05\n",
            "Epoch 852/1000, Training Loss: 2.817368761393428e-05, Test Loss: 3.776778100909168e-05\n",
            "Epoch 853/1000, Training Loss: 2.814847480583406e-05, Test Loss: 3.773360034748221e-05\n",
            "Epoch 854/1000, Training Loss: 2.8123322270197292e-05, Test Loss: 3.769950094529783e-05\n",
            "Epoch 855/1000, Training Loss: 2.8098229693627055e-05, Test Loss: 3.7665482402263744e-05\n",
            "Epoch 856/1000, Training Loss: 2.8073196764967164e-05, Test Loss: 3.7631544320728254e-05\n",
            "Epoch 857/1000, Training Loss: 2.8048223175281315e-05, Test Loss: 3.7597686305646726e-05\n",
            "Epoch 858/1000, Training Loss: 2.8023308617834914e-05, Test Loss: 3.7563907964557414e-05\n",
            "Epoch 859/1000, Training Loss: 2.7998452788074353e-05, Test Loss: 3.7530208907561444e-05\n",
            "Epoch 860/1000, Training Loss: 2.7973655383608174e-05, Test Loss: 3.749658874730467e-05\n",
            "Epoch 861/1000, Training Loss: 2.794891610418777e-05, Test Loss: 3.746304709895495e-05\n",
            "Epoch 862/1000, Training Loss: 2.7924234651688524e-05, Test Loss: 3.7429583580183456e-05\n",
            "Epoch 863/1000, Training Loss: 2.789961073009082e-05, Test Loss: 3.739619781114776e-05\n",
            "Epoch 864/1000, Training Loss: 2.7875044045461953e-05, Test Loss: 3.7362889414464454e-05\n",
            "Epoch 865/1000, Training Loss: 2.785053430593744e-05, Test Loss: 3.732965801519753e-05\n",
            "Epoch 866/1000, Training Loss: 2.782608122170293e-05, Test Loss: 3.729650324083417e-05\n",
            "Epoch 867/1000, Training Loss: 2.7801684504976525e-05, Test Loss: 3.726342472127045e-05\n",
            "Epoch 868/1000, Training Loss: 2.7777343869990756e-05, Test Loss: 3.723042208878496e-05\n",
            "Epoch 869/1000, Training Loss: 2.775305903297497e-05, Test Loss: 3.719749497803243e-05\n",
            "Epoch 870/1000, Training Loss: 2.772882971213804e-05, Test Loss: 3.716464302601077e-05\n",
            "Epoch 871/1000, Training Loss: 2.7704655627651495e-05, Test Loss: 3.713186587205297e-05\n",
            "Epoch 872/1000, Training Loss: 2.768053650163218e-05, Test Loss: 3.70991631578065e-05\n",
            "Epoch 873/1000, Training Loss: 2.7656472058125087e-05, Test Loss: 3.706653452721239e-05\n",
            "Epoch 874/1000, Training Loss: 2.7632462023087387e-05, Test Loss: 3.703397962648953e-05\n",
            "Epoch 875/1000, Training Loss: 2.7608506124371807e-05, Test Loss: 3.7001498104119576e-05\n",
            "Epoch 876/1000, Training Loss: 2.7584604091709773e-05, Test Loss: 3.69690896108266e-05\n",
            "Epoch 877/1000, Training Loss: 2.7560755656696098e-05, Test Loss: 3.6936753799559366e-05\n",
            "Epoch 878/1000, Training Loss: 2.7536960552772236e-05, Test Loss: 3.6904490325473994e-05\n",
            "Epoch 879/1000, Training Loss: 2.7513218515210733e-05, Test Loss: 3.687229884591951e-05\n",
            "Epoch 880/1000, Training Loss: 2.748952928109975e-05, Test Loss: 3.684017902042072e-05\n",
            "Epoch 881/1000, Training Loss: 2.7465892589327864e-05, Test Loss: 3.680813051065795e-05\n",
            "Epoch 882/1000, Training Loss: 2.7442308180567775e-05, Test Loss: 3.677615298045412e-05\n",
            "Epoch 883/1000, Training Loss: 2.7418775797262464e-05, Test Loss: 3.6744246095756985e-05\n",
            "Epoch 884/1000, Training Loss: 2.7395295183608708e-05, Test Loss: 3.671240952462552e-05\n",
            "Epoch 885/1000, Training Loss: 2.737186608554367e-05, Test Loss: 3.668064293720909e-05\n",
            "Epoch 886/1000, Training Loss: 2.7348488250729055e-05, Test Loss: 3.664894600573143e-05\n",
            "Epoch 887/1000, Training Loss: 2.7325161428537624e-05, Test Loss: 3.661731840448345e-05\n",
            "Epoch 888/1000, Training Loss: 2.7301885370037476e-05, Test Loss: 3.658575980979741e-05\n",
            "Epoch 889/1000, Training Loss: 2.7278659827979224e-05, Test Loss: 3.655426990003738e-05\n",
            "Epoch 890/1000, Training Loss: 2.725548455678061e-05, Test Loss: 3.6522848355581314e-05\n",
            "Epoch 891/1000, Training Loss: 2.7232359312513702e-05, Test Loss: 3.6491494858807476e-05\n",
            "Epoch 892/1000, Training Loss: 2.7209283852889988e-05, Test Loss: 3.646020909407566e-05\n",
            "Epoch 893/1000, Training Loss: 2.7186257937247472e-05, Test Loss: 3.6428990747721064e-05\n",
            "Epoch 894/1000, Training Loss: 2.7163281326537167e-05, Test Loss: 3.639783950802811e-05\n",
            "Epoch 895/1000, Training Loss: 2.7140353783308406e-05, Test Loss: 3.636675506522423e-05\n",
            "Epoch 896/1000, Training Loss: 2.7117475071697687e-05, Test Loss: 3.6335737111460245e-05\n",
            "Epoch 897/1000, Training Loss: 2.709464495741382e-05, Test Loss: 3.6304785340799475e-05\n",
            "Epoch 898/1000, Training Loss: 2.7071863207725218e-05, Test Loss: 3.6273899449203075e-05\n",
            "Epoch 899/1000, Training Loss: 2.7049129591448163e-05, Test Loss: 3.624307913451385e-05\n",
            "Epoch 900/1000, Training Loss: 2.7026443878932368e-05, Test Loss: 3.621232409644274e-05\n",
            "Epoch 901/1000, Training Loss: 2.700380584204965e-05, Test Loss: 3.618163403655514e-05\n",
            "Epoch 902/1000, Training Loss: 2.6981215254180394e-05, Test Loss: 3.6151008658259094e-05\n",
            "Epoch 903/1000, Training Loss: 2.6958671890202477e-05, Test Loss: 3.612044766678895e-05\n",
            "Epoch 904/1000, Training Loss: 2.693617552647746e-05, Test Loss: 3.608995076919278e-05\n",
            "Epoch 905/1000, Training Loss: 2.6913725940840116e-05, Test Loss: 3.6059517674319124e-05\n",
            "Epoch 906/1000, Training Loss: 2.6891322912584767e-05, Test Loss: 3.602914809280461e-05\n",
            "Epoch 907/1000, Training Loss: 2.6868966222454857e-05, Test Loss: 3.599884173705947e-05\n",
            "Epoch 908/1000, Training Loss: 2.6846655652630228e-05, Test Loss: 3.59685983212549e-05\n",
            "Epoch 909/1000, Training Loss: 2.6824390986715663e-05, Test Loss: 3.5938417561308e-05\n",
            "Epoch 910/1000, Training Loss: 2.6802172009729473e-05, Test Loss: 3.590829917487479e-05\n",
            "Epoch 911/1000, Training Loss: 2.6779998508092038e-05, Test Loss: 3.5878242881331e-05\n",
            "Epoch 912/1000, Training Loss: 2.675787026961445e-05, Test Loss: 3.5848248401761704e-05\n",
            "Epoch 913/1000, Training Loss: 2.67357870834875e-05, Test Loss: 3.581831545895115e-05\n",
            "Epoch 914/1000, Training Loss: 2.6713748740269878e-05, Test Loss: 3.578844377736702e-05\n",
            "Epoch 915/1000, Training Loss: 2.6691755031878093e-05, Test Loss: 3.575863308315174e-05\n",
            "Epoch 916/1000, Training Loss: 2.6669805751575052e-05, Test Loss: 3.572888310410487e-05\n",
            "Epoch 917/1000, Training Loss: 2.6647900693959595e-05, Test Loss: 3.5699193569676115e-05\n",
            "Epoch 918/1000, Training Loss: 2.662603965495519e-05, Test Loss: 3.566956421095174e-05\n",
            "Epoch 919/1000, Training Loss: 2.6604222431799953e-05, Test Loss: 3.563999476064278e-05\n",
            "Epoch 920/1000, Training Loss: 2.6582448823036202e-05, Test Loss: 3.5610484953073466e-05\n",
            "Epoch 921/1000, Training Loss: 2.6560718628499555e-05, Test Loss: 3.558103452416557e-05\n",
            "Epoch 922/1000, Training Loss: 2.653903164930893e-05, Test Loss: 3.5551643211433726e-05\n",
            "Epoch 923/1000, Training Loss: 2.651738768785692e-05, Test Loss: 3.552231075397246e-05\n",
            "Epoch 924/1000, Training Loss: 2.6495786547798792e-05, Test Loss: 3.5493036892436894e-05\n",
            "Epoch 925/1000, Training Loss: 2.6474228034042937e-05, Test Loss: 3.546382136904306e-05\n",
            "Epoch 926/1000, Training Loss: 2.6452711952741014e-05, Test Loss: 3.5434663927547034e-05\n",
            "Epoch 927/1000, Training Loss: 2.6431238111277923e-05, Test Loss: 3.5405564313241085e-05\n",
            "Epoch 928/1000, Training Loss: 2.640980631826245e-05, Test Loss: 3.537652227293715e-05\n",
            "Epoch 929/1000, Training Loss: 2.6388416383517294e-05, Test Loss: 3.5347537554963154e-05\n",
            "Epoch 930/1000, Training Loss: 2.6367068118070067e-05, Test Loss: 3.531860990914175e-05\n",
            "Epoch 931/1000, Training Loss: 2.6345761334143294e-05, Test Loss: 3.52897390867881e-05\n",
            "Epoch 932/1000, Training Loss: 2.6324495845145126e-05, Test Loss: 3.5260924840696225e-05\n",
            "Epoch 933/1000, Training Loss: 2.630327146566047e-05, Test Loss: 3.523216692512791e-05\n",
            "Epoch 934/1000, Training Loss: 2.6282088011441658e-05, Test Loss: 3.520346509580309e-05\n",
            "Epoch 935/1000, Training Loss: 2.6260945299398826e-05, Test Loss: 3.517481910989105e-05\n",
            "Epoch 936/1000, Training Loss: 2.6239843147592032e-05, Test Loss: 3.514622872599685e-05\n",
            "Epoch 937/1000, Training Loss: 2.6218781375221303e-05, Test Loss: 3.5117693704153544e-05\n",
            "Epoch 938/1000, Training Loss: 2.6197759802618405e-05, Test Loss: 3.508921380581229e-05\n",
            "Epoch 939/1000, Training Loss: 2.617677825123786e-05, Test Loss: 3.5060788793828885e-05\n",
            "Epoch 940/1000, Training Loss: 2.6155836543648124e-05, Test Loss: 3.503241843245656e-05\n",
            "Epoch 941/1000, Training Loss: 2.6134934503523532e-05, Test Loss: 3.500410248734053e-05\n",
            "Epoch 942/1000, Training Loss: 2.6114071955635136e-05, Test Loss: 3.497584072549663e-05\n",
            "Epoch 943/1000, Training Loss: 2.6093248725842462e-05, Test Loss: 3.4947632915315375e-05\n",
            "Epoch 944/1000, Training Loss: 2.6072464641085878e-05, Test Loss: 3.491947882653819e-05\n",
            "Epoch 945/1000, Training Loss: 2.6051719529377068e-05, Test Loss: 3.489137823026402e-05\n",
            "Epoch 946/1000, Training Loss: 2.6031013219791745e-05, Test Loss: 3.4863330898922806e-05\n",
            "Epoch 947/1000, Training Loss: 2.601034554246151e-05, Test Loss: 3.483533660627933e-05\n",
            "Epoch 948/1000, Training Loss: 2.5989716328565147e-05, Test Loss: 3.480739512741786e-05\n",
            "Epoch 949/1000, Training Loss: 2.5969125410321042e-05, Test Loss: 3.4779506238733266e-05\n",
            "Epoch 950/1000, Training Loss: 2.5948572620979692e-05, Test Loss: 3.47516697179233e-05\n",
            "Epoch 951/1000, Training Loss: 2.5928057794815184e-05, Test Loss: 3.47238853439793e-05\n",
            "Epoch 952/1000, Training Loss: 2.590758076711787e-05, Test Loss: 3.469615289717603e-05\n",
            "Epoch 953/1000, Training Loss: 2.5887141374186583e-05, Test Loss: 3.466847215906293e-05\n",
            "Epoch 954/1000, Training Loss: 2.58667394533208e-05, Test Loss: 3.464084291245827e-05\n",
            "Epoch 955/1000, Training Loss: 2.5846374842813347e-05, Test Loss: 3.4613264941434957e-05\n",
            "Epoch 956/1000, Training Loss: 2.5826047381942462e-05, Test Loss: 3.458573803131648e-05\n",
            "Epoch 957/1000, Training Loss: 2.5805756910965425e-05, Test Loss: 3.455826196866627e-05\n",
            "Epoch 958/1000, Training Loss: 2.5785503271109955e-05, Test Loss: 3.453083654127904e-05\n",
            "Epoch 959/1000, Training Loss: 2.576528630456803e-05, Test Loss: 3.450346153817368e-05\n",
            "Epoch 960/1000, Training Loss: 2.574510585448756e-05, Test Loss: 3.447613674958146e-05\n",
            "Epoch 961/1000, Training Loss: 2.5724961764966278e-05, Test Loss: 3.4448861966942206e-05\n",
            "Epoch 962/1000, Training Loss: 2.5704853881043966e-05, Test Loss: 3.442163698289293e-05\n",
            "Epoch 963/1000, Training Loss: 2.568478204869551e-05, Test Loss: 3.4394461591262286e-05\n",
            "Epoch 964/1000, Training Loss: 2.566474611482446e-05, Test Loss: 3.4367335587058046e-05\n",
            "Epoch 965/1000, Training Loss: 2.5644745927255528e-05, Test Loss: 3.43402587664615e-05\n",
            "Epoch 966/1000, Training Loss: 2.562478133472822e-05, Test Loss: 3.431323092682204e-05\n",
            "Epoch 967/1000, Training Loss: 2.5604852186889337e-05, Test Loss: 3.4286251866643234e-05\n",
            "Epoch 968/1000, Training Loss: 2.5584958334287036e-05, Test Loss: 3.425932138558179e-05\n",
            "Epoch 969/1000, Training Loss: 2.5565099628363976e-05, Test Loss: 3.423243928443416e-05\n",
            "Epoch 970/1000, Training Loss: 2.5545275921450117e-05, Test Loss: 3.420560536513168e-05\n",
            "Epoch 971/1000, Training Loss: 2.5525487066757397e-05, Test Loss: 3.4178819430730224e-05\n",
            "Epoch 972/1000, Training Loss: 2.55057329183719e-05, Test Loss: 3.415208128540583e-05\n",
            "Epoch 973/1000, Training Loss: 2.54860133312485e-05, Test Loss: 3.412539073444646e-05\n",
            "Epoch 974/1000, Training Loss: 2.5466328161203496e-05, Test Loss: 3.4098747584241105e-05\n",
            "Epoch 975/1000, Training Loss: 2.5446677264909387e-05, Test Loss: 3.407215164227671e-05\n",
            "Epoch 976/1000, Training Loss: 2.5427060499887844e-05, Test Loss: 3.4045602717125724e-05\n",
            "Epoch 977/1000, Training Loss: 2.5407477724503747e-05, Test Loss: 3.401910061844674e-05\n",
            "Epoch 978/1000, Training Loss: 2.5387928797959065e-05, Test Loss: 3.399264515697009e-05\n",
            "Epoch 979/1000, Training Loss: 2.536841358028662e-05, Test Loss: 3.396623614449117e-05\n",
            "Epoch 980/1000, Training Loss: 2.5348931932344214e-05, Test Loss: 3.3939873393865987e-05\n",
            "Epoch 981/1000, Training Loss: 2.532948371580829e-05, Test Loss: 3.391355671900362e-05\n",
            "Epoch 982/1000, Training Loss: 2.5310068793168986e-05, Test Loss: 3.388728593485779e-05\n",
            "Epoch 983/1000, Training Loss: 2.5290687027722673e-05, Test Loss: 3.386106085742018e-05\n",
            "Epoch 984/1000, Training Loss: 2.5271338283567586e-05, Test Loss: 3.383488130371457e-05\n",
            "Epoch 985/1000, Training Loss: 2.525202242559697e-05, Test Loss: 3.380874709178773e-05\n",
            "Epoch 986/1000, Training Loss: 2.523273931949423e-05, Test Loss: 3.3782658040705255e-05\n",
            "Epoch 987/1000, Training Loss: 2.521348883172663e-05, Test Loss: 3.375661397054298e-05\n",
            "Epoch 988/1000, Training Loss: 2.5194270829539594e-05, Test Loss: 3.37306147023789e-05\n",
            "Epoch 989/1000, Training Loss: 2.517508518095177e-05, Test Loss: 3.370466005829173e-05\n",
            "Epoch 990/1000, Training Loss: 2.515593175474895e-05, Test Loss: 3.3678749861349316e-05\n",
            "Epoch 991/1000, Training Loss: 2.5136810420478438e-05, Test Loss: 3.365288393560348e-05\n",
            "Epoch 992/1000, Training Loss: 2.5117721048444405e-05, Test Loss: 3.362706210608164e-05\n",
            "Epoch 993/1000, Training Loss: 2.509866350970125e-05, Test Loss: 3.3601284198784236e-05\n",
            "Epoch 994/1000, Training Loss: 2.5079637676049668e-05, Test Loss: 3.357555004067546e-05\n",
            "Epoch 995/1000, Training Loss: 2.506064342003014e-05, Test Loss: 3.354985945967932e-05\n",
            "Epoch 996/1000, Training Loss: 2.50416806149187e-05, Test Loss: 3.3524212284667844e-05\n",
            "Epoch 997/1000, Training Loss: 2.502274913472038e-05, Test Loss: 3.349860834546202e-05\n",
            "Epoch 998/1000, Training Loss: 2.500384885416552e-05, Test Loss: 3.347304747282117e-05\n",
            "Epoch 999/1000, Training Loss: 2.498497964870357e-05, Test Loss: 3.344752949843608e-05\n",
            "Epoch 1000/1000, Training Loss: 2.4966141394498565e-05, Test Loss: 3.34220542549276e-05\n",
            "Epoch 1/1000, Training Loss: 0.003642512334103384, Test Loss: 0.0034320127117973\n",
            "Epoch 2/1000, Training Loss: 0.0035713247454079557, Test Loss: 0.003327326144005803\n",
            "Epoch 3/1000, Training Loss: 0.0035580397462761466, Test Loss: 0.0033012176512703605\n",
            "Epoch 4/1000, Training Loss: 0.0035522029792204625, Test Loss: 0.003288361099282226\n",
            "Epoch 5/1000, Training Loss: 0.003548546619941374, Test Loss: 0.003280102701323849\n",
            "Epoch 6/1000, Training Loss: 0.003545922697612952, Test Loss: 0.0032741727439393677\n",
            "Epoch 7/1000, Training Loss: 0.003543897221293735, Test Loss: 0.003269677317800791\n",
            "Epoch 8/1000, Training Loss: 0.003542245765278451, Test Loss: 0.003266161190948223\n",
            "Epoch 9/1000, Training Loss: 0.003540836865208003, Test Loss: 0.003263352246094487\n",
            "Epoch 10/1000, Training Loss: 0.003539589568082796, Test Loss: 0.003261070568402597\n",
            "Epoch 11/1000, Training Loss: 0.003538452645628507, Test Loss: 0.0032591894490962493\n",
            "Epoch 12/1000, Training Loss: 0.0035373930060626554, Test Loss: 0.003257616077785297\n",
            "Epoch 13/1000, Training Loss: 0.0035363888681782963, Test Loss: 0.0032562807727959404\n",
            "Epoch 14/1000, Training Loss: 0.0035354256195337906, Test Loss: 0.0032551303658262354\n",
            "Epoch 15/1000, Training Loss: 0.003534493252914502, Test Loss: 0.0032541238354774384\n",
            "Epoch 16/1000, Training Loss: 0.0035335847561961202, Test Loss: 0.0032532292768945354\n",
            "Epoch 17/1000, Training Loss: 0.003532695089352443, Test Loss: 0.0032524217254827235\n",
            "Epoch 18/1000, Training Loss: 0.00353182052821025, Test Loss: 0.0032516815563306063\n",
            "Epoch 19/1000, Training Loss: 0.00353095823971484, Test Loss: 0.0032509932864406036\n",
            "Epoch 20/1000, Training Loss: 0.0035301060045351117, Test Loss: 0.003250344666390367\n",
            "Epoch 21/1000, Training Loss: 0.0035292620340787793, Test Loss: 0.003249725984135218\n",
            "Epoch 22/1000, Training Loss: 0.0035284248483830214, Test Loss: 0.0032491295267560705\n",
            "Epoch 23/1000, Training Loss: 0.0035275931935114958, Test Loss: 0.003248549161327036\n",
            "Epoch 24/1000, Training Loss: 0.0035267659847759247, Test Loss: 0.0032479800066019093\n",
            "Epoch 25/1000, Training Loss: 0.0035259422669837253, Test Loss: 0.003247418174585649\n",
            "Epoch 26/1000, Training Loss: 0.0035251211860277544, Test Loss: 0.003246860566306948\n",
            "Epoch 27/1000, Training Loss: 0.0035243019681273167, Test Loss: 0.003246304709908481\n",
            "Epoch 28/1000, Training Loss: 0.0035234839043089394, Test Loss: 0.0032457486319614815\n",
            "Epoch 29/1000, Training Loss: 0.003522666338539163, Test Loss: 0.003245190754986227\n",
            "Epoch 30/1000, Training Loss: 0.0035218486584539764, Test Loss: 0.0032446298157213084\n",
            "Epoch 31/1000, Training Loss: 0.0035210302879750146, Test Loss: 0.0032440647998716802\n",
            "Epoch 32/1000, Training Loss: 0.003520210681328125, Test Loss: 0.003243494889976839\n",
            "Epoch 33/1000, Training Loss: 0.0035193893181281076, Test Loss: 0.0032429194237455837\n",
            "Epoch 34/1000, Training Loss: 0.0035185656992916493, Test Loss: 0.003242337860753433\n",
            "Epoch 35/1000, Training Loss: 0.003517739343606373, Test Loss: 0.003241749755829629\n",
            "Epoch 36/1000, Training Loss: 0.003516909784828688, Test Loss: 0.003241154737800036\n",
            "Epoch 37/1000, Training Loss: 0.0035160765692140517, Test Loss: 0.003240552492520939\n",
            "Epoch 38/1000, Training Loss: 0.0035152392534050867, Test Loss: 0.0032399427493517785\n",
            "Epoch 39/1000, Training Loss: 0.003514397402618645, Test Loss: 0.003239325270384688\n",
            "Epoch 40/1000, Training Loss: 0.0035135505890844876, Test Loss: 0.003238699841884016\n",
            "Epoch 41/1000, Training Loss: 0.0035126983906969473, Test Loss: 0.00323806626749724\n",
            "Epoch 42/1000, Training Loss: 0.003511840389847641, Test Loss: 0.0032374243628853073\n",
            "Epoch 43/1000, Training Loss: 0.0035109761724126214, Test Loss: 0.0032367739514897863\n",
            "Epoch 44/1000, Training Loss: 0.0035101053268715397, Test Loss: 0.0032361148612099445\n",
            "Epoch 45/1000, Training Loss: 0.0035092274435399246, Test Loss: 0.003235446921807484\n",
            "Epoch 46/1000, Training Loss: 0.003508342113898455, Test Loss: 0.003234769962892573\n",
            "Epoch 47/1000, Training Loss: 0.00350744893000557, Test Loss: 0.0032340838123736644\n",
            "Epoch 48/1000, Training Loss: 0.003506547483981699, Test Loss: 0.0032333882952767074\n",
            "Epoch 49/1000, Training Loss: 0.00350563736755513, Test Loss: 0.0032326832328580524\n",
            "Epoch 50/1000, Training Loss: 0.0035047181716609806, Test Loss: 0.0032319684419502732\n",
            "Epoch 51/1000, Training Loss: 0.003503789486085943, Test Loss: 0.003231243734492233\n",
            "Epoch 52/1000, Training Loss: 0.003502850899152558, Test Loss: 0.003230508917204343\n",
            "Epoch 53/1000, Training Loss: 0.0035019019974376606, Test Loss: 0.00322976379137784\n",
            "Epoch 54/1000, Training Loss: 0.003500942365520416, Test Loss: 0.003229008152753092\n",
            "Epoch 55/1000, Training Loss: 0.0034999715857560415, Test Loss: 0.003228241791467037\n",
            "Epoch 56/1000, Training Loss: 0.0034989892380718644, Test Loss: 0.0032274644920538857\n",
            "Epoch 57/1000, Training Loss: 0.003497994899782872, Test Loss: 0.0032266760334864865\n",
            "Epoch 58/1000, Training Loss: 0.003496988145424308, Test Loss: 0.003225876189248352\n",
            "Epoch 59/1000, Training Loss: 0.003495968546599272, Test Loss: 0.003225064727428453\n",
            "Epoch 60/1000, Training Loss: 0.0034949356718395213, Test Loss: 0.0032242414108325616\n",
            "Epoch 61/1000, Training Loss: 0.003493889086478028, Test Loss: 0.0032234059971063106\n",
            "Epoch 62/1000, Training Loss: 0.0034928283525319727, Test Loss: 0.003222558238866133\n",
            "Epoch 63/1000, Training Loss: 0.0034917530285951347, Test Loss: 0.0032216978838352102\n",
            "Epoch 64/1000, Training Loss: 0.00349066266973876, Test Loss: 0.0032208246749821563\n",
            "Epoch 65/1000, Training Loss: 0.0034895568274201285, Test Loss: 0.003219938350660784\n",
            "Epoch 66/1000, Training Loss: 0.0034884350493981963, Test Loss: 0.0032190386447496703\n",
            "Epoch 67/1000, Training Loss: 0.0034872968796557493, Test Loss: 0.003218125286790615\n",
            "Epoch 68/1000, Training Loss: 0.0034861418583276237, Test Loss: 0.003217198002125366\n",
            "Epoch 69/1000, Training Loss: 0.0034849695216346167, Test Loss: 0.0032162565120301646\n",
            "Epoch 70/1000, Training Loss: 0.0034837794018227554, Test Loss: 0.003215300533847826\n",
            "Epoch 71/1000, Training Loss: 0.0034825710271076736, Test Loss: 0.003214329781117237\n",
            "Epoch 72/1000, Training Loss: 0.0034813439216238766, Test Loss: 0.0032133439637001913\n",
            "Epoch 73/1000, Training Loss: 0.003480097605378701, Test Loss: 0.0032123427879055884\n",
            "Epoch 74/1000, Training Loss: 0.0034788315942108285, Test Loss: 0.003211325956611047\n",
            "Epoch 75/1000, Training Loss: 0.0034775453997532416, Test Loss: 0.0032102931693820573\n",
            "Epoch 76/1000, Training Loss: 0.0034762385294004996, Test Loss: 0.003209244122588745\n",
            "Epoch 77/1000, Training Loss: 0.0034749104862802586, Test Loss: 0.0032081785095204084\n",
            "Epoch 78/1000, Training Loss: 0.0034735607692289814, Test Loss: 0.0032070960204979445\n",
            "Epoch 79/1000, Training Loss: 0.003472188872771761, Test Loss: 0.003205996342984287\n",
            "Epoch 80/1000, Training Loss: 0.0034707942871062126, Test Loss: 0.0032048791616929775\n",
            "Epoch 81/1000, Training Loss: 0.0034693764980904184, Test Loss: 0.0032037441586949605\n",
            "Epoch 82/1000, Training Loss: 0.0034679349872348496, Test Loss: 0.0032025910135237085\n",
            "Epoch 83/1000, Training Loss: 0.003466469231698264, Test Loss: 0.003201419403278713\n",
            "Epoch 84/1000, Training Loss: 0.003464978704287546, Test Loss: 0.003200229002727427\n",
            "Epoch 85/1000, Training Loss: 0.003463462873461444, Test Loss: 0.0031990194844056467\n",
            "Epoch 86/1000, Training Loss: 0.0034619212033382006, Test Loss: 0.0031977905187163614\n",
            "Epoch 87/1000, Training Loss: 0.0034603531537070274, Test Loss: 0.0031965417740270315\n",
            "Epoch 88/1000, Training Loss: 0.003458758180043403, Test Loss: 0.0031952729167652578\n",
            "Epoch 89/1000, Training Loss: 0.0034571357335281662, Test Loss: 0.003193983611512778\n",
            "Epoch 90/1000, Training Loss: 0.003455485261070356, Test Loss: 0.0031926735210976655\n",
            "Epoch 91/1000, Training Loss: 0.003453806205333779, Test Loss: 0.003191342306684653\n",
            "Epoch 92/1000, Training Loss: 0.0034520980047672424, Test Loss: 0.0031899896278634056\n",
            "Epoch 93/1000, Training Loss: 0.003450360093638431, Test Loss: 0.00318861514273458\n",
            "Epoch 94/1000, Training Loss: 0.003448591902071363, Test Loss: 0.0031872185079934883\n",
            "Epoch 95/1000, Training Loss: 0.003446792856087372, Test Loss: 0.0031857993790111346\n",
            "Epoch 96/1000, Training Loss: 0.0034449623776495793, Test Loss: 0.003184357409912398\n",
            "Epoch 97/1000, Training Loss: 0.0034430998847107667, Test Loss: 0.003182892253651089\n",
            "Epoch 98/1000, Training Loss: 0.003441204791264616, Test Loss: 0.003181403562081607\n",
            "Epoch 99/1000, Training Loss: 0.0034392765074002245, Test Loss: 0.0031798909860268655\n",
            "Epoch 100/1000, Training Loss: 0.0034373144393598305, Test Loss: 0.0031783541753421947\n",
            "Epoch 101/1000, Training Loss: 0.003435317989599687, Test Loss: 0.003176792778974835\n",
            "Epoch 102/1000, Training Loss: 0.003433286556853983, Test Loss: 0.0031752064450186897\n",
            "Epoch 103/1000, Training Loss: 0.003431219536201737, Test Loss: 0.0031735948207639035\n",
            "Epoch 104/1000, Training Loss: 0.0034291163191365894, Test Loss: 0.003171957552740915\n",
            "Epoch 105/1000, Training Loss: 0.0034269762936393767, Test Loss: 0.003170294286758498\n",
            "Epoch 106/1000, Training Loss: 0.0034247988442534173, Test Loss: 0.003168604667935411\n",
            "Epoch 107/1000, Training Loss: 0.0034225833521624037, Test Loss: 0.0031668883407251327\n",
            "Epoch 108/1000, Training Loss: 0.0034203291952707915, Test Loss: 0.0031651449489332687\n",
            "Epoch 109/1000, Training Loss: 0.0034180357482865914, Test Loss: 0.003163374135727097\n",
            "Epoch 110/1000, Training Loss: 0.00341570238280646, Test Loss: 0.0031615755436367587\n",
            "Epoch 111/1000, Training Loss: 0.003413328467402963, Test Loss: 0.003159748814547594\n",
            "Epoch 112/1000, Training Loss: 0.003410913367713899, Test Loss: 0.0031578935896830473\n",
            "Epoch 113/1000, Training Loss: 0.003408456446533584, Test Loss: 0.003156009509577647\n",
            "Epoch 114/1000, Training Loss: 0.0034059570639059407, Test Loss: 0.0031540962140394722\n",
            "Epoch 115/1000, Training Loss: 0.003403414577219286, Test Loss: 0.0031521533421015237\n",
            "Epoch 116/1000, Training Loss: 0.0034008283413026687, Test Loss: 0.0031501805319614595\n",
            "Epoch 117/1000, Training Loss: 0.0033981977085236217, Test Loss: 0.0031481774209090618\n",
            "Epoch 118/1000, Training Loss: 0.003395522028887174, Test Loss: 0.0031461436452408474\n",
            "Epoch 119/1000, Training Loss: 0.0033928006501359664, Test Loss: 0.0031440788401611926\n",
            "Epoch 120/1000, Training Loss: 0.0033900329178512996, Test Loss: 0.0031419826396693797\n",
            "Epoch 121/1000, Training Loss: 0.0033872181755549343, Test Loss: 0.0031398546764318746\n",
            "Epoch 122/1000, Training Loss: 0.003384355764811454, Test Loss: 0.0031376945816392254\n",
            "Epoch 123/1000, Training Loss: 0.003381445025330978, Test Loss: 0.003135501984846897\n",
            "Epoch 124/1000, Training Loss: 0.0033784852950719925, Test Loss: 0.003133276513799377\n",
            "Epoch 125/1000, Training Loss: 0.003375475910344075, Test Loss: 0.003131017794236863\n",
            "Epoch 126/1000, Training Loss: 0.003372416205910218, Test Loss: 0.003128725449683809\n",
            "Epoch 127/1000, Training Loss: 0.003369305515088483, Test Loss: 0.0031263991012186482\n",
            "Epoch 128/1000, Training Loss: 0.00336614316985265, Test Loss: 0.003124038367223923\n",
            "Epoch 129/1000, Training Loss: 0.003362928500931519, Test Loss: 0.0031216428631160877\n",
            "Epoch 130/1000, Training Loss: 0.003359660837906475, Test Loss: 0.0031192122010542025\n",
            "Epoch 131/1000, Training Loss: 0.0033563395093068916, Test Loss: 0.0031167459896267436\n",
            "Epoch 132/1000, Training Loss: 0.003352963842702904, Test Loss: 0.0031142438335156797\n",
            "Epoch 133/1000, Training Loss: 0.003349533164795032, Test Loss: 0.003111705333137023\n",
            "Epoch 134/1000, Training Loss: 0.0033460468015000955, Test Loss: 0.0031091300842569385\n",
            "Epoch 135/1000, Training Loss: 0.0033425040780327864, Test Loss: 0.0031065176775825474\n",
            "Epoch 136/1000, Training Loss: 0.0033389043189822032, Test Loss: 0.003103867698326498\n",
            "Epoch 137/1000, Training Loss: 0.003335246848382599, Test Loss: 0.003101179725744309\n",
            "Epoch 138/1000, Training Loss: 0.0033315309897775074, Test Loss: 0.0030984533326435327\n",
            "Epoch 139/1000, Training Loss: 0.0033277560662763107, Test Loss: 0.0030956880848636583\n",
            "Epoch 140/1000, Training Loss: 0.0033239214006022823, Test Loss: 0.0030928835407256932\n",
            "Epoch 141/1000, Training Loss: 0.003320026315130955, Test Loss: 0.003090039250450301\n",
            "Epoch 142/1000, Training Loss: 0.003316070131917654, Test Loss: 0.0030871547555432815\n",
            "Epoch 143/1000, Training Loss: 0.0033120521727128347, Test Loss: 0.0030842295881471997\n",
            "Epoch 144/1000, Training Loss: 0.0033079717589638395, Test Loss: 0.0030812632703578445\n",
            "Epoch 145/1000, Training Loss: 0.0033038282118014583, Test Loss: 0.0030782553135041972\n",
            "Epoch 146/1000, Training Loss: 0.0032996208520096305, Test Loss: 0.003075205217390446\n",
            "Epoch 147/1000, Training Loss: 0.0032953489999764194, Test Loss: 0.003072112469498621\n",
            "Epoch 148/1000, Training Loss: 0.0032910119756242744, Test Loss: 0.0030689765441502263\n",
            "Epoch 149/1000, Training Loss: 0.0032866090983174083, Test Loss: 0.003065796901625316\n",
            "Epoch 150/1000, Training Loss: 0.003282139686743944, Test Loss: 0.003062572987237206\n",
            "Epoch 151/1000, Training Loss: 0.0032776030587703345, Test Loss: 0.0030593042303611205\n",
            "Epoch 152/1000, Training Loss: 0.0032729985312653087, Test Loss: 0.003055990043414804\n",
            "Epoch 153/1000, Training Loss: 0.003268325419890452, Test Loss: 0.0030526298207891813\n",
            "Epoch 154/1000, Training Loss: 0.0032635830388542753, Test Loss: 0.003049222937726957\n",
            "Epoch 155/1000, Training Loss: 0.0032587707006264247, Test Loss: 0.003045768749147002\n",
            "Epoch 156/1000, Training Loss: 0.0032538877156084458, Test Loss: 0.0030422665884122474\n",
            "Epoch 157/1000, Training Loss: 0.003248933391757256, Test Loss: 0.003038715766038698\n",
            "Epoch 158/1000, Training Loss: 0.0032439070341572745, Test Loss: 0.0030351155683431097\n",
            "Epoch 159/1000, Training Loss: 0.0032388079445368266, Test Loss: 0.0030314652560266913\n",
            "Epoch 160/1000, Training Loss: 0.0032336354207242353, Test Loss: 0.003027764062692182\n",
            "Epoch 161/1000, Training Loss: 0.0032283887560386783, Test Loss: 0.0030240111932914016\n",
            "Epoch 162/1000, Training Loss: 0.0032230672386106556, Test Loss: 0.003020205822500449\n",
            "Epoch 163/1000, Training Loss: 0.0032176701506265505, Test Loss: 0.0030163470930193864\n",
            "Epoch 164/1000, Training Loss: 0.0032121967674915166, Test Loss: 0.0030124341137933346\n",
            "Epoch 165/1000, Training Loss: 0.0032066463569046066, Test Loss: 0.0030084659581516467\n",
            "Epoch 166/1000, Training Loss: 0.0032010181778397095, Test Loss: 0.0030044416618618007\n",
            "Epoch 167/1000, Training Loss: 0.0031953114794255745, Test Loss: 0.003000360221094504\n",
            "Epoch 168/1000, Training Loss: 0.0031895254997178746, Test Loss: 0.002996220590296397\n",
            "Epoch 169/1000, Training Loss: 0.0031836594643559197, Test Loss: 0.0029920216799666733\n",
            "Epoch 170/1000, Training Loss: 0.0031777125850963104, Test Loss: 0.0029877623543337712\n",
            "Epoch 171/1000, Training Loss: 0.0031716840582155046, Test Loss: 0.0029834414289282903\n",
            "Epoch 172/1000, Training Loss: 0.0031655730627729384, Test Loss: 0.0029790576680481244\n",
            "Epoch 173/1000, Training Loss: 0.0031593787587260082, Test Loss: 0.0029746097821117786\n",
            "Epoch 174/1000, Training Loss: 0.0031531002848879308, Test Loss: 0.0029700964248957364\n",
            "Epoch 175/1000, Training Loss: 0.0031467367567191685, Test Loss: 0.00296551619065173\n",
            "Epoch 176/1000, Training Loss: 0.003140287263942806, Test Loss: 0.0029608676110997067\n",
            "Epoch 177/1000, Training Loss: 0.003133750867974008, Test Loss: 0.0029561491522922334\n",
            "Epoch 178/1000, Training Loss: 0.003127126599153358, Test Loss: 0.0029513592113461283\n",
            "Epoch 179/1000, Training Loss: 0.0031204134537736856, Test Loss: 0.0029464961130370826\n",
            "Epoch 180/1000, Training Loss: 0.0031136103908896906, Test Loss: 0.0029415581062530397\n",
            "Epoch 181/1000, Training Loss: 0.0031067163288995055, Test Loss: 0.002936543360302217\n",
            "Epoch 182/1000, Training Loss: 0.003099730141887108, Test Loss: 0.002931449961071643\n",
            "Epoch 183/1000, Training Loss: 0.0030926506557143753, Test Loss: 0.002926275907032247\n",
            "Epoch 184/1000, Training Loss: 0.003085476643851388, Test Loss: 0.0029210191050866295\n",
            "Epoch 185/1000, Training Loss: 0.003078206822933553, Test Loss: 0.0029156773662557924\n",
            "Epoch 186/1000, Training Loss: 0.003070839848034026, Test Loss: 0.0029102484012012986\n",
            "Epoch 187/1000, Training Loss: 0.003063374307639919, Test Loss: 0.002904729815579555\n",
            "Epoch 188/1000, Training Loss: 0.003055808718320838, Test Loss: 0.0028991191052251497\n",
            "Epoch 189/1000, Training Loss: 0.0030481415190783457, Test Loss: 0.002893413651160486\n",
            "Epoch 190/1000, Training Loss: 0.0030403710653651933, Test Loss: 0.002887610714429264\n",
            "Epoch 191/1000, Training Loss: 0.003032495622763307, Test Loss: 0.0028817074307517826\n",
            "Epoch 192/1000, Training Loss: 0.00302451336030992, Test Loss: 0.002875700805000435\n",
            "Epoch 193/1000, Training Loss: 0.0030164223434615777, Test Loss: 0.0028695877054942774\n",
            "Epoch 194/1000, Training Loss: 0.0030082205266863137, Test Loss: 0.0028633648581120833\n",
            "Epoch 195/1000, Training Loss: 0.002999905745674879, Test Loss: 0.002857028840223935\n",
            "Epoch 196/1000, Training Loss: 0.002991475709162712, Test Loss: 0.002850576074442085\n",
            "Epoch 197/1000, Training Loss: 0.0029829279903552225, Test Loss: 0.0028440028221925896\n",
            "Epoch 198/1000, Training Loss: 0.0029742600179500935, Test Loss: 0.0028373051771101183\n",
            "Epoch 199/1000, Training Loss: 0.0029654690667515874, Test Loss: 0.0028304790582592714\n",
            "Epoch 200/1000, Training Loss: 0.0029565522478734053, Test Loss: 0.0028235202031869034\n",
            "Epoch 201/1000, Training Loss: 0.002947506498528461, Test Loss: 0.0028164241608111575\n",
            "Epoch 202/1000, Training Loss: 0.002938328571406117, Test Loss: 0.0028091862841543146\n",
            "Epoch 203/1000, Training Loss: 0.00292901502363993, Test Loss: 0.0028018017229282186\n",
            "Epoch 204/1000, Training Loss: 0.0029195622053719786, Test Loss: 0.00279426541598278\n",
            "Epoch 205/1000, Training Loss: 0.0029099662479233723, Test Loss: 0.0027865720836302253\n",
            "Epoch 206/1000, Training Loss: 0.0029002230515846744, Test Loss: 0.0027787162198601102\n",
            "Epoch 207/1000, Training Loss: 0.002890328273044901, Test Loss: 0.002770692084462894\n",
            "Epoch 208/1000, Training Loss: 0.002880277312483504, Test Loss: 0.002762493695083082\n",
            "Epoch 209/1000, Training Loss: 0.002870065300356583, Test Loss: 0.0027541148192266694\n",
            "Epoch 210/1000, Training Loss: 0.0028596870839165895, Test Loss: 0.00274554896625202\n",
            "Epoch 211/1000, Training Loss: 0.0028491372135142612, Test Loss: 0.0027367893793784136\n",
            "Epoch 212/1000, Training Loss: 0.0028384099287427204, Test Loss: 0.0027278290277525778\n",
            "Epoch 213/1000, Training Loss: 0.0028274991444968297, Test Loss: 0.002718660598620803\n",
            "Epoch 214/1000, Training Loss: 0.002816398437036417, Test Loss: 0.0027092764896626143\n",
            "Epoch 215/1000, Training Loss: 0.0028051010301603186, Test Loss: 0.0026996688015525168\n",
            "Epoch 216/1000, Training Loss: 0.0027935997816195823, Test Loss: 0.0026898293308283295\n",
            "Epoch 217/1000, Training Loss: 0.0027818871699234827, Test Loss: 0.002679749563159631\n",
            "Epoch 218/1000, Training Loss: 0.00276995528172147, Test Loss: 0.0026694206671275212\n",
            "Epoch 219/1000, Training Loss: 0.0027577957999786423, Test Loss: 0.0026588334886484843\n",
            "Epoch 220/1000, Training Loss: 0.002745399993202462, Test Loss: 0.002647978546200937\n",
            "Epoch 221/1000, Training Loss: 0.002732758706024857, Test Loss: 0.002636846027044502\n",
            "Epoch 222/1000, Training Loss: 0.0027198623514974054, Test Loss: 0.0026254257846595845\n",
            "Epoch 223/1000, Training Loss: 0.002706700905518543, Test Loss: 0.002613707337680144\n",
            "Epoch 224/1000, Training Loss: 0.0026932639038814723, Test Loss: 0.002601679870646798\n",
            "Epoch 225/1000, Training Loss: 0.002679540442509839, Test Loss: 0.0025893322369717805\n",
            "Epoch 226/1000, Training Loss: 0.002665519181535555, Test Loss: 0.0025766529645835836\n",
            "Epoch 227/1000, Training Loss: 0.002651188353969186, Test Loss: 0.002563630264808538\n",
            "Epoch 228/1000, Training Loss: 0.002636535779816695, Test Loss: 0.0025502520451501022\n",
            "Epoch 229/1000, Training Loss: 0.0026215488866058877, Test Loss: 0.002536505926745241\n",
            "Epoch 230/1000, Training Loss: 0.0026062147373979595, Test Loss: 0.0025223792674104696\n",
            "Epoch 231/1000, Training Loss: 0.002590520067470322, Test Loss: 0.002507859191336892\n",
            "Epoch 232/1000, Training Loss: 0.0025744513309597333, Test Loss: 0.002492932626650519\n",
            "Epoch 233/1000, Training Loss: 0.002557994758841643, Test Loss: 0.002477586352216125\n",
            "Epoch 234/1000, Training Loss: 0.002541136429681685, Test Loss: 0.0024618070552213586\n",
            "Epoch 235/1000, Training Loss: 0.002523862354615258, Test Loss: 0.002445581401220373\n",
            "Epoch 236/1000, Training Loss: 0.002506158577975079, Test Loss: 0.002428896118426647\n",
            "Epoch 237/1000, Training Loss: 0.0024880112948763933, Test Loss: 0.0024117380981009784\n",
            "Epoch 238/1000, Training Loss: 0.002469406986865332, Test Loss: 0.002394094512856882\n",
            "Epoch 239/1000, Training Loss: 0.002450332576417572, Test Loss: 0.0023759529545708553\n",
            "Epoch 240/1000, Training Loss: 0.0024307756006230314, Test Loss: 0.002357301593305582\n",
            "Epoch 241/1000, Training Loss: 0.002410724403792955, Test Loss: 0.002338129358196147\n",
            "Epoch 242/1000, Training Loss: 0.0023901683479704665, Test Loss: 0.0023184261405820826\n",
            "Epoch 243/1000, Training Loss: 0.002369098039416709, Test Loss: 0.002298183018769244\n",
            "Epoch 244/1000, Training Loss: 0.0023475055680990935, Test Loss: 0.0022773925026675123\n",
            "Epoch 245/1000, Training Loss: 0.002325384756059951, Test Loss: 0.0022560487951867785\n",
            "Epoch 246/1000, Training Loss: 0.002302731409347204, Test Loss: 0.0022341480657250314\n",
            "Epoch 247/1000, Training Loss: 0.002279543567018305, Test Loss: 0.002211688729422192\n",
            "Epoch 248/1000, Training Loss: 0.002255821739677323, Test Loss: 0.0021886717241879\n",
            "Epoch 249/1000, Training Loss: 0.0022315691291788504, Test Loss: 0.002165100775978775\n",
            "Epoch 250/1000, Training Loss: 0.002206791820641654, Test Loss: 0.002140982641561582\n",
            "Epoch 251/1000, Training Loss: 0.002181498937862533, Test Loss: 0.002116327317222979\n",
            "Epoch 252/1000, Training Loss: 0.002155702753687306, Test Loss: 0.0020911482017335076\n",
            "Epoch 253/1000, Training Loss: 0.0021294187479271383, Test Loss: 0.002065462202470916\n",
            "Epoch 254/1000, Training Loss: 0.0021026656070004374, Test Loss: 0.002039289775026561\n",
            "Epoch 255/1000, Training Loss: 0.0020754651615754673, Test Loss: 0.0020126548888566753\n",
            "Epoch 256/1000, Training Loss: 0.0020478422609708146, Test Loss: 0.0019855849145078586\n",
            "Epoch 257/1000, Training Loss: 0.0020198245857775505, Test Loss: 0.0019581104314685942\n",
            "Epoch 258/1000, Training Loss: 0.0019914424029018173, Test Loss: 0.0019302649595253024\n",
            "Epoch 259/1000, Training Loss: 0.0019627282697844504, Test Loss: 0.0019020846203339772\n",
            "Epoch 260/1000, Training Loss: 0.0019337166967421677, Test Loss: 0.0018736077394412754\n",
            "Epoch 261/1000, Training Loss: 0.0019044437780373765, Test Loss: 0.0018448744019107517\n",
            "Epoch 262/1000, Training Loss: 0.001874946803316678, Test Loss: 0.0018159259767959722\n",
            "Epoch 263/1000, Training Loss: 0.0018452638614194076, Test Loss: 0.0017868046268052298\n",
            "Epoch 264/1000, Training Loss: 0.0018154334482621485, Test Loss: 0.0017575528195725135\n",
            "Epoch 265/1000, Training Loss: 0.0017854940896198427, Test Loss: 0.0017282128560371847\n",
            "Epoch 266/1000, Training Loss: 0.001755483988251999, Test Loss: 0.0016988264296758956\n",
            "Epoch 267/1000, Training Loss: 0.0017254407030888208, Test Loss: 0.0016694342279214294\n",
            "Epoch 268/1000, Training Loss: 0.0016954008662325812, Test Loss: 0.0016400755842771344\n",
            "Epoch 269/1000, Training Loss: 0.0016653999414780395, Test Loss: 0.001610788186629774\n",
            "Epoch 270/1000, Training Loss: 0.0016354720260379108, Test Loss: 0.0015816078443004764\n",
            "Epoch 271/1000, Training Loss: 0.001605649695286202, Test Loss: 0.001552568313639996\n",
            "Epoch 272/1000, Training Loss: 0.0015759638886949717, Test Loss: 0.0015237011796142503\n",
            "Epoch 273/1000, Training Loss: 0.0015464438338066455, Test Loss: 0.0014950357889326122\n",
            "Epoch 274/1000, Training Loss: 0.0015171170040957644, Test Loss: 0.0014665992288910312\n",
            "Epoch 275/1000, Training Loss: 0.0014880091059441746, Test Loss: 0.0014384163452364268\n",
            "Epoch 276/1000, Training Loss: 0.001459144089669149, Test Loss: 0.0014105097919742352\n",
            "Epoch 277/1000, Training Loss: 0.0014305441795686314, Test Loss: 0.0013829001060762952\n",
            "Epoch 278/1000, Training Loss: 0.0014022299182279254, Test Loss: 0.00135560580042205\n",
            "Epoch 279/1000, Training Loss: 0.001374220220804421, Test Loss: 0.0013286434689355165\n",
            "Epoch 280/1000, Training Loss: 0.001346532435604733, Test Loss: 0.0013020278986759317\n",
            "Epoch 281/1000, Training Loss: 0.001319182407928663, Test Loss: 0.0012757721845211986\n",
            "Epoch 282/1000, Training Loss: 0.0012921845448221525, Test Loss: 0.001249887842980704\n",
            "Epoch 283/1000, Training Loss: 0.0012655518790134362, Test Loss: 0.00122438492253353\n",
            "Epoch 284/1000, Training Loss: 0.0012392961308716071, Test Loss: 0.0011992721086689934\n",
            "Epoch 285/1000, Training Loss: 0.0012134277677067012, Test Loss: 0.0011745568224846837\n",
            "Epoch 286/1000, Training Loss: 0.0011879560601161234, Test Loss: 0.00115024531225933\n",
            "Epoch 287/1000, Training Loss: 0.00116288913537486, Test Loss: 0.0011263427378626607\n",
            "Epoch 288/1000, Training Loss: 0.0011382340280725997, Test Loss: 0.0011028532481974895\n",
            "Epoch 289/1000, Training Loss: 0.001113996728330597, Test Loss: 0.0010797800521017914\n",
            "Epoch 290/1000, Training Loss: 0.001090182227997417, Test Loss: 0.001057125483284941\n",
            "Epoch 291/1000, Training Loss: 0.0010667945652391617, Test Loss: 0.0010348910599479071\n",
            "Epoch 292/1000, Training Loss: 0.0010438368679191188, Test Loss: 0.0010130775397575416\n",
            "Epoch 293/1000, Training Loss: 0.001021311396115582, Test Loss: 0.000991684970824333\n",
            "Epoch 294/1000, Training Loss: 0.000999219584065152, Test Loss: 0.000970712739283971\n",
            "Epoch 295/1000, Training Loss: 0.0009775620817496521, Test Loss: 0.0009501596140160955\n",
            "Epoch 296/1000, Training Loss: 0.0009563387962747258, Test Loss: 0.0009300237889574041\n",
            "Epoch 297/1000, Training Loss: 0.0009355489331215988, Test Loss: 0.0009103029233876911\n",
            "Epoch 298/1000, Training Loss: 0.0009151910372935539, Test Loss: 0.000890994180490922\n",
            "Epoch 299/1000, Training Loss: 0.0008952630343272626, Test Loss: 0.0008720942644227037\n",
            "Epoch 300/1000, Training Loss: 0.0008757622710976013, Test Loss: 0.0008535994560528305\n",
            "Epoch 301/1000, Training Loss: 0.0008566855563121723, Test Loss: 0.000835505647497253\n",
            "Epoch 302/1000, Training Loss: 0.000838029200569518, Test Loss: 0.0008178083755096162\n",
            "Epoch 303/1000, Training Loss: 0.0008197890558406173, Test Loss: 0.0008005028537664878\n",
            "Epoch 304/1000, Training Loss: 0.00080196055422705, Test Loss: 0.0007835840040536446\n",
            "Epoch 305/1000, Training Loss: 0.0007845387458492101, Test Loss: 0.0007670464863411501\n",
            "Epoch 306/1000, Training Loss: 0.0007675183357234727, Test Loss: 0.000750884727722345\n",
            "Epoch 307/1000, Training Loss: 0.0007508937194968089, Test Loss: 0.000735092950184688\n",
            "Epoch 308/1000, Training Loss: 0.0007346590179203509, Test Loss: 0.0007196651971780516\n",
            "Epoch 309/1000, Training Loss: 0.0007188081099583323, Test Loss: 0.000704595358947175\n",
            "Epoch 310/1000, Training Loss: 0.0007033346644454587, Test Loss: 0.0006898771965991743\n",
            "Epoch 311/1000, Training Loss: 0.0006882321702228456, Test Loss: 0.0006755043648830234\n",
            "Epoch 312/1000, Training Loss: 0.000673493964699787, Test Loss: 0.000661470433665248\n",
            "Epoch 313/1000, Training Loss: 0.0006591132608055672, Test Loss: 0.0006477689080944482\n",
            "Epoch 314/1000, Training Loss: 0.0006450831723113395, Test Loss: 0.0006343932474555788\n",
            "Epoch 315/1000, Training Loss: 0.0006313967375172013, Test Loss: 0.0006213368827235399\n",
            "Epoch 316/1000, Training Loss: 0.0006180469413130015, Test Loss: 0.0006085932328334662\n",
            "Epoch 317/1000, Training Loss: 0.0006050267356339268, Test Loss: 0.0005961557196928783\n",
            "Epoch 318/1000, Training Loss: 0.0005923290583424779, Test Loss: 0.0005840177819674106\n",
            "Epoch 319/1000, Training Loss: 0.0005799468505778197, Test Loss: 0.0005721728876778686\n",
            "Epoch 320/1000, Training Loss: 0.0005678730726214155, Test Loss: 0.0005606145456515183\n",
            "Epoch 321/1000, Training Loss: 0.0005561007183341604, Test Loss: 0.0005493363158745522\n",
            "Epoch 322/1000, Training Loss: 0.0005446228282254673, Test Loss: 0.0005383318187961523\n",
            "Epoch 323/1000, Training Loss: 0.0005334325012185861, Test Loss: 0.0005275947436369443\n",
            "Epoch 324/1000, Training Loss: 0.0005225229051792984, Test Loss: 0.0005171188557564353\n",
            "Epoch 325/1000, Training Loss: 0.0005118872862767468, Test Loss: 0.0005068980031349026\n",
            "Epoch 326/1000, Training Loss: 0.0005015189772462248, Test Loss: 0.0004969261220256694\n",
            "Epoch 327/1000, Training Loss: 0.000491411404623568, Test Loss: 0.00048719724183329116\n",
            "Epoch 328/1000, Training Loss: 0.00048155809502042153, Test Loss: 0.000477705489272592\n",
            "Epoch 329/1000, Training Loss: 0.00047195268050829103, Test Loss: 0.0004684450918622089\n",
            "Epoch 330/1000, Training Loss: 0.0004625889031776308, Test Loss: 0.000459410380804748\n",
            "Epoch 331/1000, Training Loss: 0.00045346061893626095, Test Loss: 0.0004505957933039778\n",
            "Epoch 332/1000, Training Loss: 0.00044456180060879394, Test Loss: 0.00044199587436717224\n",
            "Epoch 333/1000, Training Loss: 0.000435886540396328, Test Loss: 0.0004336052781387301\n",
            "Epoch 334/1000, Training Loss: 0.00042742905175268976, Test Loss: 0.0004254187688086386\n",
            "Epoch 335/1000, Training Loss: 0.0004191836707306172, Test Loss: 0.0004174312211369942\n",
            "Epoch 336/1000, Training Loss: 0.00041114485684832445, Test Loss: 0.00040963762063332096\n",
            "Epoch 337/1000, Training Loss: 0.0004033071935236999, Test Loss: 0.0004020330634268213\n",
            "Epoch 338/1000, Training Loss: 0.00039566538812051214, Test Loss: 0.00039461275586134424\n",
            "Epoch 339/1000, Training Loss: 0.00038821427164791256, Test Loss: 0.00038737201384634186\n",
            "Epoch 340/1000, Training Loss: 0.00038094879815160144, Test Loss: 0.00038030626199272603\n",
            "Epoch 341/1000, Training Loss: 0.00037386404383222394, Test Loss: 0.0003734110325602934\n",
            "Epoch 342/1000, Training Loss: 0.000366955205923768, Test Loss: 0.00036668196424111864\n",
            "Epoch 343/1000, Training Loss: 0.00036021760136216636, Test Loss: 0.00036011480080132095\n",
            "Epoch 344/1000, Training Loss: 0.0003536466652717505, Test Loss: 0.0003537053896015336\n",
            "Epoch 345/1000, Training Loss: 0.0003472379492948458, Test Loss: 0.00034744968001457704\n",
            "Epoch 346/1000, Training Loss: 0.00034098711978757364, Test Loss: 0.00034134372175708994\n",
            "Epoch 347/1000, Training Loss: 0.0003348899559028116, Test Loss: 0.000335383663150228\n",
            "Epoch 348/1000, Training Loss: 0.0003289423475792831, Test Loss: 0.00032956574932299843\n",
            "Epoch 349/1000, Training Loss: 0.00032314029345391885, Test Loss: 0.0003238863203704055\n",
            "Epoch 350/1000, Training Loss: 0.0003174798987129186, Test Loss: 0.00031834180947727035\n",
            "Epoch 351/1000, Training Loss: 0.0003119573728953688, Test Loss: 0.00031292874101738273\n",
            "Epoch 352/1000, Training Loss: 0.0003065690276618137, Test Loss: 0.0003076437286365778\n",
            "Epoch 353/1000, Training Loss: 0.0003013112745387949, Test Loss: 0.00030248347332725454\n",
            "Epoch 354/1000, Training Loss: 0.00029618062264921246, Test Loss: 0.00029744476150104615\n",
            "Epoch 355/1000, Training Loss: 0.00029117367643716663, Test Loss: 0.00029252446306542407\n",
            "Epoch 356/1000, Training Loss: 0.00028628713339492497, Test Loss: 0.000287719529509312\n",
            "Epoch 357/1000, Training Loss: 0.0002815177817987594, Test Loss: 0.00028302699200213736\n",
            "Epoch 358/1000, Training Loss: 0.00027686249845947764, Test Loss: 0.00027844395951007026\n",
            "Epoch 359/1000, Training Loss: 0.0002723182464927534, Test Loss: 0.00027396761693270535\n",
            "Epoch 360/1000, Training Loss: 0.0002678820731136737, Test Loss: 0.0002695952232629763\n",
            "Epoch 361/1000, Training Loss: 0.00026355110745921893, Test Loss: 0.0002653241097725724\n",
            "Epoch 362/1000, Training Loss: 0.00025932255844191227, Test Loss: 0.0002611516782248247\n",
            "Epoch 363/1000, Training Loss: 0.0002551937126373307, Test Loss: 0.00025707539911666734\n",
            "Epoch 364/1000, Training Loss: 0.0002511619322076918, Test Loss: 0.00025309280995092405\n",
            "Epoch 365/1000, Training Loss: 0.00024722465286340027, Test Loss: 0.00024920151354000447\n",
            "Epoch 366/1000, Training Loss: 0.0002433793818639584, Test Loss: 0.00024539917634172045\n",
            "Epoch 367/1000, Training Loss: 0.000239623696059504, Test Loss: 0.0002416835268279172\n",
            "Epoch 368/1000, Training Loss: 0.00023595523997373802, Test Loss: 0.00023805235388621186\n",
            "Epoch 369/1000, Training Loss: 0.00023237172392896792, Test Loss: 0.0002345035052551913\n",
            "Epoch 370/1000, Training Loss: 0.00022887092221363125, Test Loss: 0.00023103488599313918\n",
            "Epoch 371/1000, Training Loss: 0.00022545067129249425, Test Loss: 0.0002276444569802742\n",
            "Epoch 372/1000, Training Loss: 0.0002221088680596453, Test Loss: 0.0002243302334544707\n",
            "Epoch 373/1000, Training Loss: 0.0002188434681341274, Test Loss: 0.00022109028358021791\n",
            "Epoch 374/1000, Training Loss: 0.00021565248419801696, Test Loss: 0.000217922727050586\n",
            "Epoch 375/1000, Training Loss: 0.00021253398437663093, Test Loss: 0.0002148257337219039\n",
            "Epoch 376/1000, Training Loss: 0.00020948609066039425, Test Loss: 0.00021179752228071944\n",
            "Epoch 377/1000, Training Loss: 0.00020650697736794996, Test Loss: 0.00020883635894272982\n",
            "Epoch 378/1000, Training Loss: 0.00020359486964984338, Test Loss: 0.0002059405561831174\n",
            "Epoch 379/1000, Training Loss: 0.00020074804203226306, Test Loss: 0.0002031084714979382\n",
            "Epoch 380/1000, Training Loss: 0.00019796481700007511, Test Loss: 0.000200338506195958\n",
            "Epoch 381/1000, Training Loss: 0.00019524356361851056, Test Loss: 0.0001976291042204873\n",
            "Epoch 382/1000, Training Loss: 0.0001925826961927478, Test Loss: 0.00019497875100066568\n",
            "Epoch 383/1000, Training Loss: 0.00018998067296461846, Test Loss: 0.00019238597233164017\n",
            "Epoch 384/1000, Training Loss: 0.00018743599484567268, Test Loss: 0.0001898493332831006\n",
            "Epoch 385/1000, Training Loss: 0.0001849472041858123, Test Loss: 0.00018736743713562042\n",
            "Epoch 386/1000, Training Loss: 0.0001825128835766889, Test Loss: 0.0001849389243442344\n",
            "Epoch 387/1000, Training Loss: 0.00018013165468906033, Test Loss: 0.00018256247152870553\n",
            "Epoch 388/1000, Training Loss: 0.0001778021771432882, Test Loss: 0.00018023679048989578\n",
            "Epoch 389/1000, Training Loss: 0.00017552314741221605, Test Loss: 0.00017796062725174266\n",
            "Epoch 390/1000, Training Loss: 0.00017329329775557062, Test Loss: 0.0001757327611282267\n",
            "Epoch 391/1000, Training Loss: 0.00017111139518512773, Test Loss: 0.00017355200381481922\n",
            "Epoch 392/1000, Training Loss: 0.0001689762404598285, Test Loss: 0.00017141719850383527\n",
            "Epoch 393/1000, Training Loss: 0.00016688666711011434, Test Loss: 0.0001693272190232106\n",
            "Epoch 394/1000, Training Loss: 0.0001648415404906427, Test Loss: 0.0001672809689980919\n",
            "Epoch 395/1000, Training Loss: 0.0001628397568607, Test Loss: 0.00016527738103479388\n",
            "Epoch 396/1000, Training Loss: 0.00016088024249150026, Test Loss: 0.0001633154159265352\n",
            "Epoch 397/1000, Training Loss: 0.0001589619527996914, Test Loss: 0.0001613940618805012\n",
            "Epoch 398/1000, Training Loss: 0.00015708387150632614, Test Loss: 0.00015951233376569347\n",
            "Epoch 399/1000, Training Loss: 0.00015524500982058514, Test Loss: 0.00015766927238107715\n",
            "Epoch 400/1000, Training Loss: 0.0001534444056475768, Test Loss: 0.00015586394374354274\n",
            "Epoch 401/1000, Training Loss: 0.00015168112281955035, Test Loss: 0.00015409543839521692\n",
            "Epoch 402/1000, Training Loss: 0.00014995425034982406, Test Loss: 0.0001523628707296162\n",
            "Epoch 403/1000, Training Loss: 0.00014826290170880753, Test Loss: 0.00015066537833619809\n",
            "Epoch 404/1000, Training Loss: 0.0001466062141215025, Test Loss: 0.00014900212136287536\n",
            "Epoch 405/1000, Training Loss: 0.00014498334788582613, Test Loss: 0.00014737228189600561\n",
            "Epoch 406/1000, Training Loss: 0.00014339348571118299, Test Loss: 0.00014577506335744083\n",
            "Epoch 407/1000, Training Loss: 0.00014183583207670142, Test Loss: 0.00014420968991821518\n",
            "Epoch 408/1000, Training Loss: 0.00014030961260853898, Test Loss: 0.00014267540592841584\n",
            "Epoch 409/1000, Training Loss: 0.00013881407347571817, Test Loss: 0.00014117147536284915\n",
            "Epoch 410/1000, Training Loss: 0.00013734848080395118, Test Loss: 0.00013969718128209498\n",
            "Epoch 411/1000, Training Loss: 0.00013591212010690814, Test Loss: 0.00013825182530853534\n",
            "Epoch 412/1000, Training Loss: 0.00013450429573442534, Test Loss: 0.0001368347271169751\n",
            "Epoch 413/1000, Training Loss: 0.0001331243303371596, Test Loss: 0.00013544522393948562\n",
            "Epoch 414/1000, Training Loss: 0.00013177156434716786, Test Loss: 0.00013408267008406424\n",
            "Epoch 415/1000, Training Loss: 0.00013044535547397356, Test Loss: 0.0001327464364667732\n",
            "Epoch 416/1000, Training Loss: 0.00012914507821564734, Test Loss: 0.00013143591015699912\n",
            "Epoch 417/1000, Training Loss: 0.00012787012338443847, Test Loss: 0.0001301504939354604\n",
            "Epoch 418/1000, Training Loss: 0.0001266198976465257, Test Loss: 0.00012888960586462724\n",
            "Epoch 419/1000, Training Loss: 0.0001253938230754833, Test Loss: 0.0001276526788712416\n",
            "Epoch 420/1000, Training Loss: 0.00012419133671902037, Test Loss: 0.00012643916034057594\n",
            "Epoch 421/1000, Training Loss: 0.00012301189017860698, Test Loss: 0.00012524851172213147\n",
            "Epoch 422/1000, Training Loss: 0.00012185494920159973, Test Loss: 0.0001240802081464564\n",
            "Epoch 423/1000, Training Loss: 0.00012071999328547796, Test Loss: 0.00012293373805277882\n",
            "Epoch 424/1000, Training Loss: 0.00011960651529383682, Test Loss: 0.00012180860282715717\n",
            "Epoch 425/1000, Training Loss: 0.00011851402108376246, Test Loss: 0.00012070431645085199\n",
            "Epoch 426/1000, Training Loss: 0.00011744202914426355, Test Loss: 0.00011962040515864678\n",
            "Epoch 427/1000, Training Loss: 0.00011639007024540532, Test Loss: 0.00011855640710682565\n",
            "Epoch 428/1000, Training Loss: 0.00011535768709783176, Test Loss: 0.00011751187205054856\n",
            "Epoch 429/1000, Training Loss: 0.00011434443402234212, Test Loss: 0.00011648636103034621\n",
            "Epoch 430/1000, Training Loss: 0.00011334987662924786, Test Loss: 0.00011547944606750921\n",
            "Epoch 431/1000, Training Loss: 0.00011237359150716418, Test Loss: 0.00011449070986806965\n",
            "Epoch 432/1000, Training Loss: 0.00011141516592099502, Test Loss: 0.00011351974553518558\n",
            "Epoch 433/1000, Training Loss: 0.00011047419751879571, Test Loss: 0.00011256615628965267\n",
            "Epoch 434/1000, Training Loss: 0.00010955029404725931, Test Loss: 0.00011162955519832451\n",
            "Epoch 435/1000, Training Loss: 0.00010864307307555773, Test Loss: 0.00011070956491021987\n",
            "Epoch 436/1000, Training Loss: 0.00010775216172727402, Test Loss: 0.00010980581740008383\n",
            "Epoch 437/1000, Training Loss: 0.00010687719642020157, Test Loss: 0.0001089179537192098\n",
            "Epoch 438/1000, Training Loss: 0.00010601782261373642, Test Loss: 0.00010804562375329221\n",
            "Epoch 439/1000, Training Loss: 0.00010517369456365552, Test Loss: 0.00010718848598711878\n",
            "Epoch 440/1000, Training Loss: 0.00010434447508404304, Test Loss: 0.00010634620727590666\n",
            "Epoch 441/1000, Training Loss: 0.00010352983531615456, Test Loss: 0.00010551846262308876\n",
            "Epoch 442/1000, Training Loss: 0.00010272945450399473, Test Loss: 0.00010470493496436158\n",
            "Epoch 443/1000, Training Loss: 0.00010194301977640631, Test Loss: 0.00010390531495780759\n",
            "Epoch 444/1000, Training Loss: 0.00010117022593548517, Test Loss: 0.00010311930077993706\n",
            "Epoch 445/1000, Training Loss: 0.00010041077525110593, Test Loss: 0.00010234659792745412\n",
            "Epoch 446/1000, Training Loss: 9.966437726139072e-05, Test Loss: 0.00010158691902459784\n",
            "Epoch 447/1000, Training Loss: 9.893074857892253e-05, Test Loss: 0.00010083998363587912\n",
            "Epoch 448/1000, Training Loss: 9.820961270254019e-05, Test Loss: 0.0001001055180840751\n",
            "Epoch 449/1000, Training Loss: 9.750069983454536e-05, Test Loss: 9.938325527331938e-05\n",
            "Epoch 450/1000, Training Loss: 9.680374670314466e-05, Test Loss: 9.867293451713927e-05\n",
            "Epoch 451/1000, Training Loss: 9.611849638997793e-05, Test Loss: 9.797430137129436e-05\n",
            "Epoch 452/1000, Training Loss: 9.544469816257607e-05, Test Loss: 9.728710747128077e-05\n",
            "Epoch 453/1000, Training Loss: 9.478210731159211e-05, Test Loss: 9.661111037436518e-05\n",
            "Epoch 454/1000, Training Loss: 9.413048499267342e-05, Test Loss: 9.594607340601448e-05\n",
            "Epoch 455/1000, Training Loss: 9.348959807281638e-05, Test Loss: 9.529176551058821e-05\n",
            "Epoch 456/1000, Training Loss: 9.285921898109256e-05, Test Loss: 9.464796110618894e-05\n",
            "Epoch 457/1000, Training Loss: 9.223912556358021e-05, Test Loss: 9.401443994351544e-05\n",
            "Epoch 458/1000, Training Loss: 9.162910094240926e-05, Test Loss: 9.339098696864036e-05\n",
            "Epoch 459/1000, Training Loss: 9.102893337877098e-05, Test Loss: 9.27773921895666e-05\n",
            "Epoch 460/1000, Training Loss: 9.043841613978418e-05, Test Loss: 9.217345054647448e-05\n",
            "Epoch 461/1000, Training Loss: 8.985734736909888e-05, Test Loss: 9.15789617855416e-05\n",
            "Epoch 462/1000, Training Loss: 8.928552996112351e-05, Test Loss: 9.099373033622838e-05\n",
            "Epoch 463/1000, Training Loss: 8.872277143876767e-05, Test Loss: 9.041756519193758e-05\n",
            "Epoch 464/1000, Training Loss: 8.816888383460243e-05, Test Loss: 8.985027979394924e-05\n",
            "Epoch 465/1000, Training Loss: 8.762368357531674e-05, Test Loss: 8.929169191852314e-05\n",
            "Epoch 466/1000, Training Loss: 8.708699136940015e-05, Test Loss: 8.87416235670958e-05\n",
            "Epoch 467/1000, Training Loss: 8.655863209792071e-05, Test Loss: 8.81999008594655e-05\n",
            "Epoch 468/1000, Training Loss: 8.603843470834622e-05, Test Loss: 8.766635392989483e-05\n",
            "Epoch 469/1000, Training Loss: 8.552623211127191e-05, Test Loss: 8.714081682602328e-05\n",
            "Epoch 470/1000, Training Loss: 8.502186108001927e-05, Test Loss: 8.662312741054204e-05\n",
            "Epoch 471/1000, Training Loss: 8.452516215297033e-05, Test Loss: 8.611312726551655e-05\n",
            "Epoch 472/1000, Training Loss: 8.403597953860124e-05, Test Loss: 8.561066159930398e-05\n",
            "Epoch 473/1000, Training Loss: 8.355416102309444e-05, Test Loss: 8.511557915597604e-05\n",
            "Epoch 474/1000, Training Loss: 8.307955788049631e-05, Test Loss: 8.462773212719551e-05\n",
            "Epoch 475/1000, Training Loss: 8.261202478530848e-05, Test Loss: 8.414697606645145e-05\n",
            "Epoch 476/1000, Training Loss: 8.215141972745614e-05, Test Loss: 8.36731698055957e-05\n",
            "Epoch 477/1000, Training Loss: 8.169760392957016e-05, Test Loss: 8.320617537362268e-05\n",
            "Epoch 478/1000, Training Loss: 8.125044176650461e-05, Test Loss: 8.274585791761933e-05\n",
            "Epoch 479/1000, Training Loss: 8.08098006870291e-05, Test Loss: 8.229208562582021e-05\n",
            "Epoch 480/1000, Training Loss: 8.03755511376314e-05, Test Loss: 8.184472965271714e-05\n",
            "Epoch 481/1000, Training Loss: 7.99475664883692e-05, Test Loss: 8.140366404615839e-05\n",
            "Epoch 482/1000, Training Loss: 7.952572296071521e-05, Test Loss: 8.096876567638524e-05\n",
            "Epoch 483/1000, Training Loss: 7.910989955733227e-05, Test Loss: 8.053991416694774e-05\n",
            "Epoch 484/1000, Training Loss: 7.869997799371975e-05, Test Loss: 8.011699182743917e-05\n",
            "Epoch 485/1000, Training Loss: 7.829584263169052e-05, Test Loss: 7.969988358801434e-05\n",
            "Epoch 486/1000, Training Loss: 7.789738041461244e-05, Test Loss: 7.928847693563197e-05\n",
            "Epoch 487/1000, Training Loss: 7.750448080437369e-05, Test Loss: 7.888266185196792e-05\n",
            "Epoch 488/1000, Training Loss: 7.711703572000883e-05, Test Loss: 7.848233075295835e-05\n",
            "Epoch 489/1000, Training Loss: 7.673493947795694e-05, Test Loss: 7.808737842992973e-05\n",
            "Epoch 490/1000, Training Loss: 7.635808873389022e-05, Test Loss: 7.769770199226252e-05\n",
            "Epoch 491/1000, Training Loss: 7.598638242607127e-05, Test Loss: 7.731320081155145e-05\n",
            "Epoch 492/1000, Training Loss: 7.561972172020099e-05, Test Loss: 7.693377646721919e-05\n",
            "Epoch 493/1000, Training Loss: 7.525800995571304e-05, Test Loss: 7.655933269354398e-05\n",
            "Epoch 494/1000, Training Loss: 7.490115259346572e-05, Test Loss: 7.618977532805813e-05\n",
            "Epoch 495/1000, Training Loss: 7.454905716480367e-05, Test Loss: 7.58250122612823e-05\n",
            "Epoch 496/1000, Training Loss: 7.420163322194496e-05, Test Loss: 7.546495338776047e-05\n",
            "Epoch 497/1000, Training Loss: 7.385879228965445e-05, Test Loss: 7.510951055835193e-05\n",
            "Epoch 498/1000, Training Loss: 7.352044781817024e-05, Test Loss: 7.475859753375427e-05\n",
            "Epoch 499/1000, Training Loss: 7.31865151373492e-05, Test Loss: 7.441212993921781e-05\n",
            "Epoch 500/1000, Training Loss: 7.28569114119965e-05, Test Loss: 7.407002522042075e-05\n",
            "Epoch 501/1000, Training Loss: 7.25315555983391e-05, Test Loss: 7.373220260047141e-05\n",
            "Epoch 502/1000, Training Loss: 7.2210368401623e-05, Test Loss: 7.339858303800758e-05\n",
            "Epoch 503/1000, Training Loss: 7.189327223479877e-05, Test Loss: 7.306908918636314e-05\n",
            "Epoch 504/1000, Training Loss: 7.158019117826194e-05, Test Loss: 7.274364535377733e-05\n",
            "Epoch 505/1000, Training Loss: 7.127105094061798e-05, Test Loss: 7.242217746460389e-05\n",
            "Epoch 506/1000, Training Loss: 7.096577882045458e-05, Test Loss: 7.210461302151167e-05\n",
            "Epoch 507/1000, Training Loss: 7.066430366907792e-05, Test Loss: 7.179088106863137e-05\n",
            "Epoch 508/1000, Training Loss: 7.036655585419794e-05, Test Loss: 7.148091215563978e-05\n",
            "Epoch 509/1000, Training Loss: 7.00724672245355e-05, Test Loss: 7.11746383027447e-05\n",
            "Epoch 510/1000, Training Loss: 6.97819710753168e-05, Test Loss: 7.087199296654878e-05\n",
            "Epoch 511/1000, Training Loss: 6.949500211464713e-05, Test Loss: 7.057291100676784e-05\n",
            "Epoch 512/1000, Training Loss: 6.92114964307193e-05, Test Loss: 7.02773286537859e-05\n",
            "Epoch 513/1000, Training Loss: 6.893139145985694e-05, Test Loss: 6.998518347701861e-05\n",
            "Epoch 514/1000, Training Loss: 6.865462595534783e-05, Test Loss: 6.969641435406148e-05\n",
            "Epoch 515/1000, Training Loss: 6.838113995706514e-05, Test Loss: 6.941096144061302e-05\n",
            "Epoch 516/1000, Training Loss: 6.81108747618412e-05, Test Loss: 6.912876614113753e-05\n",
            "Epoch 517/1000, Training Loss: 6.784377289458286e-05, Test Loss: 6.884977108026109e-05\n",
            "Epoch 518/1000, Training Loss: 6.757977808009911e-05, Test Loss: 6.85739200748697e-05\n",
            "Epoch 519/1000, Training Loss: 6.731883521563296e-05, Test Loss: 6.830115810689796e-05\n",
            "Epoch 520/1000, Training Loss: 6.706089034407163e-05, Test Loss: 6.80314312967889e-05\n",
            "Epoch 521/1000, Training Loss: 6.6805890627818e-05, Test Loss: 6.776468687760914e-05\n",
            "Epoch 522/1000, Training Loss: 6.65537843233032e-05, Test Loss: 6.750087316979282e-05\n",
            "Epoch 523/1000, Training Loss: 6.630452075613181e-05, Test Loss: 6.723993955650826e-05\n",
            "Epoch 524/1000, Training Loss: 6.605805029683372e-05, Test Loss: 6.698183645962926e-05\n",
            "Epoch 525/1000, Training Loss: 6.581432433720982e-05, Test Loss: 6.67265153162865e-05\n",
            "Epoch 526/1000, Training Loss: 6.557329526725756e-05, Test Loss: 6.647392855599496e-05\n",
            "Epoch 527/1000, Training Loss: 6.533491645266154e-05, Test Loss: 6.622402957833878e-05\n",
            "Epoch 528/1000, Training Loss: 6.509914221282774e-05, Test Loss: 6.597677273119291e-05\n",
            "Epoch 529/1000, Training Loss: 6.486592779945454e-05, Test Loss: 6.573211328947353e-05\n",
            "Epoch 530/1000, Training Loss: 6.463522937562485e-05, Test Loss: 6.549000743440605e-05\n",
            "Epoch 531/1000, Training Loss: 6.440700399540174e-05, Test Loss: 6.525041223328932e-05\n",
            "Epoch 532/1000, Training Loss: 6.41812095839194e-05, Test Loss: 6.501328561974874e-05\n",
            "Epoch 533/1000, Training Loss: 6.395780491795463e-05, Test Loss: 6.477858637446428e-05\n",
            "Epoch 534/1000, Training Loss: 6.373674960696236e-05, Test Loss: 6.454627410636188e-05\n",
            "Epoch 535/1000, Training Loss: 6.351800407457226e-05, Test Loss: 6.43163092342519e-05\n",
            "Epoch 536/1000, Training Loss: 6.330152954052572e-05, Test Loss: 6.408865296891228e-05\n",
            "Epoch 537/1000, Training Loss: 6.308728800304723e-05, Test Loss: 6.38632672955951e-05\n",
            "Epoch 538/1000, Training Loss: 6.287524222163345e-05, Test Loss: 6.364011495695211e-05\n",
            "Epoch 539/1000, Training Loss: 6.266535570025926e-05, Test Loss: 6.341915943636248e-05\n",
            "Epoch 540/1000, Training Loss: 6.245759267097663e-05, Test Loss: 6.320036494166507e-05\n",
            "Epoch 541/1000, Training Loss: 6.225191807790912e-05, Test Loss: 6.298369638926413e-05\n",
            "Epoch 542/1000, Training Loss: 6.204829756161948e-05, Test Loss: 6.276911938862237e-05\n",
            "Epoch 543/1000, Training Loss: 6.184669744385465e-05, Test Loss: 6.255660022711209e-05\n",
            "Epoch 544/1000, Training Loss: 6.164708471264566e-05, Test Loss: 6.234610585522922e-05\n",
            "Epoch 545/1000, Training Loss: 6.144942700776382e-05, Test Loss: 6.213760387215113e-05\n",
            "Epoch 546/1000, Training Loss: 6.125369260651477e-05, Test Loss: 6.19310625116364e-05\n",
            "Epoch 547/1000, Training Loss: 6.105985040986626e-05, Test Loss: 6.172645062825159e-05\n",
            "Epoch 548/1000, Training Loss: 6.086786992890933e-05, Test Loss: 6.15237376839274e-05\n",
            "Epoch 549/1000, Training Loss: 6.067772127162472e-05, Test Loss: 6.132289373481965e-05\n",
            "Epoch 550/1000, Training Loss: 6.048937512996756e-05, Test Loss: 6.112388941848358e-05\n",
            "Epoch 551/1000, Training Loss: 6.0302802767251036e-05, Test Loss: 6.0926695941345865e-05\n",
            "Epoch 552/1000, Training Loss: 6.011797600582276e-05, Test Loss: 6.073128506646575e-05\n",
            "Epoch 553/1000, Training Loss: 5.993486721502801e-05, Test Loss: 6.053762910158146e-05\n",
            "Epoch 554/1000, Training Loss: 5.9753449299454096e-05, Test Loss: 6.0345700887433336e-05\n",
            "Epoch 555/1000, Training Loss: 5.957369568744355e-05, Test Loss: 6.015547378635678e-05\n",
            "Epoch 556/1000, Training Loss: 5.9395580319878235e-05, Test Loss: 5.996692167113867e-05\n",
            "Epoch 557/1000, Training Loss: 5.921907763921687e-05, Test Loss: 5.978001891413028e-05\n",
            "Epoch 558/1000, Training Loss: 5.904416257879136e-05, Test Loss: 5.9594740376614155e-05\n",
            "Epoch 559/1000, Training Loss: 5.887081055234252e-05, Test Loss: 5.941106139840952e-05\n",
            "Epoch 560/1000, Training Loss: 5.869899744380112e-05, Test Loss: 5.9228957787717236e-05\n",
            "Epoch 561/1000, Training Loss: 5.852869959730364e-05, Test Loss: 5.904840581120441e-05\n",
            "Epoch 562/1000, Training Loss: 5.8359893807431925e-05, Test Loss: 5.8869382184303995e-05\n",
            "Epoch 563/1000, Training Loss: 5.819255730968083e-05, Test Loss: 5.8691864061744976e-05\n",
            "Epoch 564/1000, Training Loss: 5.802666777114114e-05, Test Loss: 5.851582902829451e-05\n",
            "Epoch 565/1000, Training Loss: 5.7862203281394995e-05, Test Loss: 5.834125508971248e-05\n",
            "Epoch 566/1000, Training Loss: 5.769914234361788e-05, Test Loss: 5.8168120663907064e-05\n",
            "Epoch 567/1000, Training Loss: 5.753746386588493e-05, Test Loss: 5.799640457229678e-05\n",
            "Epoch 568/1000, Training Loss: 5.7377147152669596e-05, Test Loss: 5.78260860313641e-05\n",
            "Epoch 569/1000, Training Loss: 5.7218171896542234e-05, Test Loss: 5.7657144644400556e-05\n",
            "Epoch 570/1000, Training Loss: 5.7060518170046014e-05, Test Loss: 5.7489560393439435e-05\n",
            "Epoch 571/1000, Training Loss: 5.690416641776684e-05, Test Loss: 5.732331363136798e-05\n",
            "Epoch 572/1000, Training Loss: 5.674909744857334e-05, Test Loss: 5.7158385074219673e-05\n",
            "Epoch 573/1000, Training Loss: 5.659529242803628e-05, Test Loss: 5.6994755793635545e-05\n",
            "Epoch 574/1000, Training Loss: 5.644273287101745e-05, Test Loss: 5.683240720949932e-05\n",
            "Epoch 575/1000, Training Loss: 5.629140063442132e-05, Test Loss: 5.667132108273571e-05\n",
            "Epoch 576/1000, Training Loss: 5.614127791011396e-05, Test Loss: 5.651147950826485e-05\n",
            "Epoch 577/1000, Training Loss: 5.5992347217995175e-05, Test Loss: 5.6352864908121894e-05\n",
            "Epoch 578/1000, Training Loss: 5.584459139922957e-05, Test Loss: 5.6195460024725195e-05\n",
            "Epoch 579/1000, Training Loss: 5.569799360962428e-05, Test Loss: 5.603924791429246e-05\n",
            "Epoch 580/1000, Training Loss: 5.55525373131582e-05, Test Loss: 5.588421194040948e-05\n",
            "Epoch 581/1000, Training Loss: 5.5408206275651334e-05, Test Loss: 5.573033576773334e-05\n",
            "Epoch 582/1000, Training Loss: 5.526498455857482e-05, Test Loss: 5.557760335584223e-05\n",
            "Epoch 583/1000, Training Loss: 5.51228565130005e-05, Test Loss: 5.5425998953216566e-05\n",
            "Epoch 584/1000, Training Loss: 5.498180677368165e-05, Test Loss: 5.5275507091353624e-05\n",
            "Epoch 585/1000, Training Loss: 5.4841820253264934e-05, Test Loss: 5.512611257901676e-05\n",
            "Epoch 586/1000, Training Loss: 5.470288213663093e-05, Test Loss: 5.497780049660013e-05\n",
            "Epoch 587/1000, Training Loss: 5.456497787535761e-05, Test Loss: 5.483055619063e-05\n",
            "Epoch 588/1000, Training Loss: 5.442809318230524e-05, Test Loss: 5.4684365268378715e-05\n",
            "Epoch 589/1000, Training Loss: 5.4292214026322945e-05, Test Loss: 5.4539213592596315e-05\n",
            "Epoch 590/1000, Training Loss: 5.415732662706686e-05, Test Loss: 5.4395087276363406e-05\n",
            "Epoch 591/1000, Training Loss: 5.402341744993452e-05, Test Loss: 5.425197267804951e-05\n",
            "Epoch 592/1000, Training Loss: 5.389047320110957e-05, Test Loss: 5.4109856396385515e-05\n",
            "Epoch 593/1000, Training Loss: 5.375848082271242e-05, Test Loss: 5.396872526564228e-05\n",
            "Epoch 594/1000, Training Loss: 5.362742748805914e-05, Test Loss: 5.382856635091342e-05\n",
            "Epoch 595/1000, Training Loss: 5.349730059702227e-05, Test Loss: 5.368936694350147e-05\n",
            "Epoch 596/1000, Training Loss: 5.3368087771490794e-05, Test Loss: 5.355111455640218e-05\n",
            "Epoch 597/1000, Training Loss: 5.3239776850931236e-05, Test Loss: 5.341379691988956e-05\n",
            "Epoch 598/1000, Training Loss: 5.311235588804168e-05, Test Loss: 5.3277401977194313e-05\n",
            "Epoch 599/1000, Training Loss: 5.298581314450285e-05, Test Loss: 5.3141917880272614e-05\n",
            "Epoch 600/1000, Training Loss: 5.286013708681974e-05, Test Loss: 5.300733298567318e-05\n",
            "Epoch 601/1000, Training Loss: 5.2735316382250004e-05, Test Loss: 5.2873635850487395e-05\n",
            "Epoch 602/1000, Training Loss: 5.261133989482273e-05, Test Loss: 5.27408152283857e-05\n",
            "Epoch 603/1000, Training Loss: 5.248819668144462e-05, Test Loss: 5.260886006574679e-05\n",
            "Epoch 604/1000, Training Loss: 5.236587598808507e-05, Test Loss: 5.2477759497857854e-05\n",
            "Epoch 605/1000, Training Loss: 5.22443672460458e-05, Test Loss: 5.234750284520686e-05\n",
            "Epoch 606/1000, Training Loss: 5.21236600683092e-05, Test Loss: 5.2218079609849394e-05\n",
            "Epoch 607/1000, Training Loss: 5.200374424596442e-05, Test Loss: 5.20894794718492e-05\n",
            "Epoch 608/1000, Training Loss: 5.1884609744712424e-05, Test Loss: 5.1961692285804447e-05\n",
            "Epoch 609/1000, Training Loss: 5.1766246701441236e-05, Test Loss: 5.1834708077435897e-05\n",
            "Epoch 610/1000, Training Loss: 5.164864542087578e-05, Test Loss: 5.170851704025602e-05\n",
            "Epoch 611/1000, Training Loss: 5.153179637230044e-05, Test Loss: 5.1583109532307734e-05\n",
            "Epoch 612/1000, Training Loss: 5.141569018634927e-05, Test Loss: 5.145847607296727e-05\n",
            "Epoch 613/1000, Training Loss: 5.130031765186291e-05, Test Loss: 5.133460733981729e-05\n",
            "Epoch 614/1000, Training Loss: 5.1185669712815104e-05, Test Loss: 5.121149416558972e-05\n",
            "Epoch 615/1000, Training Loss: 5.1071737465302766e-05, Test Loss: 5.108912753516762e-05\n",
            "Epoch 616/1000, Training Loss: 5.095851215459961e-05, Test Loss: 5.0967498582653253e-05\n",
            "Epoch 617/1000, Training Loss: 5.084598517226903e-05, Test Loss: 5.084659858849556e-05\n",
            "Epoch 618/1000, Training Loss: 5.0734148053342195e-05, Test Loss: 5.072641897667981e-05\n",
            "Epoch 619/1000, Training Loss: 5.062299247355257e-05, Test Loss: 5.0606951311974125e-05\n",
            "Epoch 620/1000, Training Loss: 5.0512510246629345e-05, Test Loss: 5.048818729723657e-05\n",
            "Epoch 621/1000, Training Loss: 5.0402693321646605e-05, Test Loss: 5.0370118770774297e-05\n",
            "Epoch 622/1000, Training Loss: 5.029353378042961e-05, Test Loss: 5.025273770376543e-05\n",
            "Epoch 623/1000, Training Loss: 5.018502383501202e-05, Test Loss: 5.0136036197723e-05\n",
            "Epoch 624/1000, Training Loss: 5.007715582515124e-05, Test Loss: 5.0020006482020235e-05\n",
            "Epoch 625/1000, Training Loss: 4.996992221588959e-05, Test Loss: 4.9904640911467834e-05\n",
            "Epoch 626/1000, Training Loss: 4.986331559517125e-05, Test Loss: 4.978993196393566e-05\n",
            "Epoch 627/1000, Training Loss: 4.975732867150412e-05, Test Loss: 4.967587223802776e-05\n",
            "Epoch 628/1000, Training Loss: 4.9651954271674236e-05, Test Loss: 4.956245445080438e-05\n",
            "Epoch 629/1000, Training Loss: 4.954718533850518e-05, Test Loss: 4.9449671435550585e-05\n",
            "Epoch 630/1000, Training Loss: 4.944301492866425e-05, Test Loss: 4.9337516139592646e-05\n",
            "Epoch 631/1000, Training Loss: 4.9339436210514104e-05, Test Loss: 4.92259816221574e-05\n",
            "Epoch 632/1000, Training Loss: 4.923644246200797e-05, Test Loss: 4.911506105227567e-05\n",
            "Epoch 633/1000, Training Loss: 4.913402706862857e-05, Test Loss: 4.9004747706730814e-05\n",
            "Epoch 634/1000, Training Loss: 4.903218352137171e-05, Test Loss: 4.889503496804781e-05\n",
            "Epoch 635/1000, Training Loss: 4.8930905414765956e-05, Test Loss: 4.8785916322521524e-05\n",
            "Epoch 636/1000, Training Loss: 4.883018644493921e-05, Test Loss: 4.867738535829029e-05\n",
            "Epoch 637/1000, Training Loss: 4.873002040771897e-05, Test Loss: 4.856943576344486e-05\n",
            "Epoch 638/1000, Training Loss: 4.863040119677555e-05, Test Loss: 4.8462061324176305e-05\n",
            "Epoch 639/1000, Training Loss: 4.853132280180204e-05, Test Loss: 4.835525592296368e-05\n",
            "Epoch 640/1000, Training Loss: 4.843277930673094e-05, Test Loss: 4.824901353679644e-05\n",
            "Epoch 641/1000, Training Loss: 4.833476488798681e-05, Test Loss: 4.814332823543297e-05\n",
            "Epoch 642/1000, Training Loss: 4.823727381277549e-05, Test Loss: 4.8038194179697525e-05\n",
            "Epoch 643/1000, Training Loss: 4.8140300437407235e-05, Test Loss: 4.7933605619806966e-05\n",
            "Epoch 644/1000, Training Loss: 4.804383920565345e-05, Test Loss: 4.782955689373277e-05\n",
            "Epoch 645/1000, Training Loss: 4.7947884647139153e-05, Test Loss: 4.7726042425601786e-05\n",
            "Epoch 646/1000, Training Loss: 4.78524313757645e-05, Test Loss: 4.762305672411909e-05\n",
            "Epoch 647/1000, Training Loss: 4.7757474088160335e-05, Test Loss: 4.752059438103028e-05\n",
            "Epoch 648/1000, Training Loss: 4.766300756217434e-05, Test Loss: 4.741865006961347e-05\n",
            "Epoch 649/1000, Training Loss: 4.756902665538863e-05, Test Loss: 4.731721854319887e-05\n",
            "Epoch 650/1000, Training Loss: 4.747552630366436e-05, Test Loss: 4.721629463371922e-05\n",
            "Epoch 651/1000, Training Loss: 4.7382501519718476e-05, Test Loss: 4.711587325028993e-05\n",
            "Epoch 652/1000, Training Loss: 4.7289947391728e-05, Test Loss: 4.7015949377818706e-05\n",
            "Epoch 653/1000, Training Loss: 4.7197859081962254e-05, Test Loss: 4.691651807563985e-05\n",
            "Epoch 654/1000, Training Loss: 4.710623182544193e-05, Test Loss: 4.6817574476178314e-05\n",
            "Epoch 655/1000, Training Loss: 4.701506092862557e-05, Test Loss: 4.6719113783638766e-05\n",
            "Epoch 656/1000, Training Loss: 4.6924341768122375e-05, Test Loss: 4.6621131272722354e-05\n",
            "Epoch 657/1000, Training Loss: 4.683406978942938e-05, Test Loss: 4.65236222873665e-05\n",
            "Epoch 658/1000, Training Loss: 4.674424050569528e-05, Test Loss: 4.6426582239513193e-05\n",
            "Epoch 659/1000, Training Loss: 4.665484949650903e-05, Test Loss: 4.633000660789759e-05\n",
            "Epoch 660/1000, Training Loss: 4.656589240671031e-05, Test Loss: 4.623389093686493e-05\n",
            "Epoch 661/1000, Training Loss: 4.647736494522552e-05, Test Loss: 4.613823083520552e-05\n",
            "Epoch 662/1000, Training Loss: 4.638926288392609e-05, Test Loss: 4.604302197501844e-05\n",
            "Epoch 663/1000, Training Loss: 4.630158205651009e-05, Test Loss: 4.5948260090594354e-05\n",
            "Epoch 664/1000, Training Loss: 4.621431835740276e-05, Test Loss: 4.585394097731659e-05\n",
            "Epoch 665/1000, Training Loss: 4.6127467740685106e-05, Test Loss: 4.576006049059448e-05\n",
            "Epoch 666/1000, Training Loss: 4.604102621903555e-05, Test Loss: 4.566661454480698e-05\n",
            "Epoch 667/1000, Training Loss: 4.595498986269932e-05, Test Loss: 4.5573599112270996e-05\n",
            "Epoch 668/1000, Training Loss: 4.586935479847395e-05, Test Loss: 4.548101022223288e-05\n",
            "Epoch 669/1000, Training Loss: 4.578411720871544e-05, Test Loss: 4.538884395987101e-05\n",
            "Epoch 670/1000, Training Loss: 4.569927333036513e-05, Test Loss: 4.529709646532841e-05\n",
            "Epoch 671/1000, Training Loss: 4.561481945399494e-05, Test Loss: 4.520576393275905e-05\n",
            "Epoch 672/1000, Training Loss: 4.553075192286968e-05, Test Loss: 4.5114842609388827e-05\n",
            "Epoch 673/1000, Training Loss: 4.544706713203046e-05, Test Loss: 4.502432879460268e-05\n",
            "Epoch 674/1000, Training Loss: 4.5363761527392584e-05, Test Loss: 4.493421883904281e-05\n",
            "Epoch 675/1000, Training Loss: 4.528083160486471e-05, Test Loss: 4.484450914372863e-05\n",
            "Epoch 676/1000, Training Loss: 4.519827390948231e-05, Test Loss: 4.4755196159190854e-05\n",
            "Epoch 677/1000, Training Loss: 4.5116085034558835e-05, Test Loss: 4.466627638462398e-05\n",
            "Epoch 678/1000, Training Loss: 4.503426162085213e-05, Test Loss: 4.4577746367054915e-05\n",
            "Epoch 679/1000, Training Loss: 4.495280035575068e-05, Test Loss: 4.4489602700526904e-05\n",
            "Epoch 680/1000, Training Loss: 4.4871697972469844e-05, Test Loss: 4.4401842025298876e-05\n",
            "Epoch 681/1000, Training Loss: 4.479095124926934e-05, Test Loss: 4.4314461027060654e-05\n",
            "Epoch 682/1000, Training Loss: 4.4710557008680076e-05, Test Loss: 4.422745643616489e-05\n",
            "Epoch 683/1000, Training Loss: 4.4630512116752574e-05, Test Loss: 4.414082502687076e-05\n",
            "Epoch 684/1000, Training Loss: 4.455081348231251e-05, Test Loss: 4.40545636166018e-05\n",
            "Epoch 685/1000, Training Loss: 4.44714580562363e-05, Test Loss: 4.3968669065222906e-05\n",
            "Epoch 686/1000, Training Loss: 4.439244283073645e-05, Test Loss: 4.3883138274324946e-05\n",
            "Epoch 687/1000, Training Loss: 4.431376483866447e-05, Test Loss: 4.3797968186526456e-05\n",
            "Epoch 688/1000, Training Loss: 4.423542115282313e-05, Test Loss: 4.371315578478849e-05\n",
            "Epoch 689/1000, Training Loss: 4.415740888529525e-05, Test Loss: 4.362869809174015e-05\n",
            "Epoch 690/1000, Training Loss: 4.407972518678215e-05, Test Loss: 4.35445921690201e-05\n",
            "Epoch 691/1000, Training Loss: 4.400236724595653e-05, Test Loss: 4.346083511662652e-05\n",
            "Epoch 692/1000, Training Loss: 4.392533228882719e-05, Test Loss: 4.33774240722851e-05\n",
            "Epoch 693/1000, Training Loss: 4.384861757811713e-05, Test Loss: 4.329435621082109e-05\n",
            "Epoch 694/1000, Training Loss: 4.377222041265043e-05, Test Loss: 4.3211628743551245e-05\n",
            "Epoch 695/1000, Training Loss: 4.3696138126752e-05, Test Loss: 4.312923891768247e-05\n",
            "Epoch 696/1000, Training Loss: 4.3620368089661174e-05, Test Loss: 4.3047184015721765e-05\n",
            "Epoch 697/1000, Training Loss: 4.3544907704951935e-05, Test Loss: 4.2965461354898735e-05\n",
            "Epoch 698/1000, Training Loss: 4.346975440996742e-05, Test Loss: 4.288406828659922e-05\n",
            "Epoch 699/1000, Training Loss: 4.339490567526289e-05, Test Loss: 4.280300219580459e-05\n",
            "Epoch 700/1000, Training Loss: 4.33203590040619e-05, Test Loss: 4.27222605005536e-05\n",
            "Epoch 701/1000, Training Loss: 4.324611193171766e-05, Test Loss: 4.2641840651395724e-05\n",
            "Epoch 702/1000, Training Loss: 4.31721620251912e-05, Test Loss: 4.256174013087537e-05\n",
            "Epoch 703/1000, Training Loss: 4.3098506882532536e-05, Test Loss: 4.2481956453006485e-05\n",
            "Epoch 704/1000, Training Loss: 4.302514413237684e-05, Test Loss: 4.240248716277129e-05\n",
            "Epoch 705/1000, Training Loss: 4.2952071433446545e-05, Test Loss: 4.2323329835620237e-05\n",
            "Epoch 706/1000, Training Loss: 4.2879286474063595e-05, Test Loss: 4.224448207698541e-05\n",
            "Epoch 707/1000, Training Loss: 4.280678697167235e-05, Test Loss: 4.216594152179928e-05\n",
            "Epoch 708/1000, Training Loss: 4.273457067236956e-05, Test Loss: 4.2087705834025e-05\n",
            "Epoch 709/1000, Training Loss: 4.2662635350441264e-05, Test Loss: 4.200977270619399e-05\n",
            "Epoch 710/1000, Training Loss: 4.259097880791368e-05, Test Loss: 4.193213985895284e-05\n",
            "Epoch 711/1000, Training Loss: 4.251959887410702e-05, Test Loss: 4.185480504062006e-05\n",
            "Epoch 712/1000, Training Loss: 4.2448493405199975e-05, Test Loss: 4.1777766026743736e-05\n",
            "Epoch 713/1000, Training Loss: 4.237766028380178e-05, Test Loss: 4.1701020619678795e-05\n",
            "Epoch 714/1000, Training Loss: 4.2307097418533005e-05, Test Loss: 4.16245666481606e-05\n",
            "Epoch 715/1000, Training Loss: 4.2236802743610896e-05, Test Loss: 4.154840196689624e-05\n",
            "Epoch 716/1000, Training Loss: 4.216677421844731e-05, Test Loss: 4.147252445615306e-05\n",
            "Epoch 717/1000, Training Loss: 4.209700982724832e-05, Test Loss: 4.1396932021364154e-05\n",
            "Epoch 718/1000, Training Loss: 4.202750757862598e-05, Test Loss: 4.132162259273365e-05\n",
            "Epoch 719/1000, Training Loss: 4.1958265505214596e-05, Test Loss: 4.1246594124854e-05\n",
            "Epoch 720/1000, Training Loss: 4.1889281663294676e-05, Test Loss: 4.117184459633118e-05\n",
            "Epoch 721/1000, Training Loss: 4.182055413242257e-05, Test Loss: 4.1097372009407914e-05\n",
            "Epoch 722/1000, Training Loss: 4.175208101506856e-05, Test Loss: 4.1023174389602344e-05\n",
            "Epoch 723/1000, Training Loss: 4.168386043626047e-05, Test Loss: 4.0949249785353165e-05\n",
            "Epoch 724/1000, Training Loss: 4.1615890543233605e-05, Test Loss: 4.087559626766464e-05\n",
            "Epoch 725/1000, Training Loss: 4.154816950508802e-05, Test Loss: 4.080221192976274e-05\n",
            "Epoch 726/1000, Training Loss: 4.148069551245023e-05, Test Loss: 4.072909488675731e-05\n",
            "Epoch 727/1000, Training Loss: 4.141346677714269e-05, Test Loss: 4.065624327530976e-05\n",
            "Epoch 728/1000, Training Loss: 4.13464815318578e-05, Test Loss: 4.0583655253303576e-05\n",
            "Epoch 729/1000, Training Loss: 4.1279738029838654e-05, Test Loss: 4.051132899952627e-05\n",
            "Epoch 730/1000, Training Loss: 4.121323454456589e-05, Test Loss: 4.043926271335327e-05\n",
            "Epoch 731/1000, Training Loss: 4.1146969369448524e-05, Test Loss: 4.036745461443606e-05\n",
            "Epoch 732/1000, Training Loss: 4.1080940817521e-05, Test Loss: 4.029590294240305e-05\n",
            "Epoch 733/1000, Training Loss: 4.1015147221147124e-05, Test Loss: 4.0224605956553015e-05\n",
            "Epoch 734/1000, Training Loss: 4.094958693172699e-05, Test Loss: 4.015356193556984e-05\n",
            "Epoch 735/1000, Training Loss: 4.088425831940931e-05, Test Loss: 4.008276917722822e-05\n",
            "Epoch 736/1000, Training Loss: 4.081915977281121e-05, Test Loss: 4.0012225998110934e-05\n",
            "Epoch 737/1000, Training Loss: 4.075428969873937e-05, Test Loss: 3.994193073333248e-05\n",
            "Epoch 738/1000, Training Loss: 4.068964652192095e-05, Test Loss: 3.987188173626453e-05\n",
            "Epoch 739/1000, Training Loss: 4.0625228684732803e-05, Test Loss: 3.980207737826732e-05\n",
            "Epoch 740/1000, Training Loss: 4.0561034646940976e-05, Test Loss: 3.973251604842424e-05\n",
            "Epoch 741/1000, Training Loss: 4.049706288544215e-05, Test Loss: 3.966319615328197e-05\n",
            "Epoch 742/1000, Training Loss: 4.043331189401086e-05, Test Loss: 3.9594116116599565e-05\n",
            "Epoch 743/1000, Training Loss: 4.036978018304888e-05, Test Loss: 3.952527437909279e-05\n",
            "Epoch 744/1000, Training Loss: 4.030646627934252e-05, Test Loss: 3.9456669398191415e-05\n",
            "Epoch 745/1000, Training Loss: 4.0243368725819924e-05, Test Loss: 3.938829964779357e-05\n",
            "Epoch 746/1000, Training Loss: 4.018048608131712e-05, Test Loss: 3.932016361803036e-05\n",
            "Epoch 747/1000, Training Loss: 4.011781692034391e-05, Test Loss: 3.925225981503261e-05\n",
            "Epoch 748/1000, Training Loss: 4.005535983285598e-05, Test Loss: 3.91845867606991e-05\n",
            "Epoch 749/1000, Training Loss: 3.9993113424031304e-05, Test Loss: 3.911714299247127e-05\n",
            "Epoch 750/1000, Training Loss: 3.993107631405047e-05, Test Loss: 3.904992706310997e-05\n",
            "Epoch 751/1000, Training Loss: 3.986924713787735e-05, Test Loss: 3.8982937540481834e-05\n",
            "Epoch 752/1000, Training Loss: 3.9807624545048734e-05, Test Loss: 3.891617300733975e-05\n",
            "Epoch 753/1000, Training Loss: 3.9746207199464646e-05, Test Loss: 3.8849632061113355e-05\n",
            "Epoch 754/1000, Training Loss: 3.968499377918099e-05, Test Loss: 3.878331331370321e-05\n",
            "Epoch 755/1000, Training Loss: 3.962398297620993e-05, Test Loss: 3.871721539127466e-05\n",
            "Epoch 756/1000, Training Loss: 3.9563173496318305e-05, Test Loss: 3.8651336934059916e-05\n",
            "Epoch 757/1000, Training Loss: 3.950256405883486e-05, Test Loss: 3.858567659615798e-05\n",
            "Epoch 758/1000, Training Loss: 3.944215339645724e-05, Test Loss: 3.85202330453454e-05\n",
            "Epoch 759/1000, Training Loss: 3.938194025506224e-05, Test Loss: 3.8455004962881136e-05\n",
            "Epoch 760/1000, Training Loss: 3.9321923393521604e-05, Test Loss: 3.838999104332258e-05\n",
            "Epoch 761/1000, Training Loss: 3.926210158351873e-05, Test Loss: 3.8325189994340224e-05\n",
            "Epoch 762/1000, Training Loss: 3.920247360936976e-05, Test Loss: 3.82606005365357e-05\n",
            "Epoch 763/1000, Training Loss: 3.91430382678459e-05, Test Loss: 3.819622140326612e-05\n",
            "Epoch 764/1000, Training Loss: 3.9083794368002304e-05, Test Loss: 3.813205134046549e-05\n",
            "Epoch 765/1000, Training Loss: 3.9024740731005455e-05, Test Loss: 3.806808910647747e-05\n",
            "Epoch 766/1000, Training Loss: 3.896587618996634e-05, Test Loss: 3.800433347188136e-05\n",
            "Epoch 767/1000, Training Loss: 3.890719958977581e-05, Test Loss: 3.7940783219328714e-05\n",
            "Epoch 768/1000, Training Loss: 3.8848709786941155e-05, Test Loss: 3.787743714337731e-05\n",
            "Epoch 769/1000, Training Loss: 3.879040564942827e-05, Test Loss: 3.78142940503309e-05\n",
            "Epoch 770/1000, Training Loss: 3.8732286056503315e-05, Test Loss: 3.775135275808125e-05\n",
            "Epoch 771/1000, Training Loss: 3.867434989858004e-05, Test Loss: 3.76886120959533e-05\n",
            "Epoch 772/1000, Training Loss: 3.861659607706727e-05, Test Loss: 3.762607090454827e-05\n",
            "Epoch 773/1000, Training Loss: 3.855902350421885e-05, Test Loss: 3.756372803559738e-05\n",
            "Epoch 774/1000, Training Loss: 3.8501631102988495e-05, Test Loss: 3.750158235181034e-05\n",
            "Epoch 775/1000, Training Loss: 3.844441780688488e-05, Test Loss: 3.743963272672996e-05\n",
            "Epoch 776/1000, Training Loss: 3.838738255982814e-05, Test Loss: 3.7377878044589136e-05\n",
            "Epoch 777/1000, Training Loss: 3.8330524316013004e-05, Test Loss: 3.7316317200170555e-05\n",
            "Epoch 778/1000, Training Loss: 3.827384203976934e-05, Test Loss: 3.725494909866481e-05\n",
            "Epoch 779/1000, Training Loss: 3.8217334705427847e-05, Test Loss: 3.719377265553614e-05\n",
            "Epoch 780/1000, Training Loss: 3.816100129718658e-05, Test Loss: 3.713278679639003e-05\n",
            "Epoch 781/1000, Training Loss: 3.810484080898047e-05, Test Loss: 3.7071990456833674e-05\n",
            "Epoch 782/1000, Training Loss: 3.804885224435286e-05, Test Loss: 3.701138258235203e-05\n",
            "Epoch 783/1000, Training Loss: 3.7993034616328325e-05, Test Loss: 3.695096212817914e-05\n",
            "Epoch 784/1000, Training Loss: 3.7937386947288264e-05, Test Loss: 3.68907280591676e-05\n",
            "Epoch 785/1000, Training Loss: 3.788190826884801e-05, Test Loss: 3.6830679349670016e-05\n",
            "Epoch 786/1000, Training Loss: 3.782659762173748e-05, Test Loss: 3.6770814983413975e-05\n",
            "Epoch 787/1000, Training Loss: 3.777145405568092e-05, Test Loss: 3.671113395338049e-05\n",
            "Epoch 788/1000, Training Loss: 3.771647662928156e-05, Test Loss: 3.665163526168873e-05\n",
            "Epoch 789/1000, Training Loss: 3.7661664409904546e-05, Test Loss: 3.659231791947755e-05\n",
            "Epoch 790/1000, Training Loss: 3.7607016473566103e-05, Test Loss: 3.653318094679221e-05\n",
            "Epoch 791/1000, Training Loss: 3.755253190482083e-05, Test Loss: 3.6474223372469206e-05\n",
            "Epoch 792/1000, Training Loss: 3.749820979665278e-05, Test Loss: 3.6415444234028316e-05\n",
            "Epoch 793/1000, Training Loss: 3.744404925036685e-05, Test Loss: 3.635684257756097e-05\n",
            "Epoch 794/1000, Training Loss: 3.7390049375483905e-05, Test Loss: 3.6298417457623534e-05\n",
            "Epoch 795/1000, Training Loss: 3.733620928963474e-05, Test Loss: 3.624016793713084e-05\n",
            "Epoch 796/1000, Training Loss: 3.728252811845893e-05, Test Loss: 3.6182093087253434e-05\n",
            "Epoch 797/1000, Training Loss: 3.7229004995501536e-05, Test Loss: 3.6124191987312084e-05\n",
            "Epoch 798/1000, Training Loss: 3.71756390621155e-05, Test Loss: 3.6066463724679116e-05\n",
            "Epoch 799/1000, Training Loss: 3.7122429467362505e-05, Test Loss: 3.600890739467832e-05\n",
            "Epoch 800/1000, Training Loss: 3.7069375367915916e-05, Test Loss: 3.595152210048583e-05\n",
            "Epoch 801/1000, Training Loss: 3.7016475927966897e-05, Test Loss: 3.5894306953034786e-05\n",
            "Epoch 802/1000, Training Loss: 3.696373031913047e-05, Test Loss: 3.5837261070921645e-05\n",
            "Epoch 803/1000, Training Loss: 3.691113772035229e-05, Test Loss: 3.578038358030778e-05\n",
            "Epoch 804/1000, Training Loss: 3.685869731781933e-05, Test Loss: 3.572367361483254e-05\n",
            "Epoch 805/1000, Training Loss: 3.6806408304869244e-05, Test Loss: 3.566713031551927e-05\n",
            "Epoch 806/1000, Training Loss: 3.6754269881903576e-05, Test Loss: 3.561075283068765e-05\n",
            "Epoch 807/1000, Training Loss: 3.6702281256299834e-05, Test Loss: 3.555454031586558e-05\n",
            "Epoch 808/1000, Training Loss: 3.66504416423273e-05, Test Loss: 3.549849193369989e-05\n",
            "Epoch 809/1000, Training Loss: 3.659875026106263e-05, Test Loss: 3.544260685387461e-05\n",
            "Epoch 810/1000, Training Loss: 3.654720634030644e-05, Test Loss: 3.5386884253025595e-05\n",
            "Epoch 811/1000, Training Loss: 3.649580911450294e-05, Test Loss: 3.5331323314656634e-05\n",
            "Epoch 812/1000, Training Loss: 3.644455782465866e-05, Test Loss: 3.527592322905768e-05\n",
            "Epoch 813/1000, Training Loss: 3.639345171826416e-05, Test Loss: 3.522068319322512e-05\n",
            "Epoch 814/1000, Training Loss: 3.6342490049215364e-05, Test Loss: 3.516560241078436e-05\n",
            "Epoch 815/1000, Training Loss: 3.629167207773782e-05, Test Loss: 3.511068009190998e-05\n",
            "Epoch 816/1000, Training Loss: 3.624099707031029e-05, Test Loss: 3.505591545324878e-05\n",
            "Epoch 817/1000, Training Loss: 3.619046429959036e-05, Test Loss: 3.500130771784342e-05\n",
            "Epoch 818/1000, Training Loss: 3.6140073044341954e-05, Test Loss: 3.4946856115058105e-05\n",
            "Epoch 819/1000, Training Loss: 3.608982258936205e-05, Test Loss: 3.489255988050804e-05\n",
            "Epoch 820/1000, Training Loss: 3.6039712225409995e-05, Test Loss: 3.483841825598235e-05\n",
            "Epoch 821/1000, Training Loss: 3.598974124913824e-05, Test Loss: 3.47844304893758e-05\n",
            "Epoch 822/1000, Training Loss: 3.593990896302093e-05, Test Loss: 3.473059583461728e-05\n",
            "Epoch 823/1000, Training Loss: 3.5890214675288434e-05, Test Loss: 3.467691355160239e-05\n",
            "Epoch 824/1000, Training Loss: 3.584065769985885e-05, Test Loss: 3.4623382906119935e-05\n",
            "Epoch 825/1000, Training Loss: 3.579123735627175e-05, Test Loss: 3.45700031697909e-05\n",
            "Epoch 826/1000, Training Loss: 3.57419529696245e-05, Test Loss: 3.4516773619999224e-05\n",
            "Epoch 827/1000, Training Loss: 3.5692803870506236e-05, Test Loss: 3.446369353982505e-05\n",
            "Epoch 828/1000, Training Loss: 3.564378939493569e-05, Test Loss: 3.4410762217982535e-05\n",
            "Epoch 829/1000, Training Loss: 3.559490888429881e-05, Test Loss: 3.43579789487568e-05\n",
            "Epoch 830/1000, Training Loss: 3.5546161685287475e-05, Test Loss: 3.4305343031936926e-05\n",
            "Epoch 831/1000, Training Loss: 3.5497547149838196e-05, Test Loss: 3.425285377276031e-05\n",
            "Epoch 832/1000, Training Loss: 3.544906463507297e-05, Test Loss: 3.420051048184707e-05\n",
            "Epoch 833/1000, Training Loss: 3.540071350324099e-05, Test Loss: 3.414831247514311e-05\n",
            "Epoch 834/1000, Training Loss: 3.535249312165989e-05, Test Loss: 3.409625907385779e-05\n",
            "Epoch 835/1000, Training Loss: 3.530440286265848e-05, Test Loss: 3.40443496044098e-05\n",
            "Epoch 836/1000, Training Loss: 3.5256442103521336e-05, Test Loss: 3.399258339836542e-05\n",
            "Epoch 837/1000, Training Loss: 3.520861022643238e-05, Test Loss: 3.394095979238525e-05\n",
            "Epoch 838/1000, Training Loss: 3.5160906618420714e-05, Test Loss: 3.3889478128166156e-05\n",
            "Epoch 839/1000, Training Loss: 3.511333067130638e-05, Test Loss: 3.3838137752387104e-05\n",
            "Epoch 840/1000, Training Loss: 3.506588178164741e-05, Test Loss: 3.378693801665503e-05\n",
            "Epoch 841/1000, Training Loss: 3.501855935068634e-05, Test Loss: 3.373587827744921e-05\n",
            "Epoch 842/1000, Training Loss: 3.497136278430097e-05, Test Loss: 3.36849578960717e-05\n",
            "Epoch 843/1000, Training Loss: 3.4924291492950105e-05, Test Loss: 3.363417623859368e-05\n",
            "Epoch 844/1000, Training Loss: 3.487734489162523e-05, Test Loss: 3.358353267580427e-05\n",
            "Epoch 845/1000, Training Loss: 3.483052239980174e-05, Test Loss: 3.3533026583157597e-05\n",
            "Epoch 846/1000, Training Loss: 3.4783823441387595e-05, Test Loss: 3.3482657340726545e-05\n",
            "Epoch 847/1000, Training Loss: 3.473724744467737e-05, Test Loss: 3.343242433315255e-05\n",
            "Epoch 848/1000, Training Loss: 3.4690793842302665e-05, Test Loss: 3.3382326949593335e-05\n",
            "Epoch 849/1000, Training Loss: 3.4644462071187555e-05, Test Loss: 3.333236458368134e-05\n",
            "Epoch 850/1000, Training Loss: 3.459825157250042e-05, Test Loss: 3.328253663346993e-05\n",
            "Epoch 851/1000, Training Loss: 3.4552161791609235e-05, Test Loss: 3.323284250139262e-05\n",
            "Epoch 852/1000, Training Loss: 3.450619217803591e-05, Test Loss: 3.318328159421059e-05\n",
            "Epoch 853/1000, Training Loss: 3.446034218541282e-05, Test Loss: 3.313385332297378e-05\n",
            "Epoch 854/1000, Training Loss: 3.441461127143847e-05, Test Loss: 3.3084557102972603e-05\n",
            "Epoch 855/1000, Training Loss: 3.436899889783454e-05, Test Loss: 3.30353923536932e-05\n",
            "Epoch 856/1000, Training Loss: 3.432350453030269e-05, Test Loss: 3.2986358498775674e-05\n",
            "Epoch 857/1000, Training Loss: 3.427812763848333e-05, Test Loss: 3.293745496597083e-05\n",
            "Epoch 858/1000, Training Loss: 3.423286769591394e-05, Test Loss: 3.288868118709601e-05\n",
            "Epoch 859/1000, Training Loss: 3.418772417998821e-05, Test Loss: 3.2840036597994875e-05\n",
            "Epoch 860/1000, Training Loss: 3.414269657191492e-05, Test Loss: 3.279152063849645e-05\n",
            "Epoch 861/1000, Training Loss: 3.409778435667975e-05, Test Loss: 3.274313275237338e-05\n",
            "Epoch 862/1000, Training Loss: 3.405298702300375e-05, Test Loss: 3.269487238730047e-05\n",
            "Epoch 863/1000, Training Loss: 3.400830406330736e-05, Test Loss: 3.264673899481816e-05\n",
            "Epoch 864/1000, Training Loss: 3.396373497366963e-05, Test Loss: 3.259873203028968e-05\n",
            "Epoch 865/1000, Training Loss: 3.391927925379202e-05, Test Loss: 3.255085095286528e-05\n",
            "Epoch 866/1000, Training Loss: 3.38749364069605e-05, Test Loss: 3.250309522544407e-05\n",
            "Epoch 867/1000, Training Loss: 3.383070594000894e-05, Test Loss: 3.245546431463297e-05\n",
            "Epoch 868/1000, Training Loss: 3.378658736328306e-05, Test Loss: 3.240795769071371e-05\n",
            "Epoch 869/1000, Training Loss: 3.374258019060477e-05, Test Loss: 3.236057482760305e-05\n",
            "Epoch 870/1000, Training Loss: 3.369868393923589e-05, Test Loss: 3.231331520281764e-05\n",
            "Epoch 871/1000, Training Loss: 3.3654898129843784e-05, Test Loss: 3.226617829744029e-05\n",
            "Epoch 872/1000, Training Loss: 3.361122228646838e-05, Test Loss: 3.2219163596078264e-05\n",
            "Epoch 873/1000, Training Loss: 3.3567655936485275e-05, Test Loss: 3.217227058683824e-05\n",
            "Epoch 874/1000, Training Loss: 3.3524198610575475e-05, Test Loss: 3.2125498761283205e-05\n",
            "Epoch 875/1000, Training Loss: 3.3480849842689566e-05, Test Loss: 3.207884761440271e-05\n",
            "Epoch 876/1000, Training Loss: 3.343760917001697e-05, Test Loss: 3.2032316644578675e-05\n",
            "Epoch 877/1000, Training Loss: 3.339447613295229e-05, Test Loss: 3.1985905353551486e-05\n",
            "Epoch 878/1000, Training Loss: 3.3351450275064176e-05, Test Loss: 3.193961324638779e-05\n",
            "Epoch 879/1000, Training Loss: 3.330853114306454e-05, Test Loss: 3.189343983144864e-05\n",
            "Epoch 880/1000, Training Loss: 3.326571828677587e-05, Test Loss: 3.184738462035724e-05\n",
            "Epoch 881/1000, Training Loss: 3.3223011259102154e-05, Test Loss: 3.180144712796751e-05\n",
            "Epoch 882/1000, Training Loss: 3.318040961599762e-05, Test Loss: 3.17556268723316e-05\n",
            "Epoch 883/1000, Training Loss: 3.3137912916437674e-05, Test Loss: 3.1709923374671765e-05\n",
            "Epoch 884/1000, Training Loss: 3.309552072238847e-05, Test Loss: 3.166433615934883e-05\n",
            "Epoch 885/1000, Training Loss: 3.3053232598778925e-05, Test Loss: 3.1618864753832894e-05\n",
            "Epoch 886/1000, Training Loss: 3.301104811347043e-05, Test Loss: 3.157350868867182e-05\n",
            "Epoch 887/1000, Training Loss: 3.2968966837230244e-05, Test Loss: 3.1528267497463016e-05\n",
            "Epoch 888/1000, Training Loss: 3.292698834370255e-05, Test Loss: 3.148314071682615e-05\n",
            "Epoch 889/1000, Training Loss: 3.288511220938022e-05, Test Loss: 3.1438127886373555e-05\n",
            "Epoch 890/1000, Training Loss: 3.284333801357884e-05, Test Loss: 3.1393228548680716e-05\n",
            "Epoch 891/1000, Training Loss: 3.28016653384078e-05, Test Loss: 3.134844224926047e-05\n",
            "Epoch 892/1000, Training Loss: 3.2760093768746014e-05, Test Loss: 3.1303768536533114e-05\n",
            "Epoch 893/1000, Training Loss: 3.271862289221281e-05, Test Loss: 3.12592069618044e-05\n",
            "Epoch 894/1000, Training Loss: 3.267725229914464e-05, Test Loss: 3.1214757079232757e-05\n",
            "Epoch 895/1000, Training Loss: 3.2635981582568145e-05, Test Loss: 3.117041844580541e-05\n",
            "Epoch 896/1000, Training Loss: 3.259481033817343e-05, Test Loss: 3.112619062131381e-05\n",
            "Epoch 897/1000, Training Loss: 3.255373816429145e-05, Test Loss: 3.108207316832327e-05\n",
            "Epoch 898/1000, Training Loss: 3.251276466186704e-05, Test Loss: 3.103806565215343e-05\n",
            "Epoch 899/1000, Training Loss: 3.24718894344367e-05, Test Loss: 3.0994167640848104e-05\n",
            "Epoch 900/1000, Training Loss: 3.243111208810169e-05, Test Loss: 3.095037870515222e-05\n",
            "Epoch 901/1000, Training Loss: 3.239043223150685e-05, Test Loss: 3.090669841848877e-05\n",
            "Epoch 902/1000, Training Loss: 3.234984947581524e-05, Test Loss: 3.086312635693034e-05\n",
            "Epoch 903/1000, Training Loss: 3.230936343468478e-05, Test Loss: 3.0819662099179794e-05\n",
            "Epoch 904/1000, Training Loss: 3.226897372424628e-05, Test Loss: 3.077630522654296e-05\n",
            "Epoch 905/1000, Training Loss: 3.222867996308036e-05, Test Loss: 3.073305532290729e-05\n",
            "Epoch 906/1000, Training Loss: 3.218848177219445e-05, Test Loss: 3.068991197471778e-05\n",
            "Epoch 907/1000, Training Loss: 3.214837877500036e-05, Test Loss: 3.064687477095349e-05\n",
            "Epoch 908/1000, Training Loss: 3.2108370597293066e-05, Test Loss: 3.060394330310585e-05\n",
            "Epoch 909/1000, Training Loss: 3.206845686722901e-05, Test Loss: 3.056111716515745e-05\n",
            "Epoch 910/1000, Training Loss: 3.2028637215303905e-05, Test Loss: 3.0518395953556165e-05\n",
            "Epoch 911/1000, Training Loss: 3.198891127433252e-05, Test Loss: 3.0475779267195496e-05\n",
            "Epoch 912/1000, Training Loss: 3.194927867942611e-05, Test Loss: 3.0433266707394947e-05\n",
            "Epoch 913/1000, Training Loss: 3.190973906797386e-05, Test Loss: 3.0390857877874995e-05\n",
            "Epoch 914/1000, Training Loss: 3.187029207962051e-05, Test Loss: 3.034855238473839e-05\n",
            "Epoch 915/1000, Training Loss: 3.183093735624743e-05, Test Loss: 3.0306349836447802e-05\n",
            "Epoch 916/1000, Training Loss: 3.1791674541952406e-05, Test Loss: 3.0264249843806028e-05\n",
            "Epoch 917/1000, Training Loss: 3.1752503283028354e-05, Test Loss: 3.022225201993648e-05\n",
            "Epoch 918/1000, Training Loss: 3.171342322794633e-05, Test Loss: 3.0180355980260425e-05\n",
            "Epoch 919/1000, Training Loss: 3.167443402733401e-05, Test Loss: 3.013856134247912e-05\n",
            "Epoch 920/1000, Training Loss: 3.1635535333957926e-05, Test Loss: 3.009686772655367e-05\n",
            "Epoch 921/1000, Training Loss: 3.159672680270408e-05, Test Loss: 3.005527475468564e-05\n",
            "Epoch 922/1000, Training Loss: 3.1558008090559234e-05, Test Loss: 3.0013782051296493e-05\n",
            "Epoch 923/1000, Training Loss: 3.151937885659287e-05, Test Loss: 2.9972389243009017e-05\n",
            "Epoch 924/1000, Training Loss: 3.1480838761938055e-05, Test Loss: 2.9931095958631556e-05\n",
            "Epoch 925/1000, Training Loss: 3.1442387469774935e-05, Test Loss: 2.988990182913458e-05\n",
            "Epoch 926/1000, Training Loss: 3.140402464531143e-05, Test Loss: 2.9848806487634406e-05\n",
            "Epoch 927/1000, Training Loss: 3.13657499557662e-05, Test Loss: 2.9807809569376524e-05\n",
            "Epoch 928/1000, Training Loss: 3.132756307035118e-05, Test Loss: 2.976691071171463e-05\n",
            "Epoch 929/1000, Training Loss: 3.128946366025466e-05, Test Loss: 2.9726109554094982e-05\n",
            "Epoch 930/1000, Training Loss: 3.1251451398623886e-05, Test Loss: 2.968540573803761e-05\n",
            "Epoch 931/1000, Training Loss: 3.121352596054781e-05, Test Loss: 2.9644798907119417e-05\n",
            "Epoch 932/1000, Training Loss: 3.117568702304163e-05, Test Loss: 2.9604288706956975e-05\n",
            "Epoch 933/1000, Training Loss: 3.113793426502924e-05, Test Loss: 2.9563874785188313e-05\n",
            "Epoch 934/1000, Training Loss: 3.110026736732692e-05, Test Loss: 2.9523556791458386e-05\n",
            "Epoch 935/1000, Training Loss: 3.1062686012628534e-05, Test Loss: 2.948333437740058e-05\n",
            "Epoch 936/1000, Training Loss: 3.102518988548748e-05, Test Loss: 2.944320719661978e-05\n",
            "Epoch 937/1000, Training Loss: 3.098777867230296e-05, Test Loss: 2.940317490467924e-05\n",
            "Epoch 938/1000, Training Loss: 3.095045206130329e-05, Test Loss: 2.9363237159078983e-05\n",
            "Epoch 939/1000, Training Loss: 3.0913209742531e-05, Test Loss: 2.9323393619244824e-05\n",
            "Epoch 940/1000, Training Loss: 3.08760514078266e-05, Test Loss: 2.9283643946510732e-05\n",
            "Epoch 941/1000, Training Loss: 3.083897675081472e-05, Test Loss: 2.9243987804102577e-05\n",
            "Epoch 942/1000, Training Loss: 3.080198546688847e-05, Test Loss: 2.9204424857123844e-05\n",
            "Epoch 943/1000, Training Loss: 3.076507725319553e-05, Test Loss: 2.916495477253837e-05\n",
            "Epoch 944/1000, Training Loss: 3.0728251808621956e-05, Test Loss: 2.9125577219159107e-05\n",
            "Epoch 945/1000, Training Loss: 3.0691508833779224e-05, Test Loss: 2.9086291867628606e-05\n",
            "Epoch 946/1000, Training Loss: 3.065484803098952e-05, Test Loss: 2.904709839040692e-05\n",
            "Epoch 947/1000, Training Loss: 3.061826910427131e-05, Test Loss: 2.900799646175674e-05\n",
            "Epoch 948/1000, Training Loss: 3.058177175932571e-05, Test Loss: 2.8968985757727122e-05\n",
            "Epoch 949/1000, Training Loss: 3.0545355703522165e-05, Test Loss: 2.8930065956141504e-05\n",
            "Epoch 950/1000, Training Loss: 3.0509020645885435e-05, Test Loss: 2.8891236736581298e-05\n",
            "Epoch 951/1000, Training Loss: 3.047276629708162e-05, Test Loss: 2.885249778037444e-05\n",
            "Epoch 952/1000, Training Loss: 3.0436592369405e-05, Test Loss: 2.881384877057981e-05\n",
            "Epoch 953/1000, Training Loss: 3.0400498576764825e-05, Test Loss: 2.8775289391972867e-05\n",
            "Epoch 954/1000, Training Loss: 3.0364484634670932e-05, Test Loss: 2.8736819331033186e-05\n",
            "Epoch 955/1000, Training Loss: 3.0328550260223642e-05, Test Loss: 2.86984382759313e-05\n",
            "Epoch 956/1000, Training Loss: 3.0292695172097795e-05, Test Loss: 2.866014591651377e-05\n",
            "Epoch 957/1000, Training Loss: 3.0256919090531996e-05, Test Loss: 2.8621941944291174e-05\n",
            "Epoch 958/1000, Training Loss: 3.022122173731519e-05, Test Loss: 2.8583826052426393e-05\n",
            "Epoch 959/1000, Training Loss: 3.0185602835774765e-05, Test Loss: 2.8545797935716527e-05\n",
            "Epoch 960/1000, Training Loss: 3.0150062110763734e-05, Test Loss: 2.850785729058732e-05\n",
            "Epoch 961/1000, Training Loss: 3.0114599288648464e-05, Test Loss: 2.8470003815075267e-05\n",
            "Epoch 962/1000, Training Loss: 3.0079214097297834e-05, Test Loss: 2.843223720881653e-05\n",
            "Epoch 963/1000, Training Loss: 3.00439062660697e-05, Test Loss: 2.839455717303577e-05\n",
            "Epoch 964/1000, Training Loss: 3.000867552580001e-05, Test Loss: 2.835696341053148e-05\n",
            "Epoch 965/1000, Training Loss: 2.9973521608790576e-05, Test Loss: 2.8319455625666473e-05\n",
            "Epoch 966/1000, Training Loss: 2.993844424879856e-05, Test Loss: 2.8282033524352373e-05\n",
            "Epoch 967/1000, Training Loss: 2.9903443181023446e-05, Test Loss: 2.8244696814043233e-05\n",
            "Epoch 968/1000, Training Loss: 2.9868518142097464e-05, Test Loss: 2.8207445203717045e-05\n",
            "Epoch 969/1000, Training Loss: 2.983366887007231e-05, Test Loss: 2.8170278403868945e-05\n",
            "Epoch 970/1000, Training Loss: 2.9798895104410265e-05, Test Loss: 2.813319612649844e-05\n",
            "Epoch 971/1000, Training Loss: 2.976419658597156e-05, Test Loss: 2.8096198085097197e-05\n",
            "Epoch 972/1000, Training Loss: 2.9729573057004125e-05, Test Loss: 2.805928399463782e-05\n",
            "Epoch 973/1000, Training Loss: 2.969502426113285e-05, Test Loss: 2.8022453571563558e-05\n",
            "Epoch 974/1000, Training Loss: 2.9660549943349005e-05, Test Loss: 2.798570653377668e-05\n",
            "Epoch 975/1000, Training Loss: 2.962614984999945e-05, Test Loss: 2.7949042600626466e-05\n",
            "Epoch 976/1000, Training Loss: 2.9591823728775804e-05, Test Loss: 2.791246149290132e-05\n",
            "Epoch 977/1000, Training Loss: 2.9557571328705173e-05, Test Loss: 2.787596293281267e-05\n",
            "Epoch 978/1000, Training Loss: 2.9523392400139212e-05, Test Loss: 2.783954664399082e-05\n",
            "Epoch 979/1000, Training Loss: 2.9489286694743322e-05, Test Loss: 2.7803212351470654e-05\n",
            "Epoch 980/1000, Training Loss: 2.9455253965488473e-05, Test Loss: 2.7766959781679793e-05\n",
            "Epoch 981/1000, Training Loss: 2.9421293966639515e-05, Test Loss: 2.773078866243268e-05\n",
            "Epoch 982/1000, Training Loss: 2.9387406453745943e-05, Test Loss: 2.7694698722915663e-05\n",
            "Epoch 983/1000, Training Loss: 2.9353591183632638e-05, Test Loss: 2.765868969368016e-05\n",
            "Epoch 984/1000, Training Loss: 2.931984791438913e-05, Test Loss: 2.7622761306631533e-05\n",
            "Epoch 985/1000, Training Loss: 2.9286176405361185e-05, Test Loss: 2.7586913295017707e-05\n",
            "Epoch 986/1000, Training Loss: 2.9252576417140978e-05, Test Loss: 2.7551145393422522e-05\n",
            "Epoch 987/1000, Training Loss: 2.9219047711556842e-05, Test Loss: 2.7515457337752548e-05\n",
            "Epoch 988/1000, Training Loss: 2.9185590051665415e-05, Test Loss: 2.7479848865229836e-05\n",
            "Epoch 989/1000, Training Loss: 2.9152203201741625e-05, Test Loss: 2.7444319714380543e-05\n",
            "Epoch 990/1000, Training Loss: 2.9118886927269917e-05, Test Loss: 2.740886962502733e-05\n",
            "Epoch 991/1000, Training Loss: 2.908564099493474e-05, Test Loss: 2.7373498338278016e-05\n",
            "Epoch 992/1000, Training Loss: 2.9052465172612197e-05, Test Loss: 2.7338205596517877e-05\n",
            "Epoch 993/1000, Training Loss: 2.9019359229361265e-05, Test Loss: 2.7302991143399816e-05\n",
            "Epoch 994/1000, Training Loss: 2.8986322935414107e-05, Test Loss: 2.726785472383579e-05\n",
            "Epoch 995/1000, Training Loss: 2.895335606216893e-05, Test Loss: 2.7232796083984374e-05\n",
            "Epoch 996/1000, Training Loss: 2.8920458382179813e-05, Test Loss: 2.719781497124738e-05\n",
            "Epoch 997/1000, Training Loss: 2.8887629669149313e-05, Test Loss: 2.7162911134258595e-05\n",
            "Epoch 998/1000, Training Loss: 2.8854869697919816e-05, Test Loss: 2.7128084322871915e-05\n",
            "Epoch 999/1000, Training Loss: 2.8822178244464896e-05, Test Loss: 2.7093334288157654e-05\n",
            "Epoch 1000/1000, Training Loss: 2.8789555085881372e-05, Test Loss: 2.7058660782390238e-05\n",
            "Epoch 1/1000, Training Loss: 0.0034513369956767482, Test Loss: 0.002329482078854715\n",
            "Epoch 2/1000, Training Loss: 0.003381704242849365, Test Loss: 0.002187583159142289\n",
            "Epoch 3/1000, Training Loss: 0.0033700151861245776, Test Loss: 0.002156533965758932\n",
            "Epoch 4/1000, Training Loss: 0.0033648508715158065, Test Loss: 0.002145749676148204\n",
            "Epoch 5/1000, Training Loss: 0.0033611414671825103, Test Loss: 0.002141143766262684\n",
            "Epoch 6/1000, Training Loss: 0.0033580085356778614, Test Loss: 0.0021388366377905464\n",
            "Epoch 7/1000, Training Loss: 0.003355223998414084, Test Loss: 0.0021374664549087194\n",
            "Epoch 8/1000, Training Loss: 0.003352698147964518, Test Loss: 0.0021364998823186774\n",
            "Epoch 9/1000, Training Loss: 0.003350381349322277, Test Loss: 0.002135716774046575\n",
            "Epoch 10/1000, Training Loss: 0.003348239709427062, Test Loss: 0.002135023066588988\n",
            "Epoch 11/1000, Training Loss: 0.003346247308443182, Test Loss: 0.0021343769850512436\n",
            "Epoch 12/1000, Training Loss: 0.0033443831191170893, Test Loss: 0.002133758874387231\n",
            "Epoch 13/1000, Training Loss: 0.003342629557797626, Test Loss: 0.002133158666122046\n",
            "Epoch 14/1000, Training Loss: 0.0033409716924656164, Test Loss: 0.00213257061658195\n",
            "Epoch 15/1000, Training Loss: 0.0033393967442215664, Test Loss: 0.002131991067256739\n",
            "Epoch 16/1000, Training Loss: 0.0033378937334608704, Test Loss: 0.0021314174672695217\n",
            "Epoch 17/1000, Training Loss: 0.00333645320620102, Test Loss: 0.0021308479266050405\n",
            "Epoch 18/1000, Training Loss: 0.003335067010965902, Test Loss: 0.002130280995718526\n",
            "Epoch 19/1000, Training Loss: 0.003333728111584675, Test Loss: 0.002129715544472599\n",
            "Epoch 20/1000, Training Loss: 0.0033324304278415377, Test Loss: 0.0021291506869789093\n",
            "Epoch 21/1000, Training Loss: 0.0033311686989258545, Test Loss: 0.0021285857295165918\n",
            "Epoch 22/1000, Training Loss: 0.0033299383661072497, Test Loss: 0.002128020131456334\n",
            "Epoch 23/1000, Training Loss: 0.0033287354718598756, Test Loss: 0.002127453474484721\n",
            "Epoch 24/1000, Training Loss: 0.0033275565731532676, Test Loss: 0.002126885437722626\n",
            "Epoch 25/1000, Training Loss: 0.0033263986669732075, Test Loss: 0.0021263157773525766\n",
            "Epoch 26/1000, Training Loss: 0.003325259126404363, Test Loss: 0.0021257443098527916\n",
            "Epoch 27/1000, Training Loss: 0.0033241356458281867, Test Loss: 0.0021251708981865195\n",
            "Epoch 28/1000, Training Loss: 0.003323026193979248, Test Loss: 0.0021245954404425695\n",
            "Epoch 29/1000, Training Loss: 0.0033219289737680246, Test Loss: 0.002124017860520358\n",
            "Epoch 30/1000, Training Loss: 0.003320842387922523, Test Loss: 0.0021234381005238666\n",
            "Epoch 31/1000, Training Loss: 0.00331976500962757, Test Loss: 0.002122856114584404\n",
            "Epoch 32/1000, Training Loss: 0.0033186955574514487, Test Loss: 0.002122271863876878\n",
            "Epoch 33/1000, Training Loss: 0.0033176328739464, Test Loss: 0.0021216853126314237\n",
            "Epoch 34/1000, Training Loss: 0.0033165759073940228, Test Loss: 0.0021210964249733528\n",
            "Epoch 35/1000, Training Loss: 0.003315523696240157, Test Loss: 0.002120505162450524\n",
            "Epoch 36/1000, Training Loss: 0.0033144753558276853, Test Loss: 0.0021199114821294066\n",
            "Epoch 37/1000, Training Loss: 0.00331343006709103, Test Loss: 0.0021193153351597846\n",
            "Epoch 38/1000, Training Loss: 0.003312387066923983, Test Loss: 0.00211871666572399\n",
            "Epoch 39/1000, Training Loss: 0.003311345639973825, Test Loss: 0.00211811541029993\n",
            "Epoch 40/1000, Training Loss: 0.0033103051116503135, Test Loss: 0.0021175114971786944\n",
            "Epoch 41/1000, Training Loss: 0.003309264842168744, Test Loss: 0.0021169048461870403\n",
            "Epoch 42/1000, Training Loss: 0.003308224221472692, Test Loss: 0.0021162953685734047\n",
            "Epoch 43/1000, Training Loss: 0.003307182664904602, Test Loss: 0.002115682967022889\n",
            "Epoch 44/1000, Training Loss: 0.003306139609511842, Test Loss: 0.002115067535772587\n",
            "Epoch 45/1000, Training Loss: 0.0033050945108924128, Test Loss: 0.002114448960803577\n",
            "Epoch 46/1000, Training Loss: 0.0033040468404987766, Test Loss: 0.002113827120090025\n",
            "Epoch 47/1000, Training Loss: 0.003302996083330371, Test Loss: 0.0021132018838894177\n",
            "Epoch 48/1000, Training Loss: 0.0033019417359558116, Test Loss: 0.002112573115060844\n",
            "Epoch 49/1000, Training Loss: 0.0033008833048146302, Test Loss: 0.0021119406694007766\n",
            "Epoch 50/1000, Training Loss: 0.003299820304755971, Test Loss: 0.0021113043959878354\n",
            "Epoch 51/1000, Training Loss: 0.0032987522577781142, Test Loss: 0.002110664137529789\n",
            "Epoch 52/1000, Training Loss: 0.003297678691938184, Test Loss: 0.0021100197307074623\n",
            "Epoch 53/1000, Training Loss: 0.0032965991404060803, Test Loss: 0.0021093710065114343\n",
            "Epoch 54/1000, Training Loss: 0.0032955131406406583, Test Loss: 0.0021087177905684306\n",
            "Epoch 55/1000, Training Loss: 0.0032944202336695487, Test Loss: 0.002108059903455065\n",
            "Epoch 56/1000, Training Loss: 0.0032933199634568853, Test Loss: 0.0021073971609973526\n",
            "Epoch 57/1000, Training Loss: 0.003292211876345676, Test Loss: 0.0021067293745548723\n",
            "Epoch 58/1000, Training Loss: 0.003291095520563578, Test Loss: 0.002106056351288963\n",
            "Epoch 59/1000, Training Loss: 0.0032899704457826405, Test Loss: 0.0021053778944146984\n",
            "Epoch 60/1000, Training Loss: 0.0032888362027250527, Test Loss: 0.0021046938034366192\n",
            "Epoch 61/1000, Training Loss: 0.003287692342808173, Test Loss: 0.002104003874368487\n",
            "Epoch 62/1000, Training Loss: 0.003286538417823238, Test Loss: 0.002103307899937471\n",
            "Epoch 63/1000, Training Loss: 0.003285373979643021, Test Loss: 0.0021026056697733228\n",
            "Epoch 64/1000, Training Loss: 0.0032841985799544633, Test Loss: 0.0021018969705831865\n",
            "Epoch 65/1000, Training Loss: 0.003283011770013016, Test Loss: 0.002101181586312783\n",
            "Epoch 66/1000, Training Loss: 0.0032818131004158945, Test Loss: 0.002100459298294732\n",
            "Epoch 67/1000, Training Loss: 0.0032806021208919866, Test Loss: 0.0020997298853848313\n",
            "Epoch 68/1000, Training Loss: 0.003279378380106508, Test Loss: 0.0020989931240871255\n",
            "Epoch 69/1000, Training Loss: 0.0032781414254788453, Test Loss: 0.002098248788668588\n",
            "Epoch 70/1000, Training Loss: 0.003276890803012314, Test Loss: 0.0020974966512642525\n",
            "Epoch 71/1000, Training Loss: 0.0032756260571347767, Test Loss: 0.002096736481973611\n",
            "Epoch 72/1000, Training Loss: 0.0032743467305493073, Test Loss: 0.002095968048949084\n",
            "Epoch 73/1000, Training Loss: 0.003273052364094211, Test Loss: 0.002095191118477335\n",
            "Epoch 74/1000, Training Loss: 0.003271742496611903, Test Loss: 0.0020944054550542097\n",
            "Epoch 75/1000, Training Loss: 0.003270416664826232, Test Loss: 0.002093610821454013\n",
            "Epoch 76/1000, Training Loss: 0.0032690744032279695, Test Loss: 0.0020928069787938394\n",
            "Epoch 77/1000, Training Loss: 0.0032677152439682623, Test Loss: 0.002091993686593642\n",
            "Epoch 78/1000, Training Loss: 0.0032663387167599163, Test Loss: 0.0020911707028326973\n",
            "Epoch 79/1000, Training Loss: 0.0032649443487864655, Test Loss: 0.0020903377840030663\n",
            "Epoch 80/1000, Training Loss: 0.0032635316646190114, Test Loss: 0.0020894946851606885\n",
            "Epoch 81/1000, Training Loss: 0.0032621001861408917, Test Loss: 0.0020886411599746616\n",
            "Epoch 82/1000, Training Loss: 0.0032606494324802517, Test Loss: 0.0020877769607752627\n",
            "Epoch 83/1000, Training Loss: 0.003259178919950659, Test Loss: 0.002086901838601248\n",
            "Epoch 84/1000, Training Loss: 0.0032576881619999056, Test Loss: 0.0020860155432469007\n",
            "Epoch 85/1000, Training Loss: 0.003256176669167177, Test Loss: 0.0020851178233093673\n",
            "Epoch 86/1000, Training Loss: 0.0032546439490488198, Test Loss: 0.0020842084262366915\n",
            "Epoch 87/1000, Training Loss: 0.0032530895062729027, Test Loss: 0.00208328709837702\n",
            "Epoch 88/1000, Training Loss: 0.0032515128424828415, Test Loss: 0.0020823535850293907\n",
            "Epoch 89/1000, Training Loss: 0.0032499134563303477, Test Loss: 0.0020814076304965213\n",
            "Epoch 90/1000, Training Loss: 0.0032482908434779702, Test Loss: 0.002080448978139994\n",
            "Epoch 91/1000, Training Loss: 0.003246644496611531, Test Loss: 0.002079477370438191\n",
            "Epoch 92/1000, Training Loss: 0.0032449739054627525, Test Loss: 0.0020784925490473735\n",
            "Epoch 93/1000, Training Loss: 0.003243278556842385, Test Loss: 0.002077494254866226\n",
            "Epoch 94/1000, Training Loss: 0.003241557934684166, Test Loss: 0.0020764822281042236\n",
            "Epoch 95/1000, Training Loss: 0.0032398115200999267, Test Loss: 0.0020754562083541393\n",
            "Epoch 96/1000, Training Loss: 0.0032380387914461906, Test Loss: 0.002074415934668987\n",
            "Epoch 97/1000, Training Loss: 0.0032362392244026034, Test Loss: 0.0020733611456437333\n",
            "Epoch 98/1000, Training Loss: 0.003234412292062524, Test Loss: 0.0020722915795020223\n",
            "Epoch 99/1000, Training Loss: 0.003232557465036152, Test Loss: 0.0020712069741882417\n",
            "Epoch 100/1000, Training Loss: 0.0032306742115665132, Test Loss: 0.002070107067465173\n",
            "Epoch 101/1000, Training Loss: 0.003228761997658676, Test Loss: 0.00206899159701747\n",
            "Epoch 102/1000, Training Loss: 0.0032268202872225327, Test Loss: 0.002067860300561232\n",
            "Epoch 103/1000, Training Loss: 0.0032248485422295137, Test Loss: 0.00206671291595992\n",
            "Epoch 104/1000, Training Loss: 0.0032228462228835676, Test Loss: 0.0020655491813468055\n",
            "Epoch 105/1000, Training Loss: 0.003220812787806762, Test Loss: 0.0020643688352542052\n",
            "Epoch 106/1000, Training Loss: 0.003218747694239844, Test Loss: 0.0020631716167496855\n",
            "Epoch 107/1000, Training Loss: 0.0032166503982581024, Test Loss: 0.002061957265579439\n",
            "Epoch 108/1000, Training Loss: 0.003214520355002839, Test Loss: 0.002060725522319008\n",
            "Epoch 109/1000, Training Loss: 0.003212357018928804, Test Loss: 0.002059476128531544\n",
            "Epoch 110/1000, Training Loss: 0.003210159844067884, Test Loss: 0.002058208826933747\n",
            "Epoch 111/1000, Training Loss: 0.003207928284309337, Test Loss: 0.002056923361569624\n",
            "Epoch 112/1000, Training Loss: 0.003205661793696892, Test Loss: 0.002055619477992246\n",
            "Epoch 113/1000, Training Loss: 0.0032033598267429565, Test Loss: 0.002054296923453537\n",
            "Epoch 114/1000, Training Loss: 0.003201021838760212, Test Loss: 0.00205295544710231\n",
            "Epoch 115/1000, Training Loss: 0.0031986472862108126, Test Loss: 0.0020515948001905374\n",
            "Epoch 116/1000, Training Loss: 0.0031962356270734404, Test Loss: 0.0020502147362879974\n",
            "Epoch 117/1000, Training Loss: 0.00319378632122838, Test Loss: 0.002048815011505318\n",
            "Epoch 118/1000, Training Loss: 0.0031912988308608094, Test Loss: 0.002047395384725473\n",
            "Epoch 119/1000, Training Loss: 0.0031887726208824374, Test Loss: 0.002045955617843743\n",
            "Epoch 120/1000, Training Loss: 0.0031862071593716118, Test Loss: 0.0020444954760161197\n",
            "Epoch 121/1000, Training Loss: 0.0031836019180319763, Test Loss: 0.00204301472791617\n",
            "Epoch 122/1000, Training Loss: 0.003180956372669723, Test Loss: 0.002041513146000272\n",
            "Epoch 123/1000, Training Loss: 0.0031782700036894463, Test Loss: 0.0020399905067811645\n",
            "Epoch 124/1000, Training Loss: 0.003175542296608555, Test Loss: 0.0020384465911097147\n",
            "Epoch 125/1000, Training Loss: 0.003172772742590174, Test Loss: 0.002036881184464756\n",
            "Epoch 126/1000, Training Loss: 0.003169960838994377, Test Loss: 0.002035294077250855\n",
            "Epoch 127/1000, Training Loss: 0.0031671060899475744, Test Loss: 0.0020336850651037973\n",
            "Epoch 128/1000, Training Loss: 0.0031642080069297935, Test Loss: 0.0020320539492035693\n",
            "Epoch 129/1000, Training Loss: 0.003161266109379539, Test Loss: 0.0020304005365945764\n",
            "Epoch 130/1000, Training Loss: 0.0031582799253158304, Test Loss: 0.0020287246405127734\n",
            "Epoch 131/1000, Training Loss: 0.003155248991976951, Test Loss: 0.0020270260807193634\n",
            "Epoch 132/1000, Training Loss: 0.0031521728564753637, Test Loss: 0.0020253046838406706\n",
            "Epoch 133/1000, Training Loss: 0.003149051076468133, Test Loss: 0.002023560283713741\n",
            "Epoch 134/1000, Training Loss: 0.003145883220842127, Test Loss: 0.002021792721737165\n",
            "Epoch 135/1000, Training Loss: 0.0031426688704131443, Test Loss: 0.002020001847226577\n",
            "Epoch 136/1000, Training Loss: 0.003139407618638013, Test Loss: 0.0020181875177742166\n",
            "Epoch 137/1000, Training Loss: 0.0031360990723385803, Test Loss: 0.0020163495996118625\n",
            "Epoch 138/1000, Training Loss: 0.0031327428524363984, Test Loss: 0.002014487967976413\n",
            "Epoch 139/1000, Training Loss: 0.003129338594696759, Test Loss: 0.002012602507477311\n",
            "Epoch 140/1000, Training Loss: 0.0031258859504806045, Test Loss: 0.0020106931124648886\n",
            "Epoch 141/1000, Training Loss: 0.0031223845875026648, Test Loss: 0.0020087596873987136\n",
            "Epoch 142/1000, Training Loss: 0.0031188341905940458, Test Loss: 0.0020068021472148642\n",
            "Epoch 143/1000, Training Loss: 0.0031152344624673085, Test Loss: 0.0020048204176910233\n",
            "Epoch 144/1000, Training Loss: 0.003111585124481876, Test Loss: 0.0020028144358081555\n",
            "Epoch 145/1000, Training Loss: 0.0031078859174074915, Test Loss: 0.002000784150107476\n",
            "Epoch 146/1000, Training Loss: 0.0031041366021831718, Test Loss: 0.0019987295210413107\n",
            "Epoch 147/1000, Training Loss: 0.003100336960668988, Test Loss: 0.001996650521316331\n",
            "Epoch 148/1000, Training Loss: 0.003096486796387721, Test Loss: 0.0019945471362275645\n",
            "Epoch 149/1000, Training Loss: 0.0030925859352533124, Test Loss: 0.0019924193639815034\n",
            "Epoch 150/1000, Training Loss: 0.003088634226282733, Test Loss: 0.001990267216006463\n",
            "Epoch 151/1000, Training Loss: 0.003084631542287737, Test Loss: 0.0019880907172483034\n",
            "Epoch 152/1000, Training Loss: 0.003080577780542727, Test Loss: 0.001985889906449494\n",
            "Epoch 153/1000, Training Loss: 0.003076472863424701, Test Loss: 0.001983664836409354\n",
            "Epoch 154/1000, Training Loss: 0.0030723167390210976, Test Loss: 0.0019814155742232933\n",
            "Epoch 155/1000, Training Loss: 0.0030681093817010643, Test Loss: 0.0019791422014986595\n",
            "Epoch 156/1000, Training Loss: 0.0030638507926454917, Test Loss: 0.001976844814544764\n",
            "Epoch 157/1000, Training Loss: 0.0030595410003309447, Test Loss: 0.001974523524534547\n",
            "Epoch 158/1000, Training Loss: 0.003055180060962403, Test Loss: 0.001972178457635243\n",
            "Epoch 159/1000, Training Loss: 0.003050768058849512, Test Loss: 0.0019698097551052642\n",
            "Epoch 160/1000, Training Loss: 0.0030463051067209064, Test Loss: 0.0019674175733545448\n",
            "Epoch 161/1000, Training Loss: 0.0030417913459709323, Test Loss: 0.0019650020839654028\n",
            "Epoch 162/1000, Training Loss: 0.003037226946832997, Test Loss: 0.0019625634736709376\n",
            "Epoch 163/1000, Training Loss: 0.0030326121084735913, Test Loss: 0.001960101944287955\n",
            "Epoch 164/1000, Training Loss: 0.0030279470590009296, Test Loss: 0.0019576177126013044\n",
            "Epoch 165/1000, Training Loss: 0.003023232055382086, Test Loss: 0.001955111010196525\n",
            "Epoch 166/1000, Training Loss: 0.0030184673832623766, Test Loss: 0.0019525820832375928\n",
            "Epoch 167/1000, Training Loss: 0.0030136533566807637, Test Loss: 0.0019500311921866464\n",
            "Epoch 168/1000, Training Loss: 0.0030087903176750086, Test Loss: 0.0019474586114624848\n",
            "Epoch 169/1000, Training Loss: 0.003003878635770345, Test Loss: 0.0019448646290346828\n",
            "Epoch 170/1000, Training Loss: 0.0029989187073455113, Test Loss: 0.0019422495459502048\n",
            "Epoch 171/1000, Training Loss: 0.002993910954870061, Test Loss: 0.0019396136757894354\n",
            "Epoch 172/1000, Training Loss: 0.0029888558260070352, Test Loss: 0.0019369573440486004\n",
            "Epoch 173/1000, Training Loss: 0.0029837537925752515, Test Loss: 0.001934280887445676\n",
            "Epoch 174/1000, Training Loss: 0.002978605349365684, Test Loss: 0.0019315846531469513\n",
            "Epoch 175/1000, Training Loss: 0.0029734110128066932, Test Loss: 0.0019288689979115436\n",
            "Epoch 176/1000, Training Loss: 0.002968171319473158, Test Loss: 0.0019261342871513295\n",
            "Epoch 177/1000, Training Loss: 0.0029628868244349345, Test Loss: 0.0019233808939038643\n",
            "Epoch 178/1000, Training Loss: 0.002957558099440459, Test Loss: 0.0019206091977161089\n",
            "Epoch 179/1000, Training Loss: 0.002952185730931749, Test Loss: 0.001917819583436926\n",
            "Epoch 180/1000, Training Loss: 0.0029467703178875566, Test Loss: 0.0019150124399165505\n",
            "Epoch 181/1000, Training Loss: 0.0029413124694919347, Test Loss: 0.0019121881586114404\n",
            "Epoch 182/1000, Training Loss: 0.0029358128026260332, Test Loss: 0.0019093471320931821\n",
            "Epoch 183/1000, Training Loss: 0.002930271939181559, Test Loss: 0.0019064897524603635\n",
            "Epoch 184/1000, Training Loss: 0.0029246905031949317, Test Loss: 0.001903616409652611\n",
            "Epoch 185/1000, Training Loss: 0.002919069117801806, Test Loss: 0.0019007274896661907\n",
            "Epoch 186/1000, Training Loss: 0.002913408402012344, Test Loss: 0.001897823372670964\n",
            "Epoch 187/1000, Training Loss: 0.002907708967308248, Test Loss: 0.0018949044310286362\n",
            "Epoch 188/1000, Training Loss: 0.0029019714140632847, Test Loss: 0.0018919710272126075\n",
            "Epoch 189/1000, Training Loss: 0.002896196327789747, Test Loss: 0.0018890235116299759\n",
            "Epoch 190/1000, Training Loss: 0.002890384275213941, Test Loss: 0.0018860622203464817\n",
            "Epoch 191/1000, Training Loss: 0.002884535800184523, Test Loss: 0.0018830874727154793\n",
            "Epoch 192/1000, Training Loss: 0.002878651419418153, Test Loss: 0.0018800995689122382\n",
            "Epoch 193/1000, Training Loss: 0.0028727316180875706, Test Loss: 0.0018770987873750937\n",
            "Epoch 194/1000, Training Loss: 0.002866776845257859, Test Loss: 0.0018740853821552176\n",
            "Epoch 195/1000, Training Loss: 0.002860787509177184, Test Loss: 0.001871059580176885\n",
            "Epoch 196/1000, Training Loss: 0.002854763972428908, Test Loss: 0.001868021578410348\n",
            "Epoch 197/1000, Training Loss: 0.0028487065469524207, Test Loss: 0.0018649715409595037\n",
            "Epoch 198/1000, Training Loss: 0.0028426154889405115, Test Loss: 0.0018619095960666898\n",
            "Epoch 199/1000, Training Loss: 0.002836490993621494, Test Loss: 0.001858835833036974\n",
            "Epoch 200/1000, Training Loss: 0.002830333189934633, Test Loss: 0.0018557502990843586\n",
            "Epoch 201/1000, Training Loss: 0.002824142135107698, Test Loss: 0.0018526529961023599\n",
            "Epoch 202/1000, Training Loss: 0.00281791780914571, Test Loss: 0.0018495438773613595\n",
            "Epoch 203/1000, Training Loss: 0.0028116601092400954, Test Loss: 0.0018464228441351061\n",
            "Epoch 204/1000, Training Loss: 0.0028053688441075737, Test Loss: 0.0018432897422586605\n",
            "Epoch 205/1000, Training Loss: 0.0027990437282681617, Test Loss: 0.0018401443586200015\n",
            "Epoch 206/1000, Training Loss: 0.00279268437627171, Test Loss: 0.0018369864175873636\n",
            "Epoch 207/1000, Training Loss: 0.0027862902968823038, Test Loss: 0.0018338155773742631\n",
            "Epoch 208/1000, Training Loss: 0.0027798608872298476, Test Loss: 0.0018306314263440604\n",
            "Epoch 209/1000, Training Loss: 0.0027733954269380436, Test Loss: 0.0018274334792557161\n",
            "Epoch 210/1000, Training Loss: 0.002766893072237867, Test Loss: 0.001824221173452334\n",
            "Epoch 211/1000, Training Loss: 0.002760352850075562, Test Loss: 0.0018209938649939377\n",
            "Epoch 212/1000, Training Loss: 0.002753773652224062, Test Loss: 0.0018177508247358308\n",
            "Epoch 213/1000, Training Loss: 0.0027471542294067387, Test Loss: 0.0018144912343539502\n",
            "Epoch 214/1000, Training Loss: 0.0027404931854423226, Test Loss: 0.0018112141823185123\n",
            "Epoch 215/1000, Training Loss: 0.002733788971419918, Test Loss: 0.0018079186598174465\n",
            "Epoch 216/1000, Training Loss: 0.002727039879913178, Test Loss: 0.0018046035566312258\n",
            "Epoch 217/1000, Training Loss: 0.0027202440392429337, Test Loss: 0.00180126765696101\n",
            "Epoch 218/1000, Training Loss: 0.0027133994077979413, Test Loss: 0.0017979096352124138\n",
            "Epoch 219/1000, Training Loss: 0.0027065037684239146, Test Loss: 0.0017945280517377316\n",
            "Epoch 220/1000, Training Loss: 0.0026995547228916695, Test Loss: 0.0017911213485401803\n",
            "Epoch 221/1000, Training Loss: 0.0026925496864560507, Test Loss: 0.0017876878449445601\n",
            "Epoch 222/1000, Training Loss: 0.0026854858825183753, Test Loss: 0.0017842257332398692\n",
            "Epoch 223/1000, Training Loss: 0.002678360337406348, Test Loss: 0.0017807330743006792\n",
            "Epoch 224/1000, Training Loss: 0.0026711698752869295, Test Loss: 0.001777207793195599\n",
            "Epoch 225/1000, Training Loss: 0.0026639111132293635, Test Loss: 0.0017736476747931024\n",
            "Epoch 226/1000, Training Loss: 0.0026565804564375505, Test Loss: 0.0017700503593769592\n",
            "Epoch 227/1000, Training Loss: 0.00264917409367325, Test Loss: 0.0017664133382860844\n",
            "Epoch 228/1000, Training Loss: 0.0026416879928940606, Test Loss: 0.0017627339495962474\n",
            "Epoch 229/1000, Training Loss: 0.0026341178971330324, Test Loss: 0.0017590093738644131\n",
            "Epoch 230/1000, Training Loss: 0.002626459320649719, Test Loss: 0.0017552366299597372\n",
            "Epoch 231/1000, Training Loss: 0.0026187075453859275, Test Loss: 0.001751412571009356\n",
            "Epoch 232/1000, Training Loss: 0.0026108576177629574, Test Loss: 0.0017475338804912599\n",
            "Epoch 233/1000, Training Loss: 0.0026029043458609684, Test Loss: 0.0017435970685112758\n",
            "Epoch 234/1000, Training Loss: 0.0025948422970252117, Test Loss: 0.0017395984683063383\n",
            "Epoch 235/1000, Training Loss: 0.0025866657959480403, Test Loss: 0.001735534233021601\n",
            "Epoch 236/1000, Training Loss: 0.0025783689232801643, Test Loss: 0.001731400332815009\n",
            "Epoch 237/1000, Training Loss: 0.002569945514829085, Test Loss: 0.0017271925523490307\n",
            "Epoch 238/1000, Training Loss: 0.0025613891614074446, Test Loss: 0.0017229064887359967\n",
            "Epoch 239/1000, Training Loss: 0.002552693209398799, Test Loss: 0.0017185375500102604\n",
            "Epoch 240/1000, Training Loss: 0.0025438507621132382, Test Loss: 0.0017140809542075894\n",
            "Epoch 241/1000, Training Loss: 0.002534854682010217, Test Loss: 0.0017095317291394353\n",
            "Epoch 242/1000, Training Loss: 0.0025256975938710634, Test Loss: 0.0017048847129571634\n",
            "Epoch 243/1000, Training Loss: 0.002516371889008688, Test Loss: 0.0017001345556086054\n",
            "Epoch 244/1000, Training Loss: 0.00250686973060729, Test Loss: 0.0016952757212965975\n",
            "Epoch 245/1000, Training Loss: 0.002497183060290261, Test Loss: 0.0016903024920561101\n",
            "Epoch 246/1000, Training Loss: 0.0024873036060200464, Test Loss: 0.0016852089725731445\n",
            "Epoch 247/1000, Training Loss: 0.002477222891439682, Test Loss: 0.0016799890963745186\n",
            "Epoch 248/1000, Training Loss: 0.002466932246771957, Test Loss: 0.0016746366335229081\n",
            "Epoch 249/1000, Training Loss: 0.002456422821398965, Test Loss: 0.0016691451999556834\n",
            "Epoch 250/1000, Training Loss: 0.002445685598252077, Test Loss: 0.0016635082686089942\n",
            "Epoch 251/1000, Training Loss: 0.002434711410150272, Test Loss: 0.0016577191824700523\n",
            "Epoch 252/1000, Training Loss: 0.002423490958233292, Test Loss: 0.0016517711696999173\n",
            "Epoch 253/1000, Training Loss: 0.002412014832644988, Test Loss: 0.0016456573609665113\n",
            "Epoch 254/1000, Training Loss: 0.0024002735356314707, Test Loss: 0.0016393708091219502\n",
            "Epoch 255/1000, Training Loss: 0.0023882575072276574, Test Loss: 0.0016329045113495611\n",
            "Epoch 256/1000, Training Loss: 0.0023759571537140655, Test Loss: 0.001626251433893274\n",
            "Epoch 257/1000, Training Loss: 0.0023633628790320884, Test Loss: 0.001619404539464693\n",
            "Epoch 258/1000, Training Loss: 0.0023504651193495298, Test Loss: 0.0016123568174003444\n",
            "Epoch 259/1000, Training Loss: 0.0023372543809672113, Test Loss: 0.0016051013166124722\n",
            "Epoch 260/1000, Training Loss: 0.002323721281749934, Test Loss: 0.0015976311813400423\n",
            "Epoch 261/1000, Training Loss: 0.002309856596249128, Test Loss: 0.0015899396896618223\n",
            "Epoch 262/1000, Training Loss: 0.002295651304657383, Test Loss: 0.0015820202946792757\n",
            "Epoch 263/1000, Training Loss: 0.0022810966456944227, Test Loss: 0.0015738666682129125\n",
            "Epoch 264/1000, Training Loss: 0.0022661841734673404, Test Loss: 0.0015654727467815553\n",
            "Epoch 265/1000, Training Loss: 0.0022509058182730675, Test Loss: 0.0015568327795497166\n",
            "Epoch 266/1000, Training Loss: 0.0022352539512164337, Test Loss: 0.0015479413778349716\n",
            "Epoch 267/1000, Training Loss: 0.002219221452402337, Test Loss: 0.001538793565666895\n",
            "Epoch 268/1000, Training Loss: 0.0022028017823260953, Test Loss: 0.001529384830784814\n",
            "Epoch 269/1000, Training Loss: 0.0021859890559344174, Test Loss: 0.00151971117535774\n",
            "Epoch 270/1000, Training Loss: 0.0021687781186647357, Test Loss: 0.0015097691656122793\n",
            "Epoch 271/1000, Training Loss: 0.002151164623599247, Test Loss: 0.0014995559794696556\n",
            "Epoch 272/1000, Training Loss: 0.0021331451087001027, Test Loss: 0.001489069451229682\n",
            "Epoch 273/1000, Training Loss: 0.002114717072933763, Test Loss: 0.0014783081123051595\n",
            "Epoch 274/1000, Training Loss: 0.002095879049957117, Test Loss: 0.0014672712270135226\n",
            "Epoch 275/1000, Training Loss: 0.002076630677937083, Test Loss: 0.0014559588224797519\n",
            "Epoch 276/1000, Training Loss: 0.002056972764020833, Test Loss: 0.0014443717118008742\n",
            "Epoch 277/1000, Training Loss: 0.0020369073419750513, Test Loss: 0.0014325115097691121\n",
            "Epoch 278/1000, Training Loss: 0.0020164377215767747, Test Loss: 0.0014203806406455524\n",
            "Epoch 279/1000, Training Loss: 0.0019955685284687367, Test Loss: 0.001407982337712867\n",
            "Epoch 280/1000, Training Loss: 0.0019743057333862658, Test Loss: 0.00139532063460219\n",
            "Epoch 281/1000, Training Loss: 0.001952656669914502, Test Loss: 0.001382400348671631\n",
            "Epoch 282/1000, Training Loss: 0.0019306300402309163, Test Loss: 0.0013692270569930606\n",
            "Epoch 283/1000, Training Loss: 0.0019082359086126726, Test Loss: 0.0013558070657612444\n",
            "Epoch 284/1000, Training Loss: 0.0018854856828209954, Test Loss: 0.001342147374155494\n",
            "Epoch 285/1000, Training Loss: 0.0018623920837943912, Test Loss: 0.0013282556338430535\n",
            "Epoch 286/1000, Training Loss: 0.001838969104368484, Test Loss: 0.0013141401054025312\n",
            "Epoch 287/1000, Training Loss: 0.0018152319579750283, Test Loss: 0.0012998096129593843\n",
            "Epoch 288/1000, Training Loss: 0.0017911970184428206, Test Loss: 0.0012852734982627156\n",
            "Epoch 289/1000, Training Loss: 0.001766881752122073, Test Loss: 0.0012705415753006641\n",
            "Epoch 290/1000, Training Loss: 0.0017423046435804202, Test Loss: 0.0012556240863620693\n",
            "Epoch 291/1000, Training Loss: 0.0017174851160790393, Test Loss: 0.0012405316602213824\n",
            "Epoch 292/1000, Training Loss: 0.0016924434479424243, Test Loss: 0.0012252752728710299\n",
            "Epoch 293/1000, Training Loss: 0.0016672006858002048, Test Loss: 0.001209866210969868\n",
            "Epoch 294/1000, Training Loss: 0.0016417785555216157, Test Loss: 0.0011943160379370664\n",
            "Epoch 295/1000, Training Loss: 0.0016161993714994983, Test Loss: 0.0011786365624124563\n",
            "Epoch 296/1000, Training Loss: 0.001590485944787855, Test Loss: 0.0011628398086401576\n",
            "Epoch 297/1000, Training Loss: 0.0015646614904673152, Test Loss: 0.0011469379882186944\n",
            "Epoch 298/1000, Training Loss: 0.0015387495345159912, Test Loss: 0.0011309434726015505\n",
            "Epoch 299/1000, Training Loss: 0.0015127738204042047, Test Loss: 0.001114868765725797\n",
            "Epoch 300/1000, Training Loss: 0.0014867582156109316, Test Loss: 0.0010987264761884198\n",
            "Epoch 301/1000, Training Loss: 0.0014607266182753722, Test Loss: 0.0010825292884727989\n",
            "Epoch 302/1000, Training Loss: 0.001434702864242293, Test Loss: 0.0010662899328422542\n",
            "Epoch 303/1000, Training Loss: 0.0014087106348274385, Test Loss: 0.0010500211536532556\n",
            "Epoch 304/1000, Training Loss: 0.0013827733657097501, Test Loss: 0.0010337356759872816\n",
            "Epoch 305/1000, Training Loss: 0.0013569141574416232, Test Loss: 0.0010174461706479506\n",
            "Epoch 306/1000, Training Loss: 0.0013311556881472392, Test Loss: 0.001001165217709156\n",
            "Epoch 307/1000, Training Loss: 0.0013055201290454236, Test Loss: 0.0009849052689243133\n",
            "Epoch 308/1000, Training Loss: 0.0012800290634803746, Test Loss: 0.0009686786094096083\n",
            "Epoch 309/1000, Training Loss: 0.0012547034101670958, Test Loss: 0.0009524973190923484\n",
            "Epoch 310/1000, Training Loss: 0.0012295633513559193, Test Loss: 0.0009363732344664102\n",
            "Epoch 311/1000, Training Loss: 0.0012046282665913715, Test Loss: 0.0009203179112203574\n",
            "Epoch 312/1000, Training Loss: 0.0011799166726861846, Test Loss: 0.000904342588301009\n",
            "Epoch 313/1000, Training Loss: 0.0011554461704536712, Test Loss: 0.0008884581539481789\n",
            "Epoch 314/1000, Training Loss: 0.0011312333986452073, Test Loss: 0.0008726751141888905\n",
            "Epoch 315/1000, Training Loss: 0.0011072939954281547, Test Loss: 0.0008570035642149183\n",
            "Epoch 316/1000, Training Loss: 0.0010836425676184899, Test Loss: 0.0008414531629909079\n",
            "Epoch 317/1000, Training Loss: 0.0010602926677568357, Test Loss: 0.0008260331113559827\n",
            "Epoch 318/1000, Training Loss: 0.001037256778990988, Test Loss: 0.0008107521337937517\n",
            "Epoch 319/1000, Training Loss: 0.001014546307607488, Test Loss: 0.0007956184639584942\n",
            "Epoch 320/1000, Training Loss: 0.0009921715829427325, Test Loss: 0.0007806398339620386\n",
            "Epoch 321/1000, Training Loss: 0.0009701418643039353, Test Loss: 0.000765823467349642\n",
            "Epoch 322/1000, Training Loss: 0.0009484653544443975, Test Loss: 0.000751176075626127\n",
            "Epoch 323/1000, Training Loss: 0.000927149219067194, Test Loss: 0.000736703858136973\n",
            "Epoch 324/1000, Training Loss: 0.0009061996117779888, Test Loss: 0.0007224125050640779\n",
            "Epoch 325/1000, Training Loss: 0.0008856217038704647, Test Loss: 0.0007083072032623408\n",
            "Epoch 326/1000, Training Loss: 0.0008654197183069955, Test Loss: 0.0006943926446409933\n",
            "Epoch 327/1000, Training Loss: 0.0008455969672509995, Test Loss: 0.000680673036781727\n",
            "Epoch 328/1000, Training Loss: 0.00082615589251498, Test Loss: 0.0006671521154832916\n",
            "Epoch 329/1000, Training Loss: 0.0008070981083071128, Test Loss: 0.0006538331589275948\n",
            "Epoch 330/1000, Training Loss: 0.0007884244456882745, Test Loss: 0.0006407190031746181\n",
            "Epoch 331/1000, Training Loss: 0.0007701349981880382, Test Loss: 0.0006278120587106641\n",
            "Epoch 332/1000, Training Loss: 0.000752229168070686, Test Loss: 0.0006151143277954111\n",
            "Epoch 333/1000, Training Loss: 0.0007347057127889165, Test Loss: 0.0006026274223767546\n",
            "Epoch 334/1000, Training Loss: 0.0007175627912118757, Test Loss: 0.0005903525823671232\n",
            "Epoch 335/1000, Training Loss: 0.0007007980092639318, Test Loss: 0.0005782906940998636\n",
            "Epoch 336/1000, Training Loss: 0.0006844084646601495, Test Loss: 0.0005664423088090111\n",
            "Epoch 337/1000, Training Loss: 0.0006683907904722821, Test Loss: 0.0005548076609988758\n",
            "Epoch 338/1000, Training Loss: 0.0006527411973049863, Test Loss: 0.0005433866865919778\n",
            "Epoch 339/1000, Training Loss: 0.000637455513904551, Test Loss: 0.0005321790407633956\n",
            "Epoch 340/1000, Training Loss: 0.0006225292260621308, Test Loss: 0.000521184115387781\n",
            "Epoch 341/1000, Training Loss: 0.00060795751370914, Test Loss: 0.0005104010560407401\n",
            "Epoch 342/1000, Training Loss: 0.0005937352861347041, Test Loss: 0.0004998287785100374\n",
            "Epoch 343/1000, Training Loss: 0.0005798572152833252, Test Loss: 0.0004894659847836099\n",
            "Epoch 344/1000, Training Loss: 0.0005663177671157111, Test Loss: 0.00047931117849122013\n",
            "Epoch 345/1000, Training Loss: 0.0005531112310368198, Test Loss: 0.0004693626797845399\n",
            "Epoch 346/1000, Training Loss: 0.0005402317474131375, Test Loss: 0.00045961863964713313\n",
            "Epoch 347/1000, Training Loss: 0.0005276733332160857, Test Loss: 0.0004500770536310783\n",
            "Epoch 348/1000, Training Loss: 0.0005154299058404961, Test Loss: 0.00044073577502110024\n",
            "Epoch 349/1000, Training Loss: 0.0005034953051567355, Test Loss: 0.00043159252743039977\n",
            "Epoch 350/1000, Training Loss: 0.0004918633138624836, Test Loss: 0.0004226449168348489\n",
            "Epoch 351/1000, Training Loss: 0.0004805276762056411, Test Loss: 0.0004138904430541477\n",
            "Epoch 352/1000, Training Loss: 0.0004694821151535648, Test Loss: 0.0004053265106898497\n",
            "Epoch 353/1000, Training Loss: 0.0004587203480862993, Test Loss: 0.0003969504395313417\n",
            "Epoch 354/1000, Training Loss: 0.000448236101092547, Test Loss: 0.0003887594744415221\n",
            "Epoch 355/1000, Training Loss: 0.00043802312194725704, Test Loss: 0.000380750794734485\n",
            "Epoch 356/1000, Training Loss: 0.00042807519184901387, Test Loss: 0.00037292152305792555\n",
            "Epoch 357/1000, Training Loss: 0.00041838613599407557, Test Loss: 0.0003652687337933031\n",
            "Epoch 358/1000, Training Loss: 0.0004089498330619047, Test Loss: 0.00035778946098693484\n",
            "Epoch 359/1000, Training Loss: 0.00039976022368482076, Test Loss: 0.00035048070582546743\n",
            "Epoch 360/1000, Training Loss: 0.00039081131797164855, Test Loss: 0.00034333944366921987\n",
            "Epoch 361/1000, Training Loss: 0.00038209720215247426, Test Loss: 0.0003363626306570834\n",
            "Epoch 362/1000, Training Loss: 0.0003736120444085198, Test Loss: 0.00032954720989669374\n",
            "Epoch 363/1000, Training Loss: 0.00036535009994808227, Test Loss: 0.00032289011725373325\n",
            "Epoch 364/1000, Training Loss: 0.0003573057153863687, Test Loss: 0.0003163882867542832\n",
            "Epoch 365/1000, Training Loss: 0.0003494733324838107, Test Loss: 0.00031003865561414896\n",
            "Epoch 366/1000, Training Loss: 0.00034184749129445905, Test Loss: 0.0003038381689091807\n",
            "Epoch 367/1000, Training Loss: 0.00033442283277292746, Test Loss: 0.0002977837839006009\n",
            "Epoch 368/1000, Training Loss: 0.00032719410088538343, Test Loss: 0.0002918724740292916\n",
            "Epoch 369/1000, Training Loss: 0.00032015614426728593, Test Loss: 0.00028610123259303904\n",
            "Epoch 370/1000, Training Loss: 0.00031330391746772155, Test Loss: 0.0002804670761205805\n",
            "Epoch 371/1000, Training Loss: 0.0003066324818175847, Test Loss: 0.0002749670474562461\n",
            "Epoch 372/1000, Training Loss: 0.0003001370059562894, Test Loss: 0.00026959821856879963\n",
            "Epoch 373/1000, Training Loss: 0.0002938127660493388, Test Loss: 0.0002643576930979954\n",
            "Epoch 374/1000, Training Loss: 0.0002876551457267416, Test Loss: 0.00025924260865208546\n",
            "Epoch 375/1000, Training Loss: 0.0002816596357700947, Test Loss: 0.0002542501388693065\n",
            "Epoch 376/1000, Training Loss: 0.00027582183357418964, Test Loss: 0.0002493774952561644\n",
            "Epoch 377/1000, Training Loss: 0.00027013744240695123, Test Loss: 0.0002446219288149536\n",
            "Epoch 378/1000, Training Loss: 0.0002646022704898105, Test Loss: 0.0002399807314727289\n",
            "Epoch 379/1000, Training Loss: 0.00025921222991883444, Test Loss: 0.00023545123732353305\n",
            "Epoch 380/1000, Training Loss: 0.000253963335445426, Test Loss: 0.00023103082369542764\n",
            "Epoch 381/1000, Training Loss: 0.0002488517031337822, Test Loss: 0.00022671691205336118\n",
            "Epoch 382/1000, Training Loss: 0.00024387354891102918, Test Loss: 0.0002225069687486725\n",
            "Epoch 383/1000, Training Loss: 0.00023902518702460053, Test Loss: 0.0002183985056255508\n",
            "Epoch 384/1000, Training Loss: 0.00023430302842014693, Test Loss: 0.0002143890804943594\n",
            "Epoch 385/1000, Training Loss: 0.00022970357905224072, Test Loss: 0.00021047629748138387\n",
            "Epoch 386/1000, Training Loss: 0.00022522343813898888, Test Loss: 0.00020665780726408643\n",
            "Epoch 387/1000, Training Loss: 0.0002208592963707294, Test Loss: 0.0002029313072005743\n",
            "Epoch 388/1000, Training Loss: 0.00021660793408207515, Test Loss: 0.00019929454136158247\n",
            "Epoch 389/1000, Training Loss: 0.00021246621939569144, Test Loss: 0.0001957453004728154\n",
            "Epoch 390/1000, Training Loss: 0.0002084311063454574, Test Loss: 0.00019228142177516754\n",
            "Epoch 391/1000, Training Loss: 0.000204499632985876, Test Loss: 0.0001889007888098546\n",
            "Epoch 392/1000, Training Loss: 0.00020066891949398078, Test Loss: 0.00018560133113518478\n",
            "Epoch 393/1000, Training Loss: 0.0001969361662693303, Test Loss: 0.0001823810239812615\n",
            "Epoch 394/1000, Training Loss: 0.00019329865203711872, Test Loss: 0.0001792378878485677\n",
            "Epoch 395/1000, Training Loss: 0.00018975373195889174, Test Loss: 0.0001761699880560044\n",
            "Epoch 396/1000, Training Loss: 0.00018629883575493653, Test Loss: 0.00017317543424368213\n",
            "Epoch 397/1000, Training Loss: 0.0001829314658418056, Test Loss: 0.00017025237983527022\n",
            "Epoch 398/1000, Training Loss: 0.00017964919548824973, Test Loss: 0.0001673990214646152\n",
            "Epoch 399/1000, Training Loss: 0.00017644966699223254, Test Loss: 0.00016461359837081116\n",
            "Epoch 400/1000, Training Loss: 0.00017333058988153025, Test Loss: 0.00016189439176577988\n",
            "Epoch 401/1000, Training Loss: 0.0001702897391399781, Test Loss: 0.00015923972417803084\n",
            "Epoch 402/1000, Training Loss: 0.00016732495346121506, Test Loss: 0.00015664795877606854\n",
            "Epoch 403/1000, Training Loss: 0.00016443413353147223, Test Loss: 0.00015411749867462525\n",
            "Epoch 404/1000, Training Loss: 0.0001616152403427285, Test Loss: 0.00015164678622668054\n",
            "Epoch 405/1000, Training Loss: 0.0001588662935373094, Test Loss: 0.00014923430230397183\n",
            "Epoch 406/1000, Training Loss: 0.0001561853697848508, Test Loss: 0.0001468785655685273\n",
            "Epoch 407/1000, Training Loss: 0.00015357060119232933, Test Loss: 0.00014457813173750924\n",
            "Epoch 408/1000, Training Loss: 0.00015102017374770803, Test Loss: 0.00014233159284348957\n",
            "Epoch 409/1000, Training Loss: 0.00014853232579760788, Test Loss: 0.0001401375764920901\n",
            "Epoch 410/1000, Training Loss: 0.00014610534655926783, Test Loss: 0.0001379947451187528\n",
            "Epoch 411/1000, Training Loss: 0.00014373757466694425, Test Loss: 0.00013590179524624864\n",
            "Epoch 412/1000, Training Loss: 0.00014142739675279407, Test Loss: 0.0001338574567443839\n",
            "Epoch 413/1000, Training Loss: 0.00013917324606218043, Test Loss: 0.00013186049209323037\n",
            "Epoch 414/1000, Training Loss: 0.00013697360110325734, Test Loss: 0.00012990969565105734\n",
            "Epoch 415/1000, Training Loss: 0.00013482698433062878, Test Loss: 0.00012800389292806023\n",
            "Epoch 416/1000, Training Loss: 0.00013273196086277883, Test Loss: 0.00012614193986682442\n",
            "Epoch 417/1000, Training Loss: 0.0001306871372329392, Test Loss: 0.0001243227221303939\n",
            "Epoch 418/1000, Training Loss: 0.0001286911601730014, Test Loss: 0.0001225451543987078\n",
            "Epoch 419/1000, Training Loss: 0.00012674271543000457, Test Loss: 0.0001208081796740514\n",
            "Epoch 420/1000, Training Loss: 0.0001248405266147508, Test Loss: 0.00011911076859614268\n",
            "Epoch 421/1000, Training Loss: 0.00012298335408200816, Test Loss: 0.00011745191876734333\n",
            "Epoch 422/1000, Training Loss: 0.00012116999384176715, Test Loss: 0.00011583065408844345\n",
            "Epoch 423/1000, Training Loss: 0.00011939927650097979, Test Loss: 0.00011424602410539672\n",
            "Epoch 424/1000, Training Loss: 0.00011767006623518234, Test Loss: 0.00011269710336731186\n",
            "Epoch 425/1000, Training Loss: 0.00011598125978942521, Test Loss: 0.00011118299079599036\n",
            "Epoch 426/1000, Training Loss: 0.00011433178550784546, Test Loss: 0.00010970280906717593\n",
            "Epoch 427/1000, Training Loss: 0.00011272060239130667, Test Loss: 0.00010825570400373103\n",
            "Epoch 428/1000, Training Loss: 0.00011114669918244225, Test Loss: 0.00010684084398083424\n",
            "Epoch 429/1000, Training Loss: 0.00010960909347746518, Test Loss: 0.00010545741934329016\n",
            "Epoch 430/1000, Training Loss: 0.0001081068308641122, Test Loss: 0.00010410464183500395\n",
            "Epoch 431/1000, Training Loss: 0.0001066389840850767, Test Loss: 0.00010278174404063825\n",
            "Epoch 432/1000, Training Loss: 0.00010520465222630307, Test Loss: 0.00010148797883945307\n",
            "Epoch 433/1000, Training Loss: 0.0001038029599294893, Test Loss: 0.00010022261887127641\n",
            "Epoch 434/1000, Training Loss: 0.00010243305662818287, Test Loss: 9.898495601456139e-05\n",
            "Epoch 435/1000, Training Loss: 0.00010109411580684204, Test Loss: 9.7774300876448e-05\n",
            "Epoch 436/1000, Training Loss: 9.97853342822366e-05, Test Loss: 9.658998229472435e-05\n",
            "Epoch 437/1000, Training Loss: 9.85059315065949e-05, Test Loss: 9.543134685158707e-05\n",
            "Epoch 438/1000, Training Loss: 9.725514889188068e-05, Test Loss: 9.42977583990558e-05\n",
            "Epoch 439/1000, Training Loss: 9.603224915461891e-05, Test Loss: 9.318859759591603e-05\n",
            "Epoch 440/1000, Training Loss: 9.483651568069011e-05, Test Loss: 9.210326145602745e-05\n",
            "Epoch 441/1000, Training Loss: 9.366725190951373e-05, Test Loss: 9.104116290783396e-05\n",
            "Epoch 442/1000, Training Loss: 9.252378073707487e-05, Test Loss: 9.000173036491335e-05\n",
            "Epoch 443/1000, Training Loss: 9.14054439372301e-05, Test Loss: 8.898440730737159e-05\n",
            "Epoch 444/1000, Training Loss: 9.031160160077729e-05, Test Loss: 8.798865187391945e-05\n",
            "Epoch 445/1000, Training Loss: 8.924163159174904e-05, Test Loss: 8.701393646442303e-05\n",
            "Epoch 446/1000, Training Loss: 8.819492902041464e-05, Test Loss: 8.605974735273127e-05\n",
            "Epoch 447/1000, Training Loss: 8.717090573251055e-05, Test Loss: 8.512558430960557e-05\n",
            "Epoch 448/1000, Training Loss: 8.61689898141885e-05, Test Loss: 8.42109602355251e-05\n",
            "Epoch 449/1000, Training Loss: 8.518862511221732e-05, Test Loss: 8.331540080317981e-05\n",
            "Epoch 450/1000, Training Loss: 8.42292707689767e-05, Test Loss: 8.243844410944857e-05\n",
            "Epoch 451/1000, Training Loss: 8.329040077178276e-05, Test Loss: 8.157964033665149e-05\n",
            "Epoch 452/1000, Training Loss: 8.237150351610852e-05, Test Loss: 8.073855142287547e-05\n",
            "Epoch 453/1000, Training Loss: 8.147208138227533e-05, Test Loss: 7.991475074117063e-05\n",
            "Epoch 454/1000, Training Loss: 8.05916503251943e-05, Test Loss: 7.910782278741477e-05\n",
            "Epoch 455/1000, Training Loss: 7.972973947675391e-05, Test Loss: 7.831736287664082e-05\n",
            "Epoch 456/1000, Training Loss: 7.888589076046177e-05, Test Loss: 7.75429768476299e-05\n",
            "Epoch 457/1000, Training Loss: 7.805965851795638e-05, Test Loss: 7.678428077557053e-05\n",
            "Epoch 458/1000, Training Loss: 7.725060914701693e-05, Test Loss: 7.604090069258833e-05\n",
            "Epoch 459/1000, Training Loss: 7.64583207507198e-05, Test Loss: 7.531247231595462e-05\n",
            "Epoch 460/1000, Training Loss: 7.568238279737791e-05, Test Loss: 7.459864078377698e-05\n",
            "Epoch 461/1000, Training Loss: 7.492239579093037e-05, Test Loss: 7.389906039798518e-05\n",
            "Epoch 462/1000, Training Loss: 7.41779709514571e-05, Test Loss: 7.32133943744303e-05\n",
            "Epoch 463/1000, Training Loss: 7.344872990549079e-05, Test Loss: 7.254131459990951e-05\n",
            "Epoch 464/1000, Training Loss: 7.27343043858268e-05, Test Loss: 7.188250139593999e-05\n",
            "Epoch 465/1000, Training Loss: 7.20343359405207e-05, Test Loss: 7.123664328910565e-05\n",
            "Epoch 466/1000, Training Loss: 7.134847565078747e-05, Test Loss: 7.06034367877975e-05\n",
            "Epoch 467/1000, Training Loss: 7.067638385751903e-05, Test Loss: 6.998258616518621e-05\n",
            "Epoch 468/1000, Training Loss: 7.00177298961513e-05, Test Loss: 6.937380324825617e-05\n",
            "Epoch 469/1000, Training Loss: 6.937219183961028e-05, Test Loss: 6.877680721273821e-05\n",
            "Epoch 470/1000, Training Loss: 6.873945624908472e-05, Test Loss: 6.819132438378281e-05\n",
            "Epoch 471/1000, Training Loss: 6.811921793237133e-05, Test Loss: 6.761708804221341e-05\n",
            "Epoch 472/1000, Training Loss: 6.751117970956331e-05, Test Loss: 6.705383823621381e-05\n",
            "Epoch 473/1000, Training Loss: 6.691505218583681e-05, Test Loss: 6.650132159829633e-05\n",
            "Epoch 474/1000, Training Loss: 6.633055353112099e-05, Test Loss: 6.595929116740736e-05\n",
            "Epoch 475/1000, Training Loss: 6.575740926642332e-05, Test Loss: 6.542750621602256e-05\n",
            "Epoch 476/1000, Training Loss: 6.519535205660754e-05, Test Loss: 6.490573208210187e-05\n",
            "Epoch 477/1000, Training Loss: 6.464412150941666e-05, Test Loss: 6.439374000576301e-05\n",
            "Epoch 478/1000, Training Loss: 6.410346398053943e-05, Test Loss: 6.389130697053829e-05\n",
            "Epoch 479/1000, Training Loss: 6.357313238453672e-05, Test Loss: 6.339821554909607e-05\n",
            "Epoch 480/1000, Training Loss: 6.305288601143466e-05, Test Loss: 6.291425375329109e-05\n",
            "Epoch 481/1000, Training Loss: 6.254249034881217e-05, Test Loss: 6.243921488842837e-05\n",
            "Epoch 482/1000, Training Loss: 6.204171690920329e-05, Test Loss: 6.197289741161706e-05\n",
            "Epoch 483/1000, Training Loss: 6.15503430626501e-05, Test Loss: 6.151510479410026e-05\n",
            "Epoch 484/1000, Training Loss: 6.106815187424538e-05, Test Loss: 6.106564538744902e-05\n",
            "Epoch 485/1000, Training Loss: 6.059493194650118e-05, Test Loss: 6.06243322935064e-05\n",
            "Epoch 486/1000, Training Loss: 6.0130477266397534e-05, Test Loss: 6.0190983237979e-05\n",
            "Epoch 487/1000, Training Loss: 5.967458705696739e-05, Test Loss: 5.9765420447572586e-05\n",
            "Epoch 488/1000, Training Loss: 5.922706563325875e-05, Test Loss: 5.934747053055891e-05\n",
            "Epoch 489/1000, Training Loss: 5.878772226256157e-05, Test Loss: 5.893696436069862e-05\n",
            "Epoch 490/1000, Training Loss: 5.8356371028743524e-05, Test Loss: 5.853373696439688e-05\n",
            "Epoch 491/1000, Training Loss: 5.79328307005769e-05, Test Loss: 5.813762741101738e-05\n",
            "Epoch 492/1000, Training Loss: 5.75169246039337e-05, Test Loss: 5.774847870626066e-05\n",
            "Epoch 493/1000, Training Loss: 5.710848049772014e-05, Test Loss: 5.7366137688513016e-05\n",
            "Epoch 494/1000, Training Loss: 5.6707330453442345e-05, Test Loss: 5.699045492808491e-05\n",
            "Epoch 495/1000, Training Loss: 5.6313310738283304e-05, Test Loss: 5.662128462925624e-05\n",
            "Epoch 496/1000, Training Loss: 5.592626170158789e-05, Test Loss: 5.62584845350455e-05\n",
            "Epoch 497/1000, Training Loss: 5.554602766464428e-05, Test Loss: 5.59019158346229e-05\n",
            "Epoch 498/1000, Training Loss: 5.51724568136689e-05, Test Loss: 5.5551443073299604e-05\n",
            "Epoch 499/1000, Training Loss: 5.480540109588286e-05, Test Loss: 5.5206934065005226e-05\n",
            "Epoch 500/1000, Training Loss: 5.444471611859488e-05, Test Loss: 5.486825980719484e-05\n",
            "Epoch 501/1000, Training Loss: 5.409026105119815e-05, Test Loss: 5.4535294398110354e-05\n",
            "Epoch 502/1000, Training Loss: 5.374189852997864e-05, Test Loss: 5.420791495632193e-05\n",
            "Epoch 503/1000, Training Loss: 5.339949456566698e-05, Test Loss: 5.388600154249726e-05\n",
            "Epoch 504/1000, Training Loss: 5.3062918453636293e-05, Test Loss: 5.3569437083322255e-05\n",
            "Epoch 505/1000, Training Loss: 5.2732042686667964e-05, Test Loss: 5.3258107297518246e-05\n",
            "Epoch 506/1000, Training Loss: 5.240674287021475e-05, Test Loss: 5.295190062389596e-05\n",
            "Epoch 507/1000, Training Loss: 5.2086897640067456e-05, Test Loss: 5.2650708151376865e-05\n",
            "Epoch 508/1000, Training Loss: 5.177238858237253e-05, Test Loss: 5.235442355094053e-05\n",
            "Epoch 509/1000, Training Loss: 5.146310015591737e-05, Test Loss: 5.2062943009433884e-05\n",
            "Epoch 510/1000, Training Loss: 5.1158919616610795e-05, Test Loss: 5.177616516518807e-05\n",
            "Epoch 511/1000, Training Loss: 5.085973694410755e-05, Test Loss: 5.14939910453955e-05\n",
            "Epoch 512/1000, Training Loss: 5.056544477049315e-05, Test Loss: 5.121632400519253e-05\n",
            "Epoch 513/1000, Training Loss: 5.027593831098418e-05, Test Loss: 5.094306966840324e-05\n",
            "Epoch 514/1000, Training Loss: 4.999111529656843e-05, Test Loss: 5.06741358698912e-05\n",
            "Epoch 515/1000, Training Loss: 4.971087590854066e-05, Test Loss: 5.040943259947865e-05\n",
            "Epoch 516/1000, Training Loss: 4.943512271486191e-05, Test Loss: 5.014887194738373e-05\n",
            "Epoch 517/1000, Training Loss: 4.9163760608303876e-05, Test Loss: 4.989236805113648e-05\n",
            "Epoch 518/1000, Training Loss: 4.889669674630998e-05, Test Loss: 4.963983704392586e-05\n",
            "Epoch 519/1000, Training Loss: 4.8633840492533956e-05, Test Loss: 4.939119700434437e-05\n",
            "Epoch 520/1000, Training Loss: 4.8375103359995906e-05, Test Loss: 4.9146367907482054e-05\n",
            "Epoch 521/1000, Training Loss: 4.812039895581859e-05, Test Loss: 4.8905271577342144e-05\n",
            "Epoch 522/1000, Training Loss: 4.786964292748724e-05, Test Loss: 4.866783164052661e-05\n",
            "Epoch 523/1000, Training Loss: 4.762275291059149e-05, Test Loss: 4.843397348117078e-05\n",
            "Epoch 524/1000, Training Loss: 4.737964847800791e-05, Test Loss: 4.8203624197076235e-05\n",
            "Epoch 525/1000, Training Loss: 4.7140251090476386e-05, Test Loss: 4.797671255702462e-05\n",
            "Epoch 526/1000, Training Loss: 4.6904484048534684e-05, Test Loss: 4.7753168959223804e-05\n",
            "Epoch 527/1000, Training Loss: 4.667227244576515e-05, Test Loss: 4.7532925390863335e-05\n",
            "Epoch 528/1000, Training Loss: 4.644354312331811e-05, Test Loss: 4.731591538874385e-05\n",
            "Epoch 529/1000, Training Loss: 4.621822462567676e-05, Test Loss: 4.710207400095316e-05\n",
            "Epoch 530/1000, Training Loss: 4.599624715762472e-05, Test Loss: 4.689133774955917e-05\n",
            "Epoch 531/1000, Training Loss: 4.57775425423808e-05, Test Loss: 4.6683644594287405e-05\n",
            "Epoch 532/1000, Training Loss: 4.5562044180866995e-05, Test Loss: 4.6478933897156864e-05\n",
            "Epoch 533/1000, Training Loss: 4.534968701207782e-05, Test Loss: 4.6277146388051885e-05\n",
            "Epoch 534/1000, Training Loss: 4.514040747452251e-05, Test Loss: 4.6078224131198554e-05\n",
            "Epoch 535/1000, Training Loss: 4.4934143468701776e-05, Test Loss: 4.5882110492521734e-05\n",
            "Epoch 536/1000, Training Loss: 4.473083432059279e-05, Test Loss: 4.568875010785915e-05\n",
            "Epoch 537/1000, Training Loss: 4.4530420746115624e-05, Test Loss: 4.549808885200715e-05\n",
            "Epoch 538/1000, Training Loss: 4.4332844816548574e-05, Test Loss: 4.5310073808574844e-05\n",
            "Epoch 539/1000, Training Loss: 4.413804992487171e-05, Test Loss: 4.512465324062914e-05\n",
            "Epoch 540/1000, Training Loss: 4.3945980753002255e-05, Test Loss: 4.494177656209989e-05\n",
            "Epoch 541/1000, Training Loss: 4.375658323990641e-05, Test Loss: 4.4761394309930675e-05\n",
            "Epoch 542/1000, Training Loss: 4.356980455055563e-05, Test Loss: 4.458345811695301e-05\n",
            "Epoch 543/1000, Training Loss: 4.3385593045706834e-05, Test Loss: 4.4407920685464435e-05\n",
            "Epoch 544/1000, Training Loss: 4.3203898252481795e-05, Test Loss: 4.423473576148699e-05\n",
            "Epoch 545/1000, Training Loss: 4.302467083572252e-05, Test Loss: 4.406385810969318e-05\n",
            "Epoch 546/1000, Training Loss: 4.2847862570104534e-05, Test Loss: 4.389524348897769e-05\n",
            "Epoch 547/1000, Training Loss: 4.2673426312980525e-05, Test Loss: 4.3728848628660845e-05\n",
            "Epoch 548/1000, Training Loss: 4.250131597793787e-05, Test Loss: 4.3564631205296505e-05\n",
            "Epoch 549/1000, Training Loss: 4.233148650904959e-05, Test Loss: 4.34025498200802e-05\n",
            "Epoch 550/1000, Training Loss: 4.2163893855800584e-05, Test Loss: 4.324256397683756e-05\n",
            "Epoch 551/1000, Training Loss: 4.1998494948663746e-05, Test Loss: 4.30846340605702e-05\n",
            "Epoch 552/1000, Training Loss: 4.183524767531605e-05, Test Loss: 4.292872131654995e-05\n",
            "Epoch 553/1000, Training Loss: 4.1674110857472597e-05, Test Loss: 4.277478782994823e-05\n",
            "Epoch 554/1000, Training Loss: 4.1515044228322805e-05, Test Loss: 4.262279650598387e-05\n",
            "Epoch 555/1000, Training Loss: 4.1358008410547696e-05, Test Loss: 4.247271105056965e-05\n",
            "Epoch 556/1000, Training Loss: 4.120296489491053e-05, Test Loss: 4.232449595145403e-05\n",
            "Epoch 557/1000, Training Loss: 4.104987601939707e-05, Test Loss: 4.217811645983998e-05\n",
            "Epoch 558/1000, Training Loss: 4.089870494889293e-05, Test Loss: 4.2033538572462285e-05\n",
            "Epoch 559/1000, Training Loss: 4.074941565538526e-05, Test Loss: 4.189072901412586e-05\n",
            "Epoch 560/1000, Training Loss: 4.06019728986742e-05, Test Loss: 4.174965522067457e-05\n",
            "Epoch 561/1000, Training Loss: 4.0456342207573544e-05, Test Loss: 4.161028532239251e-05\n",
            "Epoch 562/1000, Training Loss: 4.03124898615973e-05, Test Loss: 4.147258812782136e-05\n",
            "Epoch 563/1000, Training Loss: 4.017038287310992e-05, Test Loss: 4.133653310798141e-05\n",
            "Epoch 564/1000, Training Loss: 4.002998896993547e-05, Test Loss: 4.120209038098696e-05\n",
            "Epoch 565/1000, Training Loss: 3.98912765784058e-05, Test Loss: 4.106923069704695e-05\n",
            "Epoch 566/1000, Training Loss: 3.9754214806840846e-05, Test Loss: 4.093792542383807e-05\n",
            "Epoch 567/1000, Training Loss: 3.961877342944703e-05, Test Loss: 4.0808146532241574e-05\n",
            "Epoch 568/1000, Training Loss: 3.948492287062337e-05, Test Loss: 4.067986658243249e-05\n",
            "Epoch 569/1000, Training Loss: 3.935263418966531e-05, Test Loss: 4.055305871031816e-05\n",
            "Epoch 570/1000, Training Loss: 3.9221879065851107e-05, Test Loss: 4.0427696614306784e-05\n",
            "Epoch 571/1000, Training Loss: 3.9092629783907036e-05, Test Loss: 4.030375454240533e-05\n",
            "Epoch 572/1000, Training Loss: 3.896485921983501e-05, Test Loss: 4.018120727963558e-05\n",
            "Epoch 573/1000, Training Loss: 3.883854082709795e-05, Test Loss: 4.006003013575865e-05\n",
            "Epoch 574/1000, Training Loss: 3.871364862314964e-05, Test Loss: 3.9940198933301385e-05\n",
            "Epoch 575/1000, Training Loss: 3.859015717630084e-05, Test Loss: 3.982168999587649e-05\n",
            "Epoch 576/1000, Training Loss: 3.8468041592914664e-05, Test Loss: 3.9704480136789896e-05\n",
            "Epoch 577/1000, Training Loss: 3.834727750492172e-05, Test Loss: 3.9588546647924966e-05\n",
            "Epoch 578/1000, Training Loss: 3.8227841057641906e-05, Test Loss: 3.947386728889837e-05\n",
            "Epoch 579/1000, Training Loss: 3.810970889791149e-05, Test Loss: 3.9360420276477054e-05\n",
            "Epoch 580/1000, Training Loss: 3.799285816250397e-05, Test Loss: 3.924818427426007e-05\n",
            "Epoch 581/1000, Training Loss: 3.78772664668371e-05, Test Loss: 3.9137138382601664e-05\n",
            "Epoch 582/1000, Training Loss: 3.776291189395942e-05, Test Loss: 3.902726212878665e-05\n",
            "Epoch 583/1000, Training Loss: 3.7649772983807453e-05, Test Loss: 3.89185354574381e-05\n",
            "Epoch 584/1000, Training Loss: 3.753782872272798e-05, Test Loss: 3.881093872115853e-05\n",
            "Epoch 585/1000, Training Loss: 3.742705853325752e-05, Test Loss: 3.8704452671397546e-05\n",
            "Epoch 586/1000, Training Loss: 3.731744226415264e-05, Test Loss: 3.8599058449537325e-05\n",
            "Epoch 587/1000, Training Loss: 3.7208960180664646e-05, Test Loss: 3.849473757819452e-05\n",
            "Epoch 588/1000, Training Loss: 3.710159295505149e-05, Test Loss: 3.839147195272786e-05\n",
            "Epoch 589/1000, Training Loss: 3.69953216573226e-05, Test Loss: 3.8289243832953305e-05\n",
            "Epoch 590/1000, Training Loss: 3.6890127746208727e-05, Test Loss: 3.8188035835054726e-05\n",
            "Epoch 591/1000, Training Loss: 3.678599306034961e-05, Test Loss: 3.8087830923687105e-05\n",
            "Epoch 592/1000, Training Loss: 3.668289980969991e-05, Test Loss: 3.798861240426922e-05\n",
            "Epoch 593/1000, Training Loss: 3.6580830567138624e-05, Test Loss: 3.7890363915458265e-05\n",
            "Epoch 594/1000, Training Loss: 3.647976826028645e-05, Test Loss: 3.779306942180735e-05\n",
            "Epoch 595/1000, Training Loss: 3.637969616351633e-05, Test Loss: 3.769671320659095e-05\n",
            "Epoch 596/1000, Training Loss: 3.628059789016193e-05, Test Loss: 3.760127986480535e-05\n",
            "Epoch 597/1000, Training Loss: 3.618245738490974e-05, Test Loss: 3.750675429633131e-05\n",
            "Epoch 598/1000, Training Loss: 3.608525891637602e-05, Test Loss: 3.741312169926108e-05\n",
            "Epoch 599/1000, Training Loss: 3.59889870698619e-05, Test Loss: 3.73203675633795e-05\n",
            "Epoch 600/1000, Training Loss: 3.5893626740282194e-05, Test Loss: 3.7228477663800586e-05\n",
            "Epoch 601/1000, Training Loss: 3.5799163125263853e-05, Test Loss: 3.713743805475264e-05\n",
            "Epoch 602/1000, Training Loss: 3.5705581718408474e-05, Test Loss: 3.704723506350921e-05\n",
            "Epoch 603/1000, Training Loss: 3.561286830271758e-05, Test Loss: 3.695785528446329e-05\n",
            "Epoch 604/1000, Training Loss: 3.5521008944172674e-05, Test Loss: 3.6869285573339114e-05\n",
            "Epoch 605/1000, Training Loss: 3.5429989985468815e-05, Test Loss: 3.6781513041537475e-05\n",
            "Epoch 606/1000, Training Loss: 3.533979803989754e-05, Test Loss: 3.66945250506149e-05\n",
            "Epoch 607/1000, Training Loss: 3.5250419985374165e-05, Test Loss: 3.6608309206890804e-05\n",
            "Epoch 608/1000, Training Loss: 3.516184295860785e-05, Test Loss: 3.652285335618214e-05\n",
            "Epoch 609/1000, Training Loss: 3.5074054349407505e-05, Test Loss: 3.643814557865282e-05\n",
            "Epoch 610/1000, Training Loss: 3.498704179512449e-05, Test Loss: 3.635417418379117e-05\n",
            "Epoch 611/1000, Training Loss: 3.4900793175225676e-05, Test Loss: 3.6270927705500036e-05\n",
            "Epoch 612/1000, Training Loss: 3.4815296605992206e-05, Test Loss: 3.618839489729669e-05\n",
            "Epoch 613/1000, Training Loss: 3.473054043534654e-05, Test Loss: 3.610656472762949e-05\n",
            "Epoch 614/1000, Training Loss: 3.464651323779778e-05, Test Loss: 3.602542637529695e-05\n",
            "Epoch 615/1000, Training Loss: 3.456320380950792e-05, Test Loss: 3.594496922497697e-05\n",
            "Epoch 616/1000, Training Loss: 3.448060116347152e-05, Test Loss: 3.586518286285342e-05\n",
            "Epoch 617/1000, Training Loss: 3.439869452480951e-05, Test Loss: 3.578605707234872e-05\n",
            "Epoch 618/1000, Training Loss: 3.431747332617251e-05, Test Loss: 3.5707581829946515e-05\n",
            "Epoch 619/1000, Training Loss: 3.423692720325042e-05, Test Loss: 3.562974730111912e-05\n",
            "Epoch 620/1000, Training Loss: 3.415704599038821e-05, Test Loss: 3.555254383633806e-05\n",
            "Epoch 621/1000, Training Loss: 3.407781971630156e-05, Test Loss: 3.547596196718349e-05\n",
            "Epoch 622/1000, Training Loss: 3.3999238599893536e-05, Test Loss: 3.539999240253611e-05\n",
            "Epoch 623/1000, Training Loss: 3.3921293046167386e-05, Test Loss: 3.532462602485955e-05\n",
            "Epoch 624/1000, Training Loss: 3.3843973642234004e-05, Test Loss: 3.524985388656475e-05\n",
            "Epoch 625/1000, Training Loss: 3.376727115341247e-05, Test Loss: 3.5175667206457524e-05\n",
            "Epoch 626/1000, Training Loss: 3.369117651941845e-05, Test Loss: 3.5102057366266025e-05\n",
            "Epoch 627/1000, Training Loss: 3.3615680850643086e-05, Test Loss: 3.502901590724737e-05\n",
            "Epoch 628/1000, Training Loss: 3.3540775424514686e-05, Test Loss: 3.495653452686702e-05\n",
            "Epoch 629/1000, Training Loss: 3.346645168194715e-05, Test Loss: 3.4884605075559515e-05\n",
            "Epoch 630/1000, Training Loss: 3.339270122386669e-05, Test Loss: 3.4813219553555905e-05\n",
            "Epoch 631/1000, Training Loss: 3.331951580781985e-05, Test Loss: 3.474237010778398e-05\n",
            "Epoch 632/1000, Training Loss: 3.324688734465994e-05, Test Loss: 3.467204902884064e-05\n",
            "Epoch 633/1000, Training Loss: 3.317480789530618e-05, Test Loss: 3.4602248748027633e-05\n",
            "Epoch 634/1000, Training Loss: 3.3103269667580026e-05, Test Loss: 3.453296183445638e-05\n",
            "Epoch 635/1000, Training Loss: 3.303226501311134e-05, Test Loss: 3.446418099221699e-05\n",
            "Epoch 636/1000, Training Loss: 3.2961786424315864e-05, Test Loss: 3.439589905760738e-05\n",
            "Epoch 637/1000, Training Loss: 3.289182653144148e-05, Test Loss: 3.432810899643014e-05\n",
            "Epoch 638/1000, Training Loss: 3.282237809968038e-05, Test Loss: 3.4260803901340184e-05\n",
            "Epoch 639/1000, Training Loss: 3.27534340263479e-05, Test Loss: 3.41939769892632e-05\n",
            "Epoch 640/1000, Training Loss: 3.268498733812444e-05, Test Loss: 3.412762159885713e-05\n",
            "Epoch 641/1000, Training Loss: 3.26170311883599e-05, Test Loss: 3.4061731188042715e-05\n",
            "Epoch 642/1000, Training Loss: 3.2549558854438776e-05, Test Loss: 3.399629933158015e-05\n",
            "Epoch 643/1000, Training Loss: 3.248256373520447e-05, Test Loss: 3.393131971870225e-05\n",
            "Epoch 644/1000, Training Loss: 3.241603934844216e-05, Test Loss: 3.386678615079838e-05\n",
            "Epoch 645/1000, Training Loss: 3.234997932841742e-05, Test Loss: 3.380269253915169e-05\n",
            "Epoch 646/1000, Training Loss: 3.228437742347001e-05, Test Loss: 3.3739032902723335e-05\n",
            "Epoch 647/1000, Training Loss: 3.221922749366232e-05, Test Loss: 3.367580136598432e-05\n",
            "Epoch 648/1000, Training Loss: 3.215452350848037e-05, Test Loss: 3.361299215679957e-05\n",
            "Epoch 649/1000, Training Loss: 3.209025954458435e-05, Test Loss: 3.355059960435369e-05\n",
            "Epoch 650/1000, Training Loss: 3.2026429783612414e-05, Test Loss: 3.348861813712222e-05\n",
            "Epoch 651/1000, Training Loss: 3.196302851002979e-05, Test Loss: 3.342704228088975e-05\n",
            "Epoch 652/1000, Training Loss: 3.190005010902967e-05, Test Loss: 3.336586665680886e-05\n",
            "Epoch 653/1000, Training Loss: 3.183748906447686e-05, Test Loss: 3.330508597950257e-05\n",
            "Epoch 654/1000, Training Loss: 3.177533995689973e-05, Test Loss: 3.324469505520466e-05\n",
            "Epoch 655/1000, Training Loss: 3.171359746152541e-05, Test Loss: 3.318468877994534e-05\n",
            "Epoch 656/1000, Training Loss: 3.165225634635881e-05, Test Loss: 3.3125062137771896e-05\n",
            "Epoch 657/1000, Training Loss: 3.159131147030419e-05, Test Loss: 3.3065810199010044e-05\n",
            "Epoch 658/1000, Training Loss: 3.153075778132758e-05, Test Loss: 3.300692811856113e-05\n",
            "Epoch 659/1000, Training Loss: 3.14705903146602e-05, Test Loss: 3.2948411134237586e-05\n",
            "Epoch 660/1000, Training Loss: 3.141080419104127e-05, Test Loss: 3.2890254565132264e-05\n",
            "Epoch 661/1000, Training Loss: 3.1351394614999625e-05, Test Loss: 3.2832453810024674e-05\n",
            "Epoch 662/1000, Training Loss: 3.129235687317183e-05, Test Loss: 3.277500434581919e-05\n",
            "Epoch 663/1000, Training Loss: 3.123368633265913e-05, Test Loss: 3.271790172601926e-05\n",
            "Epoch 664/1000, Training Loss: 3.117537843941916e-05, Test Loss: 3.26611415792318e-05\n",
            "Epoch 665/1000, Training Loss: 3.1117428716692084e-05, Test Loss: 3.260471960770498e-05\n",
            "Epoch 666/1000, Training Loss: 3.105983276346236e-05, Test Loss: 3.25486315858952e-05\n",
            "Epoch 667/1000, Training Loss: 3.100258625295372e-05, Test Loss: 3.2492873359068035e-05\n",
            "Epoch 668/1000, Training Loss: 3.094568493115641e-05, Test Loss: 3.243744084192408e-05\n",
            "Epoch 669/1000, Training Loss: 3.088912461538671e-05, Test Loss: 3.2382330017259874e-05\n",
            "Epoch 670/1000, Training Loss: 3.0832901192877734e-05, Test Loss: 3.232753693465043e-05\n",
            "Epoch 671/1000, Training Loss: 3.0777010619400636e-05, Test Loss: 3.227305770916596e-05\n",
            "Epoch 672/1000, Training Loss: 3.07214489179154e-05, Test Loss: 3.221888852010997e-05\n",
            "Epoch 673/1000, Training Loss: 3.066621217725156e-05, Test Loss: 3.2165025609787174e-05\n",
            "Epoch 674/1000, Training Loss: 3.0611296550816776e-05, Test Loss: 3.211146528230026e-05\n",
            "Epoch 675/1000, Training Loss: 3.055669825533311e-05, Test Loss: 3.2058203902362914e-05\n",
            "Epoch 676/1000, Training Loss: 3.0502413569600435e-05, Test Loss: 3.200523789414694e-05\n",
            "Epoch 677/1000, Training Loss: 3.044843883328733e-05, Test Loss: 3.1952563740149656e-05\n",
            "Epoch 678/1000, Training Loss: 3.0394770445746708e-05, Test Loss: 3.1900177980083867e-05\n",
            "Epoch 679/1000, Training Loss: 3.0341404864857907e-05, Test Loss: 3.184807720979291e-05\n",
            "Epoch 680/1000, Training Loss: 3.0288338605892283e-05, Test Loss: 3.1796258080188166e-05\n",
            "Epoch 681/1000, Training Loss: 3.0235568240403985e-05, Test Loss: 3.174471729620793e-05\n",
            "Epoch 682/1000, Training Loss: 3.018309039514446e-05, Test Loss: 3.169345161579649e-05\n",
            "Epoch 683/1000, Training Loss: 3.0130901750999366e-05, Test Loss: 3.1642457848910355e-05\n",
            "Epoch 684/1000, Training Loss: 3.0078999041948272e-05, Test Loss: 3.159173285653615e-05\n",
            "Epoch 685/1000, Training Loss: 3.002737905404747e-05, Test Loss: 3.1541273549736114e-05\n",
            "Epoch 686/1000, Training Loss: 2.9976038624432868e-05, Test Loss: 3.149107688871122e-05\n",
            "Epoch 687/1000, Training Loss: 2.9924974640344553e-05, Test Loss: 3.144113988188134e-05\n",
            "Epoch 688/1000, Training Loss: 2.9874184038172965e-05, Test Loss: 3.139145958498689e-05\n",
            "Epoch 689/1000, Training Loss: 2.982366380252322e-05, Test Loss: 3.1342033100208005e-05\n",
            "Epoch 690/1000, Training Loss: 2.977341096530086e-05, Test Loss: 3.129285757530348e-05\n",
            "Epoch 691/1000, Training Loss: 2.972342260481638e-05, Test Loss: 3.1243930202764254e-05\n",
            "Epoch 692/1000, Training Loss: 2.967369584490854e-05, Test Loss: 3.119524821898567e-05\n",
            "Epoch 693/1000, Training Loss: 2.9624227854085778e-05, Test Loss: 3.1146808903457864e-05\n",
            "Epoch 694/1000, Training Loss: 2.9575015844685438e-05, Test Loss: 3.1098609577972185e-05\n",
            "Epoch 695/1000, Training Loss: 2.9526057072052076e-05, Test Loss: 3.10506476058413e-05\n",
            "Epoch 696/1000, Training Loss: 2.9477348833731072e-05, Test Loss: 3.100292039113924e-05\n",
            "Epoch 697/1000, Training Loss: 2.942888846867996e-05, Test Loss: 3.0955425377952715e-05\n",
            "Epoch 698/1000, Training Loss: 2.9380673356496964e-05, Test Loss: 3.090816004965263e-05\n",
            "Epoch 699/1000, Training Loss: 2.933270091666362e-05, Test Loss: 3.08611219281733e-05\n",
            "Epoch 700/1000, Training Loss: 2.9284968607804687e-05, Test Loss: 3.081430857331548e-05\n",
            "Epoch 701/1000, Training Loss: 2.9237473926963284e-05, Test Loss: 3.0767717582052994e-05\n",
            "Epoch 702/1000, Training Loss: 2.9190214408890805e-05, Test Loss: 3.0721346587863463e-05\n",
            "Epoch 703/1000, Training Loss: 2.9143187625350598e-05, Test Loss: 3.067519326006469e-05\n",
            "Epoch 704/1000, Training Loss: 2.9096391184438277e-05, Test Loss: 3.062925530316852e-05\n",
            "Epoch 705/1000, Training Loss: 2.904982272991334e-05, Test Loss: 3.0583530456246455e-05\n",
            "Epoch 706/1000, Training Loss: 2.9003479940546808e-05, Test Loss: 3.053801649230811e-05\n",
            "Epoch 707/1000, Training Loss: 2.895736052948148e-05, Test Loss: 3.0492711217692923e-05\n",
            "Epoch 708/1000, Training Loss: 2.8911462243604025e-05, Test Loss: 3.044761247147263e-05\n",
            "Epoch 709/1000, Training Loss: 2.8865782862932482e-05, Test Loss: 3.0402718124865967e-05\n",
            "Epoch 710/1000, Training Loss: 2.8820320200013892e-05, Test Loss: 3.035802608066415e-05\n",
            "Epoch 711/1000, Training Loss: 2.877507209933643e-05, Test Loss: 3.0313534272673306e-05\n",
            "Epoch 712/1000, Training Loss: 2.8730036436751475e-05, Test Loss: 3.02692406651585e-05\n",
            "Epoch 713/1000, Training Loss: 2.868521111890888e-05, Test Loss: 3.0225143252307738e-05\n",
            "Epoch 714/1000, Training Loss: 2.8640594082702818e-05, Test Loss: 3.0181240057699526e-05\n",
            "Epoch 715/1000, Training Loss: 2.8596183294729765e-05, Test Loss: 3.0137529133787553e-05\n",
            "Epoch 716/1000, Training Loss: 2.8551976750756677e-05, Test Loss: 3.009400856138969e-05\n",
            "Epoch 717/1000, Training Loss: 2.8507972475200658e-05, Test Loss: 3.0050676449189542e-05\n",
            "Epoch 718/1000, Training Loss: 2.8464168520618122e-05, Test Loss: 3.0007530933249963e-05\n",
            "Epoch 719/1000, Training Loss: 2.8420562967205607e-05, Test Loss: 2.9964570176532927e-05\n",
            "Epoch 720/1000, Training Loss: 2.8377153922309566e-05, Test Loss: 2.9921792368428155e-05\n",
            "Epoch 721/1000, Training Loss: 2.8333939519946428e-05, Test Loss: 2.9879195724294583e-05\n",
            "Epoch 722/1000, Training Loss: 2.8290917920332712e-05, Test Loss: 2.9836778485009353e-05\n",
            "Epoch 723/1000, Training Loss: 2.824808730942338e-05, Test Loss: 2.97945389165229e-05\n",
            "Epoch 724/1000, Training Loss: 2.8205445898460694e-05, Test Loss: 2.9752475309426294e-05\n",
            "Epoch 725/1000, Training Loss: 2.8162991923531478e-05, Test Loss: 2.9710585978526995e-05\n",
            "Epoch 726/1000, Training Loss: 2.8120723645133455e-05, Test Loss: 2.966886926242853e-05\n",
            "Epoch 727/1000, Training Loss: 2.807863934774966e-05, Test Loss: 2.962732352312322e-05\n",
            "Epoch 728/1000, Training Loss: 2.8036737339432902e-05, Test Loss: 2.9585947145591387e-05\n",
            "Epoch 729/1000, Training Loss: 2.799501595139541e-05, Test Loss: 2.9544738537405172e-05\n",
            "Epoch 730/1000, Training Loss: 2.7953473537610513e-05, Test Loss: 2.9503696128345823e-05\n",
            "Epoch 731/1000, Training Loss: 2.791210847441854e-05, Test Loss: 2.9462818370023193e-05\n",
            "Epoch 732/1000, Training Loss: 2.787091916014419e-05, Test Loss: 2.9422103735504474e-05\n",
            "Epoch 733/1000, Training Loss: 2.7829904014717265e-05, Test Loss: 2.9381550718951347e-05\n",
            "Epoch 734/1000, Training Loss: 2.7789061479304755e-05, Test Loss: 2.9341157835261234e-05\n",
            "Epoch 735/1000, Training Loss: 2.774839001594735e-05, Test Loss: 2.930092361971819e-05\n",
            "Epoch 736/1000, Training Loss: 2.770788810720493e-05, Test Loss: 2.9260846627648648e-05\n",
            "Epoch 737/1000, Training Loss: 2.7667554255808107e-05, Test Loss: 2.9220925434084885e-05\n",
            "Epoch 738/1000, Training Loss: 2.762738698431663e-05, Test Loss: 2.9181158633434026e-05\n",
            "Epoch 739/1000, Training Loss: 2.75873848347847e-05, Test Loss: 2.9141544839154042e-05\n",
            "Epoch 740/1000, Training Loss: 2.7547546368432697e-05, Test Loss: 2.9102082683434603e-05\n",
            "Epoch 741/1000, Training Loss: 2.750787016532508e-05, Test Loss: 2.906277081688626e-05\n",
            "Epoch 742/1000, Training Loss: 2.7468354824054997e-05, Test Loss: 2.9023607908233084e-05\n",
            "Epoch 743/1000, Training Loss: 2.7428998961434836e-05, Test Loss: 2.8984592644012318e-05\n",
            "Epoch 744/1000, Training Loss: 2.7389801212192622e-05, Test Loss: 2.8945723728280693e-05\n",
            "Epoch 745/1000, Training Loss: 2.7350760228674298e-05, Test Loss: 2.890699988232372e-05\n",
            "Epoch 746/1000, Training Loss: 2.731187468055186e-05, Test Loss: 2.8868419844372703e-05\n",
            "Epoch 747/1000, Training Loss: 2.727314325453697e-05, Test Loss: 2.8829982369325573e-05\n",
            "Epoch 748/1000, Training Loss: 2.723456465410028e-05, Test Loss: 2.8791686228474726e-05\n",
            "Epoch 749/1000, Training Loss: 2.7196137599196257e-05, Test Loss: 2.8753530209237023e-05\n",
            "Epoch 750/1000, Training Loss: 2.7157860825992656e-05, Test Loss: 2.8715513114892832e-05\n",
            "Epoch 751/1000, Training Loss: 2.711973308660556e-05, Test Loss: 2.8677633764325304e-05\n",
            "Epoch 752/1000, Training Loss: 2.7081753148839844e-05, Test Loss: 2.8639890991769237e-05\n",
            "Epoch 753/1000, Training Loss: 2.7043919795934013e-05, Test Loss: 2.860228364656144e-05\n",
            "Epoch 754/1000, Training Loss: 2.7006231826310175e-05, Test Loss: 2.8564810592895655e-05\n",
            "Epoch 755/1000, Training Loss: 2.6968688053328302e-05, Test Loss: 2.852747070958314e-05\n",
            "Epoch 756/1000, Training Loss: 2.693128730504572e-05, Test Loss: 2.8490262889818797e-05\n",
            "Epoch 757/1000, Training Loss: 2.68940284239813e-05, Test Loss: 2.8453186040949173e-05\n",
            "Epoch 758/1000, Training Loss: 2.6856910266883613e-05, Test Loss: 2.841623908424614e-05\n",
            "Epoch 759/1000, Training Loss: 2.6819931704503295e-05, Test Loss: 2.8379420954684742e-05\n",
            "Epoch 760/1000, Training Loss: 2.6783091621369573e-05, Test Loss: 2.834273060072288e-05\n",
            "Epoch 761/1000, Training Loss: 2.6746388915572464e-05, Test Loss: 2.830616698408962e-05\n",
            "Epoch 762/1000, Training Loss: 2.670982249854745e-05, Test Loss: 2.8269729079573918e-05\n",
            "Epoch 763/1000, Training Loss: 2.6673391294864332e-05, Test Loss: 2.82334158748163e-05\n",
            "Epoch 764/1000, Training Loss: 2.6637094242020718e-05, Test Loss: 2.8197226370107495e-05\n",
            "Epoch 765/1000, Training Loss: 2.660093029023937e-05, Test Loss: 2.8161159578190065e-05\n",
            "Epoch 766/1000, Training Loss: 2.6564898402268227e-05, Test Loss: 2.8125214524060143e-05\n",
            "Epoch 767/1000, Training Loss: 2.6528997553185432e-05, Test Loss: 2.8089390244778093e-05\n",
            "Epoch 768/1000, Training Loss: 2.649322673020737e-05, Test Loss: 2.8053685789278197e-05\n",
            "Epoch 769/1000, Training Loss: 2.645758493249983e-05, Test Loss: 2.8018100218184403e-05\n",
            "Epoch 770/1000, Training Loss: 2.642207117099375e-05, Test Loss: 2.7982632603627957e-05\n",
            "Epoch 771/1000, Training Loss: 2.6386684468203014e-05, Test Loss: 2.7947282029069757e-05\n",
            "Epoch 772/1000, Training Loss: 2.6351423858047004e-05, Test Loss: 2.791204758912308e-05\n",
            "Epoch 773/1000, Training Loss: 2.631628838567523e-05, Test Loss: 2.7876928389383926e-05\n",
            "Epoch 774/1000, Training Loss: 2.6281277107295746e-05, Test Loss: 2.7841923546258895e-05\n",
            "Epoch 775/1000, Training Loss: 2.624638909000645e-05, Test Loss: 2.7807032186801567e-05\n",
            "Epoch 776/1000, Training Loss: 2.6211623411629594e-05, Test Loss: 2.777225344854828e-05\n",
            "Epoch 777/1000, Training Loss: 2.6176979160549914e-05, Test Loss: 2.7737586479358695e-05\n",
            "Epoch 778/1000, Training Loss: 2.6142455435553763e-05, Test Loss: 2.7703030437256247e-05\n",
            "Epoch 779/1000, Training Loss: 2.610805134567358e-05, Test Loss: 2.7668584490276006e-05\n",
            "Epoch 780/1000, Training Loss: 2.6073766010033693e-05, Test Loss: 2.763424781631103e-05\n",
            "Epoch 781/1000, Training Loss: 2.6039598557698912e-05, Test Loss: 2.7600019602964217e-05\n",
            "Epoch 782/1000, Training Loss: 2.600554812752684e-05, Test Loss: 2.75658990474015e-05\n",
            "Epoch 783/1000, Training Loss: 2.5971613868021258e-05, Test Loss: 2.7531885356206376e-05\n",
            "Epoch 784/1000, Training Loss: 2.5937794937190058e-05, Test Loss: 2.7497977745241417e-05\n",
            "Epoch 785/1000, Training Loss: 2.590409050240431e-05, Test Loss: 2.746417543950595e-05\n",
            "Epoch 786/1000, Training Loss: 2.5870499740259328e-05, Test Loss: 2.743047767300156e-05\n",
            "Epoch 787/1000, Training Loss: 2.58370218364406e-05, Test Loss: 2.7396883688597283e-05\n",
            "Epoch 788/1000, Training Loss: 2.5803655985589985e-05, Test Loss: 2.736339273789882e-05\n",
            "Epoch 789/1000, Training Loss: 2.5770401391174146e-05, Test Loss: 2.733000408111547e-05\n",
            "Epoch 790/1000, Training Loss: 2.5737257265357497e-05, Test Loss: 2.7296716986937192e-05\n",
            "Epoch 791/1000, Training Loss: 2.570422282887472e-05, Test Loss: 2.7263530732407712e-05\n",
            "Epoch 792/1000, Training Loss: 2.56712973109071e-05, Test Loss: 2.723044460279919e-05\n",
            "Epoch 793/1000, Training Loss: 2.5638479948961146e-05, Test Loss: 2.7197457891495568e-05\n",
            "Epoch 794/1000, Training Loss: 2.560576998874833e-05, Test Loss: 2.716456989987004e-05\n",
            "Epoch 795/1000, Training Loss: 2.5573166684067445e-05, Test Loss: 2.7131779937172106e-05\n",
            "Epoch 796/1000, Training Loss: 2.5540669296689678e-05, Test Loss: 2.7099087320409335e-05\n",
            "Epoch 797/1000, Training Loss: 2.5508277096244178e-05, Test Loss: 2.706649137423643e-05\n",
            "Epoch 798/1000, Training Loss: 2.5475989360107263e-05, Test Loss: 2.703399143084342e-05\n",
            "Epoch 799/1000, Training Loss: 2.5443805373292098e-05, Test Loss: 2.70015868298482e-05\n",
            "Epoch 800/1000, Training Loss: 2.54117244283412e-05, Test Loss: 2.696927691818785e-05\n",
            "Epoch 801/1000, Training Loss: 2.5379745825220697e-05, Test Loss: 2.6937061050014165e-05\n",
            "Epoch 802/1000, Training Loss: 2.5347868871215905e-05, Test Loss: 2.690493858658994e-05\n",
            "Epoch 803/1000, Training Loss: 2.5316092880829482e-05, Test Loss: 2.687290889618764e-05\n",
            "Epoch 804/1000, Training Loss: 2.5284417175680272e-05, Test Loss: 2.6840971353988106e-05\n",
            "Epoch 805/1000, Training Loss: 2.5252841084404965e-05, Test Loss: 2.6809125341984483e-05\n",
            "Epoch 806/1000, Training Loss: 2.5221363942560253e-05, Test Loss: 2.6777370248882312e-05\n",
            "Epoch 807/1000, Training Loss: 2.5189985092528527e-05, Test Loss: 2.6745705470007775e-05\n",
            "Epoch 808/1000, Training Loss: 2.5158703883422772e-05, Test Loss: 2.67141304072122e-05\n",
            "Epoch 809/1000, Training Loss: 2.512751967099494e-05, Test Loss: 2.6682644468780798e-05\n",
            "Epoch 810/1000, Training Loss: 2.5096431817544746e-05, Test Loss: 2.665124706934103e-05\n",
            "Epoch 811/1000, Training Loss: 2.5065439691831096e-05, Test Loss: 2.6619937629776566e-05\n",
            "Epoch 812/1000, Training Loss: 2.5034542668983732e-05, Test Loss: 2.6588715577136893e-05\n",
            "Epoch 813/1000, Training Loss: 2.5003740130417167e-05, Test Loss: 2.6557580344552488e-05\n",
            "Epoch 814/1000, Training Loss: 2.4973031463746023e-05, Test Loss: 2.652653137115107e-05\n",
            "Epoch 815/1000, Training Loss: 2.4942416062701337e-05, Test Loss: 2.64955681019728e-05\n",
            "Epoch 816/1000, Training Loss: 2.4911893327049137e-05, Test Loss: 2.6464689987890644e-05\n",
            "Epoch 817/1000, Training Loss: 2.488146266250877e-05, Test Loss: 2.643389648552707e-05\n",
            "Epoch 818/1000, Training Loss: 2.4851123480674988e-05, Test Loss: 2.6403187057178393e-05\n",
            "Epoch 819/1000, Training Loss: 2.482087519893799e-05, Test Loss: 2.637256117073267e-05\n",
            "Epoch 820/1000, Training Loss: 2.4790717240408895e-05, Test Loss: 2.6342018299598652e-05\n",
            "Epoch 821/1000, Training Loss: 2.4760649033842482e-05, Test Loss: 2.6311557922625132e-05\n",
            "Epoch 822/1000, Training Loss: 2.473067001356387e-05, Test Loss: 2.6281179524029344e-05\n",
            "Epoch 823/1000, Training Loss: 2.4700779619395446e-05, Test Loss: 2.625088259332422e-05\n",
            "Epoch 824/1000, Training Loss: 2.4670977296584573e-05, Test Loss: 2.6220666625247343e-05\n",
            "Epoch 825/1000, Training Loss: 2.4641262495733303e-05, Test Loss: 2.6190531119688396e-05\n",
            "Epoch 826/1000, Training Loss: 2.4611634672728803e-05, Test Loss: 2.616047558162008e-05\n",
            "Epoch 827/1000, Training Loss: 2.4582093288674545e-05, Test Loss: 2.61304995210312e-05\n",
            "Epoch 828/1000, Training Loss: 2.4552637809823722e-05, Test Loss: 2.610060245285986e-05\n",
            "Epoch 829/1000, Training Loss: 2.452326770751266e-05, Test Loss: 2.6070783896924307e-05\n",
            "Epoch 830/1000, Training Loss: 2.449398245809552e-05, Test Loss: 2.6041043377861036e-05\n",
            "Epoch 831/1000, Training Loss: 2.4464781542880114e-05, Test Loss: 2.6011380425059447e-05\n",
            "Epoch 832/1000, Training Loss: 2.4435664448065154e-05, Test Loss: 2.598179457259779e-05\n",
            "Epoch 833/1000, Training Loss: 2.4406630664678354e-05, Test Loss: 2.5952285359183655e-05\n",
            "Epoch 834/1000, Training Loss: 2.4377679688514578e-05, Test Loss: 2.5922852328091853e-05\n",
            "Epoch 835/1000, Training Loss: 2.434881102007622e-05, Test Loss: 2.5893495027102427e-05\n",
            "Epoch 836/1000, Training Loss: 2.43200241645137e-05, Test Loss: 2.5864213008444462e-05\n",
            "Epoch 837/1000, Training Loss: 2.42913186315678e-05, Test Loss: 2.583500582873683e-05\n",
            "Epoch 838/1000, Training Loss: 2.426269393551163e-05, Test Loss: 2.580587304893006e-05\n",
            "Epoch 839/1000, Training Loss: 2.4234149595094835e-05, Test Loss: 2.5776814234251304e-05\n",
            "Epoch 840/1000, Training Loss: 2.42056851334874e-05, Test Loss: 2.5747828954147813e-05\n",
            "Epoch 841/1000, Training Loss: 2.4177300078225237e-05, Test Loss: 2.5718916782232344e-05\n",
            "Epoch 842/1000, Training Loss: 2.4148993961156393e-05, Test Loss: 2.569007729623137e-05\n",
            "Epoch 843/1000, Training Loss: 2.4120766318388085e-05, Test Loss: 2.566131007792868e-05\n",
            "Epoch 844/1000, Training Loss: 2.409261669023416e-05, Test Loss: 2.5632614713116416e-05\n",
            "Epoch 845/1000, Training Loss: 2.4064544621164304e-05, Test Loss: 2.5603990791542254e-05\n",
            "Epoch 846/1000, Training Loss: 2.4036549659752726e-05, Test Loss: 2.5575437906858197e-05\n",
            "Epoch 847/1000, Training Loss: 2.400863135862922e-05, Test Loss: 2.554695565657406e-05\n",
            "Epoch 848/1000, Training Loss: 2.3980789274428713e-05, Test Loss: 2.5518543642003975e-05\n",
            "Epoch 849/1000, Training Loss: 2.3953022967744442e-05, Test Loss: 2.549020146822078e-05\n",
            "Epoch 850/1000, Training Loss: 2.3925332003079792e-05, Test Loss: 2.5461928744010536e-05\n",
            "Epoch 851/1000, Training Loss: 2.389771594880118e-05, Test Loss: 2.543372508182004e-05\n",
            "Epoch 852/1000, Training Loss: 2.3870174377092058e-05, Test Loss: 2.5405590097717345e-05\n",
            "Epoch 853/1000, Training Loss: 2.3842706863907576e-05, Test Loss: 2.53775234113402e-05\n",
            "Epoch 854/1000, Training Loss: 2.3815312988929548e-05, Test Loss: 2.5349524645857246e-05\n",
            "Epoch 855/1000, Training Loss: 2.378799233552323e-05, Test Loss: 2.5321593427920414e-05\n",
            "Epoch 856/1000, Training Loss: 2.37607444906923e-05, Test Loss: 2.5293729387620787e-05\n",
            "Epoch 857/1000, Training Loss: 2.3733569045037454e-05, Test Loss: 2.52659321584503e-05\n",
            "Epoch 858/1000, Training Loss: 2.3706465592713627e-05, Test Loss: 2.5238201377254842e-05\n",
            "Epoch 859/1000, Training Loss: 2.367943373138878e-05, Test Loss: 2.521053668419736e-05\n",
            "Epoch 860/1000, Training Loss: 2.3652473062202424e-05, Test Loss: 2.518293772271239e-05\n",
            "Epoch 861/1000, Training Loss: 2.3625583189725947e-05, Test Loss: 2.5155404139469766e-05\n",
            "Epoch 862/1000, Training Loss: 2.3598763721922512e-05, Test Loss: 2.5127935584334155e-05\n",
            "Epoch 863/1000, Training Loss: 2.357201427010791e-05, Test Loss: 2.5100531710323523e-05\n",
            "Epoch 864/1000, Training Loss: 2.354533444891233e-05, Test Loss: 2.5073192173575134e-05\n",
            "Epoch 865/1000, Training Loss: 2.351872387624201e-05, Test Loss: 2.5045916633303428e-05\n",
            "Epoch 866/1000, Training Loss: 2.3492182173241934e-05, Test Loss: 2.5018704751766768e-05\n",
            "Epoch 867/1000, Training Loss: 2.346570896425852e-05, Test Loss: 2.499155619422667e-05\n",
            "Epoch 868/1000, Training Loss: 2.3439303876804057e-05, Test Loss: 2.4964470628914844e-05\n",
            "Epoch 869/1000, Training Loss: 2.341296654152026e-05, Test Loss: 2.493744772699624e-05\n",
            "Epoch 870/1000, Training Loss: 2.338669659214296e-05, Test Loss: 2.4910487162533116e-05\n",
            "Epoch 871/1000, Training Loss: 2.3360493665467598e-05, Test Loss: 2.4883588612452692e-05\n",
            "Epoch 872/1000, Training Loss: 2.3334357401314345e-05, Test Loss: 2.4856751756510633e-05\n",
            "Epoch 873/1000, Training Loss: 2.3308287442494877e-05, Test Loss: 2.4829976277259263e-05\n",
            "Epoch 874/1000, Training Loss: 2.3282283434778796e-05, Test Loss: 2.4803261860014307e-05\n",
            "Epoch 875/1000, Training Loss: 2.3256345026860342e-05, Test Loss: 2.477660819281975e-05\n",
            "Epoch 876/1000, Training Loss: 2.3230471870326554e-05, Test Loss: 2.475001496641861e-05\n",
            "Epoch 877/1000, Training Loss: 2.3204663619625045e-05, Test Loss: 2.472348187422131e-05\n",
            "Epoch 878/1000, Training Loss: 2.3178919932032234e-05, Test Loss: 2.46970086122713e-05\n",
            "Epoch 879/1000, Training Loss: 2.3153240467623082e-05, Test Loss: 2.4670594879218184e-05\n",
            "Epoch 880/1000, Training Loss: 2.31276248892392e-05, Test Loss: 2.4644240376284002e-05\n",
            "Epoch 881/1000, Training Loss: 2.3102072862459725e-05, Test Loss: 2.4617944807235145e-05\n",
            "Epoch 882/1000, Training Loss: 2.3076584055571007e-05, Test Loss: 2.459170787835338e-05\n",
            "Epoch 883/1000, Training Loss: 2.3051158139537554e-05, Test Loss: 2.4565529298405387e-05\n",
            "Epoch 884/1000, Training Loss: 2.3025794787972578e-05, Test Loss: 2.453940877861407e-05\n",
            "Epoch 885/1000, Training Loss: 2.3000493677109768e-05, Test Loss: 2.4513346032631785e-05\n",
            "Epoch 886/1000, Training Loss: 2.2975254485774917e-05, Test Loss: 2.4487340776509904e-05\n",
            "Epoch 887/1000, Training Loss: 2.295007689535831e-05, Test Loss: 2.4461392728674947e-05\n",
            "Epoch 888/1000, Training Loss: 2.2924960589787712e-05, Test Loss: 2.4435501609898826e-05\n",
            "Epoch 889/1000, Training Loss: 2.2899905255499944e-05, Test Loss: 2.4409667143272805e-05\n",
            "Epoch 890/1000, Training Loss: 2.2874910581415515e-05, Test Loss: 2.438388905418057e-05\n",
            "Epoch 891/1000, Training Loss: 2.284997625891195e-05, Test Loss: 2.4358167070273675e-05\n",
            "Epoch 892/1000, Training Loss: 2.2825101981797734e-05, Test Loss: 2.4332500921445183e-05\n",
            "Epoch 893/1000, Training Loss: 2.2800287446286607e-05, Test Loss: 2.4306890339804806e-05\n",
            "Epoch 894/1000, Training Loss: 2.277553235097245e-05, Test Loss: 2.4281335059652458e-05\n",
            "Epoch 895/1000, Training Loss: 2.2750836396804352e-05, Test Loss: 2.4255834817455816e-05\n",
            "Epoch 896/1000, Training Loss: 2.272619928706198e-05, Test Loss: 2.4230389351825483e-05\n",
            "Epoch 897/1000, Training Loss: 2.2701620727331397e-05, Test Loss: 2.4204998403490105e-05\n",
            "Epoch 898/1000, Training Loss: 2.2677100425480892e-05, Test Loss: 2.41796617152746e-05\n",
            "Epoch 899/1000, Training Loss: 2.2652638091637443e-05, Test Loss: 2.415437903207474e-05\n",
            "Epoch 900/1000, Training Loss: 2.2628233438163528e-05, Test Loss: 2.4129150100836026e-05\n",
            "Epoch 901/1000, Training Loss: 2.260388617963391e-05, Test Loss: 2.410397467053019e-05\n",
            "Epoch 902/1000, Training Loss: 2.2579596032813107e-05, Test Loss: 2.4078852492134118e-05\n",
            "Epoch 903/1000, Training Loss: 2.2555362716632906e-05, Test Loss: 2.4053783318605196e-05\n",
            "Epoch 904/1000, Training Loss: 2.253118595217013e-05, Test Loss: 2.402876690486372e-05\n",
            "Epoch 905/1000, Training Loss: 2.2507065462625056e-05, Test Loss: 2.4003803007768116e-05\n",
            "Epoch 906/1000, Training Loss: 2.2483000973299632e-05, Test Loss: 2.3978891386094264e-05\n",
            "Epoch 907/1000, Training Loss: 2.2458992211576343e-05, Test Loss: 2.3954031800515822e-05\n",
            "Epoch 908/1000, Training Loss: 2.243503890689731e-05, Test Loss: 2.392922401358441e-05\n",
            "Epoch 909/1000, Training Loss: 2.2411140790743006e-05, Test Loss: 2.3904467789705664e-05\n",
            "Epoch 910/1000, Training Loss: 2.2387297596612546e-05, Test Loss: 2.3879762895124484e-05\n",
            "Epoch 911/1000, Training Loss: 2.2363509060003094e-05, Test Loss: 2.385510909790132e-05\n",
            "Epoch 912/1000, Training Loss: 2.2339774918389816e-05, Test Loss: 2.3830506167893316e-05\n",
            "Epoch 913/1000, Training Loss: 2.2316094911206415e-05, Test Loss: 2.380595387673774e-05\n",
            "Epoch 914/1000, Training Loss: 2.2292468779825724e-05, Test Loss: 2.378145199782835e-05\n",
            "Epoch 915/1000, Training Loss: 2.226889626754043e-05, Test Loss: 2.3757000306302235e-05\n",
            "Epoch 916/1000, Training Loss: 2.224537711954423e-05, Test Loss: 2.3732598579018345e-05\n",
            "Epoch 917/1000, Training Loss: 2.2221911082912286e-05, Test Loss: 2.3708246594535865e-05\n",
            "Epoch 918/1000, Training Loss: 2.219849790658421e-05, Test Loss: 2.3683944133103698e-05\n",
            "Epoch 919/1000, Training Loss: 2.2175137341344844e-05, Test Loss: 2.365969097663824e-05\n",
            "Epoch 920/1000, Training Loss: 2.2151829139806426e-05, Test Loss: 2.3635486908706076e-05\n",
            "Epoch 921/1000, Training Loss: 2.2128573056390605e-05, Test Loss: 2.3611331714506762e-05\n",
            "Epoch 922/1000, Training Loss: 2.210536884731144e-05, Test Loss: 2.3587225180857088e-05\n",
            "Epoch 923/1000, Training Loss: 2.2082216270557482e-05, Test Loss: 2.3563167096173427e-05\n",
            "Epoch 924/1000, Training Loss: 2.205911508587502e-05, Test Loss: 2.353915725045491e-05\n",
            "Epoch 925/1000, Training Loss: 2.203606505475074e-05, Test Loss: 2.3515195435266983e-05\n",
            "Epoch 926/1000, Training Loss: 2.2013065940395228e-05, Test Loss: 2.3491281443725963e-05\n",
            "Epoch 927/1000, Training Loss: 2.1990117507726627e-05, Test Loss: 2.3467415070482356e-05\n",
            "Epoch 928/1000, Training Loss: 2.1967219523354e-05, Test Loss: 2.3443596111704875e-05\n",
            "Epoch 929/1000, Training Loss: 2.1944371755561456e-05, Test Loss: 2.3419824365066817e-05\n",
            "Epoch 930/1000, Training Loss: 2.1921573974291892e-05, Test Loss: 2.339609962972819e-05\n",
            "Epoch 931/1000, Training Loss: 2.1898825951131426e-05, Test Loss: 2.3372421706321348e-05\n",
            "Epoch 932/1000, Training Loss: 2.1876127459294237e-05, Test Loss: 2.334879039693668e-05\n",
            "Epoch 933/1000, Training Loss: 2.1853478273606712e-05, Test Loss: 2.3325205505107504e-05\n",
            "Epoch 934/1000, Training Loss: 2.183087817049254e-05, Test Loss: 2.3301666835795518e-05\n",
            "Epoch 935/1000, Training Loss: 2.180832692795718e-05, Test Loss: 2.327817419537659e-05\n",
            "Epoch 936/1000, Training Loss: 2.1785824325574054e-05, Test Loss: 2.3254727391624758e-05\n",
            "Epoch 937/1000, Training Loss: 2.1763370144468886e-05, Test Loss: 2.323132623370123e-05\n",
            "Epoch 938/1000, Training Loss: 2.1740964167306038e-05, Test Loss: 2.3207970532138552e-05\n",
            "Epoch 939/1000, Training Loss: 2.1718606178273665e-05, Test Loss: 2.3184660098826386e-05\n",
            "Epoch 940/1000, Training Loss: 2.1696295963069624e-05, Test Loss: 2.3161394746999795e-05\n",
            "Epoch 941/1000, Training Loss: 2.167403330888805e-05, Test Loss: 2.3138174291224893e-05\n",
            "Epoch 942/1000, Training Loss: 2.165181800440478e-05, Test Loss: 2.311499854738511e-05\n",
            "Epoch 943/1000, Training Loss: 2.1629649839764687e-05, Test Loss: 2.3091867332669823e-05\n",
            "Epoch 944/1000, Training Loss: 2.1607528606567205e-05, Test Loss: 2.306878046555952e-05\n",
            "Epoch 945/1000, Training Loss: 2.1585454097853633e-05, Test Loss: 2.3045737765814942e-05\n",
            "Epoch 946/1000, Training Loss: 2.1563426108094044e-05, Test Loss: 2.302273905446209e-05\n",
            "Epoch 947/1000, Training Loss: 2.154144443317351e-05, Test Loss: 2.2999784153783476e-05\n",
            "Epoch 948/1000, Training Loss: 2.1519508870379836e-05, Test Loss: 2.2976872887301514e-05\n",
            "Epoch 949/1000, Training Loss: 2.1497619218391217e-05, Test Loss: 2.295400507977038e-05\n",
            "Epoch 950/1000, Training Loss: 2.147577527726304e-05, Test Loss: 2.2931180557162447e-05\n",
            "Epoch 951/1000, Training Loss: 2.145397684841563e-05, Test Loss: 2.2908399146654864e-05\n",
            "Epoch 952/1000, Training Loss: 2.1432223734621793e-05, Test Loss: 2.2885660676620554e-05\n",
            "Epoch 953/1000, Training Loss: 2.1410515739995012e-05, Test Loss: 2.2862964976615194e-05\n",
            "Epoch 954/1000, Training Loss: 2.1388852669977508e-05, Test Loss: 2.2840311877367225e-05\n",
            "Epoch 955/1000, Training Loss: 2.1367234331328007e-05, Test Loss: 2.281770121076444e-05\n",
            "Epoch 956/1000, Training Loss: 2.1345660532110345e-05, Test Loss: 2.2795132809844822e-05\n",
            "Epoch 957/1000, Training Loss: 2.1324131081680993e-05, Test Loss: 2.2772606508783742e-05\n",
            "Epoch 958/1000, Training Loss: 2.1302645790679166e-05, Test Loss: 2.2750122142885294e-05\n",
            "Epoch 959/1000, Training Loss: 2.128120447101435e-05, Test Loss: 2.2727679548570956e-05\n",
            "Epoch 960/1000, Training Loss: 2.1259806935855105e-05, Test Loss: 2.270527856336744e-05\n",
            "Epoch 961/1000, Training Loss: 2.123845299961839e-05, Test Loss: 2.2682919025898383e-05\n",
            "Epoch 962/1000, Training Loss: 2.121714247795849e-05, Test Loss: 2.2660600775871644e-05\n",
            "Epoch 963/1000, Training Loss: 2.1195875187755894e-05, Test Loss: 2.2638323654071716e-05\n",
            "Epoch 964/1000, Training Loss: 2.1174650947106904e-05, Test Loss: 2.2616087502347848e-05\n",
            "Epoch 965/1000, Training Loss: 2.1153469575313013e-05, Test Loss: 2.2593892163604504e-05\n",
            "Epoch 966/1000, Training Loss: 2.11323308928699e-05, Test Loss: 2.257173748179237e-05\n",
            "Epoch 967/1000, Training Loss: 2.1111234721457912e-05, Test Loss: 2.2549623301897133e-05\n",
            "Epoch 968/1000, Training Loss: 2.1090180883931185e-05, Test Loss: 2.2527549469931458e-05\n",
            "Epoch 969/1000, Training Loss: 2.1069169204307518e-05, Test Loss: 2.2505515832923368e-05\n",
            "Epoch 970/1000, Training Loss: 2.1048199507758678e-05, Test Loss: 2.2483522238909725e-05\n",
            "Epoch 971/1000, Training Loss: 2.1027271620600273e-05, Test Loss: 2.2461568536924625e-05\n",
            "Epoch 972/1000, Training Loss: 2.1006385370281722e-05, Test Loss: 2.2439654576990528e-05\n",
            "Epoch 973/1000, Training Loss: 2.09855405853769e-05, Test Loss: 2.241778021011036e-05\n",
            "Epoch 974/1000, Training Loss: 2.09647370955742e-05, Test Loss: 2.2395945288257265e-05\n",
            "Epoch 975/1000, Training Loss: 2.094397473166751e-05, Test Loss: 2.2374149664367406e-05\n",
            "Epoch 976/1000, Training Loss: 2.0923253325546232e-05, Test Loss: 2.2352393192328467e-05\n",
            "Epoch 977/1000, Training Loss: 2.090257271018631e-05, Test Loss: 2.2330675726974932e-05\n",
            "Epoch 978/1000, Training Loss: 2.088193271964107e-05, Test Loss: 2.2308997124074854e-05\n",
            "Epoch 979/1000, Training Loss: 2.0861333189031848e-05, Test Loss: 2.2287357240325072e-05\n",
            "Epoch 980/1000, Training Loss: 2.0840773954539207e-05, Test Loss: 2.226575593334125e-05\n",
            "Epoch 981/1000, Training Loss: 2.0820254853393896e-05, Test Loss: 2.2244193061650816e-05\n",
            "Epoch 982/1000, Training Loss: 2.0799775723868057e-05, Test Loss: 2.222266848468267e-05\n",
            "Epoch 983/1000, Training Loss: 2.077933640526648e-05, Test Loss: 2.220118206276068e-05\n",
            "Epoch 984/1000, Training Loss: 2.0758936737918248e-05, Test Loss: 2.217973365709638e-05\n",
            "Epoch 985/1000, Training Loss: 2.0738576563167672e-05, Test Loss: 2.2158323129778404e-05\n",
            "Epoch 986/1000, Training Loss: 2.0718255723366237e-05, Test Loss: 2.213695034376826e-05\n",
            "Epoch 987/1000, Training Loss: 2.069797406186409e-05, Test Loss: 2.211561516288892e-05\n",
            "Epoch 988/1000, Training Loss: 2.0677731423001666e-05, Test Loss: 2.2094317451820286e-05\n",
            "Epoch 989/1000, Training Loss: 2.0657527652102e-05, Test Loss: 2.207305707608988e-05\n",
            "Epoch 990/1000, Training Loss: 2.063736259546172e-05, Test Loss: 2.205183390206529e-05\n",
            "Epoch 991/1000, Training Loss: 2.0617236100343746e-05, Test Loss: 2.203064779694805e-05\n",
            "Epoch 992/1000, Training Loss: 2.0597148014969255e-05, Test Loss: 2.2009498628765924e-05\n",
            "Epoch 993/1000, Training Loss: 2.0577098188509406e-05, Test Loss: 2.1988386266364367e-05\n",
            "Epoch 994/1000, Training Loss: 2.055708647107825e-05, Test Loss: 2.1967310579401828e-05\n",
            "Epoch 995/1000, Training Loss: 2.0537112713724242e-05, Test Loss: 2.1946271438340334e-05\n",
            "Epoch 996/1000, Training Loss: 2.0517176768423043e-05, Test Loss: 2.1925268714438783e-05\n",
            "Epoch 997/1000, Training Loss: 2.0497278488070224e-05, Test Loss: 2.1904302279749055e-05\n",
            "Epoch 998/1000, Training Loss: 2.047741772647318e-05, Test Loss: 2.188337200710466e-05\n",
            "Epoch 999/1000, Training Loss: 2.0457594338344305e-05, Test Loss: 2.186247777011726e-05\n",
            "Epoch 1000/1000, Training Loss: 2.04378081792934e-05, Test Loss: 2.1841619443168813e-05\n",
            "Epoch 1/1000, Training Loss: 0.003312674983818167, Test Loss: 0.0021109995303847293\n",
            "Epoch 2/1000, Training Loss: 0.003258264116912743, Test Loss: 0.0019608279609700185\n",
            "Epoch 3/1000, Training Loss: 0.0032512511959383304, Test Loss: 0.001926380070645041\n",
            "Epoch 4/1000, Training Loss: 0.00324886656838163, Test Loss: 0.0019141995821056218\n",
            "Epoch 5/1000, Training Loss: 0.0032473205124115404, Test Loss: 0.0019090308844176653\n",
            "Epoch 6/1000, Training Loss: 0.003246053177312555, Test Loss: 0.0019065395724886835\n",
            "Epoch 7/1000, Training Loss: 0.0032449373916479196, Test Loss: 0.00190517590200164\n",
            "Epoch 8/1000, Training Loss: 0.0032439229144297864, Test Loss: 0.0019043228346816215\n",
            "Epoch 9/1000, Training Loss: 0.0032429808684650095, Test Loss: 0.0019037189415190647\n",
            "Epoch 10/1000, Training Loss: 0.0032420923749270337, Test Loss: 0.0019032472104687894\n",
            "Epoch 11/1000, Training Loss: 0.0032412446433734625, Test Loss: 0.001902851609997376\n",
            "Epoch 12/1000, Training Loss: 0.003240428842959469, Test Loss: 0.001902502873255746\n",
            "Epoch 13/1000, Training Loss: 0.003239638742871732, Test Loss: 0.0019021841242604898\n",
            "Epoch 14/1000, Training Loss: 0.003238869825367213, Test Loss: 0.0019018846788831478\n",
            "Epoch 15/1000, Training Loss: 0.003238118711046799, Test Loss: 0.0019015972634684423\n",
            "Epoch 16/1000, Training Loss: 0.0032373827886893387, Test Loss: 0.0019013166871444684\n",
            "Epoch 17/1000, Training Loss: 0.0032366599768616543, Test Loss: 0.0019010391500067004\n",
            "Epoch 18/1000, Training Loss: 0.003235948569430812, Test Loss: 0.0019007618428560683\n",
            "Epoch 19/1000, Training Loss: 0.0032352471342027705, Test Loss: 0.0019004826907435763\n",
            "Epoch 20/1000, Training Loss: 0.003234554445195391, Test Loss: 0.0019002001747298902\n",
            "Epoch 21/1000, Training Loss: 0.0032338694363054305, Test Loss: 0.0018999132010325338\n",
            "Epoch 22/1000, Training Loss: 0.0032331911687119965, Test Loss: 0.0018996210017926385\n",
            "Epoch 23/1000, Training Loss: 0.0032325188072259547, Test Loss: 0.0018993230584965128\n",
            "Epoch 24/1000, Training Loss: 0.0032318516025780382, Test Loss: 0.0018990190423779183\n",
            "Epoch 25/1000, Training Loss: 0.003231188877744827, Test Loss: 0.0018987087678787864\n",
            "Epoch 26/1000, Training Loss: 0.003230530017098241, Test Loss: 0.0018983921562878764\n",
            "Epoch 27/1000, Training Loss: 0.0032298744575911696, Test Loss: 0.0018980692073639644\n",
            "Epoch 28/1000, Training Loss: 0.003229221681458787, Test Loss: 0.0018977399772399483\n",
            "Epoch 29/1000, Training Loss: 0.0032285712100831765, Test Loss: 0.001897404561271388\n",
            "Epoch 30/1000, Training Loss: 0.003227922598775997, Test Loss: 0.0018970630807762393\n",
            "Epoch 31/1000, Training Loss: 0.003227275432303104, Test Loss: 0.0018967156728340456\n",
            "Epoch 32/1000, Training Loss: 0.003226629321020646, Test Loss: 0.0018963624824876737\n",
            "Epoch 33/1000, Training Loss: 0.003225983897522907, Test Loss: 0.0018960036568286861\n",
            "Epoch 34/1000, Training Loss: 0.003225338813723464, Test Loss: 0.0018956393405568429\n",
            "Epoch 35/1000, Training Loss: 0.0032246937383064943, Test Loss: 0.001895269672690698\n",
            "Epoch 36/1000, Training Loss: 0.0032240483544962793, Test Loss: 0.0018948947841746992\n",
            "Epoch 37/1000, Training Loss: 0.0032234023581015152, Test Loss: 0.001894514796182287\n",
            "Epoch 38/1000, Training Loss: 0.003222755455797732, Test Loss: 0.0018941298189572593\n",
            "Epoch 39/1000, Training Loss: 0.003222107363616524, Test Loss: 0.0018937399510694224\n",
            "Epoch 40/1000, Training Loss: 0.0032214578056146657, Test Loss: 0.00189334527898713\n",
            "Epoch 41/1000, Training Loss: 0.0032208065126998952, Test Loss: 0.0018929458768904119\n",
            "Epoch 42/1000, Training Loss: 0.0032201532215932454, Test Loss: 0.0018925418066648683\n",
            "Epoch 43/1000, Training Loss: 0.0032194976739104387, Test Loss: 0.0018921331180295961\n",
            "Epoch 44/1000, Training Loss: 0.003218839615347132, Test Loss: 0.001891719848762716\n",
            "Epoch 45/1000, Training Loss: 0.0032181787949547825, Test Loss: 0.0018913020249960938\n",
            "Epoch 46/1000, Training Loss: 0.0032175149644955627, Test Loss: 0.0018908796615572756\n",
            "Epoch 47/1000, Training Loss: 0.003216847877866271, Test Loss: 0.0018904527623416617\n",
            "Epoch 48/1000, Training Loss: 0.0032161772905824285, Test Loss: 0.0018900213207018277\n",
            "Epoch 49/1000, Training Loss: 0.0032155029593148978, Test Loss: 0.001889585319844045\n",
            "Epoch 50/1000, Training Loss: 0.0032148246414723204, Test Loss: 0.0018891447332244759\n",
            "Epoch 51/1000, Training Loss: 0.0032141420948235333, Test Loss: 0.0018886995249393528\n",
            "Epoch 52/1000, Training Loss: 0.003213455077154834, Test Loss: 0.0018882496501050332\n",
            "Epoch 53/1000, Training Loss: 0.003212763345957669, Test Loss: 0.0018877950552249197\n",
            "Epoch 54/1000, Training Loss: 0.0032120666581428336, Test Loss: 0.0018873356785411317\n",
            "Epoch 55/1000, Training Loss: 0.00321136476977779, Test Loss: 0.001886871450369565\n",
            "Epoch 56/1000, Training Loss: 0.0032106574358441507, Test Loss: 0.0018864022934174862\n",
            "Epoch 57/1000, Training Loss: 0.003209944410012731, Test Loss: 0.001885928123083221\n",
            "Epoch 58/1000, Training Loss: 0.0032092254444339263, Test Loss: 0.001885448847737811\n",
            "Epoch 59/1000, Training Loss: 0.0032085002895414416, Test Loss: 0.0018849643689888263\n",
            "Epoch 60/1000, Training Loss: 0.0032077686938676688, Test Loss: 0.0018844745819265615\n",
            "Epoch 61/1000, Training Loss: 0.0032070304038692115, Test Loss: 0.0018839793753531217\n",
            "Epoch 62/1000, Training Loss: 0.003206285163761271, Test Loss: 0.001883478631994915\n",
            "Epoch 63/1000, Training Loss: 0.0032055327153597568, Test Loss: 0.0018829722286991298\n",
            "Epoch 64/1000, Training Loss: 0.003204772797930143, Test Loss: 0.001882460036614908\n",
            "Epoch 65/1000, Training Loss: 0.0032040051480422237, Test Loss: 0.001881941921359801\n",
            "Epoch 66/1000, Training Loss: 0.003203229499430029, Test Loss: 0.001881417743172284\n",
            "Epoch 67/1000, Training Loss: 0.003202445582856267, Test Loss: 0.001880887357050948\n",
            "Epoch 68/1000, Training Loss: 0.003201653125980736, Test Loss: 0.0018803506128810946\n",
            "Epoch 69/1000, Training Loss: 0.0032008518532322416, Test Loss: 0.0018798073555493775\n",
            "Epoch 70/1000, Training Loss: 0.003200041485683619, Test Loss: 0.0018792574250471744\n",
            "Epoch 71/1000, Training Loss: 0.003199221740929498, Test Loss: 0.0018787006565632982\n",
            "Epoch 72/1000, Training Loss: 0.0031983923329665397, Test Loss: 0.0018781368805666806\n",
            "Epoch 73/1000, Training Loss: 0.0031975529720758887, Test Loss: 0.0018775659228796283\n",
            "Epoch 74/1000, Training Loss: 0.0031967033647076286, Test Loss: 0.0018769876047421948\n",
            "Epoch 75/1000, Training Loss: 0.003195843213367093, Test Loss: 0.0018764017428682584\n",
            "Epoch 76/1000, Training Loss: 0.003194972216502879, Test Loss: 0.0018758081494937853\n",
            "Epoch 77/1000, Training Loss: 0.003194090068396469, Test Loss: 0.001875206632417827\n",
            "Epoch 78/1000, Training Loss: 0.0031931964590533723, Test Loss: 0.0018745969950366911\n",
            "Epoch 79/1000, Training Loss: 0.003192291074095732, Test Loss: 0.0018739790363717728\n",
            "Epoch 80/1000, Training Loss: 0.003191373594656373, Test Loss: 0.001873352551091481\n",
            "Epoch 81/1000, Training Loss: 0.0031904436972742547, Test Loss: 0.001872717329527667\n",
            "Epoch 82/1000, Training Loss: 0.00318950105379135, Test Loss: 0.0018720731576869743\n",
            "Epoch 83/1000, Training Loss: 0.0031885453312509617, Test Loss: 0.001871419817257483\n",
            "Epoch 84/1000, Training Loss: 0.0031875761917975042, Test Loss: 0.001870757085611007\n",
            "Epoch 85/1000, Training Loss: 0.0031865932925778033, Test Loss: 0.0018700847358014336\n",
            "Epoch 86/1000, Training Loss: 0.003185596285643983, Test Loss: 0.0018694025365593914\n",
            "Epoch 87/1000, Training Loss: 0.0031845848178579887, Test Loss: 0.0018687102522836146\n",
            "Epoch 88/1000, Training Loss: 0.003183558530797861, Test Loss: 0.0018680076430292972\n",
            "Epoch 89/1000, Training Loss: 0.0031825170606658293, Test Loss: 0.0018672944644937386\n",
            "Epoch 90/1000, Training Loss: 0.0031814600381983526, Test Loss: 0.001866570467999574\n",
            "Epoch 91/1000, Training Loss: 0.003180387088578211, Test Loss: 0.0018658354004758846\n",
            "Epoch 92/1000, Training Loss: 0.0031792978313487865, Test Loss: 0.0018650890044374416\n",
            "Epoch 93/1000, Training Loss: 0.0031781918803306546, Test Loss: 0.0018643310179623585\n",
            "Epoch 94/1000, Training Loss: 0.003177068843540658, Test Loss: 0.001863561174668428\n",
            "Epoch 95/1000, Training Loss: 0.0031759283231135903, Test Loss: 0.0018627792036883758\n",
            "Epoch 96/1000, Training Loss: 0.0031747699152266874, Test Loss: 0.0018619848296442896\n",
            "Epoch 97/1000, Training Loss: 0.0031735932100270714, Test Loss: 0.0018611777726214803\n",
            "Epoch 98/1000, Training Loss: 0.0031723977915623634, Test Loss: 0.0018603577481420145\n",
            "Epoch 99/1000, Training Loss: 0.0031711832377146363, Test Loss: 0.0018595244671381502\n",
            "Epoch 100/1000, Training Loss: 0.003169949120137924, Test Loss: 0.0018586776359259274\n",
            "Epoch 101/1000, Training Loss: 0.003168695004199501, Test Loss: 0.0018578169561791586\n",
            "Epoch 102/1000, Training Loss: 0.003167420448925157, Test Loss: 0.0018569421249040377\n",
            "Epoch 103/1000, Training Loss: 0.003166125006948696, Test Loss: 0.0018560528344146471\n",
            "Epoch 104/1000, Training Loss: 0.0031648082244659145, Test Loss: 0.0018551487723095562\n",
            "Epoch 105/1000, Training Loss: 0.0031634696411932924, Test Loss: 0.0018542296214498113\n",
            "Epoch 106/1000, Training Loss: 0.0031621087903316958, Test Loss: 0.0018532950599385097\n",
            "Epoch 107/1000, Training Loss: 0.0031607251985353237, Test Loss: 0.0018523447611022716\n",
            "Epoch 108/1000, Training Loss: 0.003159318385886222, Test Loss: 0.0018513783934748064\n",
            "Epoch 109/1000, Training Loss: 0.0031578878658746323, Test Loss: 0.001850395620782881\n",
            "Epoch 110/1000, Training Loss: 0.0031564331453855007, Test Loss: 0.001849396101934936\n",
            "Epoch 111/1000, Training Loss: 0.003154953724691449, Test Loss: 0.0018483794910126384\n",
            "Epoch 112/1000, Training Loss: 0.0031534490974525486, Test Loss: 0.0018473454372656308\n",
            "Epoch 113/1000, Training Loss: 0.0031519187507232237, Test Loss: 0.0018462935851097895\n",
            "Epoch 114/1000, Training Loss: 0.00315036216496664, Test Loss: 0.0018452235741292728\n",
            "Epoch 115/1000, Training Loss: 0.0031487788140769422, Test Loss: 0.001844135039082693\n",
            "Epoch 116/1000, Training Loss: 0.003147168165409704, Test Loss: 0.0018430276099136843\n",
            "Epoch 117/1000, Training Loss: 0.0031455296798209766, Test Loss: 0.0018419009117662296\n",
            "Epoch 118/1000, Training Loss: 0.003143862811715335, Test Loss: 0.0018407545650050716\n",
            "Epoch 119/1000, Training Loss: 0.00314216700910332, Test Loss: 0.0018395881852415565\n",
            "Epoch 120/1000, Training Loss: 0.0031404417136686917, Test Loss: 0.0018384013833652691\n",
            "Epoch 121/1000, Training Loss: 0.0031386863608459303, Test Loss: 0.0018371937655818254\n",
            "Epoch 122/1000, Training Loss: 0.003136900379908416, Test Loss: 0.0018359649334572265\n",
            "Epoch 123/1000, Training Loss: 0.0031350831940677383, Test Loss: 0.001834714483969142\n",
            "Epoch 124/1000, Training Loss: 0.0031332342205845905, Test Loss: 0.0018334420095655625\n",
            "Epoch 125/1000, Training Loss: 0.0031313528708917316, Test Loss: 0.0018321470982312282\n",
            "Epoch 126/1000, Training Loss: 0.0031294385507294837, Test Loss: 0.001830829333562269\n",
            "Epoch 127/1000, Training Loss: 0.00312749066029426, Test Loss: 0.0018294882948495241\n",
            "Epoch 128/1000, Training Loss: 0.003125508594400619, Test Loss: 0.0018281235571709852\n",
            "Epoch 129/1000, Training Loss: 0.0031234917426573626, Test Loss: 0.0018267346914938687\n",
            "Epoch 130/1000, Training Loss: 0.003121439489658181, Test Loss: 0.0018253212647867836\n",
            "Epoch 131/1000, Training Loss: 0.0031193512151873725, Test Loss: 0.0018238828401425239\n",
            "Epoch 132/1000, Training Loss: 0.0031172262944411766, Test Loss: 0.0018224189769120006\n",
            "Epoch 133/1000, Training Loss: 0.0031150640982652382, Test Loss: 0.0018209292308498425\n",
            "Epoch 134/1000, Training Loss: 0.003112863993408759, Test Loss: 0.001819413154272219\n",
            "Epoch 135/1000, Training Loss: 0.0031106253427958617, Test Loss: 0.0018178702962274437\n",
            "Epoch 136/1000, Training Loss: 0.003108347505814731, Test Loss: 0.0018163002026799322\n",
            "Epoch 137/1000, Training Loss: 0.003106029838625052, Test Loss: 0.0018147024167080893\n",
            "Epoch 138/1000, Training Loss: 0.0031036716944843057, Test Loss: 0.0018130764787167245\n",
            "Epoch 139/1000, Training Loss: 0.003101272424093449, Test Loss: 0.001811421926664586\n",
            "Epoch 140/1000, Training Loss: 0.00309883137596251, Test Loss: 0.0018097382963076195\n",
            "Epoch 141/1000, Training Loss: 0.0030963478967966276, Test Loss: 0.001808025121458573\n",
            "Epoch 142/1000, Training Loss: 0.003093821331903035, Test Loss: 0.001806281934263545\n",
            "Epoch 143/1000, Training Loss: 0.003091251025619487, Test Loss: 0.001804508265496092\n",
            "Epoch 144/1000, Training Loss: 0.0030886363217646094, Test Loss: 0.001802703644869502\n",
            "Epoch 145/1000, Training Loss: 0.0030859765641106135, Test Loss: 0.001800867601367866\n",
            "Epoch 146/1000, Training Loss: 0.0030832710968788177, Test Loss: 0.001798999663596498\n",
            "Epoch 147/1000, Training Loss: 0.0030805192652583555, Test Loss: 0.0017970993601523178\n",
            "Epoch 148/1000, Training Loss: 0.00307772041594844, Test Loss: 0.0017951662200147653\n",
            "Epoch 149/1000, Training Loss: 0.003074873897724499, Test Loss: 0.0017931997729577806\n",
            "Epoch 150/1000, Training Loss: 0.003071979062028449, Test Loss: 0.0017911995499833872\n",
            "Epoch 151/1000, Training Loss: 0.003069035263583334, Test Loss: 0.0017891650837773543\n",
            "Epoch 152/1000, Training Loss: 0.003066041861032463, Test Loss: 0.0017870959091874194\n",
            "Epoch 153/1000, Training Loss: 0.0030629982176031702, Test Loss: 0.0017849915637244758\n",
            "Epoch 154/1000, Training Loss: 0.0030599037017951716, Test Loss: 0.0017828515880870862\n",
            "Epoch 155/1000, Training Loss: 0.0030567576880934856, Test Loss: 0.0017806755267096557\n",
            "Epoch 156/1000, Training Loss: 0.0030535595577057227, Test Loss: 0.001778462928334505\n",
            "Epoch 157/1000, Training Loss: 0.003050308699323505, Test Loss: 0.0017762133466080357\n",
            "Epoch 158/1000, Training Loss: 0.003047004509907632, Test Loss: 0.0017739263407010877\n",
            "Epoch 159/1000, Training Loss: 0.003043646395496494, Test Loss: 0.0017716014759535175\n",
            "Epoch 160/1000, Training Loss: 0.0030402337720371155, Test Loss: 0.001769238324542923\n",
            "Epoch 161/1000, Training Loss: 0.003036766066238049, Test Loss: 0.0017668364661773215\n",
            "Epoch 162/1000, Training Loss: 0.0030332427164432105, Test Loss: 0.0017643954888115192\n",
            "Epoch 163/1000, Training Loss: 0.003029663173525551, Test Loss: 0.0017619149893867273\n",
            "Epoch 164/1000, Training Loss: 0.0030260269017993104, Test Loss: 0.0017593945745928796\n",
            "Epoch 165/1000, Training Loss: 0.0030223333799493934, Test Loss: 0.001756833861652991\n",
            "Epoch 166/1000, Training Loss: 0.0030185821019762145, Test Loss: 0.001754232479128649\n",
            "Epoch 167/1000, Training Loss: 0.003014772578154128, Test Loss: 0.0017515900677456567\n",
            "Epoch 168/1000, Training Loss: 0.0030109043360013487, Test Loss: 0.0017489062812386432\n",
            "Epoch 169/1000, Training Loss: 0.003006976921259041, Test Loss: 0.0017461807872132201\n",
            "Epoch 170/1000, Training Loss: 0.003002989898876948, Test Loss: 0.0017434132680241394\n",
            "Epoch 171/1000, Training Loss: 0.0029989428540027497, Test Loss: 0.001740603421667643\n",
            "Epoch 172/1000, Training Loss: 0.002994835392971994, Test Loss: 0.001737750962685998\n",
            "Epoch 173/1000, Training Loss: 0.0029906671442951906, Test Loss: 0.001734855623081998\n",
            "Epoch 174/1000, Training Loss: 0.0029864377596383824, Test Loss: 0.0017319171532409277\n",
            "Epoch 175/1000, Training Loss: 0.0029821469147931533, Test Loss: 0.0017289353228573023\n",
            "Epoch 176/1000, Training Loss: 0.002977794310631786, Test Loss: 0.001725909921863416\n",
            "Epoch 177/1000, Training Loss: 0.002973379674042905, Test Loss: 0.0017228407613564393\n",
            "Epoch 178/1000, Training Loss: 0.002968902758842675, Test Loss: 0.0017197276745206666\n",
            "Epoch 179/1000, Training Loss: 0.0029643633466562543, Test Loss: 0.0017165705175410913\n",
            "Epoch 180/1000, Training Loss: 0.0029597612477639253, Test Loss: 0.0017133691705043606\n",
            "Epoch 181/1000, Training Loss: 0.0029550963019059685, Test Loss: 0.0017101235382828495\n",
            "Epoch 182/1000, Training Loss: 0.002950368379040047, Test Loss: 0.0017068335513973404\n",
            "Epoch 183/1000, Training Loss: 0.0029455773800445583, Test Loss: 0.0017034991668535512\n",
            "Epoch 184/1000, Training Loss: 0.002940723237361118, Test Loss: 0.0017001203689475592\n",
            "Epoch 185/1000, Training Loss: 0.0029358059155690374, Test Loss: 0.00169669717003486\n",
            "Epoch 186/1000, Training Loss: 0.002930825411884417, Test Loss: 0.001693229611257706\n",
            "Epoch 187/1000, Training Loss: 0.0029257817565762085, Test Loss: 0.0016897177632250939\n",
            "Epoch 188/1000, Training Loss: 0.0029206750132913916, Test Loss: 0.0016861617266396714\n",
            "Epoch 189/1000, Training Loss: 0.002915505279281202, Test Loss: 0.0016825616328656446\n",
            "Epoch 190/1000, Training Loss: 0.0029102726855202033, Test Loss: 0.0016789176444317253\n",
            "Epoch 191/1000, Training Loss: 0.0029049773967098572, Test Loss: 0.0016752299554630462\n",
            "Epoch 192/1000, Training Loss: 0.002899619611158154, Test Loss: 0.001671498792035942\n",
            "Epoch 193/1000, Training Loss: 0.0028941995605268446, Test Loss: 0.0016677244124495474\n",
            "Epoch 194/1000, Training Loss: 0.002888717509437787, Test Loss: 0.0016639071074081868\n",
            "Epoch 195/1000, Training Loss: 0.002883173754929996, Test Loss: 0.0016600472001086118\n",
            "Epoch 196/1000, Training Loss: 0.00287756862575909, Test Loss: 0.0016561450462264041\n",
            "Epoch 197/1000, Training Loss: 0.002871902481530967, Test Loss: 0.0016522010337959442\n",
            "Epoch 198/1000, Training Loss: 0.0028661757116617993, Test Loss: 0.0016482155829787128\n",
            "Epoch 199/1000, Training Loss: 0.0028603887341566913, Test Loss: 0.0016441891457149773\n",
            "Epoch 200/1000, Training Loss: 0.0028545419941996795, Test Loss: 0.001640122205254339\n",
            "Epoch 201/1000, Training Loss: 0.0028486359625481982, Test Loss: 0.0016360152755610158\n",
            "Epoch 202/1000, Training Loss: 0.0028426711337255434, Test Loss: 0.001631868900590297\n",
            "Epoch 203/1000, Training Loss: 0.0028366480240054393, Test Loss: 0.0016276836534331594\n",
            "Epoch 204/1000, Training Loss: 0.0028305671691833834, Test Loss: 0.0016234601353266502\n",
            "Epoch 205/1000, Training Loss: 0.0028244291221300732, Test Loss: 0.001619198974528339\n",
            "Epoch 206/1000, Training Loss: 0.0028182344501229417, Test Loss: 0.001614900825053863\n",
            "Epoch 207/1000, Training Loss: 0.0028119837319525386, Test Loss: 0.0016105663652773903\n",
            "Epoch 208/1000, Training Loss: 0.00280567755480133, Test Loss: 0.001606196296395605\n",
            "Epoch 209/1000, Training Loss: 0.0027993165108932566, Test Loss: 0.0016017913407567405\n",
            "Epoch 210/1000, Training Loss: 0.002792901193913313, Test Loss: 0.0015973522400569568\n",
            "Epoch 211/1000, Training Loss: 0.002786432195197215, Test Loss: 0.0015928797534073733\n",
            "Epoch 212/1000, Training Loss: 0.002779910099692201, Test Loss: 0.001588374655275889\n",
            "Epoch 213/1000, Training Loss: 0.0027733354816908275, Test Loss: 0.0015838377333088148\n",
            "Epoch 214/1000, Training Loss: 0.002766708900340581, Test Loss: 0.0015792697860382938\n",
            "Epoch 215/1000, Training Loss: 0.0027600308949329573, Test Loss: 0.0015746716204822373\n",
            "Epoch 216/1000, Training Loss: 0.002753301979976548, Test Loss: 0.0015700440496443943\n",
            "Epoch 217/1000, Training Loss: 0.002746522640059459, Test Loss: 0.0015653878899228439\n",
            "Epoch 218/1000, Training Loss: 0.002739693324507183, Test Loss: 0.0015607039584359748\n",
            "Epoch 219/1000, Training Loss: 0.0027328144418427335, Test Loss: 0.0015559930702755162\n",
            "Epoch 220/1000, Training Loss: 0.0027258863540565127, Test Loss: 0.001551256035696729\n",
            "Epoch 221/1000, Training Loss: 0.002718909370693915, Test Loss: 0.0015464936572562467\n",
            "Epoch 222/1000, Training Loss: 0.0027118837427692107, Test Loss: 0.0015417067269082683\n",
            "Epoch 223/1000, Training Loss: 0.002704809656514541, Test Loss: 0.0015368960230699752\n",
            "Epoch 224/1000, Training Loss: 0.0026976872269732486, Test Loss: 0.0015320623076669178\n",
            "Epoch 225/1000, Training Loss: 0.002690516491446847, Test Loss: 0.00152720632316899\n",
            "Epoch 226/1000, Training Loss: 0.002683297402805037, Test Loss: 0.0015223287896270784\n",
            "Epoch 227/1000, Training Loss: 0.0026760298226681466, Test Loss: 0.0015174304017200248\n",
            "Epoch 228/1000, Training Loss: 0.002668713514471149, Test Loss: 0.0015125118258206393\n",
            "Epoch 229/1000, Training Loss: 0.0026613481364181745, Test Loss: 0.0015075736970884687\n",
            "Epoch 230/1000, Training Loss: 0.002653933234336039, Test Loss: 0.0015026166165958926\n",
            "Epoch 231/1000, Training Loss: 0.0026464682344347636, Test Loss: 0.0014976411484925374\n",
            "Epoch 232/1000, Training Loss: 0.0026389524359825235, Test Loss: 0.0014926478172113153\n",
            "Epoch 233/1000, Training Loss: 0.0026313850039017235, Test Loss: 0.0014876371047175033\n",
            "Epoch 234/1000, Training Loss: 0.0026237649612921255, Test Loss: 0.0014826094477999885\n",
            "Epoch 235/1000, Training Loss: 0.0026160911818861442, Test Loss: 0.0014775652354014237\n",
            "Epoch 236/1000, Training Loss: 0.00260836238244051, Test Loss: 0.0014725048059813573\n",
            "Epoch 237/1000, Training Loss: 0.0026005771150675594, Test Loss: 0.0014674284449033958\n",
            "Epoch 238/1000, Training Loss: 0.0025927337595085267, Test Loss: 0.0014623363818343576\n",
            "Epoch 239/1000, Training Loss: 0.0025848305153502314, Test Loss: 0.0014572287881399913\n",
            "Epoch 240/1000, Training Loss: 0.002576865394185759, Test Loss: 0.001452105774258147\n",
            "Epoch 241/1000, Training Loss: 0.0025688362117188635, Test Loss: 0.0014469673870265614\n",
            "Epoch 242/1000, Training Loss: 0.0025607405798112287, Test Loss: 0.0014418136069384371\n",
            "Epoch 243/1000, Training Loss: 0.0025525758984711613, Test Loss: 0.001436644345294856\n",
            "Epoch 244/1000, Training Loss: 0.002544339347782093, Test Loss: 0.0014314594412189402\n",
            "Epoch 245/1000, Training Loss: 0.0025360278797692346, Test Loss: 0.0014262586584924159\n",
            "Epoch 246/1000, Training Loss: 0.0025276382102031804, Test Loss: 0.0014210416821710565\n",
            "Epoch 247/1000, Training Loss: 0.0025191668103401238, Test Loss: 0.0014158081149314788\n",
            "Epoch 248/1000, Training Loss: 0.0025106098985998564, Test Loss: 0.0014105574730979165\n",
            "Epoch 249/1000, Training Loss: 0.002501963432184885, Test Loss: 0.001405289182294164\n",
            "Epoch 250/1000, Training Loss: 0.002493223098647221, Test Loss: 0.0014000025726631233\n",
            "Epoch 251/1000, Training Loss: 0.0024843843074135786, Test Loss: 0.0013946968735941416\n",
            "Epoch 252/1000, Training Loss: 0.0024754421812853704, Test Loss: 0.0013893712078973369\n",
            "Epoch 253/1000, Training Loss: 0.0024663915479371896, Test Loss: 0.0013840245853642724\n",
            "Epoch 254/1000, Training Loss: 0.002457226931446779, Test Loss: 0.0013786558956563008\n",
            "Epoch 255/1000, Training Loss: 0.0024479425439012836, Test Loss: 0.0013732639004658009\n",
            "Epoch 256/1000, Training Loss: 0.0024385322771392904, Test Loss: 0.0013678472249021951\n",
            "Epoch 257/1000, Training Loss: 0.002428989694706451, Test Loss: 0.0013624043480642998\n",
            "Epoch 258/1000, Training Loss: 0.0024193080241249583, Test Loss: 0.0013569335927742271\n",
            "Epoch 259/1000, Training Loss: 0.002409480149604589, Test Loss: 0.0013514331144661084\n",
            "Epoch 260/1000, Training Loss: 0.0023994986053563015, Test Loss: 0.0013459008892465006\n",
            "Epoch 261/1000, Training Loss: 0.002389355569709423, Test Loss: 0.0013403347011731977\n",
            "Epoch 262/1000, Training Loss: 0.0023790428602812335, Test Loss: 0.0013347321288363228\n",
            "Epoch 263/1000, Training Loss: 0.0023685519305043316, Test Loss: 0.001329090531371062\n",
            "Epoch 264/1000, Training Loss: 0.0023578738678834456, Test Loss: 0.0013234070340861733\n",
            "Epoch 265/1000, Training Loss: 0.002346999394430255, Test Loss: 0.0013176785139574728\n",
            "Epoch 266/1000, Training Loss: 0.00233591886981291, Test Loss: 0.0013119015853116068\n",
            "Epoch 267/1000, Training Loss: 0.002324622297856409, Test Loss: 0.0013060725861129133\n",
            "Epoch 268/1000, Training Loss: 0.002313099337140397, Test Loss: 0.0013001875653653493\n",
            "Epoch 269/1000, Training Loss: 0.0023013393165609145, Test Loss: 0.0012942422722511733\n",
            "Epoch 270/1000, Training Loss: 0.0022893312568493963, Test Loss: 0.0012882321477470276\n",
            "Epoch 271/1000, Training Loss: 0.0022770638991714085, Test Loss: 0.001282152319582857\n",
            "Epoch 272/1000, Training Loss: 0.002264525742052667, Test Loss: 0.0012759976015351338\n",
            "Epoch 273/1000, Training Loss: 0.002251705087991417, Test Loss: 0.0012697624981661082\n",
            "Epoch 274/1000, Training Loss: 0.0022385901012017526, Test Loss: 0.0012634412162254958\n",
            "Epoch 275/1000, Training Loss: 0.002225168877976027, Test Loss: 0.0012570276840080227\n",
            "Epoch 276/1000, Training Loss: 0.0022114295311360644, Test Loss: 0.0012505155799935517\n",
            "Epoch 277/1000, Training Loss: 0.002197360289939493, Test Loss: 0.0012438983720677625\n",
            "Epoch 278/1000, Training Loss: 0.0021829496165930704, Test Loss: 0.0012371693685099044\n",
            "Epoch 279/1000, Training Loss: 0.002168186340172413, Test Loss: 0.0012303217817178106\n",
            "Epoch 280/1000, Training Loss: 0.0021530598082319997, Test Loss: 0.0012233488052989776\n",
            "Epoch 281/1000, Training Loss: 0.002137560055690141, Test Loss: 0.001216243704673459\n",
            "Epoch 282/1000, Training Loss: 0.002121677989681858, Test Loss: 0.001208999920702184\n",
            "Epoch 283/1000, Training Loss: 0.002105405587993926, Test Loss: 0.0012016111850775267\n",
            "Epoch 284/1000, Training Loss: 0.002088736107459706, Test Loss: 0.001194071645315291\n",
            "Epoch 285/1000, Training Loss: 0.0020716642973527964, Test Loss: 0.0011863759962131857\n",
            "Epoch 286/1000, Training Loss: 0.002054186611464938, Test Loss: 0.0011785196136593402\n",
            "Epoch 287/1000, Training Loss: 0.00203630141130279, Test Loss: 0.0011704986857771539\n",
            "Epoch 288/1000, Training Loss: 0.0020180091518327815, Test Loss: 0.0011623103356883005\n",
            "Epoch 289/1000, Training Loss: 0.001999312540598738, Test Loss: 0.0011539527297785392\n",
            "Epoch 290/1000, Training Loss: 0.001980216660983848, Test Loss: 0.0011454251653659814\n",
            "Epoch 291/1000, Training Loss: 0.0019607290510076274, Test Loss: 0.0011367281321749934\n",
            "Epoch 292/1000, Training Loss: 0.0019408597304071866, Test Loss: 0.0011278633430400003\n",
            "Epoch 293/1000, Training Loss: 0.0019206211708402193, Test Loss: 0.0011188337307697894\n",
            "Epoch 294/1000, Training Loss: 0.001900028206764523, Test Loss: 0.001109643409994052\n",
            "Epoch 295/1000, Training Loss: 0.001879097887703736, Test Loss: 0.0011002976049283252\n",
            "Epoch 296/1000, Training Loss: 0.001857849275933752, Test Loss: 0.0010908025461252274\n",
            "Epoch 297/1000, Training Loss: 0.0018363031968068435, Test Loss: 0.0010811653412085408\n",
            "Epoch 298/1000, Training Loss: 0.0018144819516560113, Test Loss: 0.0010713938261096073\n",
            "Epoch 299/1000, Training Loss: 0.0017924090052199278, Test Loss: 0.0010614964042912987\n",
            "Epoch 300/1000, Training Loss: 0.001770108660610866, Test Loss: 0.0010514818817737547\n",
            "Epoch 301/1000, Training Loss: 0.0017476057349381014, Test Loss: 0.001041359305473103\n",
            "Epoch 302/1000, Training Loss: 0.001724925247836685, Test Loss: 0.0010311378115088854\n",
            "Epoch 303/1000, Training Loss: 0.001702092133482752, Test Loss: 0.0010208264888677528\n",
            "Epoch 304/1000, Training Loss: 0.0016791309844230806, Test Loss: 0.0010104342622990487\n",
            "Epoch 305/1000, Training Loss: 0.0016560658329700733, Test Loss: 0.0009999697967361494\n",
            "Epoch 306/1000, Training Loss: 0.001632919973274776, Test Loss: 0.0009894414240374052\n",
            "Epoch 307/1000, Training Loss: 0.0016097158247184284, Test Loss: 0.0009788570915369595\n",
            "Epoch 308/1000, Training Loss: 0.0015864748351310364, Test Loss: 0.0009682243308596847\n",
            "Epoch 309/1000, Training Loss: 0.0015632174206615765, Test Loss: 0.0009575502447133871\n",
            "Epoch 310/1000, Training Loss: 0.0015399629379334678, Test Loss: 0.0009468415089181794\n",
            "Epoch 311/1000, Training Loss: 0.0015167296834104299, Test Loss: 0.0009361043867348131\n",
            "Epoch 312/1000, Training Loss: 0.0014935349146214833, Test Loss: 0.0009253447525641102\n",
            "Epoch 313/1000, Training Loss: 0.0014703948879743094, Test Loss: 0.0009145681222567206\n",
            "Epoch 314/1000, Training Loss: 0.0014473249082366695, Test Loss: 0.0009037796875459886\n",
            "Epoch 315/1000, Training Loss: 0.0014243393853006915, Test Loss: 0.00089298435245342\n",
            "Epoch 316/1000, Training Loss: 0.0014014518944866943, Test Loss: 0.0008821867698801934\n",
            "Epoch 317/1000, Training Loss: 0.0013786752373265005, Test Loss: 0.0008713913769621194\n",
            "Epoch 318/1000, Training Loss: 0.0013560215004415166, Test Loss: 0.0008606024281108058\n",
            "Epoch 319/1000, Training Loss: 0.0013335021107611638, Test Loss: 0.000849824024977572\n",
            "Epoch 320/1000, Training Loss: 0.001311127885890215, Test Loss: 0.0008390601428518156\n",
            "Epoch 321/1000, Training Loss: 0.0012889090789165126, Test Loss: 0.0008283146532395082\n",
            "Epoch 322/1000, Training Loss: 0.0012668554173491666, Test Loss: 0.0008175913425590333\n",
            "Epoch 323/1000, Training Loss: 0.001244976136194043, Test Loss: 0.0008068939270441638\n",
            "Epoch 324/1000, Training Loss: 0.001223280005413671, Test Loss: 0.0007962260640589644\n",
            "Epoch 325/1000, Training Loss: 0.0012017753521914641, Test Loss: 0.0007855913601125099\n",
            "Epoch 326/1000, Training Loss: 0.0011804700785347762, Test Loss: 0.0007749933759156073\n",
            "Epoch 327/1000, Training Loss: 0.0011593716748175894, Test Loss: 0.0007644356288523951\n",
            "Epoch 328/1000, Training Loss: 0.0011384872298912175, Test Loss: 0.0007539215932507933\n",
            "Epoch 329/1000, Training Loss: 0.0011178234383886693, Test Loss: 0.0007434546988314114\n",
            "Epoch 330/1000, Training Loss: 0.0010973866058229939, Test Loss: 0.000733038327698292\n",
            "Epoch 331/1000, Training Loss: 0.001077182652038788, Test Loss: 0.0007226758102102747\n",
            "Epoch 332/1000, Training Loss: 0.0010572171135239988, Test Loss: 0.0007123704200411149\n",
            "Epoch 333/1000, Training Loss: 0.001037495145031158, Test Loss: 0.0007021253687028047\n",
            "Epoch 334/1000, Training Loss: 0.0010180215208965156, Test Loss: 0.0006919437997709916\n",
            "Epoch 335/1000, Training Loss: 0.0009988006363847147, Test Loss: 0.0006818287830163461\n",
            "Epoch 336/1000, Training Loss: 0.0009798365093280708, Test Loss: 0.0006717833086111557\n",
            "Epoch 337/1000, Training Loss: 0.0009611327822742649, Test Loss: 0.0006618102815489782\n",
            "Epoch 338/1000, Training Loss: 0.0009426927253053315, Test Loss: 0.0006519125163850097\n",
            "Epoch 339/1000, Training Loss: 0.0009245192396451277, Test Loss: 0.0006420927323789444\n",
            "Epoch 340/1000, Training Loss: 0.000906614862131642, Test Loss: 0.0006323535490984273\n",
            "Epoch 341/1000, Training Loss: 0.0008889817705953568, Test Loss: 0.0006226974825210807\n",
            "Epoch 342/1000, Training Loss: 0.00087162179015463, Test Loss: 0.0006131269416560911\n",
            "Epoch 343/1000, Training Loss: 0.0008545364004138345, Test Loss: 0.0006036442256921568\n",
            "Epoch 344/1000, Training Loss: 0.0008377267435294586, Test Loss: 0.0005942515216670322\n",
            "Epoch 345/1000, Training Loss: 0.0008211936330929247, Test Loss: 0.0005849509026451644\n",
            "Epoch 346/1000, Training Loss: 0.0008049375637661856, Test Loss: 0.0005757443263827007\n",
            "Epoch 347/1000, Training Loss: 0.0007889587215971842, Test Loss: 0.0005666336344545774\n",
            "Epoch 348/1000, Training Loss: 0.0007732569949357274, Test Loss: 0.0005576205518148227\n",
            "Epoch 349/1000, Training Loss: 0.0007578319858668671, Test Loss: 0.000548706686759307\n",
            "Epoch 350/1000, Training Loss: 0.0007426830220771526, Test Loss: 0.0005398935312592682\n",
            "Epoch 351/1000, Training Loss: 0.000727809169069554, Test Loss: 0.0005311824616339675\n",
            "Epoch 352/1000, Training Loss: 0.000713209242644557, Test Loss: 0.0005225747395314855\n",
            "Epoch 353/1000, Training Loss: 0.0006988818215678509, Test Loss: 0.0005140715131878531\n",
            "Epoch 354/1000, Training Loss: 0.0006848252603490365, Test Loss: 0.0005056738189362582\n",
            "Epoch 355/1000, Training Loss: 0.0006710377020601988, Test Loss: 0.0004973825829398244\n",
            "Epoch 356/1000, Training Loss: 0.000657517091128374, Test Loss: 0.0004891986231233214\n",
            "Epoch 357/1000, Training Loss: 0.0006442611860412995, Test Loss: 0.00048112265128117575\n",
            "Epoch 358/1000, Training Loss: 0.00063126757191134, Test Loss: 0.00047315527534090454\n",
            "Epoch 359/1000, Training Loss: 0.0006185336728481372, Test Loss: 0.0004652970017630968\n",
            "Epoch 360/1000, Training Loss: 0.0006060567640960309, Test Loss: 0.0004575482380605691\n",
            "Epoch 361/1000, Training Loss: 0.0005938339838977994, Test Loss: 0.0004499092954211102\n",
            "Epoch 362/1000, Training Loss: 0.0005818623450513193, Test Loss: 0.000442380391419483\n",
            "Epoch 363/1000, Training Loss: 0.0005701387461309044, Test Loss: 0.00043496165280572645\n",
            "Epoch 364/1000, Training Loss: 0.0005586599823496711, Test Loss: 0.0004276531183579761\n",
            "Epoch 365/1000, Training Loss: 0.0005474227560436153, Test Loss: 0.00042045474178882005\n",
            "Epoch 366/1000, Training Loss: 0.000536423686762295, Test Loss: 0.00041336639469528413\n",
            "Epoch 367/1000, Training Loss: 0.0005256593209546558, Test Loss: 0.00040638786954319843\n",
            "Epoch 368/1000, Training Loss: 0.0005151261412419547, Test Loss: 0.00039951888267726653\n",
            "Epoch 369/1000, Training Loss: 0.0005048205752728924, Test Loss: 0.0003927590773487623\n",
            "Epoch 370/1000, Training Loss: 0.0004947390041588549, Test Loss: 0.00038610802675338035\n",
            "Epoch 371/1000, Training Loss: 0.00048487777048958743, Test Loss: 0.00037956523707185515\n",
            "Epoch 372/1000, Training Loss: 0.00047523318593190955, Test Loss: 0.0003731301505067008\n",
            "Epoch 373/1000, Training Loss: 0.00046580153841599024, Test Loss: 0.00036680214830845455\n",
            "Epoch 374/1000, Training Loss: 0.00045657909891543924, Test Loss: 0.00036058055378535725\n",
            "Epoch 375/1000, Training Loss: 0.00044756212782885274, Test Loss: 0.00035446463529043915\n",
            "Epoch 376/1000, Training Loss: 0.00043874688097176554, Test Loss: 0.00034845360918048766\n",
            "Epoch 377/1000, Training Loss: 0.00043012961518899996, Test Loss: 0.00034254664274148617\n",
            "Epoch 378/1000, Training Loss: 0.000421706593598236, Test Loss: 0.00033674285707547153\n",
            "Epoch 379/1000, Training Loss: 0.0004134740904763935, Test Loss: 0.00033104132994401554\n",
            "Epoch 380/1000, Training Loss: 0.00040542839580090073, Test Loss: 0.00032544109856381807\n",
            "Epoch 381/1000, Training Loss: 0.0003975658194583813, Test Loss: 0.0003199411623501446\n",
            "Epoch 382/1000, Training Loss: 0.00038988269513361737, Test Loss: 0.0003145404856042888\n",
            "Epoch 383/1000, Training Loss: 0.0003823753838917882, Test Loss: 0.0003092380001413357\n",
            "Epoch 384/1000, Training Loss: 0.0003750402774671267, Test Loss: 0.00030403260785499695\n",
            "Epoch 385/1000, Training Loss: 0.00036787380127112926, Test Loss: 0.00029892318321653286\n",
            "Epoch 386/1000, Training Loss: 0.0003608724171333799, Test Loss: 0.0002939085757050359\n",
            "Epoch 387/1000, Training Loss: 0.00035403262578801055, Test Loss: 0.0002889876121668467\n",
            "Epoch 388/1000, Training Loss: 0.0003473509691185131, Test Loss: 0.00028415909910198256\n",
            "Epoch 389/1000, Training Loss: 0.00034082403217351715, Test Loss: 0.0002794218248759299\n",
            "Epoch 390/1000, Training Loss: 0.00033444844496580985, Test Loss: 0.0002747745618554362\n",
            "Epoch 391/1000, Training Loss: 0.00032822088406660773, Test Loss: 0.00027021606846715256\n",
            "Epoch 392/1000, Training Loss: 0.00032213807400674596, Test Loss: 0.0002657450911784136\n",
            "Epoch 393/1000, Training Loss: 0.00031619678849612395, Test Loss: 0.0002613603663996098\n",
            "Epoch 394/1000, Training Loss: 0.0003103938514723902, Test Loss: 0.00025706062230798876\n",
            "Epoch 395/1000, Training Loss: 0.0003047261379893951, Test Loss: 0.0002528445805928277\n",
            "Epoch 396/1000, Training Loss: 0.00029919057495568827, Test Loss: 0.0002487109581223904\n",
            "Epoch 397/1000, Training Loss: 0.000293784141732773, Test Loss: 0.0002446584685330551\n",
            "Epoch 398/1000, Training Loss: 0.00028850387060263064, Test Loss: 0.0002406858237414813\n",
            "Epoch 399/1000, Training Loss: 0.0002833468471133928, Test Loss: 0.00023679173538061199\n",
            "Epoch 400/1000, Training Loss: 0.0002783102103118809, Test Loss: 0.00023297491616073478\n",
            "Epoch 401/1000, Training Loss: 0.00027339115287116896, Test Loss: 0.00022923408115685777\n",
            "Epoch 402/1000, Training Loss: 0.00026858692112099596, Test Loss: 0.0002255679490237874\n",
            "Epoch 403/1000, Training Loss: 0.0002638948149884901, Test Loss: 0.00022197524314054395\n",
            "Epoch 404/1000, Training Loss: 0.0002593121878562387, Test Loss: 0.00021845469268574034\n",
            "Epoch 405/1000, Training Loss: 0.00025483644634444815, Test Loss: 0.00021500503364577997\n",
            "Epoch 406/1000, Training Loss: 0.00025046505002348466, Test Loss: 0.0002116250097576704\n",
            "Epoch 407/1000, Training Loss: 0.00024619551106283164, Test Loss: 0.00020831337338846447\n",
            "Epoch 408/1000, Training Loss: 0.00024202539382210065, Test Loss: 0.00020506888635331462\n",
            "Epoch 409/1000, Training Loss: 0.00023795231438945472, Test Loss: 0.00020189032067420524\n",
            "Epoch 410/1000, Training Loss: 0.00023397394007242987, Test Loss: 0.00019877645928143725\n",
            "Epoch 411/1000, Training Loss: 0.0002300879888459232, Test Loss: 0.0001957260966600115\n",
            "Epoch 412/1000, Training Loss: 0.00022629222876174922, Test Loss: 0.0001927380394430226\n",
            "Epoch 413/1000, Training Loss: 0.00022258447732392223, Test Loss: 0.00018981110695418253\n",
            "Epoch 414/1000, Training Loss: 0.00021896260083356864, Test Loss: 0.00018694413170163596\n",
            "Epoch 415/1000, Training Loss: 0.0002154245137071159, Test Loss: 0.00018413595982518657\n",
            "Epoch 416/1000, Training Loss: 0.00021196817777110536, Test Loss: 0.00018138545149896584\n",
            "Epoch 417/1000, Training Loss: 0.00020859160153687524, Test Loss: 0.0001786914812917173\n",
            "Epoch 418/1000, Training Loss: 0.00020529283945796631, Test Loss: 0.00017605293848664286\n",
            "Epoch 419/1000, Training Loss: 0.00020206999117307632, Test Loss: 0.00017346872736286433\n",
            "Epoch 420/1000, Training Loss: 0.00019892120073705246, Test Loss: 0.00017093776744044647\n",
            "Epoch 421/1000, Training Loss: 0.00019584465584228565, Test Loss: 0.00016845899369088177\n",
            "Epoch 422/1000, Training Loss: 0.00019283858703269863, Test Loss: 0.00016603135671492536\n",
            "Epoch 423/1000, Training Loss: 0.000189901266912333, Test Loss: 0.00016365382288956594\n",
            "Epoch 424/1000, Training Loss: 0.00018703100935039133, Test Loss: 0.0001613253744859316\n",
            "Epoch 425/1000, Training Loss: 0.0001842261686844199, Test Loss: 0.0001590450097597786\n",
            "Epoch 426/1000, Training Loss: 0.0001814851389232335, Test Loss: 0.00015681174301626245\n",
            "Epoch 427/1000, Training Loss: 0.00017880635295099332, Test Loss: 0.00015462460465055607\n",
            "Epoch 428/1000, Training Loss: 0.0001761882817337448, Test Loss: 0.00015248264116582964\n",
            "Epoch 429/1000, Training Loss: 0.00017362943352962468, Test Loss: 0.000150384915170107\n",
            "Epoch 430/1000, Training Loss: 0.00017112835310383013, Test Loss: 0.000148330505353383\n",
            "Epoch 431/1000, Training Loss: 0.00016868362094928826, Test Loss: 0.00014631850644632847\n",
            "Epoch 432/1000, Training Loss: 0.00016629385251397996, Test Loss: 0.0001443480291619643\n",
            "Epoch 433/1000, Training Loss: 0.00016395769743568603, Test Loss: 0.00014241820012144953\n",
            "Epoch 434/1000, Training Loss: 0.0001616738387848554, Test Loss: 0.0001405281617652289\n",
            "Epoch 435/1000, Training Loss: 0.00015944099231627787, Test Loss: 0.00013867707225064755\n",
            "Epoch 436/1000, Training Loss: 0.00015725790573008943, Test Loss: 0.00013686410533709194\n",
            "Epoch 437/1000, Training Loss: 0.00015512335794264355, Test Loss: 0.00013508845025971504\n",
            "Epoch 438/1000, Training Loss: 0.00015303615836767133, Test Loss: 0.00013334931159267628\n",
            "Epoch 439/1000, Training Loss: 0.00015099514620811084, Test Loss: 0.00013164590910284292\n",
            "Epoch 440/1000, Training Loss: 0.00014899918975895754, Test Loss: 0.0001299774775948233\n",
            "Epoch 441/1000, Training Loss: 0.00014704718572137096, Test Loss: 0.00012834326674811768\n",
            "Epoch 442/1000, Training Loss: 0.00014513805852831648, Test Loss: 0.0001267425409472267\n",
            "Epoch 443/1000, Training Loss: 0.0001432707596819131, Test Loss: 0.00012517457910540507\n",
            "Epoch 444/1000, Training Loss: 0.00014144426710263229, Test Loss: 0.00012363867448277158\n",
            "Epoch 445/1000, Training Loss: 0.00013965758449049532, Test Loss: 0.00012213413449943948\n",
            "Epoch 446/1000, Training Loss: 0.00013790974069830388, Test Loss: 0.00012066028054423148\n",
            "Epoch 447/1000, Training Loss: 0.00013619978911700954, Test Loss: 0.00011921644777964169\n",
            "Epoch 448/1000, Training Loss: 0.00013452680707322576, Test Loss: 0.00011780198494348551\n",
            "Epoch 449/1000, Training Loss: 0.00013288989523887036, Test Loss: 0.0001164162541478311\n",
            "Epoch 450/1000, Training Loss: 0.00013128817705294326, Test Loss: 0.00011505863067562042\n",
            "Epoch 451/1000, Training Loss: 0.0001297207981553912, Test Loss: 0.00011372850277547725\n",
            "Epoch 452/1000, Training Loss: 0.0001281869258329825, Test Loss: 0.00011242527145506408\n",
            "Epoch 453/1000, Training Loss: 0.0001266857484771312, Test Loss: 0.00011114835027341206\n",
            "Epoch 454/1000, Training Loss: 0.00012521647505357294, Test Loss: 0.00010989716513256514\n",
            "Epoch 455/1000, Training Loss: 0.0001237783345837808, Test Loss: 0.00010867115406887134\n",
            "Epoch 456/1000, Training Loss: 0.00012237057563801193, Test Loss: 0.00010746976704423765\n",
            "Epoch 457/1000, Training Loss: 0.00012099246583983629, Test Loss: 0.00010629246573761063\n",
            "Epoch 458/1000, Training Loss: 0.00011964329138203992, Test Loss: 0.00010513872333700948\n",
            "Epoch 459/1000, Training Loss: 0.00011832235655369828, Test Loss: 0.00010400802433224638\n",
            "Epoch 460/1000, Training Loss: 0.00011702898327834093, Test Loss: 0.0001028998643086858\n",
            "Epoch 461/1000, Training Loss: 0.00011576251066296976, Test Loss: 0.00010181374974214158\n",
            "Epoch 462/1000, Training Loss: 0.00011452229455780841, Test Loss: 0.00010074919779516911\n",
            "Epoch 463/1000, Training Loss: 0.00011330770712659689, Test Loss: 9.970573611489774e-05\n",
            "Epoch 464/1000, Training Loss: 0.00011211813642724222, Test Loss: 9.868290263254529e-05\n",
            "Epoch 465/1000, Training Loss: 0.00011095298600266378, Test Loss: 9.768024536479962e-05\n",
            "Epoch 466/1000, Training Loss: 0.00010981167448164363, Test Loss: 9.669732221716521e-05\n",
            "Epoch 467/1000, Training Loss: 0.00010869363518948507, Test Loss: 9.57337007893932e-05\n",
            "Epoch 468/1000, Training Loss: 0.00010759831576830942, Test Loss: 9.478895818310706e-05\n",
            "Epoch 469/1000, Training Loss: 0.00010652517780678863, Test Loss: 9.386268081172278e-05\n",
            "Epoch 470/1000, Training Loss: 0.0001054736964791393, Test Loss: 9.295446421273278e-05\n",
            "Epoch 471/1000, Training Loss: 0.00010444336019317303, Test Loss: 9.206391286243015e-05\n",
            "Epoch 472/1000, Training Loss: 0.00010343367024723171, Test Loss: 9.119063999315271e-05\n",
            "Epoch 473/1000, Training Loss: 0.00010244414049580497, Test Loss: 9.033426741307533e-05\n",
            "Epoch 474/1000, Training Loss: 0.00010147429702365444, Test Loss: 8.949442532862527e-05\n",
            "Epoch 475/1000, Training Loss: 0.00010052367782825578, Test Loss: 8.867075216954608e-05\n",
            "Epoch 476/1000, Training Loss: 9.95918325103607e-05, Test Loss: 8.786289441662817e-05\n",
            "Epoch 477/1000, Training Loss: 9.867832197252707e-05, Test Loss: 8.707050643218852e-05\n",
            "Epoch 478/1000, Training Loss: 9.778271812539786e-05, Test Loss: 8.629325029323481e-05\n",
            "Epoch 479/1000, Training Loss: 9.690460360158227e-05, Test Loss: 8.553079562741514e-05\n",
            "Epoch 480/1000, Training Loss: 9.604357147694685e-05, Test Loss: 8.478281945170498e-05\n",
            "Epoch 481/1000, Training Loss: 9.519922499914777e-05, Test Loss: 8.404900601386782e-05\n",
            "Epoch 482/1000, Training Loss: 9.437117732321695e-05, Test Loss: 8.332904663666218e-05\n",
            "Epoch 483/1000, Training Loss: 9.355905125406337e-05, Test Loss: 8.26226395648352e-05\n",
            "Epoch 484/1000, Training Loss: 9.27624789956934e-05, Test Loss: 8.192948981484491e-05\n",
            "Epoch 485/1000, Training Loss: 9.198110190700086e-05, Test Loss: 8.124930902734677e-05\n",
            "Epoch 486/1000, Training Loss: 9.121457026396769e-05, Test Loss: 8.058181532240003e-05\n",
            "Epoch 487/1000, Training Loss: 9.046254302810609e-05, Test Loss: 7.992673315739395e-05\n",
            "Epoch 488/1000, Training Loss: 8.972468762099256e-05, Test Loss: 7.928379318766586e-05\n",
            "Epoch 489/1000, Training Loss: 8.90006797047484e-05, Test Loss: 7.865273212980483e-05\n",
            "Epoch 490/1000, Training Loss: 8.829020296830472e-05, Test Loss: 7.8033292627595e-05\n",
            "Epoch 491/1000, Training Loss: 8.759294891931462e-05, Test Loss: 7.742522312059769e-05\n",
            "Epoch 492/1000, Training Loss: 8.690861668155988e-05, Test Loss: 7.68282777153222e-05\n",
            "Epoch 493/1000, Training Loss: 8.623691279772631e-05, Test Loss: 7.624221605898581e-05\n",
            "Epoch 494/1000, Training Loss: 8.557755103739319e-05, Test Loss: 7.566680321579648e-05\n",
            "Epoch 495/1000, Training Loss: 8.493025221011499e-05, Test Loss: 7.510180954576377e-05\n",
            "Epoch 496/1000, Training Loss: 8.42947439834573e-05, Test Loss: 7.454701058598149e-05\n",
            "Epoch 497/1000, Training Loss: 8.36707607058592e-05, Test Loss: 7.400218693435566e-05\n",
            "Epoch 498/1000, Training Loss: 8.30580432341964e-05, Test Loss: 7.346712413574531e-05\n",
            "Epoch 499/1000, Training Loss: 8.245633876592422e-05, Test Loss: 7.294161257048515e-05\n",
            "Epoch 500/1000, Training Loss: 8.186540067567687e-05, Test Loss: 7.242544734524553e-05\n",
            "Epoch 501/1000, Training Loss: 8.12849883562091e-05, Test Loss: 7.19184281862047e-05\n",
            "Epoch 502/1000, Training Loss: 8.071486706356072e-05, Test Loss: 7.142035933448092e-05\n",
            "Epoch 503/1000, Training Loss: 8.015480776634279e-05, Test Loss: 7.093104944381573e-05\n",
            "Epoch 504/1000, Training Loss: 7.960458699901399e-05, Test Loss: 7.045031148043094e-05\n",
            "Epoch 505/1000, Training Loss: 7.906398671907449e-05, Test Loss: 6.997796262507043e-05\n",
            "Epoch 506/1000, Training Loss: 7.853279416803866e-05, Test Loss: 6.951382417714494e-05\n",
            "Epoch 507/1000, Training Loss: 7.801080173611292e-05, Test Loss: 6.905772146097623e-05\n",
            "Epoch 508/1000, Training Loss: 7.749780683045864e-05, Test Loss: 6.860948373408144e-05\n",
            "Epoch 509/1000, Training Loss: 7.699361174696285e-05, Test Loss: 6.816894409747588e-05\n",
            "Epoch 510/1000, Training Loss: 7.649802354540469e-05, Test Loss: 6.773593940794263e-05\n",
            "Epoch 511/1000, Training Loss: 7.601085392794164e-05, Test Loss: 6.731031019224884e-05\n",
            "Epoch 512/1000, Training Loss: 7.553191912081518e-05, Test Loss: 6.689190056325605e-05\n",
            "Epoch 513/1000, Training Loss: 7.506103975920123e-05, Test Loss: 6.648055813790684e-05\n",
            "Epoch 514/1000, Training Loss: 7.459804077510125e-05, Test Loss: 6.607613395702197e-05\n",
            "Epoch 515/1000, Training Loss: 7.414275128821295e-05, Test Loss: 6.567848240690902e-05\n",
            "Epoch 516/1000, Training Loss: 7.369500449969266e-05, Test Loss: 6.528746114272108e-05\n",
            "Epoch 517/1000, Training Loss: 7.325463758871971e-05, Test Loss: 6.49029310135375e-05\n",
            "Epoch 518/1000, Training Loss: 7.282149161180743e-05, Test Loss: 6.452475598913747e-05\n",
            "Epoch 519/1000, Training Loss: 7.239541140476762e-05, Test Loss: 6.415280308841949e-05\n",
            "Epoch 520/1000, Training Loss: 7.197624548727321e-05, Test Loss: 6.378694230945325e-05\n",
            "Epoch 521/1000, Training Loss: 7.156384596993263e-05, Test Loss: 6.34270465611069e-05\n",
            "Epoch 522/1000, Training Loss: 7.11580684638211e-05, Test Loss: 6.307299159623298e-05\n",
            "Epoch 523/1000, Training Loss: 7.075877199239376e-05, Test Loss: 6.272465594637487e-05\n",
            "Epoch 524/1000, Training Loss: 7.036581890572246e-05, Test Loss: 6.238192085796342e-05\n",
            "Epoch 525/1000, Training Loss: 6.997907479698351e-05, Test Loss: 6.20446702299687e-05\n",
            "Epoch 526/1000, Training Loss: 6.959840842115274e-05, Test Loss: 6.171279055298071e-05\n",
            "Epoch 527/1000, Training Loss: 6.922369161582224e-05, Test Loss: 6.13861708496861e-05\n",
            "Epoch 528/1000, Training Loss: 6.88547992241125e-05, Test Loss: 6.1064702616706e-05\n",
            "Epoch 529/1000, Training Loss: 6.849160901959056e-05, Test Loss: 6.0748279767778085e-05\n",
            "Epoch 530/1000, Training Loss: 6.813400163316861e-05, Test Loss: 6.043679857823757e-05\n",
            "Epoch 531/1000, Training Loss: 6.778186048190816e-05, Test Loss: 6.013015763078256e-05\n",
            "Epoch 532/1000, Training Loss: 6.743507169969185e-05, Test Loss: 5.982825776248716e-05\n",
            "Epoch 533/1000, Training Loss: 6.709352406970369e-05, Test Loss: 5.953100201303585e-05\n",
            "Epoch 534/1000, Training Loss: 6.675710895867238e-05, Test Loss: 5.923829557415821e-05\n",
            "Epoch 535/1000, Training Loss: 6.642572025283257e-05, Test Loss: 5.8950045740227025e-05\n",
            "Epoch 536/1000, Training Loss: 6.609925429554761e-05, Test Loss: 5.866616186000479e-05\n",
            "Epoch 537/1000, Training Loss: 6.577760982656662e-05, Test Loss: 5.8386555289502544e-05\n",
            "Epoch 538/1000, Training Loss: 6.546068792285026e-05, Test Loss: 5.8111139345936685e-05\n",
            "Epoch 539/1000, Training Loss: 6.514839194093768e-05, Test Loss: 5.783982926274714e-05\n",
            "Epoch 540/1000, Training Loss: 6.484062746081156e-05, Test Loss: 5.7572542145664804e-05\n",
            "Epoch 541/1000, Training Loss: 6.453730223121204e-05, Test Loss: 5.73091969297943e-05\n",
            "Epoch 542/1000, Training Loss: 6.423832611637062e-05, Test Loss: 5.704971433769545e-05\n",
            "Epoch 543/1000, Training Loss: 6.394361104411949e-05, Test Loss: 5.679401683844161e-05\n",
            "Epoch 544/1000, Training Loss: 6.365307095534262e-05, Test Loss: 5.654202860762454e-05\n",
            "Epoch 545/1000, Training Loss: 6.336662175473045e-05, Test Loss: 5.6293675488293045e-05\n",
            "Epoch 546/1000, Training Loss: 6.308418126279963e-05, Test Loss: 5.6048884952795705e-05\n",
            "Epoch 547/1000, Training Loss: 6.280566916915674e-05, Test Loss: 5.580758606552096e-05\n",
            "Epoch 548/1000, Training Loss: 6.253100698695363e-05, Test Loss: 5.556970944649181e-05\n",
            "Epoch 549/1000, Training Loss: 6.226011800852136e-05, Test Loss: 5.5335187235819925e-05\n",
            "Epoch 550/1000, Training Loss: 6.199292726213303e-05, Test Loss: 5.510395305897572e-05\n",
            "Epoch 551/1000, Training Loss: 6.172936146988278e-05, Test Loss: 5.487594199288309e-05\n",
            "Epoch 552/1000, Training Loss: 6.146934900663668e-05, Test Loss: 5.465109053278337e-05\n",
            "Epoch 553/1000, Training Loss: 6.12128198600363e-05, Test Loss: 5.442933655989002e-05\n",
            "Epoch 554/1000, Training Loss: 6.095970559152191e-05, Test Loss: 5.421061930978572e-05\n",
            "Epoch 555/1000, Training Loss: 6.0709939298349086e-05, Test Loss: 5.39948793415584e-05\n",
            "Epoch 556/1000, Training Loss: 6.0463455576572066e-05, Test Loss: 5.3782058507659866e-05\n",
            "Epoch 557/1000, Training Loss: 6.022019048497057e-05, Test Loss: 5.3572099924457524e-05\n",
            "Epoch 558/1000, Training Loss: 5.998008150989286e-05, Test Loss: 5.336494794348559e-05\n",
            "Epoch 559/1000, Training Loss: 5.974306753098667e-05, Test Loss: 5.316054812335154e-05\n",
            "Epoch 560/1000, Training Loss: 5.9509088787804795e-05, Test Loss: 5.295884720230666e-05\n",
            "Epoch 561/1000, Training Loss: 5.9278086847250105e-05, Test Loss: 5.2759793071453975e-05\n",
            "Epoch 562/1000, Training Loss: 5.905000457184907e-05, Test Loss: 5.256333474857441e-05\n",
            "Epoch 563/1000, Training Loss: 5.882478608881852e-05, Test Loss: 5.2369422352569774e-05\n",
            "Epoch 564/1000, Training Loss: 5.860237675992322e-05, Test Loss: 5.217800707849615e-05\n",
            "Epoch 565/1000, Training Loss: 5.838272315207876e-05, Test Loss: 5.198904117317775e-05\n",
            "Epoch 566/1000, Training Loss: 5.816577300870813e-05, Test Loss: 5.180247791139218e-05\n",
            "Epoch 567/1000, Training Loss: 5.795147522180608e-05, Test Loss: 5.1618271572607406e-05\n",
            "Epoch 568/1000, Training Loss: 5.7739779804711415e-05, Test Loss: 5.1436377418255354e-05\n",
            "Epoch 569/1000, Training Loss: 5.753063786555808e-05, Test Loss: 5.125675166954363e-05\n",
            "Epoch 570/1000, Training Loss: 5.732400158139288e-05, Test Loss: 5.107935148577325e-05\n",
            "Epoch 571/1000, Training Loss: 5.7119824172935756e-05, Test Loss: 5.0904134943167554e-05\n",
            "Epoch 572/1000, Training Loss: 5.6918059879976574e-05, Test Loss: 5.073106101418593e-05\n",
            "Epoch 573/1000, Training Loss: 5.6718663937379306e-05, Test Loss: 5.056008954732618e-05\n",
            "Epoch 574/1000, Training Loss: 5.652159255168693e-05, Test Loss: 5.0391181247385704e-05\n",
            "Epoch 575/1000, Training Loss: 5.632680287830637e-05, Test Loss: 5.0224297656191604e-05\n",
            "Epoch 576/1000, Training Loss: 5.613425299926013e-05, Test Loss: 5.005940113376927e-05\n",
            "Epoch 577/1000, Training Loss: 5.594390190149174e-05, Test Loss: 4.9896454839950735e-05\n",
            "Epoch 578/1000, Training Loss: 5.5755709455705965e-05, Test Loss: 4.973542271640929e-05\n",
            "Epoch 579/1000, Training Loss: 5.556963639573266e-05, Test Loss: 4.957626946910986e-05\n",
            "Epoch 580/1000, Training Loss: 5.5385644298402444e-05, Test Loss: 4.9418960551168906e-05\n",
            "Epoch 581/1000, Training Loss: 5.520369556391658e-05, Test Loss: 4.9263462146106146e-05\n",
            "Epoch 582/1000, Training Loss: 5.5023753396703644e-05, Test Loss: 4.910974115148427e-05\n",
            "Epoch 583/1000, Training Loss: 5.4845781786743775e-05, Test Loss: 4.8957765162933555e-05\n",
            "Epoch 584/1000, Training Loss: 5.466974549135093e-05, Test Loss: 4.8807502458533224e-05\n",
            "Epoch 585/1000, Training Loss: 5.449561001741175e-05, Test Loss: 4.865892198356956e-05\n",
            "Epoch 586/1000, Training Loss: 5.432334160404747e-05, Test Loss: 4.851199333563545e-05\n",
            "Epoch 587/1000, Training Loss: 5.415290720570626e-05, Test Loss: 4.8366686750071325e-05\n",
            "Epoch 588/1000, Training Loss: 5.39842744756706e-05, Test Loss: 4.822297308575318e-05\n",
            "Epoch 589/1000, Training Loss: 5.381741174996494e-05, Test Loss: 4.808082381120009e-05\n",
            "Epoch 590/1000, Training Loss: 5.3652288031657574e-05, Test Loss: 4.794021099100192e-05\n",
            "Epoch 591/1000, Training Loss: 5.3488872975541275e-05, Test Loss: 4.78011072725654e-05\n",
            "Epoch 592/1000, Training Loss: 5.332713687319082e-05, Test Loss: 4.7663485873158046e-05\n",
            "Epoch 593/1000, Training Loss: 5.316705063838038e-05, Test Loss: 4.752732056725382e-05\n",
            "Epoch 594/1000, Training Loss: 5.300858579286022e-05, Test Loss: 4.73925856741738e-05\n",
            "Epoch 595/1000, Training Loss: 5.285171445247098e-05, Test Loss: 4.725925604600373e-05\n",
            "Epoch 596/1000, Training Loss: 5.269640931359685e-05, Test Loss: 4.712730705579186e-05\n",
            "Epoch 597/1000, Training Loss: 5.2542643639949226e-05, Test Loss: 4.699671458601958e-05\n",
            "Epoch 598/1000, Training Loss: 5.2390391249664704e-05, Test Loss: 4.686745501733557e-05\n",
            "Epoch 599/1000, Training Loss: 5.2239626502718105e-05, Test Loss: 4.67395052175474e-05\n",
            "Epoch 600/1000, Training Loss: 5.209032428863818e-05, Test Loss: 4.661284253086443e-05\n",
            "Epoch 601/1000, Training Loss: 5.194246001451325e-05, Test Loss: 4.6487444767391086e-05\n",
            "Epoch 602/1000, Training Loss: 5.17960095932944e-05, Test Loss: 4.636329019285976e-05\n",
            "Epoch 603/1000, Training Loss: 5.1650949432371076e-05, Test Loss: 4.624035751859318e-05\n",
            "Epoch 604/1000, Training Loss: 5.150725642242439e-05, Test Loss: 4.6118625891702e-05\n",
            "Epoch 605/1000, Training Loss: 5.136490792654816e-05, Test Loss: 4.5998074885505704e-05\n",
            "Epoch 606/1000, Training Loss: 5.1223881769626324e-05, Test Loss: 4.587868449016987e-05\n",
            "Epoch 607/1000, Training Loss: 5.108415622796626e-05, Test Loss: 4.5760435103560395e-05\n",
            "Epoch 608/1000, Training Loss: 5.094571001917908e-05, Test Loss: 4.56433075223007e-05\n",
            "Epoch 609/1000, Training Loss: 5.080852229230208e-05, Test Loss: 4.552728293303871e-05\n",
            "Epoch 610/1000, Training Loss: 5.067257261815279e-05, Test Loss: 4.541234290391617e-05\n",
            "Epoch 611/1000, Training Loss: 5.053784097991609e-05, Test Loss: 4.529846937621634e-05\n",
            "Epoch 612/1000, Training Loss: 5.040430776395093e-05, Test Loss: 4.518564465622021e-05\n",
            "Epoch 613/1000, Training Loss: 5.027195375081799e-05, Test Loss: 4.507385140723519e-05\n",
            "Epoch 614/1000, Training Loss: 5.014076010651774e-05, Test Loss: 4.496307264180894e-05\n",
            "Epoch 615/1000, Training Loss: 5.0010708373936925e-05, Test Loss: 4.485329171412659e-05\n",
            "Epoch 616/1000, Training Loss: 4.9881780464495134e-05, Test Loss: 4.474449231256613e-05\n",
            "Epoch 617/1000, Training Loss: 4.9753958649990075e-05, Test Loss: 4.4636658452437835e-05\n",
            "Epoch 618/1000, Training Loss: 4.962722555463605e-05, Test Loss: 4.452977446887811e-05\n",
            "Epoch 619/1000, Training Loss: 4.9501564147284036e-05, Test Loss: 4.4423825009907204e-05\n",
            "Epoch 620/1000, Training Loss: 4.937695773383235e-05, Test Loss: 4.431879502964402e-05\n",
            "Epoch 621/1000, Training Loss: 4.925338994980902e-05, Test Loss: 4.421466978167631e-05\n",
            "Epoch 622/1000, Training Loss: 4.9130844753130625e-05, Test Loss: 4.411143481258127e-05\n",
            "Epoch 623/1000, Training Loss: 4.900930641703187e-05, Test Loss: 4.400907595558209e-05\n",
            "Epoch 624/1000, Training Loss: 4.888875952315717e-05, Test Loss: 4.390757932436565e-05\n",
            "Epoch 625/1000, Training Loss: 4.876918895481668e-05, Test Loss: 4.380693130702613e-05\n",
            "Epoch 626/1000, Training Loss: 4.865057989039718e-05, Test Loss: 4.370711856014604e-05\n",
            "Epoch 627/1000, Training Loss: 4.853291779692796e-05, Test Loss: 4.360812800301544e-05\n",
            "Epoch 628/1000, Training Loss: 4.841618842379651e-05, Test Loss: 4.350994681198198e-05\n",
            "Epoch 629/1000, Training Loss: 4.830037779660772e-05, Test Loss: 4.3412562414922844e-05\n",
            "Epoch 630/1000, Training Loss: 4.818547221118891e-05, Test Loss: 4.3315962485844525e-05\n",
            "Epoch 631/1000, Training Loss: 4.8071458227732594e-05, Test Loss: 4.322013493960268e-05\n",
            "Epoch 632/1000, Training Loss: 4.795832266507396e-05, Test Loss: 4.3125067926739264e-05\n",
            "Epoch 633/1000, Training Loss: 4.7846052595100634e-05, Test Loss: 4.303074982843814e-05\n",
            "Epoch 634/1000, Training Loss: 4.7734635337293044e-05, Test Loss: 4.293716925159553e-05\n",
            "Epoch 635/1000, Training Loss: 4.762405845338958e-05, Test Loss: 4.2844315023992906e-05\n",
            "Epoch 636/1000, Training Loss: 4.7514309742174596e-05, Test Loss: 4.275217618958507e-05\n",
            "Epoch 637/1000, Training Loss: 4.740537723438806e-05, Test Loss: 4.266074200389468e-05\n",
            "Epoch 638/1000, Training Loss: 4.729724918774894e-05, Test Loss: 4.257000192950122e-05\n",
            "Epoch 639/1000, Training Loss: 4.7189914082095015e-05, Test Loss: 4.247994563163964e-05\n",
            "Epoch 640/1000, Training Loss: 4.70833606146346e-05, Test Loss: 4.2390562973891686e-05\n",
            "Epoch 641/1000, Training Loss: 4.697757769530614e-05, Test Loss: 4.23018440139806e-05\n",
            "Epoch 642/1000, Training Loss: 4.687255444224207e-05, Test Loss: 4.22137789996457e-05\n",
            "Epoch 643/1000, Training Loss: 4.6768280177340675e-05, Test Loss: 4.212635836462389e-05\n",
            "Epoch 644/1000, Training Loss: 4.666474442193454e-05, Test Loss: 4.203957272471252e-05\n",
            "Epoch 645/1000, Training Loss: 4.65619368925612e-05, Test Loss: 4.195341287392155e-05\n",
            "Epoch 646/1000, Training Loss: 4.6459847496826566e-05, Test Loss: 4.1867869780711245e-05\n",
            "Epoch 647/1000, Training Loss: 4.635846632936622e-05, Test Loss: 4.178293458431136e-05\n",
            "Epoch 648/1000, Training Loss: 4.6257783667894684e-05, Test Loss: 4.169859859112788e-05\n",
            "Epoch 649/1000, Training Loss: 4.615778996934789e-05, Test Loss: 4.161485327122401e-05\n",
            "Epoch 650/1000, Training Loss: 4.60584758661093e-05, Test Loss: 4.153169025487816e-05\n",
            "Epoch 651/1000, Training Loss: 4.59598321623254e-05, Test Loss: 4.144910132922487e-05\n",
            "Epoch 652/1000, Training Loss: 4.586184983029966e-05, Test Loss: 4.1367078434960064e-05\n",
            "Epoch 653/1000, Training Loss: 4.576452000697394e-05, Test Loss: 4.128561366313289e-05\n",
            "Epoch 654/1000, Training Loss: 4.566783399048246e-05, Test Loss: 4.120469925199467e-05\n",
            "Epoch 655/1000, Training Loss: 4.557178323678886e-05, Test Loss: 4.112432758392305e-05\n",
            "Epoch 656/1000, Training Loss: 4.5476359356395914e-05, Test Loss: 4.104449118241689e-05\n",
            "Epoch 657/1000, Training Loss: 4.5381554111130946e-05, Test Loss: 4.0965182709156356e-05\n",
            "Epoch 658/1000, Training Loss: 4.52873594110007e-05, Test Loss: 4.088639496111938e-05\n",
            "Epoch 659/1000, Training Loss: 4.519376731111982e-05, Test Loss: 4.0808120867775484e-05\n",
            "Epoch 660/1000, Training Loss: 4.510077000870549e-05, Test Loss: 4.073035348832918e-05\n",
            "Epoch 661/1000, Training Loss: 4.5008359840142214e-05, Test Loss: 4.0653086009033275e-05\n",
            "Epoch 662/1000, Training Loss: 4.4916529278107244e-05, Test Loss: 4.057631174055129e-05\n",
            "Epoch 663/1000, Training Loss: 4.482527092876608e-05, Test Loss: 4.0500024115387864e-05\n",
            "Epoch 664/1000, Training Loss: 4.4734577529025283e-05, Test Loss: 4.04242166853694e-05\n",
            "Epoch 665/1000, Training Loss: 4.464444194385053e-05, Test Loss: 4.034888311918136e-05\n",
            "Epoch 666/1000, Training Loss: 4.4554857163641004e-05, Test Loss: 4.0274017199958527e-05\n",
            "Epoch 667/1000, Training Loss: 4.446581630166307e-05, Test Loss: 4.019961282293017e-05\n",
            "Epoch 668/1000, Training Loss: 4.43773125915421e-05, Test Loss: 4.0125663993116633e-05\n",
            "Epoch 669/1000, Training Loss: 4.428933938480755e-05, Test Loss: 4.005216482307163e-05\n",
            "Epoch 670/1000, Training Loss: 4.420189014849501e-05, Test Loss: 3.9979109530679155e-05\n",
            "Epoch 671/1000, Training Loss: 4.41149584627991e-05, Test Loss: 3.990649243699927e-05\n",
            "Epoch 672/1000, Training Loss: 4.40285380187778e-05, Test Loss: 3.9834307964150273e-05\n",
            "Epoch 673/1000, Training Loss: 4.394262261610981e-05, Test Loss: 3.976255063325366e-05\n",
            "Epoch 674/1000, Training Loss: 4.385720616089895e-05, Test Loss: 3.969121506240996e-05\n",
            "Epoch 675/1000, Training Loss: 4.3772282663526436e-05, Test Loss: 3.962029596472275e-05\n",
            "Epoch 676/1000, Training Loss: 4.368784623655444e-05, Test Loss: 3.954978814636914e-05\n",
            "Epoch 677/1000, Training Loss: 4.360389109267012e-05, Test Loss: 3.947968650470737e-05\n",
            "Epoch 678/1000, Training Loss: 4.3520411542677314e-05, Test Loss: 3.9409986026428294e-05\n",
            "Epoch 679/1000, Training Loss: 4.343740199353314e-05, Test Loss: 3.9340681785745996e-05\n",
            "Epoch 680/1000, Training Loss: 4.335485694642507e-05, Test Loss: 3.927176894262755e-05\n",
            "Epoch 681/1000, Training Loss: 4.3272770994890986e-05, Test Loss: 3.920324274105518e-05\n",
            "Epoch 682/1000, Training Loss: 4.319113882298203e-05, Test Loss: 3.913509850734342e-05\n",
            "Epoch 683/1000, Training Loss: 4.31099552034616e-05, Test Loss: 3.9067331648472056e-05\n",
            "Epoch 684/1000, Training Loss: 4.302921499604656e-05, Test Loss: 3.899993765046725e-05\n",
            "Epoch 685/1000, Training Loss: 4.294891314568602e-05, Test Loss: 3.893291207681057e-05\n",
            "Epoch 686/1000, Training Loss: 4.286904468087679e-05, Test Loss: 3.886625056689193e-05\n",
            "Epoch 687/1000, Training Loss: 4.2789604712014945e-05, Test Loss: 3.8799948834485935e-05\n",
            "Epoch 688/1000, Training Loss: 4.2710588429784054e-05, Test Loss: 3.8734002666265265e-05\n",
            "Epoch 689/1000, Training Loss: 4.263199110357692e-05, Test Loss: 3.8668407920345475e-05\n",
            "Epoch 690/1000, Training Loss: 4.2553808079953006e-05, Test Loss: 3.860316052486114e-05\n",
            "Epoch 691/1000, Training Loss: 4.2476034781129573e-05, Test Loss: 3.853825647657625e-05\n",
            "Epoch 692/1000, Training Loss: 4.239866670350063e-05, Test Loss: 3.847369183951396e-05\n",
            "Epoch 693/1000, Training Loss: 4.2321699416193375e-05, Test Loss: 3.840946274362604e-05\n",
            "Epoch 694/1000, Training Loss: 4.224512855965295e-05, Test Loss: 3.8345565383489686e-05\n",
            "Epoch 695/1000, Training Loss: 4.216894984425683e-05, Test Loss: 3.82819960170247e-05\n",
            "Epoch 696/1000, Training Loss: 4.20931590489604e-05, Test Loss: 3.821875096424358e-05\n",
            "Epoch 697/1000, Training Loss: 4.201775201997108e-05, Test Loss: 3.81558266060311e-05\n",
            "Epoch 698/1000, Training Loss: 4.194272466945018e-05, Test Loss: 3.809321938294418e-05\n",
            "Epoch 699/1000, Training Loss: 4.186807297424246e-05, Test Loss: 3.803092579403591e-05\n",
            "Epoch 700/1000, Training Loss: 4.179379297463327e-05, Test Loss: 3.796894239571799e-05\n",
            "Epoch 701/1000, Training Loss: 4.171988077313187e-05, Test Loss: 3.7907265800625374e-05\n",
            "Epoch 702/1000, Training Loss: 4.1646332533281477e-05, Test Loss: 3.7845892676527696e-05\n",
            "Epoch 703/1000, Training Loss: 4.157314447849081e-05, Test Loss: 3.77848197452452e-05\n",
            "Epoch 704/1000, Training Loss: 4.150031289089656e-05, Test Loss: 3.772404378160346e-05\n",
            "Epoch 705/1000, Training Loss: 4.14278341102443e-05, Test Loss: 3.7663561612393975e-05\n",
            "Epoch 706/1000, Training Loss: 4.1355704532795506e-05, Test Loss: 3.760337011537108e-05\n",
            "Epoch 707/1000, Training Loss: 4.128392061025768e-05, Test Loss: 3.7543466218261875e-05\n",
            "Epoch 708/1000, Training Loss: 4.121247884873675e-05, Test Loss: 3.7483846897801386e-05\n",
            "Epoch 709/1000, Training Loss: 4.114137580771119e-05, Test Loss: 3.7424509178784e-05\n",
            "Epoch 710/1000, Training Loss: 4.1070608099027375e-05, Test Loss: 3.736545013313798e-05\n",
            "Epoch 711/1000, Training Loss: 4.100017238591685e-05, Test Loss: 3.7306666879017176e-05\n",
            "Epoch 712/1000, Training Loss: 4.093006538203408e-05, Test Loss: 3.724815657991484e-05\n",
            "Epoch 713/1000, Training Loss: 4.086028385051372e-05, Test Loss: 3.718991644379281e-05\n",
            "Epoch 714/1000, Training Loss: 4.079082460304802e-05, Test Loss: 3.713194372223057e-05\n",
            "Epoch 715/1000, Training Loss: 4.072168449898278e-05, Test Loss: 3.707423570959417e-05\n",
            "Epoch 716/1000, Training Loss: 4.065286044443308e-05, Test Loss: 3.701678974221732e-05\n",
            "Epoch 717/1000, Training Loss: 4.058434939141772e-05, Test Loss: 3.695960319760424e-05\n",
            "Epoch 718/1000, Training Loss: 4.0516148337009176e-05, Test Loss: 3.690267349364777e-05\n",
            "Epoch 719/1000, Training Loss: 4.044825432250486e-05, Test Loss: 3.6845998087863656e-05\n",
            "Epoch 720/1000, Training Loss: 4.038066443261136e-05, Test Loss: 3.678957447663792e-05\n",
            "Epoch 721/1000, Training Loss: 4.031337579464887e-05, Test Loss: 3.673340019449601e-05\n",
            "Epoch 722/1000, Training Loss: 4.024638557777186e-05, Test Loss: 3.66774728133853e-05\n",
            "Epoch 723/1000, Training Loss: 4.01796909922031e-05, Test Loss: 3.662178994196789e-05\n",
            "Epoch 724/1000, Training Loss: 4.0113289288485935e-05, Test Loss: 3.656634922493151e-05\n",
            "Epoch 725/1000, Training Loss: 4.004717775675144e-05, Test Loss: 3.6511148342314286e-05\n",
            "Epoch 726/1000, Training Loss: 3.998135372599983e-05, Test Loss: 3.6456185008847256e-05\n",
            "Epoch 727/1000, Training Loss: 3.9915814563397493e-05, Test Loss: 3.6401456973301484e-05\n",
            "Epoch 728/1000, Training Loss: 3.985055767358784e-05, Test Loss: 3.634696201786354e-05\n",
            "Epoch 729/1000, Training Loss: 3.978558049801596e-05, Test Loss: 3.629269795749893e-05\n",
            "Epoch 730/1000, Training Loss: 3.9720880514267956e-05, Test Loss: 3.6238662639359244e-05\n",
            "Epoch 731/1000, Training Loss: 3.965645523542394e-05, Test Loss: 3.618485394217914e-05\n",
            "Epoch 732/1000, Training Loss: 3.9592302209421824e-05, Test Loss: 3.613126977569401e-05\n",
            "Epoch 733/1000, Training Loss: 3.952841901843737e-05, Test Loss: 3.607790808007171e-05\n",
            "Epoch 734/1000, Training Loss: 3.946480327827386e-05, Test Loss: 3.602476682534751e-05\n",
            "Epoch 735/1000, Training Loss: 3.940145263776639e-05, Test Loss: 3.5971844010882016e-05\n",
            "Epoch 736/1000, Training Loss: 3.933836477819673e-05, Test Loss: 3.591913766481926e-05\n",
            "Epoch 737/1000, Training Loss: 3.927553741272017e-05, Test Loss: 3.586664584356467e-05\n",
            "Epoch 738/1000, Training Loss: 3.921296828580459e-05, Test Loss: 3.581436663126536e-05\n",
            "Epoch 739/1000, Training Loss: 3.915065517268066e-05, Test Loss: 3.576229813931083e-05\n",
            "Epoch 740/1000, Training Loss: 3.908859587880239e-05, Test Loss: 3.571043850583572e-05\n",
            "Epoch 741/1000, Training Loss: 3.902678823931845e-05, Test Loss: 3.565878589523442e-05\n",
            "Epoch 742/1000, Training Loss: 3.8965230118556186e-05, Test Loss: 3.560733849769087e-05\n",
            "Epoch 743/1000, Training Loss: 3.89039194095129e-05, Test Loss: 3.555609452871085e-05\n",
            "Epoch 744/1000, Training Loss: 3.8842854033358876e-05, Test Loss: 3.550505222866331e-05\n",
            "Epoch 745/1000, Training Loss: 3.878203193895105e-05, Test Loss: 3.545420986234589e-05\n",
            "Epoch 746/1000, Training Loss: 3.872145110235439e-05, Test Loss: 3.540356571853228e-05\n",
            "Epoch 747/1000, Training Loss: 3.8661109526373466e-05, Test Loss: 3.53531181095546e-05\n",
            "Epoch 748/1000, Training Loss: 3.860100524009552e-05, Test Loss: 3.530286537087941e-05\n",
            "Epoch 749/1000, Training Loss: 3.8541136298439246e-05, Test Loss: 3.5252805860699454e-05\n",
            "Epoch 750/1000, Training Loss: 3.8481500781713453e-05, Test Loss: 3.520293795953033e-05\n",
            "Epoch 751/1000, Training Loss: 3.842209679518696e-05, Test Loss: 3.515326006981441e-05\n",
            "Epoch 752/1000, Training Loss: 3.836292246866248e-05, Test Loss: 3.51037706155328e-05\n",
            "Epoch 753/1000, Training Loss: 3.8303975956063594e-05, Test Loss: 3.505446804183093e-05\n",
            "Epoch 754/1000, Training Loss: 3.8245255435025396e-05, Test Loss: 3.500535081463905e-05\n",
            "Epoch 755/1000, Training Loss: 3.818675910649654e-05, Test Loss: 3.495641742031727e-05\n",
            "Epoch 756/1000, Training Loss: 3.8128485194348076e-05, Test Loss: 3.490766636529381e-05\n",
            "Epoch 757/1000, Training Loss: 3.8070431944989176e-05, Test Loss: 3.485909617571625e-05\n",
            "Epoch 758/1000, Training Loss: 3.801259762699025e-05, Test Loss: 3.481070539710834e-05\n",
            "Epoch 759/1000, Training Loss: 3.795498053071497e-05, Test Loss: 3.47624925940339e-05\n",
            "Epoch 760/1000, Training Loss: 3.78975789679585e-05, Test Loss: 3.471445634977315e-05\n",
            "Epoch 761/1000, Training Loss: 3.784039127159106e-05, Test Loss: 3.466659526598637e-05\n",
            "Epoch 762/1000, Training Loss: 3.778341579521274e-05, Test Loss: 3.4618907962417585e-05\n",
            "Epoch 763/1000, Training Loss: 3.772665091280929e-05, Test Loss: 3.457139307656391e-05\n",
            "Epoch 764/1000, Training Loss: 3.7670095018419534e-05, Test Loss: 3.452404926338835e-05\n",
            "Epoch 765/1000, Training Loss: 3.7613746525806274e-05, Test Loss: 3.447687519500986e-05\n",
            "Epoch 766/1000, Training Loss: 3.7557603868134804e-05, Test Loss: 3.4429869560418915e-05\n",
            "Epoch 767/1000, Training Loss: 3.7501665497656744e-05, Test Loss: 3.438303106518848e-05\n",
            "Epoch 768/1000, Training Loss: 3.744592988540053e-05, Test Loss: 3.433635843119363e-05\n",
            "Epoch 769/1000, Training Loss: 3.73903955208677e-05, Test Loss: 3.4289850396336096e-05\n",
            "Epoch 770/1000, Training Loss: 3.7335060911735326e-05, Test Loss: 3.424350571427407e-05\n",
            "Epoch 771/1000, Training Loss: 3.7279924583561825e-05, Test Loss: 3.419732315416075e-05\n",
            "Epoch 772/1000, Training Loss: 3.722498507950232e-05, Test Loss: 3.4151301500378265e-05\n",
            "Epoch 773/1000, Training Loss: 3.7170240960026554e-05, Test Loss: 3.41054395522945e-05\n",
            "Epoch 774/1000, Training Loss: 3.711569080264268e-05, Test Loss: 3.405973612399961e-05\n",
            "Epoch 775/1000, Training Loss: 3.706133320162647e-05, Test Loss: 3.4014190044077524e-05\n",
            "Epoch 776/1000, Training Loss: 3.70071667677554e-05, Test Loss: 3.39688001553489e-05\n",
            "Epoch 777/1000, Training Loss: 3.695319012804849e-05, Test Loss: 3.3923565314650005e-05\n",
            "Epoch 778/1000, Training Loss: 3.689940192550996e-05, Test Loss: 3.3878484392594744e-05\n",
            "Epoch 779/1000, Training Loss: 3.6845800818879e-05, Test Loss: 3.383355627335807e-05\n",
            "Epoch 780/1000, Training Loss: 3.6792385482381465e-05, Test Loss: 3.378877985443737e-05\n",
            "Epoch 781/1000, Training Loss: 3.673915460549059e-05, Test Loss: 3.374415404644865e-05\n",
            "Epoch 782/1000, Training Loss: 3.6686106892688176e-05, Test Loss: 3.369967777291246e-05\n",
            "Epoch 783/1000, Training Loss: 3.663324106323201e-05, Test Loss: 3.3655349970036805e-05\n",
            "Epoch 784/1000, Training Loss: 3.6580555850928496e-05, Test Loss: 3.361116958652216e-05\n",
            "Epoch 785/1000, Training Loss: 3.652805000390616e-05, Test Loss: 3.3567135583350623e-05\n",
            "Epoch 786/1000, Training Loss: 3.6475722284397445e-05, Test Loss: 3.35232469335944e-05\n",
            "Epoch 787/1000, Training Loss: 3.642357146852041e-05, Test Loss: 3.3479502622216826e-05\n",
            "Epoch 788/1000, Training Loss: 3.637159634607023e-05, Test Loss: 3.3435901645894836e-05\n",
            "Epoch 789/1000, Training Loss: 3.631979572030719e-05, Test Loss: 3.339244301281665e-05\n",
            "Epoch 790/1000, Training Loss: 3.626816840775452e-05, Test Loss: 3.3349125742508794e-05\n",
            "Epoch 791/1000, Training Loss: 3.621671323799668e-05, Test Loss: 3.330594886565405e-05\n",
            "Epoch 792/1000, Training Loss: 3.616542905348301e-05, Test Loss: 3.326291142391398e-05\n",
            "Epoch 793/1000, Training Loss: 3.6114314709333666e-05, Test Loss: 3.322001246976076e-05\n",
            "Epoch 794/1000, Training Loss: 3.6063369073150354e-05, Test Loss: 3.317725106630428e-05\n",
            "Epoch 795/1000, Training Loss: 3.601259102482891e-05, Test Loss: 3.313462628712466e-05\n",
            "Epoch 796/1000, Training Loss: 3.5961979456377044e-05, Test Loss: 3.309213721611125e-05\n",
            "Epoch 797/1000, Training Loss: 3.591153327173438e-05, Test Loss: 3.3049782947305596e-05\n",
            "Epoch 798/1000, Training Loss: 3.586125138659531e-05, Test Loss: 3.3007562584738294e-05\n",
            "Epoch 799/1000, Training Loss: 3.581113272823647e-05, Test Loss: 3.2965475242278175e-05\n",
            "Epoch 800/1000, Training Loss: 3.5761176235345834e-05, Test Loss: 3.292352004348316e-05\n",
            "Epoch 801/1000, Training Loss: 3.5711380857855084e-05, Test Loss: 3.288169612144286e-05\n",
            "Epoch 802/1000, Training Loss: 3.5661745556777146e-05, Test Loss: 3.284000261864915e-05\n",
            "Epoch 803/1000, Training Loss: 3.561226930404225e-05, Test Loss: 3.279843868683654e-05\n",
            "Epoch 804/1000, Training Loss: 3.556295108234159e-05, Test Loss: 3.275700348684939e-05\n",
            "Epoch 805/1000, Training Loss: 3.5513789884970376e-05, Test Loss: 3.271569618850524e-05\n",
            "Epoch 806/1000, Training Loss: 3.546478471567524e-05, Test Loss: 3.267451597045673e-05\n",
            "Epoch 807/1000, Training Loss: 3.5415934588504716e-05, Test Loss: 3.263346202005894e-05\n",
            "Epoch 808/1000, Training Loss: 3.5367238527661065e-05, Test Loss: 3.2592533533244456e-05\n",
            "Epoch 809/1000, Training Loss: 3.531869556735503e-05, Test Loss: 3.2551729714386044e-05\n",
            "Epoch 810/1000, Training Loss: 3.527030475166333e-05, Test Loss: 3.25110497761762e-05\n",
            "Epoch 811/1000, Training Loss: 3.5222065134390295e-05, Test Loss: 3.247049293950653e-05\n",
            "Epoch 812/1000, Training Loss: 3.517397577892782e-05, Test Loss: 3.243005843334565e-05\n",
            "Epoch 813/1000, Training Loss: 3.512603575812213e-05, Test Loss: 3.2389745494612024e-05\n",
            "Epoch 814/1000, Training Loss: 3.507824415414029e-05, Test Loss: 3.234955336806954e-05\n",
            "Epoch 815/1000, Training Loss: 3.503060005834059e-05, Test Loss: 3.2309481306206694e-05\n",
            "Epoch 816/1000, Training Loss: 3.498310257114258e-05, Test Loss: 3.226952856912113e-05\n",
            "Epoch 817/1000, Training Loss: 3.49357508019027e-05, Test Loss: 3.2229694424415024e-05\n",
            "Epoch 818/1000, Training Loss: 3.488854386878989e-05, Test Loss: 3.218997814708556e-05\n",
            "Epoch 819/1000, Training Loss: 3.4841480898663116e-05, Test Loss: 3.215037901941158e-05\n",
            "Epoch 820/1000, Training Loss: 3.4794561026953856e-05, Test Loss: 3.211089633086189e-05\n",
            "Epoch 821/1000, Training Loss: 3.4747783397545434e-05, Test Loss: 3.2071529377979724e-05\n",
            "Epoch 822/1000, Training Loss: 3.470114716265974e-05, Test Loss: 3.203227746428515e-05\n",
            "Epoch 823/1000, Training Loss: 3.4654651482743414e-05, Test Loss: 3.1993139900183835e-05\n",
            "Epoch 824/1000, Training Loss: 3.460829552635536e-05, Test Loss: 3.195411600286052e-05\n",
            "Epoch 825/1000, Training Loss: 3.456207847005709e-05, Test Loss: 3.191520509618331e-05\n",
            "Epoch 826/1000, Training Loss: 3.451599949830501e-05, Test Loss: 3.187640651061664e-05\n",
            "Epoch 827/1000, Training Loss: 3.44700578033443e-05, Test Loss: 3.183771958312257e-05\n",
            "Epoch 828/1000, Training Loss: 3.442425258510431e-05, Test Loss: 3.1799143657075823e-05\n",
            "Epoch 829/1000, Training Loss: 3.4378583051095924e-05, Test Loss: 3.176067808216583e-05\n",
            "Epoch 830/1000, Training Loss: 3.4333048416311316e-05, Test Loss: 3.1722322214320626e-05\n",
            "Epoch 831/1000, Training Loss: 3.4287647903123674e-05, Test Loss: 3.1684075415608305e-05\n",
            "Epoch 832/1000, Training Loss: 3.4242380741190497e-05, Test Loss: 3.164593705416568e-05\n",
            "Epoch 833/1000, Training Loss: 3.41972461673567e-05, Test Loss: 3.1607906504101e-05\n",
            "Epoch 834/1000, Training Loss: 3.4152243425561244e-05, Test Loss: 3.1569983145425506e-05\n",
            "Epoch 835/1000, Training Loss: 3.4107371766744556e-05, Test Loss: 3.1532166363966686e-05\n",
            "Epoch 836/1000, Training Loss: 3.40626304487551e-05, Test Loss: 3.1494455551285077e-05\n",
            "Epoch 837/1000, Training Loss: 3.401801873626256e-05, Test Loss: 3.145685010461074e-05\n",
            "Epoch 838/1000, Training Loss: 3.3973535900667004e-05, Test Loss: 3.141934942675135e-05\n",
            "Epoch 839/1000, Training Loss: 3.392918122001332e-05, Test Loss: 3.138195292602612e-05\n",
            "Epoch 840/1000, Training Loss: 3.388495397890592e-05, Test Loss: 3.1344660016190976e-05\n",
            "Epoch 841/1000, Training Loss: 3.384085346842362e-05, Test Loss: 3.1307470116365016e-05\n",
            "Epoch 842/1000, Training Loss: 3.379687898603774e-05, Test Loss: 3.127038265095945e-05\n",
            "Epoch 843/1000, Training Loss: 3.375302983552983e-05, Test Loss: 3.1233397049608304e-05\n",
            "Epoch 844/1000, Training Loss: 3.3709305326913016e-05, Test Loss: 3.1196512747103495e-05\n",
            "Epoch 845/1000, Training Loss: 3.366570477635173e-05, Test Loss: 3.11597291833164e-05\n",
            "Epoch 846/1000, Training Loss: 3.362222750608588e-05, Test Loss: 3.11230458031458e-05\n",
            "Epoch 847/1000, Training Loss: 3.357887284435226e-05, Test Loss: 3.10864620564398e-05\n",
            "Epoch 848/1000, Training Loss: 3.353564012531175e-05, Test Loss: 3.104997739794046e-05\n",
            "Epoch 849/1000, Training Loss: 3.349252868897391e-05, Test Loss: 3.1013591287214696e-05\n",
            "Epoch 850/1000, Training Loss: 3.344953788112459e-05, Test Loss: 3.097730318859637e-05\n",
            "Epoch 851/1000, Training Loss: 3.340666705325505e-05, Test Loss: 3.0941112571123776e-05\n",
            "Epoch 852/1000, Training Loss: 3.336391556249123e-05, Test Loss: 3.090501890848215e-05\n",
            "Epoch 853/1000, Training Loss: 3.332128277152333e-05, Test Loss: 3.0869021678935555e-05\n",
            "Epoch 854/1000, Training Loss: 3.327876804853903e-05, Test Loss: 3.083312036527884e-05\n",
            "Epoch 855/1000, Training Loss: 3.323637076715599e-05, Test Loss: 3.079731445477936e-05\n",
            "Epoch 856/1000, Training Loss: 3.319409030635523e-05, Test Loss: 3.0761603439113415e-05\n",
            "Epoch 857/1000, Training Loss: 3.3151926050416046e-05, Test Loss: 3.072598681432103e-05\n",
            "Epoch 858/1000, Training Loss: 3.310987738885236e-05, Test Loss: 3.0690464080743404e-05\n",
            "Epoch 859/1000, Training Loss: 3.306794371634968e-05, Test Loss: 3.065503474297374e-05\n",
            "Epoch 860/1000, Training Loss: 3.302612443270224e-05, Test Loss: 3.061969830979941e-05\n",
            "Epoch 861/1000, Training Loss: 3.298441894275265e-05, Test Loss: 3.058445429416191e-05\n",
            "Epoch 862/1000, Training Loss: 3.294282665633139e-05, Test Loss: 3.0549302213094544e-05\n",
            "Epoch 863/1000, Training Loss: 3.290134698819714e-05, Test Loss: 3.051424158767363e-05\n",
            "Epoch 864/1000, Training Loss: 3.285997935797899e-05, Test Loss: 3.047927194297463e-05\n",
            "Epoch 865/1000, Training Loss: 3.281872319011801e-05, Test Loss: 3.0444392808017655e-05\n",
            "Epoch 866/1000, Training Loss: 3.277757791381181e-05, Test Loss: 3.0409603715725757e-05\n",
            "Epoch 867/1000, Training Loss: 3.2736542962957825e-05, Test Loss: 3.0374904202874288e-05\n",
            "Epoch 868/1000, Training Loss: 3.2695617776098816e-05, Test Loss: 3.034029381004597e-05\n",
            "Epoch 869/1000, Training Loss: 3.2654801796368316e-05, Test Loss: 3.030577208158177e-05\n",
            "Epoch 870/1000, Training Loss: 3.261409447143768e-05, Test Loss: 3.0271338565540256e-05\n",
            "Epoch 871/1000, Training Loss: 3.2573495253463885e-05, Test Loss: 3.0236992813654442e-05\n",
            "Epoch 872/1000, Training Loss: 3.253300359903695e-05, Test Loss: 3.0202734381284195e-05\n",
            "Epoch 873/1000, Training Loss: 3.249261896912976e-05, Test Loss: 3.016856282737579e-05\n",
            "Epoch 874/1000, Training Loss: 3.2452340829047345e-05, Test Loss: 3.013447771442078e-05\n",
            "Epoch 875/1000, Training Loss: 3.24121686483775e-05, Test Loss: 3.0100478608411007e-05\n",
            "Epoch 876/1000, Training Loss: 3.2372101900942695e-05, Test Loss: 3.006656507880379e-05\n",
            "Epoch 877/1000, Training Loss: 3.233214006475049e-05, Test Loss: 3.0032736698476758e-05\n",
            "Epoch 878/1000, Training Loss: 3.2292282621948375e-05, Test Loss: 2.99989930436865e-05\n",
            "Epoch 879/1000, Training Loss: 3.2252529058774975e-05, Test Loss: 2.9965333694038247e-05\n",
            "Epoch 880/1000, Training Loss: 3.221287886551585e-05, Test Loss: 2.9931758232441577e-05\n",
            "Epoch 881/1000, Training Loss: 3.217333153645755e-05, Test Loss: 2.989826624507058e-05\n",
            "Epoch 882/1000, Training Loss: 3.213388656984234e-05, Test Loss: 2.9864857321331854e-05\n",
            "Epoch 883/1000, Training Loss: 3.209454346782554e-05, Test Loss: 2.9831531053826564e-05\n",
            "Epoch 884/1000, Training Loss: 3.205530173643098e-05, Test Loss: 2.9798287038312046e-05\n",
            "Epoch 885/1000, Training Loss: 3.2016160885508705e-05, Test Loss: 2.976512487366658e-05\n",
            "Epoch 886/1000, Training Loss: 3.197712042869353e-05, Test Loss: 2.9732044161858177e-05\n",
            "Epoch 887/1000, Training Loss: 3.193817988336299e-05, Test Loss: 2.9699044507909343e-05\n",
            "Epoch 888/1000, Training Loss: 3.189933877059552e-05, Test Loss: 2.966612551985747e-05\n",
            "Epoch 889/1000, Training Loss: 3.186059661513131e-05, Test Loss: 2.963328680872736e-05\n",
            "Epoch 890/1000, Training Loss: 3.182195294533248e-05, Test Loss: 2.9600527988495943e-05\n",
            "Epoch 891/1000, Training Loss: 3.1783407293144066e-05, Test Loss: 2.9567848676063433e-05\n",
            "Epoch 892/1000, Training Loss: 3.1744959194053645e-05, Test Loss: 2.9535248491213216e-05\n",
            "Epoch 893/1000, Training Loss: 3.170660818705526e-05, Test Loss: 2.9502727056587285e-05\n",
            "Epoch 894/1000, Training Loss: 3.1668353814611285e-05, Test Loss: 2.9470283997655675e-05\n",
            "Epoch 895/1000, Training Loss: 3.163019562261505e-05, Test Loss: 2.9437918942682206e-05\n",
            "Epoch 896/1000, Training Loss: 3.159213316035397e-05, Test Loss: 2.940563152269553e-05\n",
            "Epoch 897/1000, Training Loss: 3.1554165980474356e-05, Test Loss: 2.9373421371460448e-05\n",
            "Epoch 898/1000, Training Loss: 3.151629363894501e-05, Test Loss: 2.9341288125447562e-05\n",
            "Epoch 899/1000, Training Loss: 3.147851569502348e-05, Test Loss: 2.930923142380359e-05\n",
            "Epoch 900/1000, Training Loss: 3.1440831711220535e-05, Test Loss: 2.9277250908329227e-05\n",
            "Epoch 901/1000, Training Loss: 3.140324125326589e-05, Test Loss: 2.9245346223440747e-05\n",
            "Epoch 902/1000, Training Loss: 3.136574389007463e-05, Test Loss: 2.9213517016148524e-05\n",
            "Epoch 903/1000, Training Loss: 3.1328339193714934e-05, Test Loss: 2.91817629360314e-05\n",
            "Epoch 904/1000, Training Loss: 3.129102673937479e-05, Test Loss: 2.9150083635205536e-05\n",
            "Epoch 905/1000, Training Loss: 3.1253806105329244e-05, Test Loss: 2.911847876830203e-05\n",
            "Epoch 906/1000, Training Loss: 3.12166768729097e-05, Test Loss: 2.908694799243821e-05\n",
            "Epoch 907/1000, Training Loss: 3.117963862647097e-05, Test Loss: 2.905549096719085e-05\n",
            "Epoch 908/1000, Training Loss: 3.1142690953362194e-05, Test Loss: 2.9024107354574895e-05\n",
            "Epoch 909/1000, Training Loss: 3.110583344389424e-05, Test Loss: 2.8992796819013985e-05\n",
            "Epoch 910/1000, Training Loss: 3.106906569131149e-05, Test Loss: 2.896155902732149e-05\n",
            "Epoch 911/1000, Training Loss: 3.103238729176083e-05, Test Loss: 2.8930393648668318e-05\n",
            "Epoch 912/1000, Training Loss: 3.0995797844262564e-05, Test Loss: 2.8899300354566355e-05\n",
            "Epoch 913/1000, Training Loss: 3.095929695068151e-05, Test Loss: 2.8868278818839675e-05\n",
            "Epoch 914/1000, Training Loss: 3.09228842156981e-05, Test Loss: 2.8837328717604266e-05\n",
            "Epoch 915/1000, Training Loss: 3.088655924678096e-05, Test Loss: 2.88064497292468e-05\n",
            "Epoch 916/1000, Training Loss: 3.0850321654158316e-05, Test Loss: 2.8775641534395175e-05\n",
            "Epoch 917/1000, Training Loss: 3.081417105079093e-05, Test Loss: 2.8744903815902736e-05\n",
            "Epoch 918/1000, Training Loss: 3.0778107052344605e-05, Test Loss: 2.8714236258822263e-05\n",
            "Epoch 919/1000, Training Loss: 3.0742129277163876e-05, Test Loss: 2.8683638550388167e-05\n",
            "Epoch 920/1000, Training Loss: 3.0706237346244804e-05, Test Loss: 2.8653110379989872e-05\n",
            "Epoch 921/1000, Training Loss: 3.0670430883210015e-05, Test Loss: 2.8622651439155052e-05\n",
            "Epoch 922/1000, Training Loss: 3.063470951428196e-05, Test Loss: 2.8592261421525998e-05\n",
            "Epoch 923/1000, Training Loss: 3.059907286825819e-05, Test Loss: 2.8561940022841505e-05\n",
            "Epoch 924/1000, Training Loss: 3.056352057648567e-05, Test Loss: 2.8531686940912338e-05\n",
            "Epoch 925/1000, Training Loss: 3.052805227283681e-05, Test Loss: 2.850150187560626e-05\n",
            "Epoch 926/1000, Training Loss: 3.0492667593683844e-05, Test Loss: 2.8471384528824945e-05\n",
            "Epoch 927/1000, Training Loss: 3.045736617787582e-05, Test Loss: 2.844133460448743e-05\n",
            "Epoch 928/1000, Training Loss: 3.042214766671417e-05, Test Loss: 2.8411351808506365e-05\n",
            "Epoch 929/1000, Training Loss: 3.038701170392959e-05, Test Loss: 2.8381435848770518e-05\n",
            "Epoch 930/1000, Training Loss: 3.035195793565791e-05, Test Loss: 2.835158643512962e-05\n",
            "Epoch 931/1000, Training Loss: 3.0316986010418396e-05, Test Loss: 2.832180327937166e-05\n",
            "Epoch 932/1000, Training Loss: 3.0282095579090228e-05, Test Loss: 2.8292086095204377e-05\n",
            "Epoch 933/1000, Training Loss: 3.0247286294890234e-05, Test Loss: 2.8262434598242e-05\n",
            "Epoch 934/1000, Training Loss: 3.0212557813350887e-05, Test Loss: 2.8232848505981307e-05\n",
            "Epoch 935/1000, Training Loss: 3.0177909792298406e-05, Test Loss: 2.8203327537786445e-05\n",
            "Epoch 936/1000, Training Loss: 3.0143341891831753e-05, Test Loss: 2.81738714148764e-05\n",
            "Epoch 937/1000, Training Loss: 3.010885377430066e-05, Test Loss: 2.8144479860298378e-05\n",
            "Epoch 938/1000, Training Loss: 3.0074445104284597e-05, Test Loss: 2.8115152598918894e-05\n",
            "Epoch 939/1000, Training Loss: 3.0040115548572315e-05, Test Loss: 2.808588935740243e-05\n",
            "Epoch 940/1000, Training Loss: 3.000586477614136e-05, Test Loss: 2.805668986419767e-05\n",
            "Epoch 941/1000, Training Loss: 2.997169245813747e-05, Test Loss: 2.802755384951791e-05\n",
            "Epoch 942/1000, Training Loss: 2.9937598267854956e-05, Test Loss: 2.7998481045331185e-05\n",
            "Epoch 943/1000, Training Loss: 2.99035818807168e-05, Test Loss: 2.7969471185335736e-05\n",
            "Epoch 944/1000, Training Loss: 2.986964297425506e-05, Test Loss: 2.7940524004952585e-05\n",
            "Epoch 945/1000, Training Loss: 2.9835781228091134e-05, Test Loss: 2.791163924130009e-05\n",
            "Epoch 946/1000, Training Loss: 2.9801996323917463e-05, Test Loss: 2.7882816633191813e-05\n",
            "Epoch 947/1000, Training Loss: 2.9768287945478622e-05, Test Loss: 2.7854055921111262e-05\n",
            "Epoch 948/1000, Training Loss: 2.9734655778551847e-05, Test Loss: 2.782535684719937e-05\n",
            "Epoch 949/1000, Training Loss: 2.970109951092946e-05, Test Loss: 2.7796719155243117e-05\n",
            "Epoch 950/1000, Training Loss: 2.9667618832400866e-05, Test Loss: 2.77681425906542e-05\n",
            "Epoch 951/1000, Training Loss: 2.9634213434733427e-05, Test Loss: 2.7739626900460833e-05\n",
            "Epoch 952/1000, Training Loss: 2.9600883011655776e-05, Test Loss: 2.7711171833292163e-05\n",
            "Epoch 953/1000, Training Loss: 2.956762725884038e-05, Test Loss: 2.7682777139360673e-05\n",
            "Epoch 954/1000, Training Loss: 2.953444587388502e-05, Test Loss: 2.7654442570453507e-05\n",
            "Epoch 955/1000, Training Loss: 2.9501338556297222e-05, Test Loss: 2.7626167879912648e-05\n",
            "Epoch 956/1000, Training Loss: 2.9468305007475866e-05, Test Loss: 2.7597952822628926e-05\n",
            "Epoch 957/1000, Training Loss: 2.9435344930695255e-05, Test Loss: 2.7569797155021432e-05\n",
            "Epoch 958/1000, Training Loss: 2.9402458031088674e-05, Test Loss: 2.7541700635031193e-05\n",
            "Epoch 959/1000, Training Loss: 2.9369644015631893e-05, Test Loss: 2.7513663022099756e-05\n",
            "Epoch 960/1000, Training Loss: 2.933690259312634e-05, Test Loss: 2.7485684077162153e-05\n",
            "Epoch 961/1000, Training Loss: 2.9304233474184263e-05, Test Loss: 2.745776356263482e-05\n",
            "Epoch 962/1000, Training Loss: 2.9271636371212048e-05, Test Loss: 2.742990124239768e-05\n",
            "Epoch 963/1000, Training Loss: 2.923911099839559e-05, Test Loss: 2.740209688178696e-05\n",
            "Epoch 964/1000, Training Loss: 2.9206657071682886e-05, Test Loss: 2.7374350247580012e-05\n",
            "Epoch 965/1000, Training Loss: 2.9174274308771663e-05, Test Loss: 2.7346661107983836e-05\n",
            "Epoch 966/1000, Training Loss: 2.9141962429091867e-05, Test Loss: 2.7319029232622156e-05\n",
            "Epoch 967/1000, Training Loss: 2.910972115379167e-05, Test Loss: 2.7291454392525627e-05\n",
            "Epoch 968/1000, Training Loss: 2.9077550205722823e-05, Test Loss: 2.726393636011865e-05\n",
            "Epoch 969/1000, Training Loss: 2.9045449309425542e-05, Test Loss: 2.7236474909204862e-05\n",
            "Epoch 970/1000, Training Loss: 2.901341819111524e-05, Test Loss: 2.7209069814962733e-05\n",
            "Epoch 971/1000, Training Loss: 2.898145657866718e-05, Test Loss: 2.7181720853928204e-05\n",
            "Epoch 972/1000, Training Loss: 2.8949564201602335e-05, Test Loss: 2.7154427803984262e-05\n",
            "Epoch 973/1000, Training Loss: 2.891774079107426e-05, Test Loss: 2.7127190444352108e-05\n",
            "Epoch 974/1000, Training Loss: 2.888598607985479e-05, Test Loss: 2.71000085555764e-05\n",
            "Epoch 975/1000, Training Loss: 2.8854299802320466e-05, Test Loss: 2.7072881919520107e-05\n",
            "Epoch 976/1000, Training Loss: 2.882268169443947e-05, Test Loss: 2.704581031934864e-05\n",
            "Epoch 977/1000, Training Loss: 2.879113149375735e-05, Test Loss: 2.7018793539520133e-05\n",
            "Epoch 978/1000, Training Loss: 2.875964893938497e-05, Test Loss: 2.699183136577656e-05\n",
            "Epoch 979/1000, Training Loss: 2.8728233771984236e-05, Test Loss: 2.696492358513221e-05\n",
            "Epoch 980/1000, Training Loss: 2.8696885733756574e-05, Test Loss: 2.6938069985865483e-05\n",
            "Epoch 981/1000, Training Loss: 2.8665604568428805e-05, Test Loss: 2.6911270357502645e-05\n",
            "Epoch 982/1000, Training Loss: 2.8634390021242117e-05, Test Loss: 2.688452449081686e-05\n",
            "Epoch 983/1000, Training Loss: 2.8603241838937535e-05, Test Loss: 2.6857832177807787e-05\n",
            "Epoch 984/1000, Training Loss: 2.8572159769745682e-05, Test Loss: 2.683119321170314e-05\n",
            "Epoch 985/1000, Training Loss: 2.8541143563373022e-05, Test Loss: 2.6804607386939897e-05\n",
            "Epoch 986/1000, Training Loss: 2.8510192970990542e-05, Test Loss: 2.6778074499155478e-05\n",
            "Epoch 987/1000, Training Loss: 2.847930774522167e-05, Test Loss: 2.675159434518471e-05\n",
            "Epoch 988/1000, Training Loss: 2.8448487640130217e-05, Test Loss: 2.6725166723045084e-05\n",
            "Epoch 989/1000, Training Loss: 2.8417732411208937e-05, Test Loss: 2.669879143192741e-05\n",
            "Epoch 990/1000, Training Loss: 2.8387041815367874e-05, Test Loss: 2.6672468272188618e-05\n",
            "Epoch 991/1000, Training Loss: 2.835641561092311e-05, Test Loss: 2.66461970453404e-05\n",
            "Epoch 992/1000, Training Loss: 2.8325853557584602e-05, Test Loss: 2.6619977554042634e-05\n",
            "Epoch 993/1000, Training Loss: 2.829535541644592e-05, Test Loss: 2.6593809602093683e-05\n",
            "Epoch 994/1000, Training Loss: 2.8264920949973406e-05, Test Loss: 2.656769299441864e-05\n",
            "Epoch 995/1000, Training Loss: 2.82345499219937e-05, Test Loss: 2.654162753706529e-05\n",
            "Epoch 996/1000, Training Loss: 2.8204242097684015e-05, Test Loss: 2.65156130371927e-05\n",
            "Epoch 997/1000, Training Loss: 2.817399724356202e-05, Test Loss: 2.6489649303061933e-05\n",
            "Epoch 998/1000, Training Loss: 2.814381512747331e-05, Test Loss: 2.6463736144028663e-05\n",
            "Epoch 999/1000, Training Loss: 2.8113695518582188e-05, Test Loss: 2.643787337053724e-05\n",
            "Epoch 1000/1000, Training Loss: 2.808363818736121e-05, Test Loss: 2.641206079410791e-05\n",
            "Optimal Number of Hidden Nodes (H*): 9\n",
            "Corresponding Optimal Epoch: 1000\n",
            "Minimum Test Loss: 1.7331159113314552e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# MLP class with one hidden layer\n",
        "class MLP:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        # Initialize the number of nodes in each layer\n",
        "        self.input_nodes = input_nodes + 1  # Including bias\n",
        "        self.hidden_nodes = hidden_nodes + 1  # Including bias\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights_input_hidden = np.random.rand(self.input_nodes, self.hidden_nodes - 1) - 0.5\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_nodes, self.output_nodes) - 0.5\n",
        "\n",
        "        # Include bias in the inputs\n",
        "        self.bias_input = np.random.rand(self.input_nodes) - 0.5\n",
        "        self.bias_hidden = np.random.rand(self.hidden_nodes) - 0.5\n",
        "\n",
        "        # Initialize lists to store training and test losses\n",
        "        self.training_loss_history = []\n",
        "        self.test_loss_history = []\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # Add bias to inputs\n",
        "        inputs_with_bias = np.concatenate((inputs, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the hidden layer\n",
        "        self.hidden_input = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        # Add bias to hidden layer outputs\n",
        "        hidden_output_with_bias = np.concatenate((self.hidden_output, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the output layer\n",
        "        self.final_input = np.dot(hidden_output_with_bias, self.weights_hidden_output)\n",
        "        self.final_output = sigmoid(self.final_input)\n",
        "\n",
        "        return self.final_output\n",
        "\n",
        "    def backward_pass(self, inputs, expected_output, output, learning_rate):\n",
        "        # Compute error\n",
        "        error = expected_output - output\n",
        "\n",
        "        # Gradient for output weights\n",
        "        d_weights_hidden_output = np.dot(np.concatenate((self.hidden_output, [1]), axis=0).reshape(-1,1),\n",
        "                                         error * sigmoid_derivative(output).reshape(1, -1))\n",
        "\n",
        "        # Error for hidden layer\n",
        "        hidden_error = np.dot(self.weights_hidden_output, error * sigmoid_derivative(output))[:-1]\n",
        "\n",
        "        # Gradient for input weights\n",
        "        d_weights_input_hidden = np.dot(np.concatenate((inputs, [1]), axis=0).reshape(-1,1),\n",
        "                                        hidden_error * sigmoid_derivative(self.hidden_output).reshape(1, -1))\n",
        "\n",
        "        # Update the weights\n",
        "        self.weights_hidden_output += learning_rate * d_weights_hidden_output\n",
        "        self.weights_input_hidden += learning_rate * d_weights_input_hidden\n",
        "\n",
        "    def train_and_evaluate(self, dataset, test_inputs, test_expected_output, max_epochs, learning_rate):\n",
        "        # Split dataset into inputs and expected outputs\n",
        "        inputs = dataset[:, :2]\n",
        "        expected_output = dataset[:, 2:]\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(max_epochs):\n",
        "            for j in range(inputs.shape[0]):\n",
        "                input_sample = inputs[j]\n",
        "                output = self.forward_pass(input_sample)\n",
        "                self.backward_pass(input_sample, expected_output[j], output, learning_rate)\n",
        "\n",
        "            # Calculate training loss\n",
        "            training_loss = np.mean(np.square(expected_output - self.predict(inputs)))\n",
        "            self.training_loss_history.append(training_loss)\n",
        "\n",
        "            # Calculate test loss\n",
        "            test_loss = np.mean(np.square(test_expected_output - self.predict(test_inputs)))\n",
        "            self.test_loss_history.append(test_loss)\n",
        "\n",
        "            # Print out progress\n",
        "            print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {training_loss}, Test Loss: {test_loss}\")\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        outputs = np.array([self.forward_pass(input_sample) for input_sample in inputs])\n",
        "        return outputs\n",
        "\n",
        "# Function to calculate the multivariate normal density\n",
        "def multivariate_gaussian_density(x, mu, cov):\n",
        "    n = mu.shape[0]\n",
        "    diff = x - mu\n",
        "    return (1. / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \\\n",
        "           np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))\n",
        "\n",
        "# Parameters for the Gaussian\n",
        "mu_x = np.array([0, 0])\n",
        "cov_x = np.array([[0.3, -0.5],\n",
        "                  [-0.5, 2]])\n",
        "\n",
        "# Set the number of input samples and epochs\n",
        "N = 1600\n",
        "E = 1000\n",
        "\n",
        "# Set the range of hidden nodes\n",
        "hidden_nodes_range = range(1, 21)  # From 1 to 20 hidden nodes\n",
        "\n",
        "# Initialize lists to store test loss and optimal epochs\n",
        "test_loss_values = []\n",
        "training_loss_values = []\n",
        "\n",
        "for H in hidden_nodes_range:\n",
        "    # Generate N training samples randomly\n",
        "    samples = np.zeros((N, 3))\n",
        "    samples[:, 0] = np.random.uniform(-2, 2, N)  # x1\n",
        "    samples[:, 1] = np.random.uniform(-4, 4, N)  # x2\n",
        "\n",
        "    # Calculate the function value for each sample\n",
        "    for i in range(N):\n",
        "        samples[i, 2] = multivariate_gaussian_density(samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Generate new test data\n",
        "    N_test = 100  # Change this to your dataset size for testing\n",
        "    test_samples = np.zeros((N_test, 3))\n",
        "    test_samples[:, 0] = np.random.uniform(-2, 2, N_test)  # x1 range\n",
        "    test_samples[:, 1] = np.random.uniform(-4, 4, N_test)  # x2 range\n",
        "\n",
        "    for i in range(N_test):\n",
        "        test_samples[i, 2] = multivariate_gaussian_density(test_samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Initialize the MLP\n",
        "    mlp = MLP(input_nodes=2, hidden_nodes=H, output_nodes=1)\n",
        "    mlp.train_and_evaluate(samples, test_samples[:, :2], test_samples[:, 2:], max_epochs=E, learning_rate=0.1)\n",
        "\n",
        "    # Record test loss and training loss\n",
        "    test_loss_values.append(mlp.test_loss_history[-1])\n",
        "    training_loss_values.append(mlp.training_loss_history[-1])\n",
        "\n",
        "# Find the index of the minimum test loss value\n",
        "optimal_H_index = np.argmin(test_loss_values)\n",
        "optimal_H = hidden_nodes_range[optimal_H_index]\n",
        "min_test_loss = test_loss_values[optimal_H_index]\n",
        "optimal_epoch_for_optimal_H = E  # Optimal epoch is fixed at E\n",
        "\n",
        "print(f\"Optimal Number of Hidden Nodes (H*): {optimal_H}\")\n",
        "print(f\"Corresponding Optimal Epoch: {optimal_epoch_for_optimal_H}\")\n",
        "print(f\"Minimum Test Loss: {min_test_loss}\")\n",
        "\n",
        "# Plot test loss and training loss against the number of hidden nodes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(hidden_nodes_range, training_loss_values, marker='o', linestyle='-', label='Training Loss')\n",
        "plt.plot(hidden_nodes_range, test_loss_values, marker='o', linestyle='-', label='Test Loss')\n",
        "plt.title('Loss vs Number of Hidden Nodes')\n",
        "plt.xlabel('Number of Hidden Nodes (H)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.axvline(x=optimal_H, color='r', linestyle='--', label=f'Optimal H: {optimal_H}')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}