{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPw/ZlnGfZAUemkewGmSdN9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhivyaaSP/Deep-Learning/blob/main/Exe_2_2_3_Hidden_Nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R6P2-vHHrPjd",
        "outputId": "8cec9ef0-90a3-4026-bc6d-6d249d84295d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 4/1000, Training Loss: 0.0034523631347154076, Test Loss: 0.002818263252092433\n",
            "Epoch 5/1000, Training Loss: 0.0034477818895917766, Test Loss: 0.0028147702798226827\n",
            "Epoch 6/1000, Training Loss: 0.0034445548036296735, Test Loss: 0.002812408353583007\n",
            "Epoch 7/1000, Training Loss: 0.0034421077468936813, Test Loss: 0.002810628093111774\n",
            "Epoch 8/1000, Training Loss: 0.0034401326171473847, Test Loss: 0.0028091517390920534\n",
            "Epoch 9/1000, Training Loss: 0.0034384461626874013, Test Loss: 0.002807828048700282\n",
            "Epoch 10/1000, Training Loss: 0.003436937178484362, Test Loss: 0.0028065753714230372\n",
            "Epoch 11/1000, Training Loss: 0.003435538193952216, Test Loss: 0.0028053513816098585\n",
            "Epoch 12/1000, Training Loss: 0.003434208349736424, Test Loss: 0.0028041355906089473\n",
            "Epoch 13/1000, Training Loss: 0.0034329228756925425, Test Loss: 0.002802919251057579\n",
            "Epoch 14/1000, Training Loss: 0.0034316666756283643, Test Loss: 0.002801699642633727\n",
            "Epoch 15/1000, Training Loss: 0.0034304304514071386, Test Loss: 0.0028004769143587693\n",
            "Epoch 16/1000, Training Loss: 0.0034292083762287925, Test Loss: 0.0027992523902438033\n",
            "Epoch 17/1000, Training Loss: 0.0034279967032919577, Test Loss: 0.0027980276977371656\n",
            "Epoch 18/1000, Training Loss: 0.0034267929354675195, Test Loss: 0.0027968043500580238\n",
            "Epoch 19/1000, Training Loss: 0.003425595330074668, Test Loss: 0.0027955835727088217\n",
            "Epoch 20/1000, Training Loss: 0.0034244026033391026, Test Loss: 0.0027943662564329135\n",
            "Epoch 21/1000, Training Loss: 0.003423213753687994, Test Loss: 0.002793152971521942\n",
            "Epoch 22/1000, Training Loss: 0.0034220279557401197, Test Loss: 0.0027919440082674247\n",
            "Epoch 23/1000, Training Loss: 0.0034208444963687652, Test Loss: 0.0027907394251679147\n",
            "Epoch 24/1000, Training Loss: 0.003419662735835075, Test Loss: 0.002789539095841215\n",
            "Epoch 25/1000, Training Loss: 0.0034184820838953946, Test Loss: 0.002788342750673223\n",
            "Epoch 26/1000, Training Loss: 0.0034173019848857842, Test Loss: 0.0027871500119136016\n",
            "Epoch 27/1000, Training Loss: 0.0034161219082189365, Test Loss: 0.0027859604222657606\n",
            "Epoch 28/1000, Training Loss: 0.0034149413421709863, Test Loss: 0.002784773467621568\n",
            "Epoch 29/1000, Training Loss: 0.0034137597896911442, Test Loss: 0.0027835885948033264\n",
            "Epoch 30/1000, Training Loss: 0.003412576765474567, Test Loss: 0.0027824052251901733\n",
            "Epoch 31/1000, Training Loss: 0.0034113917938404293, Test Loss: 0.0027812227650292187\n",
            "Epoch 32/1000, Training Loss: 0.0034102044071365125, Test Loss: 0.002780040613120481\n",
            "Epoch 33/1000, Training Loss: 0.0034090141444987464, Test Loss: 0.002778858166448229\n",
            "Epoch 34/1000, Training Loss: 0.0034078205508583015, Test Loss: 0.0027776748242234945\n",
            "Epoch 35/1000, Training Loss: 0.003406623176127569, Test Loss: 0.0027764899907087837\n",
            "Epoch 36/1000, Training Loss: 0.003405421574519977, Test Loss: 0.0027753030771175387\n",
            "Epoch 37/1000, Training Loss: 0.0034042153039730966, Test Loss: 0.002774113502816902\n",
            "Epoch 38/1000, Training Loss: 0.0034030039256536977, Test Loss: 0.00277292069601092\n",
            "Epoch 39/1000, Training Loss: 0.003401787003529206, Test Loss: 0.0027717240940406075\n",
            "Epoch 40/1000, Training Loss: 0.0034005641039939593, Test Loss: 0.0027705231434052924\n",
            "Epoch 41/1000, Training Loss: 0.0033993347955412543, Test Loss: 0.0027693172995847536\n",
            "Epoch 42/1000, Training Loss: 0.0033980986484740665, Test Loss: 0.0027681060267222995\n",
            "Epoch 43/1000, Training Loss: 0.0033968552346486896, Test Loss: 0.002766888797214075\n",
            "Epoch 44/1000, Training Loss: 0.0033956041272465813, Test Loss: 0.0027656650912383824\n",
            "Epoch 45/1000, Training Loss: 0.003394344900570503, Test Loss: 0.002764434396250094\n",
            "Epoch 46/1000, Training Loss: 0.0033930771298616913, Test Loss: 0.0027631962064585277\n",
            "Epoch 47/1000, Training Loss: 0.003391800391135341, Test Loss: 0.0027619500223021332\n",
            "Epoch 48/1000, Training Loss: 0.0033905142610320847, Test Loss: 0.0027606953499294557\n",
            "Epoch 49/1000, Training Loss: 0.003389218316683561, Test Loss: 0.0027594317006930298\n",
            "Epoch 50/1000, Training Loss: 0.003387912135590432, Test Loss: 0.0027581585906606503\n",
            "Epoch 51/1000, Training Loss: 0.0033865952955114864, Test Loss: 0.0027568755401469495\n",
            "Epoch 52/1000, Training Loss: 0.003385267374362709, Test Loss: 0.0027555820732669124\n",
            "Epoch 53/1000, Training Loss: 0.003383927950125336, Test Loss: 0.002754277717512252\n",
            "Epoch 54/1000, Training Loss: 0.0033825766007621215, Test Loss: 0.002752962003350829\n",
            "Epoch 55/1000, Training Loss: 0.0033812129041411653, Test Loss: 0.0027516344638489547\n",
            "Epoch 56/1000, Training Loss: 0.003379836437966751, Test Loss: 0.002750294634316055\n",
            "Epoch 57/1000, Training Loss: 0.003378446779716776, Test Loss: 0.002748942051971027\n",
            "Epoch 58/1000, Training Loss: 0.0033770435065864097, Test Loss: 0.002747576255629479\n",
            "Epoch 59/1000, Training Loss: 0.0033756261954377286, Test Loss: 0.0027461967854109683\n",
            "Epoch 60/1000, Training Loss: 0.0033741944227550923, Test Loss: 0.0027448031824653597\n",
            "Epoch 61/1000, Training Loss: 0.0033727477646061264, Test Loss: 0.002743394988717387\n",
            "Epoch 62/1000, Training Loss: 0.003371285796608202, Test Loss: 0.0027419717466285755\n",
            "Epoch 63/1000, Training Loss: 0.003369808093900334, Test Loss: 0.0027405329989756704\n",
            "Epoch 64/1000, Training Loss: 0.0033683142311204907, Test Loss: 0.002739078288644803\n",
            "Epoch 65/1000, Training Loss: 0.0033668037823882946, Test Loss: 0.0027376071584406603\n",
            "Epoch 66/1000, Training Loss: 0.0033652763212931626, Test Loss: 0.0027361191509099776\n",
            "Epoch 67/1000, Training Loss: 0.003363731420887915, Test Loss: 0.0027346138081787404\n",
            "Epoch 68/1000, Training Loss: 0.00336216865368796, Test Loss: 0.002733090671802534\n",
            "Epoch 69/1000, Training Loss: 0.0033605875916761087, Test Loss: 0.0027315492826295506\n",
            "Epoch 70/1000, Training Loss: 0.0033589878063131594, Test Loss: 0.0027299891806757647\n",
            "Epoch 71/1000, Training Loss: 0.0033573688685543555, Test Loss: 0.002728409905011933\n",
            "Epoch 72/1000, Training Loss: 0.0033557303488718653, Test Loss: 0.0027268109936620243\n",
            "Epoch 73/1000, Training Loss: 0.0033540718172834177, Test Loss: 0.0027251919835128207\n",
            "Epoch 74/1000, Training Loss: 0.0033523928433872663, Test Loss: 0.002723552410234406\n",
            "Epoch 75/1000, Training Loss: 0.0033506929964036347, Test Loss: 0.0027218918082113487\n",
            "Epoch 76/1000, Training Loss: 0.003348971845222834, Test Loss: 0.0027202097104844173\n",
            "Epoch 77/1000, Training Loss: 0.003347228958460212, Test Loss: 0.0027185056487026744\n",
            "Epoch 78/1000, Training Loss: 0.00334546390451815, Test Loss: 0.0027167791530858863\n",
            "Epoch 79/1000, Training Loss: 0.0033436762516552755, Test Loss: 0.0027150297523971766\n",
            "Epoch 80/1000, Training Loss: 0.0033418655680631116, Test Loss: 0.0027132569739258833\n",
            "Epoch 81/1000, Training Loss: 0.0033400314219503443, Test Loss: 0.0027114603434806647\n",
            "Epoch 82/1000, Training Loss: 0.003338173381634936, Test Loss: 0.002709639385392834\n",
            "Epoch 83/1000, Training Loss: 0.003336291015644275, Test Loss: 0.00270779362253004\n",
            "Epoch 84/1000, Training Loss: 0.0033343838928235814, Test Loss: 0.0027059225763203386\n",
            "Epoch 85/1000, Training Loss: 0.0033324515824527732, Test Loss: 0.0027040257667867796\n",
            "Epoch 86/1000, Training Loss: 0.0033304936543720144, Test Loss: 0.0027021027125926507\n",
            "Epoch 87/1000, Training Loss: 0.003328509679116134, Test Loss: 0.0027001529310975176\n",
            "Epoch 88/1000, Training Loss: 0.0033264992280581497, Test Loss: 0.0026981759384242378\n",
            "Epoch 89/1000, Training Loss: 0.0033244618735620724, Test Loss: 0.0026961712495371434\n",
            "Epoch 90/1000, Training Loss: 0.0033223971891452194, Test Loss: 0.002694138378331599\n",
            "Epoch 91/1000, Training Loss: 0.0033203047496502035, Test Loss: 0.002692076837735144\n",
            "Epoch 92/1000, Training Loss: 0.003318184131426814, Test Loss: 0.0026899861398204756\n",
            "Epoch 93/1000, Training Loss: 0.0033160349125239386, Test Loss: 0.0026878657959305058\n",
            "Epoch 94/1000, Training Loss: 0.003313856672891736, Test Loss: 0.0026857153168157568\n",
            "Epoch 95/1000, Training Loss: 0.0033116489945941795, Test Loss: 0.0026835342127843596\n",
            "Epoch 96/1000, Training Loss: 0.0033094114620321542, Test Loss: 0.0026813219938649413\n",
            "Epoch 97/1000, Training Loss: 0.0033071436621772182, Test Loss: 0.00267907816998266\n",
            "Epoch 98/1000, Training Loss: 0.0033048451848161652, Test Loss: 0.0026768022511487128\n",
            "Epoch 99/1000, Training Loss: 0.0033025156228064833, Test Loss: 0.0026744937476635674\n",
            "Epoch 100/1000, Training Loss: 0.0033001545723427904, Test Loss: 0.0026721521703342277\n",
            "Epoch 101/1000, Training Loss: 0.00329776163323432, Test Loss: 0.002669777030705827\n",
            "Epoch 102/1000, Training Loss: 0.003295336409193486, Test Loss: 0.002667367841307831\n",
            "Epoch 103/1000, Training Loss: 0.003292878508135549, Test Loss: 0.002664924115915128\n",
            "Epoch 104/1000, Training Loss: 0.003290387542489377, Test Loss: 0.0026624453698243\n",
            "Epoch 105/1000, Training Loss: 0.0032878631295192406, Test Loss: 0.0026599311201453302\n",
            "Epoch 106/1000, Training Loss: 0.0032853048916575943, Test Loss: 0.0026573808861090043\n",
            "Epoch 107/1000, Training Loss: 0.003282712456848717, Test Loss: 0.002654794189390271\n",
            "Epoch 108/1000, Training Loss: 0.003280085458903087, Test Loss: 0.0026521705544477513\n",
            "Epoch 109/1000, Training Loss: 0.0032774235378622884, Test Loss: 0.0026495095088796506\n",
            "Epoch 110/1000, Training Loss: 0.0032747263403742444, Test Loss: 0.0026468105837962326\n",
            "Epoch 111/1000, Training Loss: 0.0032719935200784945, Test Loss: 0.002644073314209035\n",
            "Epoch 112/1000, Training Loss: 0.003269224738001215, Test Loss: 0.0026412972394369637\n",
            "Epoch 113/1000, Training Loss: 0.00326641966295961, Test Loss: 0.0026384819035293657\n",
            "Epoch 114/1000, Training Loss: 0.003263577971975258, Test Loss: 0.002635626855706175\n",
            "Epoch 115/1000, Training Loss: 0.003260699350695939, Test Loss: 0.0026327316508151515\n",
            "Epoch 116/1000, Training Loss: 0.003257783493825433, Test Loss: 0.002629795849806223\n",
            "Epoch 117/1000, Training Loss: 0.0032548301055606767, Test Loss: 0.002626819020222891\n",
            "Epoch 118/1000, Training Loss: 0.0032518389000356445, Test Loss: 0.0026238007367105974\n",
            "Epoch 119/1000, Training Loss: 0.003248809601771241, Test Loss: 0.0026207405815419243\n",
            "Epoch 120/1000, Training Loss: 0.0032457419461304126, Test Loss: 0.0026176381451584246\n",
            "Epoch 121/1000, Training Loss: 0.0032426356797776302, Test Loss: 0.0026144930267288356\n",
            "Epoch 122/1000, Training Loss: 0.0032394905611418403, Test Loss: 0.0026113048347233586\n",
            "Epoch 123/1000, Training Loss: 0.0032363063608818565, Test Loss: 0.0026080731875036266\n",
            "Epoch 124/1000, Training Loss: 0.003233082862353154, Test Loss: 0.002604797713927911\n",
            "Epoch 125/1000, Training Loss: 0.003229819862074906, Test Loss: 0.002601478053971038\n",
            "Epoch 126/1000, Training Loss: 0.0032265171701960425, Test Loss: 0.002598113859358441\n",
            "Epoch 127/1000, Training Loss: 0.003223174610959041, Test Loss: 0.002594704794213632\n",
            "Epoch 128/1000, Training Loss: 0.0032197920231600663, Test Loss: 0.002591250535718363\n",
            "Epoch 129/1000, Training Loss: 0.003216369260604006, Test Loss: 0.002587750774784609\n",
            "Epoch 130/1000, Training Loss: 0.0032129061925528744, Test Loss: 0.0025842052167374357\n",
            "Epoch 131/1000, Training Loss: 0.00320940270416597, Test Loss: 0.0025806135820077196\n",
            "Epoch 132/1000, Training Loss: 0.003205858696930113, Test Loss: 0.002576975606833593\n",
            "Epoch 133/1000, Training Loss: 0.0032022740890782, Test Loss: 0.0025732910439693725\n",
            "Epoch 134/1000, Training Loss: 0.003198648815994257, Test Loss: 0.002569559663400659\n",
            "Epoch 135/1000, Training Loss: 0.003194982830603078, Test Loss: 0.0025657812530641617\n",
            "Epoch 136/1000, Training Loss: 0.0031912761037425274, Test Loss: 0.0025619556195707245\n",
            "Epoch 137/1000, Training Loss: 0.0031875286245164358, Test Loss: 0.0025580825889298994\n",
            "Epoch 138/1000, Training Loss: 0.0031837404006260744, Test Loss: 0.002554162007274333\n",
            "Epoch 139/1000, Training Loss: 0.003179911458678033, Test Loss: 0.0025501937415821256\n",
            "Epoch 140/1000, Training Loss: 0.0031760418444663763, Test Loss: 0.0025461776803951818\n",
            "Epoch 141/1000, Training Loss: 0.003172131623226845, Test Loss: 0.0025421137345315314\n",
            "Epoch 142/1000, Training Loss: 0.003168180879860903, Test Loss: 0.0025380018377894496\n",
            "Epoch 143/1000, Training Loss: 0.003164189719127347, Test Loss: 0.0025338419476411294\n",
            "Epoch 144/1000, Training Loss: 0.003160158265799245, Test Loss: 0.0025296340459135713\n",
            "Epoch 145/1000, Training Loss: 0.0031560866647839265, Test Loss: 0.002525378139454254\n",
            "Epoch 146/1000, Training Loss: 0.0031519750812037633, Test Loss: 0.0025210742607790714\n",
            "Epoch 147/1000, Training Loss: 0.0031478237004355326, Test Loss: 0.0025167224686999706\n",
            "Epoch 148/1000, Training Loss: 0.00314363272810612, Test Loss: 0.0025123228489296063\n",
            "Epoch 149/1000, Training Loss: 0.003139402390042439, Test Loss: 0.0025078755146603154\n",
            "Epoch 150/1000, Training Loss: 0.0031351329321734345, Test Loss: 0.0025033806071146493\n",
            "Epoch 151/1000, Training Loss: 0.003130824620382141, Test Loss: 0.002498838296064637\n",
            "Epoch 152/1000, Training Loss: 0.0031264777403058543, Test Loss: 0.0024942487803169696\n",
            "Epoch 153/1000, Training Loss: 0.003122092597082513, Test Loss: 0.0024896122881612223\n",
            "Epoch 154/1000, Training Loss: 0.0031176695150415866, Test Loss: 0.0024849290777782876\n",
            "Epoch 155/1000, Training Loss: 0.0031132088373377973, Test Loss: 0.0024801994376061377\n",
            "Epoch 156/1000, Training Loss: 0.0031087109255261785, Test Loss: 0.0024754236866600973\n",
            "Epoch 157/1000, Training Loss: 0.0031041761590771422, Test Loss: 0.002470602174804848\n",
            "Epoch 158/1000, Training Loss: 0.0030996049348303184, Test Loss: 0.002465735282975395\n",
            "Epoch 159/1000, Training Loss: 0.0030949976663861766, Test Loss: 0.002460823423344344\n",
            "Epoch 160/1000, Training Loss: 0.0030903547834345603, Test Loss: 0.0024558670394328836\n",
            "Epoch 161/1000, Training Loss: 0.0030856767310194987, Test Loss: 0.002450866606162999\n",
            "Epoch 162/1000, Training Loss: 0.0030809639687398344, Test Loss: 0.002445822629848515\n",
            "Epoch 163/1000, Training Loss: 0.003076216969885425, Test Loss: 0.0024407356481227368\n",
            "Epoch 164/1000, Training Loss: 0.0030714362205089118, Test Loss: 0.002435606229800604\n",
            "Epoch 165/1000, Training Loss: 0.0030666222184332135, Test Loss: 0.0024304349746733945\n",
            "Epoch 166/1000, Training Loss: 0.0030617754721952105, Test Loss: 0.0024252225132342284\n",
            "Epoch 167/1000, Training Loss: 0.0030568964999262305, Test Loss: 0.0024199695063328096\n",
            "Epoch 168/1000, Training Loss: 0.003051985828170236, Test Loss: 0.002414676644758021\n",
            "Epoch 169/1000, Training Loss: 0.003047043990640806, Test Loss: 0.0024093446487472386\n",
            "Epoch 170/1000, Training Loss: 0.003042071526918232, Test Loss: 0.002403974267421398\n",
            "Epoch 171/1000, Training Loss: 0.0030370689810882766, Test Loss: 0.002398566278145154\n",
            "Epoch 172/1000, Training Loss: 0.003032036900324345, Test Loss: 0.0023931214858116355\n",
            "Epoch 173/1000, Training Loss: 0.0030269758334149997, Test Loss: 0.002387640722051586\n",
            "Epoch 174/1000, Training Loss: 0.0030218863292389665, Test Loss: 0.0023821248443669065\n",
            "Epoch 175/1000, Training Loss: 0.0030167689351899285, Test Loss: 0.0023765747351888592\n",
            "Epoch 176/1000, Training Loss: 0.003011624195553555, Test Loss: 0.0023709913008614367\n",
            "Epoch 177/1000, Training Loss: 0.0030064526498393674, Test Loss: 0.0023653754705506144\n",
            "Epoch 178/1000, Training Loss: 0.003001254831070116, Test Loss: 0.002359728195080454\n",
            "Epoch 179/1000, Training Loss: 0.0029960312640314995, Test Loss: 0.0023540504456972203\n",
            "Epoch 180/1000, Training Loss: 0.002990782463485018, Test Loss: 0.0023483432127628548\n",
            "Epoch 181/1000, Training Loss: 0.0029855089323469063, Test Loss: 0.0023426075043793794\n",
            "Epoch 182/1000, Training Loss: 0.0029802111598359817, Test Loss: 0.0023368443449458764\n",
            "Epoch 183/1000, Training Loss: 0.0029748896195933073, Test Loss: 0.002331054773649925\n",
            "Epoch 184/1000, Training Loss: 0.0029695447677764637, Test Loss: 0.002325239842895363\n",
            "Epoch 185/1000, Training Loss: 0.0029641770411311347, Test Loss: 0.002319400616668446\n",
            "Epoch 186/1000, Training Loss: 0.002958786855042615, Test Loss: 0.0023135381688443764\n",
            "Epoch 187/1000, Training Loss: 0.0029533746015696282, Test Loss: 0.0023076535814363012\n",
            "Epoch 188/1000, Training Loss: 0.002947940647462729, Test Loss: 0.00230174794278876\n",
            "Epoch 189/1000, Training Loss: 0.0029424853321692456, Test Loss: 0.0022958223457175244\n",
            "Epoch 190/1000, Training Loss: 0.0029370089658264965, Test Loss: 0.002289877885597645\n",
            "Epoch 191/1000, Training Loss: 0.002931511827244699, Test Loss: 0.0022839156584013344\n",
            "Epoch 192/1000, Training Loss: 0.0029259941618806394, Test Loss: 0.0022779367586871374\n",
            "Epoch 193/1000, Training Loss: 0.0029204561798027912, Test Loss: 0.002271942277541533\n",
            "Epoch 194/1000, Training Loss: 0.002914898053648175, Test Loss: 0.002265933300473847\n",
            "Epoch 195/1000, Training Loss: 0.0029093199165707824, Test Loss: 0.002259910905264938\n",
            "Epoch 196/1000, Training Loss: 0.00290372186018093, Test Loss: 0.0022538761597697566\n",
            "Epoch 197/1000, Training Loss: 0.002898103932474374, Test Loss: 0.0022478301196733746\n",
            "Epoch 198/1000, Training Loss: 0.002892466135749514, Test Loss: 0.002241773826199572\n",
            "Epoch 199/1000, Training Loss: 0.002886808424510411, Test Loss: 0.002235708303770535\n",
            "Epoch 200/1000, Training Loss: 0.0028811307033527766, Test Loss: 0.0022296345576155448\n",
            "Epoch 201/1000, Training Loss: 0.00287543282482948, Test Loss: 0.0022235535713259346\n",
            "Epoch 202/1000, Training Loss: 0.0028697145872914488, Test Loss: 0.0022174663043528567\n",
            "Epoch 203/1000, Training Loss: 0.0028639757326992432, Test Loss: 0.0022113736894436406\n",
            "Epoch 204/1000, Training Loss: 0.002858215944399865, Test Loss: 0.002205276630011797\n",
            "Epoch 205/1000, Training Loss: 0.0028524348448627024, Test Loss: 0.0021991759974348095\n",
            "Epoch 206/1000, Training Loss: 0.002846631993367842, Test Loss: 0.002193072628273116\n",
            "Epoch 207/1000, Training Loss: 0.0028408068836392342, Test Loss: 0.0021869673214027024\n",
            "Epoch 208/1000, Training Loss: 0.0028349589414145646, Test Loss: 0.00218086083505291\n",
            "Epoch 209/1000, Training Loss: 0.00282908752194295, Test Loss: 0.0021747538837401247\n",
            "Epoch 210/1000, Training Loss: 0.0028231919074009006, Test Loss: 0.0021686471350871052\n",
            "Epoch 211/1000, Training Loss: 0.002817271304216359, Test Loss: 0.002162541206516809\n",
            "Epoch 212/1000, Training Loss: 0.0028113248402899284, Test Loss: 0.002156436661808676\n",
            "Epoch 213/1000, Training Loss: 0.002805351562101827, Test Loss: 0.0021503340075044656\n",
            "Epoch 214/1000, Training Loss: 0.0027993504316925124, Test Loss: 0.0021442336891499196\n",
            "Epoch 215/1000, Training Loss: 0.0027933203235043958, Test Loss: 0.0021381360873577253\n",
            "Epoch 216/1000, Training Loss: 0.002787260021071624, Test Loss: 0.002132041513676585\n",
            "Epoch 217/1000, Training Loss: 0.002781168213544456, Test Loss: 0.0021259502062505216\n",
            "Epoch 218/1000, Training Loss: 0.0027750434920345655, Test Loss: 0.002119862325252081\n",
            "Epoch 219/1000, Training Loss: 0.002768884345767322, Test Loss: 0.0021137779480726888\n",
            "Epoch 220/1000, Training Loss: 0.00276268915802712, Test Loss: 0.002107697064253198\n",
            "Epoch 221/1000, Training Loss: 0.0027564562018819426, Test Loss: 0.00210161957013762\n",
            "Epoch 222/1000, Training Loss: 0.0027501836356736303, Test Loss: 0.00209554526323326\n",
            "Epoch 223/1000, Training Loss: 0.0027438694982609003, Test Loss: 0.002089473836260868\n",
            "Epoch 224/1000, Training Loss: 0.0027375117040029763, Test Loss: 0.0020834048708792655\n",
            "Epoch 225/1000, Training Loss: 0.002731108037472828, Test Loss: 0.002077337831069953\n",
            "Epoch 226/1000, Training Loss: 0.0027246561478905403, Test Loss: 0.0020712720561688186\n",
            "Epoch 227/1000, Training Loss: 0.002718153543269282, Test Loss: 0.0020652067535340563\n",
            "Epoch 228/1000, Training Loss: 0.0027115975842688512, Test Loss: 0.0020591409908420222\n",
            "Epoch 229/1000, Training Loss: 0.0027049854777547733, Test Loss: 0.0020530736880059646\n",
            "Epoch 230/1000, Training Loss: 0.0026983142700647274, Test Loss: 0.0020470036087165346\n",
            "Epoch 231/1000, Training Loss: 0.00269158083998856, Test Loss: 0.002040929351607776\n",
            "Epoch 232/1000, Training Loss: 0.0026847818914735888, Test Loss: 0.002034849341058021\n",
            "Epoch 233/1000, Training Loss: 0.0026779139460732706, Test Loss: 0.0020287618176419078\n",
            "Epoch 234/1000, Training Loss: 0.0026709733351648946, Test Loss: 0.0020226648282577345\n",
            "Epoch 235/1000, Training Loss: 0.00266395619197076, Test Loss: 0.0020165562159636468\n",
            "Epoch 236/1000, Training Loss: 0.0026568584434275293, Test Loss: 0.0020104336095669814\n",
            "Epoch 237/1000, Training Loss: 0.0026496758019601816, Test Loss: 0.002004294413023465\n",
            "Epoch 238/1000, Training Loss: 0.0026424037572304617, Test Loss: 0.0019981357947171704\n",
            "Epoch 239/1000, Training Loss: 0.0026350375679449352, Test Loss: 0.0019919546767082283\n",
            "Epoch 240/1000, Training Loss: 0.0026275722538249176, Test Loss: 0.0019857477240535094\n",
            "Epoch 241/1000, Training Loss: 0.002620002587859638, Test Loss: 0.0019795113343257604\n",
            "Epoch 242/1000, Training Loss: 0.0026123230889851224, Test Loss: 0.0019732416274793594\n",
            "Epoch 243/1000, Training Loss: 0.0026045280153543415, Test Loss: 0.00196693443623575\n",
            "Epoch 244/1000, Training Loss: 0.0025966113583891074, Test Loss: 0.001960585297188801\n",
            "Epoch 245/1000, Training Loss: 0.00258856683783067, Test Loss: 0.0019541894428597353\n",
            "Epoch 246/1000, Training Loss: 0.0025803878980337593, Test Loss: 0.0019477417949625609\n",
            "Epoch 247/1000, Training Loss: 0.0025720677057772586, Test Loss: 0.0019412369591738404\n",
            "Epoch 248/1000, Training Loss: 0.002563599149893058, Test Loss: 0.0019346692217344938\n",
            "Epoch 249/1000, Training Loss: 0.00255497484304208, Test Loss: 0.0019280325482454906\n",
            "Epoch 250/1000, Training Loss: 0.0025461871259915016, Test Loss: 0.0019213205850525086\n",
            "Epoch 251/1000, Training Loss: 0.002537228074768515, Test Loss: 0.0019145266636456837\n",
            "Epoch 252/1000, Training Loss: 0.0025280895110815172, Test Loss: 0.0019076438085276304\n",
            "Epoch 253/1000, Training Loss: 0.0025187630164073107, Test Loss: 0.001900664749023742\n",
            "Epoch 254/1000, Training Loss: 0.002509239950140235, Test Loss: 0.0018935819355208664\n",
            "Epoch 255/1000, Training Loss: 0.0024995114721833296, Test Loss: 0.0018863875606206831\n",
            "Epoch 256/1000, Training Loss: 0.0024895685703298634, Test Loss: 0.0018790735856788682\n",
            "Epoch 257/1000, Training Loss: 0.002479402092732853, Test Loss: 0.0018716317731668087\n",
            "Epoch 258/1000, Training Loss: 0.002469002785687864, Test Loss: 0.0018640537252348826\n",
            "Epoch 259/1000, Training Loss: 0.0024583613368580792, Test Loss: 0.001856330928771151\n",
            "Epoch 260/1000, Training Loss: 0.00244746842394896, Test Loss: 0.00184845480713301\n",
            "Epoch 261/1000, Training Loss: 0.002436314768692145, Test Loss: 0.0018404167785785923\n",
            "Epoch 262/1000, Training Loss: 0.002424891195825929, Test Loss: 0.001832208321237833\n",
            "Epoch 263/1000, Training Loss: 0.002413188696565625, Test Loss: 0.0018238210442398787\n",
            "Epoch 264/1000, Training Loss: 0.0024011984958465958, Test Loss: 0.0018152467643564147\n",
            "Epoch 265/1000, Training Loss: 0.0023889121224037867, Test Loss: 0.0018064775872351072\n",
            "Epoch 266/1000, Training Loss: 0.0023763214805343487, Test Loss: 0.0017975059919928796\n",
            "Epoch 267/1000, Training Loss: 0.0023634189221873247, Test Loss: 0.001788324917628667\n",
            "Epoch 268/1000, Training Loss: 0.0023501973178510307, Test Loss: 0.0017789278494170122\n",
            "Epoch 269/1000, Training Loss: 0.002336650124580389, Test Loss: 0.0017693089031787256\n",
            "Epoch 270/1000, Training Loss: 0.002322771449438546, Test Loss: 0.0017594629051163476\n",
            "Epoch 271/1000, Training Loss: 0.0023085561066337134, Test Loss: 0.0017493854647756052\n",
            "Epoch 272/1000, Training Loss: 0.0022939996667236656, Test Loss: 0.0017390730386722202\n",
            "Epoch 273/1000, Training Loss: 0.00227909849644274, Test Loss: 0.0017285229822264255\n",
            "Epoch 274/1000, Training Loss: 0.002263849787978457, Test Loss: 0.001717733587886873\n",
            "Epoch 275/1000, Training Loss: 0.002248251576879291, Test Loss: 0.001706704107704263\n",
            "Epoch 276/1000, Training Loss: 0.0022323027481960787, Test Loss: 0.0016954347591233469\n",
            "Epoch 277/1000, Training Loss: 0.002216003030924416, Test Loss: 0.0016839267133779525\n",
            "Epoch 278/1000, Training Loss: 0.002199352981296453, Test Loss: 0.001672182066563137\n",
            "Epoch 279/1000, Training Loss: 0.0021823539559365154, Test Loss: 0.0016602037941778245\n",
            "Epoch 280/1000, Training Loss: 0.0021650080763145267, Test Loss: 0.0016479956906307008\n",
            "Epoch 281/1000, Training Loss: 0.002147318186275593, Test Loss: 0.0016355622958311908\n",
            "Epoch 282/1000, Training Loss: 0.0021292878046703808, Test Loss: 0.0016229088115008656\n",
            "Epoch 283/1000, Training Loss: 0.0021109210752435757, Test Loss: 0.0016100410102022592\n",
            "Epoch 284/1000, Training Loss: 0.002092222715951021, Test Loss: 0.0015969651402699674\n",
            "Epoch 285/1000, Training Loss: 0.002073197969773365, Test Loss: 0.001583687829836558\n",
            "Epoch 286/1000, Training Loss: 0.002053852558887751, Test Loss: 0.0015702159929824972\n",
            "Epoch 287/1000, Training Loss: 0.002034192643768294, Test Loss: 0.0015565567407284557\n",
            "Epoch 288/1000, Training Loss: 0.0020142247884350826, Test Loss: 0.0015427172991634272\n",
            "Epoch 289/1000, Training Loss: 0.0019939559326864736, Test Loss: 0.00152870493650268\n",
            "Epoch 290/1000, Training Loss: 0.0019733933717559714, Test Loss: 0.00151452690033649\n",
            "Epoch 291/1000, Training Loss: 0.0019525447434568968, Test Loss: 0.0015001903658017912\n",
            "Epoch 292/1000, Training Loss: 0.0019314180225337208, Test Loss: 0.001485702394916337\n",
            "Epoch 293/1000, Training Loss: 0.001910021521643134, Test Loss: 0.0014710699068824211\n",
            "Epoch 294/1000, Training Loss: 0.0018883638981487217, Test Loss: 0.001456299658810112\n",
            "Epoch 295/1000, Training Loss: 0.0018664541657345496, Test Loss: 0.0014413982360347846\n",
            "Epoch 296/1000, Training Loss: 0.0018443017097241063, Test Loss: 0.0014263720510101863\n",
            "Epoch 297/1000, Training Loss: 0.0018219163049277036, Test Loss: 0.001411227349640435\n",
            "Epoch 298/1000, Training Loss: 0.001799308134827188, Test Loss: 0.001395970223862047\n",
            "Epoch 299/1000, Training Loss: 0.0017764878109337867, Test Loss: 0.0013806066292885721\n",
            "Epoch 300/1000, Training Loss: 0.0017534663912146648, Test Loss: 0.0013651424067732654\n",
            "Epoch 301/1000, Training Loss: 0.0017302553965682901, Test Loss: 0.0013495833068168305\n",
            "Epoch 302/1000, Training Loss: 0.0017068668244310265, Test Loss: 0.0013339350158377572\n",
            "Epoch 303/1000, Training Loss: 0.001683313158710481, Test Loss: 0.0013182031834221477\n",
            "Epoch 304/1000, Training Loss: 0.001659607375360992, Test Loss: 0.0013023934497722336\n",
            "Epoch 305/1000, Training Loss: 0.0016357629430385433, Test Loss: 0.0012865114726719233\n",
            "Epoch 306/1000, Training Loss: 0.0016117938183942027, Test Loss: 0.0012705629533812006\n",
            "Epoch 307/1000, Training Loss: 0.0015877144356845458, Test Loss: 0.0012545536609564793\n",
            "Epoch 308/1000, Training Loss: 0.0015635396904939943, Test Loss: 0.0012384894545711853\n",
            "Epoch 309/1000, Training Loss: 0.0015392849174760185, Test Loss: 0.0012223763034793108\n",
            "Epoch 310/1000, Training Loss: 0.0015149658621283462, Test Loss: 0.0012062203043263183\n",
            "Epoch 311/1000, Training Loss: 0.0014905986467207521, Test Loss: 0.001190027695566874\n",
            "Epoch 312/1000, Training Loss: 0.0014661997305927337, Test Loss: 0.0011738048687996313\n",
            "Epoch 313/1000, Training Loss: 0.001441785865132062, Test Loss: 0.001157558376876602\n",
            "Epoch 314/1000, Training Loss: 0.001417374043833164, Test Loss: 0.0011412949386900037\n",
            "Epoch 315/1000, Training Loss: 0.0013929814479156743, Test Loss: 0.0011250214405835231\n",
            "Epoch 316/1000, Training Loss: 0.0013686253880577526, Test Loss: 0.001108744934379091\n",
            "Epoch 317/1000, Training Loss: 0.0013443232428641083, Test Loss: 0.0010924726320538354\n",
            "Epoch 318/1000, Training Loss: 0.0013200923947445068, Test Loss: 0.0010762118971458868\n",
            "Epoch 319/1000, Training Loss: 0.0012959501639231532, Test Loss: 0.0010599702330113309\n",
            "Epoch 320/1000, Training Loss: 0.0012719137413318776, Test Loss: 0.0010437552680971904\n",
            "Epoch 321/1000, Training Loss: 0.0012480001211594136, Test Loss: 0.0010275747384368467\n",
            "Epoch 322/1000, Training Loss: 0.001224226033834462, Test Loss: 0.0010114364676127733\n",
            "Epoch 323/1000, Training Loss: 0.0012006078802114183, Test Loss: 0.0009953483444669936\n",
            "Epoch 324/1000, Training Loss: 0.0011771616677046298, Test Loss: 0.0009793182988706752\n",
            "Epoch 325/1000, Training Loss: 0.0011539029490800438, Test Loss: 0.0009633542758898001\n",
            "Epoch 326/1000, Training Loss: 0.0011308467645632923, Test Loss: 0.000947464208703738\n",
            "Epoch 327/1000, Training Loss: 0.001108007587861671, Test Loss: 0.0009316559906464117\n",
            "Epoch 328/1000, Training Loss: 0.001085399276625834, Test Loss: 0.0009159374467458052\n",
            "Epoch 329/1000, Training Loss: 0.0010630350277972106, Test Loss: 0.0009003163051364353\n",
            "Epoch 330/1000, Training Loss: 0.0010409273382014645, Test Loss: 0.0008848001687111326\n",
            "Epoch 331/1000, Training Loss: 0.0010190879706588468, Test Loss: 0.0008693964873634774\n",
            "Epoch 332/1000, Training Loss: 0.0009975279257914656, Test Loss: 0.0008541125311510707\n",
            "Epoch 333/1000, Training Loss: 0.0009762574196175991, Test Loss: 0.0008389553646831524\n",
            "Epoch 334/1000, Training Loss: 0.000955285866936191, Test Loss: 0.0008239318230046662\n",
            "Epoch 335/1000, Training Loss: 0.0009346218704227131, Test Loss: 0.0008090484892141121\n",
            "Epoch 336/1000, Training Loss: 0.0009142732152820776, Test Loss: 0.000794311674014833\n",
            "Epoch 337/1000, Training Loss: 0.0008942468692365031, Test Loss: 0.000779727397360299\n",
            "Epoch 338/1000, Training Loss: 0.0008745489875676496, Test Loss: 0.0007653013723145686\n",
            "Epoch 339/1000, Training Loss: 0.0008551849228827051, Test Loss: 0.0007510389912095503\n",
            "Epoch 340/1000, Training Loss: 0.0008361592392347302, Test Loss: 0.0007369453141432597\n",
            "Epoch 341/1000, Training Loss: 0.0008174757301975713, Test Loss: 0.0007230250598271648\n",
            "Epoch 342/1000, Training Loss: 0.0007991374404753981, Test Loss: 0.0007092825987578856\n",
            "Epoch 343/1000, Training Loss: 0.000781146690615553, Test Loss: 0.0006957219486585535\n",
            "Epoch 344/1000, Training Loss: 0.0007635051043902652, Test Loss: 0.0006823467721088013\n",
            "Epoch 345/1000, Training Loss: 0.000746213638417309, Test Loss: 0.0006691603762599583\n",
            "Epoch 346/1000, Training Loss: 0.0007292726136004588, Test Loss: 0.0006561657145132082\n",
            "Epoch 347/1000, Training Loss: 0.0007126817479871776, Test Loss: 0.0006433653900237108\n",
            "Epoch 348/1000, Training Loss: 0.0006964401906619501, Test Loss: 0.0006307616608826037\n",
            "Epoch 349/1000, Training Loss: 0.0006805465563183189, Test Loss: 0.0006183564468211069\n",
            "Epoch 350/1000, Training Loss: 0.0006649989601799221, Test Loss: 0.000606151337276818\n",
            "Epoch 351/1000, Training Loss: 0.0006497950529698647, Test Loss: 0.0005941476006606411\n",
            "Epoch 352/1000, Training Loss: 0.0006349320556578933, Test Loss: 0.0005823461946643061\n",
            "Epoch 353/1000, Training Loss: 0.0006204067937450925, Test Loss: 0.00057074777745174\n",
            "Epoch 354/1000, Training Loss: 0.0006062157308760641, Test Loss: 0.0005593527195830156\n",
            "Epoch 355/1000, Training Loss: 0.0005923550015977173, Test Loss: 0.000548161116526575\n",
            "Epoch 356/1000, Training Loss: 0.0005788204431120608, Test Loss: 0.0005371728016235307\n",
            "Epoch 357/1000, Training Loss: 0.0005656076258969155, Test Loss: 0.0005263873593769803\n",
            "Epoch 358/1000, Training Loss: 0.0005527118830934231, Test Loss: 0.0005158041389489704\n",
            "Epoch 359/1000, Training Loss: 0.0005401283385822012, Test Loss: 0.0005054222677577215\n",
            "Epoch 360/1000, Training Loss: 0.0005278519336909746, Test Loss: 0.0004952406650781477\n",
            "Epoch 361/1000, Training Loss: 0.000515877452495531, Test Loss: 0.00048525805555862706\n",
            "Epoch 362/1000, Training Loss: 0.0005041995456927115, Test Loss: 0.00047547298257721816\n",
            "Epoch 363/1000, Training Loss: 0.0004928127530391774, Test Loss: 0.0004658838213700097\n",
            "Epoch 364/1000, Training Loss: 0.0004817115243626178, Test Loss: 0.0004564887918735391\n",
            "Epoch 365/1000, Training Loss: 0.00047089023916338274, Test Loss: 0.00044728597123203697\n",
            "Epoch 366/1000, Training Loss: 0.00046034322483399126, Test Loss: 0.00043827330592826686\n",
            "Epoch 367/1000, Training Loss: 0.00045006477353185095, Test Loss: 0.00042944862350441983\n",
            "Epoch 368/1000, Training Loss: 0.00044004915774710116, Test Loss: 0.00042080964384636345\n",
            "Epoch 369/1000, Training Loss: 0.0004302906446125682, Test Loss: 0.0004123539900109522\n",
            "Epoch 370/1000, Training Loss: 0.0004207835090068542, Test Loss: 0.00040407919858176443\n",
            "Epoch 371/1000, Training Loss: 0.0004115220455045071, Test Loss: 0.0003959827295437237\n",
            "Epoch 372/1000, Training Loss: 0.00040250057922918927, Test Loss: 0.00038806197567167896\n",
            "Epoch 373/1000, Training Loss: 0.00039371347566699507, Test Loss: 0.0003803142714319167\n",
            "Epoch 374/1000, Training Loss: 0.0003851551494975224, Test Loss: 0.0003727369013990957\n",
            "Epoch 375/1000, Training Loss: 0.00037682007250020114, Test Loss: 0.0003653271081940768\n",
            "Epoch 376/1000, Training Loss: 0.000368702780592753, Test Loss: 0.0003580820999506805\n",
            "Epoch 377/1000, Training Loss: 0.0003607978800576704, Test Loss: 0.0003509990573215576\n",
            "Epoch 378/1000, Training Loss: 0.000353100053011128, Test Loss: 0.00034407514003510526\n",
            "Epoch 379/1000, Training Loss: 0.0003456040621671522, Test Loss: 0.0003373074930168359\n",
            "Epoch 380/1000, Training Loss: 0.00033830475494797647, Test Loss: 0.0003306932520897151\n",
            "Epoch 381/1000, Training Loss: 0.00033119706698944327, Test Loss: 0.0003242295492689246\n",
            "Epoch 382/1000, Training Loss: 0.0003242760250882024, Test Loss: 0.00031791351766706554\n",
            "Epoch 383/1000, Training Loss: 0.0003175367496351743, Test Loss: 0.00031174229602631945\n",
            "Epoch 384/1000, Training Loss: 0.00031097445657748057, Test Loss: 0.0003057130328942833\n",
            "Epoch 385/1000, Training Loss: 0.0003045844589487687, Test Loss: 0.00029982289046033285\n",
            "Epoch 386/1000, Training Loss: 0.0002983621680055533, Test Loss: 0.00029406904806932065\n",
            "Epoch 387/1000, Training Loss: 0.00029230309400494515, Test Loss: 0.0002884487054292373\n",
            "Epoch 388/1000, Training Loss: 0.0002864028466569421, Test Loss: 0.00028295908552924634\n",
            "Epoch 389/1000, Training Loss: 0.0002806571352822802, Test Loss: 0.0002775974372841506\n",
            "Epoch 390/1000, Training Loss: 0.0002750617687048091, Test Loss: 0.0002723610379209675\n",
            "Epoch 391/1000, Training Loss: 0.00026961265490532156, Test Loss: 0.00026724719512286385\n",
            "Epoch 392/1000, Training Loss: 0.0002643058004618349, Test Loss: 0.00026225324894514005\n",
            "Epoch 393/1000, Training Loss: 0.00025913730979956333, Test Loss: 0.0002573765735175374\n",
            "Epoch 394/1000, Training Loss: 0.0002541033842719839, Test Loss: 0.0002526145785464971\n",
            "Epoch 395/1000, Training Loss: 0.00024920032109283406, Test Loss: 0.00024796471063053585\n",
            "Epoch 396/1000, Training Loss: 0.00024442451213725516, Test Loss: 0.00024342445440124862\n",
            "Epoch 397/1000, Training Loss: 0.00023977244262888935, Test Loss: 0.00023899133350194877\n",
            "Epoch 398/1000, Training Loss: 0.00023524068972828832, Test Loss: 0.00023466291141535048\n",
            "Epoch 399/1000, Training Loss: 0.00023082592103676496, Test Loss: 0.00023043679215115745\n",
            "Epoch 400/1000, Training Loss: 0.000226524893028555, Test Loss: 0.00022631062080382818\n",
            "Epoch 401/1000, Training Loss: 0.0002223344494230603, Test Loss: 0.00022228208399029052\n",
            "Epoch 402/1000, Training Loss: 0.00021825151950786835, Test Loss: 0.000218348910176833\n",
            "Epoch 403/1000, Training Loss: 0.00021427311642228505, Test Loss: 0.0002145088699038771\n",
            "Epoch 404/1000, Training Loss: 0.00021039633541017954, Test Loss: 0.0002107597759168257\n",
            "Epoch 405/1000, Training Loss: 0.0002066183520501301, Test Loss: 0.0002070994832107443\n",
            "Epoch 406/1000, Training Loss: 0.00020293642047004205, Test Loss: 0.00020352588899612647\n",
            "Epoch 407/1000, Training Loss: 0.00019934787155273414, Test Loss: 0.00020003693259256178\n",
            "Epoch 408/1000, Training Loss: 0.0001958501111382649, Test Loss: 0.00019663059525672853\n",
            "Epoch 409/1000, Training Loss: 0.00019244061822822555, Test Loss: 0.00019330489995066736\n",
            "Epoch 410/1000, Training Loss: 0.0001891169431966098, Test Loss: 0.00019005791105598848\n",
            "Epoch 411/1000, Training Loss: 0.00018587670601136956, Test Loss: 0.00018688773403920336\n",
            "Epoch 412/1000, Training Loss: 0.00018271759447030427, Test Loss: 0.00018379251507309895\n",
            "Epoch 413/1000, Training Loss: 0.00017963736245448162, Test Loss: 0.00018077044061870013\n",
            "Epoch 414/1000, Training Loss: 0.00017663382820199, Test Loss: 0.00017781973697205447\n",
            "Epoch 415/1000, Training Loss: 0.00017370487260447737, Test Loss: 0.0001749386697797971\n",
            "Epoch 416/1000, Training Loss: 0.0001708484375285684, Test Loss: 0.00017212554352715581\n",
            "Epoch 417/1000, Training Loss: 0.00016806252416398453, Test Loss: 0.00016937870100177023\n",
            "Epoch 418/1000, Training Loss: 0.00016534519139990315, Test Loss: 0.00016669652273651115\n",
            "Epoch 419/1000, Training Loss: 0.0001626945542308395, Test Loss: 0.00016407742643420264\n",
            "Epoch 420/1000, Training Loss: 0.00016010878219310332, Test Loss: 0.00016151986637690435\n",
            "Epoch 421/1000, Training Loss: 0.00015758609783271204, Test Loss: 0.0001590223328222844\n",
            "Epoch 422/1000, Training Loss: 0.00015512477520541752, Test Loss: 0.0001565833513893572\n",
            "Epoch 423/1000, Training Loss: 0.00015272313840936145, Test Loss: 0.000154201482435675\n",
            "Epoch 424/1000, Training Loss: 0.0001503795601507317, Test Loss: 0.00015187532042794307\n",
            "Epoch 425/1000, Training Loss: 0.00014809246034262356, Test Loss: 0.0001496034933078008\n",
            "Epoch 426/1000, Training Loss: 0.00014586030473724422, Test Loss: 0.00014738466185441936\n",
            "Epoch 427/1000, Training Loss: 0.00014368160359143906, Test Loss: 0.00014521751904539423\n",
            "Epoch 428/1000, Training Loss: 0.0001415549103654664, Test Loss: 0.0001431007894172865\n",
            "Epoch 429/1000, Training Loss: 0.00013947882045483282, Test Loss: 0.00014103322842705504\n",
            "Epoch 430/1000, Training Loss: 0.0001374519699549499, Test Loss: 0.0001390136218154936\n",
            "Epoch 431/1000, Training Loss: 0.0001354730344582786, Test Loss: 0.0001370407849736832\n",
            "Epoch 432/1000, Training Loss: 0.00013354072788363466, Test Loss: 0.00013511356231341083\n",
            "Epoch 433/1000, Training Loss: 0.00013165380133718465, Test Loss: 0.00013323082664233493\n",
            "Epoch 434/1000, Training Loss: 0.00012981104200469548, Test Loss: 0.00013139147854466994\n",
            "Epoch 435/1000, Training Loss: 0.0001280112720745476, Test Loss: 0.00012959444576804684\n",
            "Epoch 436/1000, Training Loss: 0.00012625334769096413, Test Loss: 0.0001278386826171422\n",
            "Epoch 437/1000, Training Loss: 0.00012453615793691462, Test Loss: 0.0001261231693545962\n",
            "Epoch 438/1000, Training Loss: 0.00012285862384610078, Test Loss: 0.0001244469116096786\n",
            "Epoch 439/1000, Training Loss: 0.00012121969744345845, Test Loss: 0.00012280893979512255\n",
            "Epoch 440/1000, Training Loss: 0.0001196183608135385, Test Loss: 0.00012120830853245174\n",
            "Epoch 441/1000, Training Loss: 0.00011805362519616748, Test Loss: 0.00011964409608612551\n",
            "Epoch 442/1000, Training Loss: 0.00011652453010876312, Test Loss: 0.00011811540380674865\n",
            "Epoch 443/1000, Training Loss: 0.00011503014249466991, Test Loss: 0.00011662135558356503\n",
            "Epoch 444/1000, Training Loss: 0.00011356955589688334, Test Loss: 0.00011516109730640746\n",
            "Epoch 445/1000, Training Loss: 0.00011214188965653472, Test Loss: 0.00011373379633724666\n",
            "Epoch 446/1000, Training Loss: 0.00011074628813551217, Test Loss: 0.00011233864099146574\n",
            "Epoch 447/1000, Training Loss: 0.00010938191996257642, Test Loss: 0.00011097484002891371\n",
            "Epoch 448/1000, Training Loss: 0.00010804797730236136, Test Loss: 0.00010964162215481356\n",
            "Epoch 449/1000, Training Loss: 0.00010674367514664401, Test Loss: 0.0001083382355305515\n",
            "Epoch 450/1000, Training Loss: 0.00010546825062727302, Test Loss: 0.00010706394729435147\n",
            "Epoch 451/1000, Training Loss: 0.0001042209623501446, Test Loss: 0.00010581804309181696\n",
            "Epoch 452/1000, Training Loss: 0.00010300108974965886, Test Loss: 0.0001045998266163293\n",
            "Epoch 453/1000, Training Loss: 0.00010180793246305041, Test Loss: 0.00010340861915923207\n",
            "Epoch 454/1000, Training Loss: 0.00010064080972403637, Test Loss: 0.00010224375916976055\n",
            "Epoch 455/1000, Training Loss: 9.949905977521614e-05, Test Loss: 0.00010110460182462467\n",
            "Epoch 456/1000, Training Loss: 9.83820392986738e-05, Test Loss: 9.999051860717348e-05\n",
            "Epoch 457/1000, Training Loss: 9.728912286424189e-05, Test Loss: 9.89008968960255e-05\n",
            "Epoch 458/1000, Training Loss: 9.621970239490675e-05, Test Loss: 9.783513956308003e-05\n",
            "Epoch 459/1000, Training Loss: 9.517318664883531e-05, Test Loss: 9.679266458077518e-05\n",
            "Epoch 460/1000, Training Loss: 9.41490007175243e-05, Test Loss: 9.577290463848073e-05\n",
            "Epoch 461/1000, Training Loss: 9.314658553957338e-05, Test Loss: 9.477530676788994e-05\n",
            "Epoch 462/1000, Training Loss: 9.216539742962129e-05, Test Loss: 9.379933197728571e-05\n",
            "Epoch 463/1000, Training Loss: 9.120490762195804e-05, Test Loss: 9.284445489452866e-05\n",
            "Epoch 464/1000, Training Loss: 9.026460182837436e-05, Test Loss: 9.191016341863569e-05\n",
            "Epoch 465/1000, Training Loss: 8.934397980979905e-05, Test Loss: 9.099595837979282e-05\n",
            "Epoch 466/1000, Training Loss: 8.844255496129598e-05, Test Loss: 9.010135320765976e-05\n",
            "Epoch 467/1000, Training Loss: 8.755985391000512e-05, Test Loss: 8.922587360781253e-05\n",
            "Epoch 468/1000, Training Loss: 8.669541612561713e-05, Test Loss: 8.836905724617043e-05\n",
            "Epoch 469/1000, Training Loss: 8.584879354299197e-05, Test Loss: 8.753045344125233e-05\n",
            "Epoch 470/1000, Training Loss: 8.501955019652627e-05, Test Loss: 8.670962286410674e-05\n",
            "Epoch 471/1000, Training Loss: 8.420726186591586e-05, Test Loss: 8.590613724576423e-05\n",
            "Epoch 472/1000, Training Loss: 8.341151573293142e-05, Test Loss: 8.511957909204988e-05\n",
            "Epoch 473/1000, Training Loss: 8.263191004887305e-05, Test Loss: 8.434954140560759e-05\n",
            "Epoch 474/1000, Training Loss: 8.186805381235413e-05, Test Loss: 8.359562741497535e-05\n",
            "Epoch 475/1000, Training Loss: 8.111956645708775e-05, Test Loss: 8.285745031056227e-05\n",
            "Epoch 476/1000, Training Loss: 8.038607754935385e-05, Test Loss: 8.213463298737199e-05\n",
            "Epoch 477/1000, Training Loss: 7.966722649483962e-05, Test Loss: 8.142680779431813e-05\n",
            "Epoch 478/1000, Training Loss: 7.896266225455365e-05, Test Loss: 8.073361628999003e-05\n",
            "Epoch 479/1000, Training Loss: 7.827204306951482e-05, Test Loss: 8.005470900471245e-05\n",
            "Epoch 480/1000, Training Loss: 7.759503619394597e-05, Test Loss: 7.93897452087562e-05\n",
            "Epoch 481/1000, Training Loss: 7.693131763668844e-05, Test Loss: 7.873839268655125e-05\n",
            "Epoch 482/1000, Training Loss: 7.62805719105781e-05, Test Loss: 7.810032751676436e-05\n",
            "Epoch 483/1000, Training Loss: 7.56424917895306e-05, Test Loss: 7.747523385809947e-05\n",
            "Epoch 484/1000, Training Loss: 7.501677807307582e-05, Test Loss: 7.686280374067685e-05\n",
            "Epoch 485/1000, Training Loss: 7.440313935811955e-05, Test Loss: 7.626273686286507e-05\n",
            "Epoch 486/1000, Training Loss: 7.380129181767837e-05, Test Loss: 7.567474039342048e-05\n",
            "Epoch 487/1000, Training Loss: 7.321095898638463e-05, Test Loss: 7.50985287788086e-05\n",
            "Epoch 488/1000, Training Loss: 7.263187155252964e-05, Test Loss: 7.453382355558141e-05\n",
            "Epoch 489/1000, Training Loss: 7.206376715643765e-05, Test Loss: 7.398035316767348e-05\n",
            "Epoch 490/1000, Training Loss: 7.1506390194972e-05, Test Loss: 7.343785278850592e-05\n",
            "Epoch 491/1000, Training Loss: 7.095949163197525e-05, Test Loss: 7.290606414776677e-05\n",
            "Epoch 492/1000, Training Loss: 7.042282881444215e-05, Test Loss: 7.238473536274642e-05\n",
            "Epoch 493/1000, Training Loss: 6.989616529425917e-05, Test Loss: 7.187362077412459e-05\n",
            "Epoch 494/1000, Training Loss: 6.937927065531901e-05, Test Loss: 7.137248078607956e-05\n",
            "Epoch 495/1000, Training Loss: 6.887192034584217e-05, Test Loss: 7.08810817106192e-05\n",
            "Epoch 496/1000, Training Loss: 6.837389551573977e-05, Test Loss: 7.039919561601442e-05\n",
            "Epoch 497/1000, Training Loss: 6.788498285885529e-05, Test Loss: 6.992660017923573e-05\n",
            "Epoch 498/1000, Training Loss: 6.740497445992929e-05, Test Loss: 6.946307854228706e-05\n",
            "Epoch 499/1000, Training Loss: 6.693366764613825e-05, Test Loss: 6.900841917233065e-05\n",
            "Epoch 500/1000, Training Loss: 6.647086484305664e-05, Test Loss: 6.856241572550845e-05\n",
            "Epoch 501/1000, Training Loss: 6.601637343491096e-05, Test Loss: 6.812486691435837e-05\n",
            "Epoch 502/1000, Training Loss: 6.557000562897772e-05, Test Loss: 6.769557637873289e-05\n",
            "Epoch 503/1000, Training Loss: 6.513157832400591e-05, Test Loss: 6.7274352560129e-05\n",
            "Epoch 504/1000, Training Loss: 6.470091298252285e-05, Test Loss: 6.686100857933323e-05\n",
            "Epoch 505/1000, Training Loss: 6.427783550691372e-05, Test Loss: 6.645536211729692e-05\n",
            "Epoch 506/1000, Training Loss: 6.386217611914843e-05, Test Loss: 6.605723529915956e-05\n",
            "Epoch 507/1000, Training Loss: 6.345376924404126e-05, Test Loss: 6.566645458132854e-05\n",
            "Epoch 508/1000, Training Loss: 6.305245339593098e-05, Test Loss: 6.52828506415393e-05\n",
            "Epoch 509/1000, Training Loss: 6.265807106867901e-05, Test Loss: 6.490625827181353e-05\n",
            "Epoch 510/1000, Training Loss: 6.227046862887567e-05, Test Loss: 6.453651627423538e-05\n",
            "Epoch 511/1000, Training Loss: 6.188949621215747e-05, Test Loss: 6.417346735948227e-05\n",
            "Epoch 512/1000, Training Loss: 6.151500762253575e-05, Test Loss: 6.381695804801344e-05\n",
            "Epoch 513/1000, Training Loss: 6.11468602346448e-05, Test Loss: 6.346683857386885e-05\n",
            "Epoch 514/1000, Training Loss: 6.078491489881594e-05, Test Loss: 6.312296279099792e-05\n",
            "Epoch 515/1000, Training Loss: 6.042903584889282e-05, Test Loss: 6.278518808203697e-05\n",
            "Epoch 516/1000, Training Loss: 6.0079090612699966e-05, Test Loss: 6.245337526950086e-05\n",
            "Epoch 517/1000, Training Loss: 5.9734949925083385e-05, Test Loss: 6.212738852929036e-05\n",
            "Epoch 518/1000, Training Loss: 5.939648764344222e-05, Test Loss: 6.180709530647083e-05\n",
            "Epoch 519/1000, Training Loss: 5.906358066567779e-05, Test Loss: 6.14923662332643e-05\n",
            "Epoch 520/1000, Training Loss: 5.8736108850483655e-05, Test Loss: 6.118307504917866e-05\n",
            "Epoch 521/1000, Training Loss: 5.841395493990091e-05, Test Loss: 6.087909852322987e-05\n",
            "Epoch 522/1000, Training Loss: 5.8097004484078144e-05, Test Loss: 6.058031637820002e-05\n",
            "Epoch 523/1000, Training Loss: 5.77851457681573e-05, Test Loss: 6.028661121686797e-05\n",
            "Epoch 524/1000, Training Loss: 5.747826974123066e-05, Test Loss: 5.9997868450165664e-05\n",
            "Epoch 525/1000, Training Loss: 5.71762699472988e-05, Test Loss: 5.9713976227209065e-05\n",
            "Epoch 526/1000, Training Loss: 5.687904245817147e-05, Test Loss: 5.943482536714725e-05\n",
            "Epoch 527/1000, Training Loss: 5.6586485808252984e-05, Test Loss: 5.916030929278609e-05\n",
            "Epoch 528/1000, Training Loss: 5.629850093115101e-05, Test Loss: 5.88903239659345e-05\n",
            "Epoch 529/1000, Training Loss: 5.6014991098059534e-05, Test Loss: 5.862476782443327e-05\n",
            "Epoch 530/1000, Training Loss: 5.5735861857856744e-05, Test Loss: 5.8363541720810615e-05\n",
            "Epoch 531/1000, Training Loss: 5.5461020978872675e-05, Test Loss: 5.810654886253359e-05\n",
            "Epoch 532/1000, Training Loss: 5.5190378392264725e-05, Test Loss: 5.785369475379839e-05\n",
            "Epoch 533/1000, Training Loss: 5.492384613696994e-05, Test Loss: 5.760488713882549e-05\n",
            "Epoch 534/1000, Training Loss: 5.466133830616853e-05, Test Loss: 5.736003594662791e-05\n",
            "Epoch 535/1000, Training Loss: 5.4402770995227804e-05, Test Loss: 5.7119053237190555e-05\n",
            "Epoch 536/1000, Training Loss: 5.414806225107606e-05, Test Loss: 5.68818531490475e-05\n",
            "Epoch 537/1000, Training Loss: 5.389713202296279e-05, Test Loss: 5.6648351848198496e-05\n",
            "Epoch 538/1000, Training Loss: 5.364990211456897e-05, Test Loss: 5.641846747834608e-05\n",
            "Epoch 539/1000, Training Loss: 5.340629613741895e-05, Test Loss: 5.61921201123999e-05\n",
            "Epoch 540/1000, Training Loss: 5.316623946556741e-05, Test Loss: 5.5969231705242496e-05\n",
            "Epoch 541/1000, Training Loss: 5.292965919151433e-05, Test Loss: 5.574972604767947e-05\n",
            "Epoch 542/1000, Training Loss: 5.269648408331332e-05, Test Loss: 5.5533528721586286e-05\n",
            "Epoch 543/1000, Training Loss: 5.2466644542841905e-05, Test Loss: 5.532056705620121e-05\n",
            "Epoch 544/1000, Training Loss: 5.224007256519963e-05, Test Loss: 5.51107700855166e-05\n",
            "Epoch 545/1000, Training Loss: 5.201670169919258e-05, Test Loss: 5.490406850677415e-05\n",
            "Epoch 546/1000, Training Loss: 5.1796467008886346e-05, Test Loss: 5.4700394640005694e-05\n",
            "Epoch 547/1000, Training Loss: 5.157930503618465e-05, Test Loss: 5.4499682388610503e-05\n",
            "Epoch 548/1000, Training Loss: 5.13651537644105e-05, Test Loss: 5.430186720092125e-05\n",
            "Epoch 549/1000, Training Loss: 5.115395258285714e-05, Test Loss: 5.4106886032761185e-05\n",
            "Epoch 550/1000, Training Loss: 5.094564225228274e-05, Test Loss: 5.391467731093185e-05\n",
            "Epoch 551/1000, Training Loss: 5.0740164871322966e-05, Test Loss: 5.372518089763907e-05\n",
            "Epoch 552/1000, Training Loss: 5.053746384379066e-05, Test Loss: 5.3538338055807436e-05\n",
            "Epoch 553/1000, Training Loss: 5.033748384684306e-05, Test Loss: 5.335409141527803e-05\n",
            "Epoch 554/1000, Training Loss: 5.014017079998463e-05, Test Loss: 5.31723849398487e-05\n",
            "Epoch 555/1000, Training Loss: 4.9945471834887456e-05, Test Loss: 5.299316389515604e-05\n",
            "Epoch 556/1000, Training Loss: 4.975333526600569e-05, Test Loss: 5.2816374817350406e-05\n",
            "Epoch 557/1000, Training Loss: 4.956371056195702e-05, Test Loss: 5.264196548256491e-05\n",
            "Epoch 558/1000, Training Loss: 4.937654831765266e-05, Test Loss: 5.2469884877156034e-05\n",
            "Epoch 559/1000, Training Loss: 4.919180022715699e-05, Test Loss: 5.230008316867511e-05\n",
            "Epoch 560/1000, Training Loss: 4.900941905724857e-05, Test Loss: 5.21325116775777e-05\n",
            "Epoch 561/1000, Training Loss: 4.8829358621670984e-05, Test Loss: 5.196712284963196e-05\n",
            "Epoch 562/1000, Training Loss: 4.8651573756052e-05, Test Loss: 5.180387022902111e-05\n",
            "Epoch 563/1000, Training Loss: 4.8476020293466236e-05, Test Loss: 5.164270843211148e-05\n",
            "Epoch 564/1000, Training Loss: 4.830265504063321e-05, Test Loss: 5.1483593121875705e-05\n",
            "Epoch 565/1000, Training Loss: 4.8131435754725696e-05, Test Loss: 5.132648098294736e-05\n",
            "Epoch 566/1000, Training Loss: 4.796232112077293e-05, Test Loss: 5.117132969730676e-05\n",
            "Epoch 567/1000, Training Loss: 4.77952707296454e-05, Test Loss: 5.101809792055183e-05\n",
            "Epoch 568/1000, Training Loss: 4.763024505660067e-05, Test Loss: 5.086674525876695e-05\n",
            "Epoch 569/1000, Training Loss: 4.746720544037643e-05, Test Loss: 5.0717232245966256e-05\n",
            "Epoch 570/1000, Training Loss: 4.7306114062818704e-05, Test Loss: 5.056952032208483e-05\n",
            "Epoch 571/1000, Training Loss: 4.7146933929024345e-05, Test Loss: 5.042357181152096e-05\n",
            "Epoch 572/1000, Training Loss: 4.6989628847988974e-05, Test Loss: 5.0279349902201374e-05\n",
            "Epoch 573/1000, Training Loss: 4.6834163413744185e-05, Test Loss: 5.0136818625164607e-05\n",
            "Epoch 574/1000, Training Loss: 4.668050298697145e-05, Test Loss: 4.9995942834643507e-05\n",
            "Epoch 575/1000, Training Loss: 4.652861367707841e-05, Test Loss: 4.9856688188641305e-05\n",
            "Epoch 576/1000, Training Loss: 4.6378462324726255e-05, Test Loss: 4.9719021129976705e-05\n",
            "Epoch 577/1000, Training Loss: 4.623001648479815e-05, Test Loss: 4.958290886779752e-05\n",
            "Epoch 578/1000, Training Loss: 4.60832444097878e-05, Test Loss: 4.944831935955181e-05\n",
            "Epoch 579/1000, Training Loss: 4.593811503361075e-05, Test Loss: 4.931522129337996e-05\n",
            "Epoch 580/1000, Training Loss: 4.579459795581368e-05, Test Loss: 4.918358407096295e-05\n",
            "Epoch 581/1000, Training Loss: 4.565266342618064e-05, Test Loss: 4.9053377790765474e-05\n",
            "Epoch 582/1000, Training Loss: 4.5512282329718924e-05, Test Loss: 4.8924573231701054e-05\n",
            "Epoch 583/1000, Training Loss: 4.537342617201963e-05, Test Loss: 4.8797141837186e-05\n",
            "Epoch 584/1000, Training Loss: 4.523606706497673e-05, Test Loss: 4.8671055699587705e-05\n",
            "Epoch 585/1000, Training Loss: 4.510017771286416e-05, Test Loss: 4.854628754503978e-05\n",
            "Epoch 586/1000, Training Loss: 4.496573139874984e-05, Test Loss: 4.842281071863589e-05\n",
            "Epoch 587/1000, Training Loss: 4.483270197125062e-05, Test Loss: 4.8300599169975674e-05\n",
            "Epoch 588/1000, Training Loss: 4.4701063831605675e-05, Test Loss: 4.8179627439060656e-05\n",
            "Epoch 589/1000, Training Loss: 4.457079192107176e-05, Test Loss: 4.805987064252944e-05\n",
            "Epoch 590/1000, Training Loss: 4.444186170862296e-05, Test Loss: 4.794130446023037e-05\n",
            "Epoch 591/1000, Training Loss: 4.4314249178954776e-05, Test Loss: 4.782390512210764e-05\n",
            "Epoch 592/1000, Training Loss: 4.418793082077538e-05, Test Loss: 4.7707649395413944e-05\n",
            "Epoch 593/1000, Training Loss: 4.406288361538525e-05, Test Loss: 4.759251457221772e-05\n",
            "Epoch 594/1000, Training Loss: 4.3939085025534086e-05, Test Loss: 4.747847845722353e-05\n",
            "Epoch 595/1000, Training Loss: 4.3816512984543585e-05, Test Loss: 4.736551935587619e-05\n",
            "Epoch 596/1000, Training Loss: 4.369514588569867e-05, Test Loss: 4.7253616062750764e-05\n",
            "Epoch 597/1000, Training Loss: 4.357496257189146e-05, Test Loss: 4.714274785021731e-05\n",
            "Epoch 598/1000, Training Loss: 4.345594232551365e-05, Test Loss: 4.7032894457389896e-05\n",
            "Epoch 599/1000, Training Loss: 4.333806485859489e-05, Test Loss: 4.6924036079316184e-05\n",
            "Epoch 600/1000, Training Loss: 4.3221310303176625e-05, Test Loss: 4.6816153356449623e-05\n",
            "Epoch 601/1000, Training Loss: 4.310565920191577e-05, Test Loss: 4.670922736435376e-05\n",
            "Epoch 602/1000, Training Loss: 4.299109249891327e-05, Test Loss: 4.660323960365893e-05\n",
            "Epoch 603/1000, Training Loss: 4.28775915307629e-05, Test Loss: 4.649817199025887e-05\n",
            "Epoch 604/1000, Training Loss: 4.276513801781131e-05, Test Loss: 4.6394006845737495e-05\n",
            "Epoch 605/1000, Training Loss: 4.265371405562724e-05, Test Loss: 4.629072688802121e-05\n",
            "Epoch 606/1000, Training Loss: 4.2543302106673434e-05, Test Loss: 4.618831522225727e-05\n",
            "Epoch 607/1000, Training Loss: 4.243388499217529e-05, Test Loss: 4.608675533190314e-05\n",
            "Epoch 608/1000, Training Loss: 4.232544588418118e-05, Test Loss: 4.598603107003005e-05\n",
            "Epoch 609/1000, Training Loss: 4.221796829781117e-05, Test Loss: 4.588612665082944e-05\n",
            "Epoch 610/1000, Training Loss: 4.2111436083690294e-05, Test Loss: 4.5787026641319685e-05\n",
            "Epoch 611/1000, Training Loss: 4.200583342055554e-05, Test Loss: 4.568871595325438e-05\n",
            "Epoch 612/1000, Training Loss: 4.190114480804094e-05, Test Loss: 4.559117983520794e-05\n",
            "Epoch 613/1000, Training Loss: 4.1797355059627966e-05, Test Loss: 4.5494403864864235e-05\n",
            "Epoch 614/1000, Training Loss: 4.16944492957649e-05, Test Loss: 4.539837394146841e-05\n",
            "Epoch 615/1000, Training Loss: 4.1592412937144044e-05, Test Loss: 4.5303076278473605e-05\n",
            "Epoch 616/1000, Training Loss: 4.149123169813584e-05, Test Loss: 4.520849739634726e-05\n",
            "Epoch 617/1000, Training Loss: 4.139089158037956e-05, Test Loss: 4.51146241155485e-05\n",
            "Epoch 618/1000, Training Loss: 4.129137886651828e-05, Test Loss: 4.502144354967853e-05\n",
            "Epoch 619/1000, Training Loss: 4.119268011408173e-05, Test Loss: 4.4928943098777134e-05\n",
            "Epoch 620/1000, Training Loss: 4.109478214951228e-05, Test Loss: 4.483711044278637e-05\n",
            "Epoch 621/1000, Training Loss: 4.0997672062325496e-05, Test Loss: 4.474593353516368e-05\n",
            "Epoch 622/1000, Training Loss: 4.090133719940884e-05, Test Loss: 4.465540059664346e-05\n",
            "Epoch 623/1000, Training Loss: 4.0805765159451105e-05, Test Loss: 4.456550010914483e-05\n",
            "Epoch 624/1000, Training Loss: 4.071094378749877e-05, Test Loss: 4.447622080981895e-05\n",
            "Epoch 625/1000, Training Loss: 4.061686116964022e-05, Test Loss: 4.4387551685238156e-05\n",
            "Epoch 626/1000, Training Loss: 4.05235056278088e-05, Test Loss: 4.429948196571897e-05\n",
            "Epoch 627/1000, Training Loss: 4.043086571470778e-05, Test Loss: 4.4212001119775635e-05\n",
            "Epoch 628/1000, Training Loss: 4.0338930208849325e-05, Test Loss: 4.4125098848705075e-05\n",
            "Epoch 629/1000, Training Loss: 4.024768810970901e-05, Test Loss: 4.403876508129151e-05\n",
            "Epoch 630/1000, Training Loss: 4.01571286329889e-05, Test Loss: 4.3952989968642234e-05\n",
            "Epoch 631/1000, Training Loss: 4.006724120598893e-05, Test Loss: 4.3867763879134804e-05\n",
            "Epoch 632/1000, Training Loss: 3.9978015463085614e-05, Test Loss: 4.378307739348963e-05\n",
            "Epoch 633/1000, Training Loss: 3.988944124131049e-05, Test Loss: 4.3698921299946144e-05\n",
            "Epoch 634/1000, Training Loss: 3.9801508576030716e-05, Test Loss: 4.3615286589559615e-05\n",
            "Epoch 635/1000, Training Loss: 3.971420769672659e-05, Test Loss: 4.353216445160033e-05\n",
            "Epoch 636/1000, Training Loss: 3.962752902286517e-05, Test Loss: 4.344954626906117e-05\n",
            "Epoch 637/1000, Training Loss: 3.954146315986705e-05, Test Loss: 4.3367423614271936e-05\n",
            "Epoch 638/1000, Training Loss: 3.9456000895164086e-05, Test Loss: 4.328578824460752e-05\n",
            "Epoch 639/1000, Training Loss: 3.93711331943447e-05, Test Loss: 4.3204632098298374e-05\n",
            "Epoch 640/1000, Training Loss: 3.9286851197387216e-05, Test Loss: 4.312394729034176e-05\n",
            "Epoch 641/1000, Training Loss: 3.9203146214977474e-05, Test Loss: 4.3043726108498836e-05\n",
            "Epoch 642/1000, Training Loss: 3.912000972490898e-05, Test Loss: 4.296396100938806e-05\n",
            "Epoch 643/1000, Training Loss: 3.90374333685621e-05, Test Loss: 4.288464461467015e-05\n",
            "Epoch 644/1000, Training Loss: 3.8955408947465486e-05, Test Loss: 4.280576970731207e-05\n",
            "Epoch 645/1000, Training Loss: 3.887392841993077e-05, Test Loss: 4.272732922794901e-05\n",
            "Epoch 646/1000, Training Loss: 3.8792983897763105e-05, Test Loss: 4.2649316271320715e-05\n",
            "Epoch 647/1000, Training Loss: 3.871256764304671e-05, Test Loss: 4.2571724082788977e-05\n",
            "Epoch 648/1000, Training Loss: 3.8632672064999625e-05, Test Loss: 4.2494546054942945e-05\n",
            "Epoch 649/1000, Training Loss: 3.855328971689969e-05, Test Loss: 4.241777572426936e-05\n",
            "Epoch 650/1000, Training Loss: 3.847441329307738e-05, Test Loss: 4.234140676791334e-05\n",
            "Epoch 651/1000, Training Loss: 3.83960356259768e-05, Test Loss: 4.2265433000498815e-05\n",
            "Epoch 652/1000, Training Loss: 3.8318149683281125e-05, Test Loss: 4.218984837103379e-05\n",
            "Epoch 653/1000, Training Loss: 3.824074856509991e-05, Test Loss: 4.211464695988036e-05\n",
            "Epoch 654/1000, Training Loss: 3.816382550122161e-05, Test Loss: 4.2039822975786946e-05\n",
            "Epoch 655/1000, Training Loss: 3.8087373848422295e-05, Test Loss: 4.196537075300529e-05\n",
            "Epoch 656/1000, Training Loss: 3.8011387087838493e-05, Test Loss: 4.189128474845387e-05\n",
            "Epoch 657/1000, Training Loss: 3.7935858822393113e-05, Test Loss: 4.181755953896373e-05\n",
            "Epoch 658/1000, Training Loss: 3.786078277428172e-05, Test Loss: 4.174418981856901e-05\n",
            "Epoch 659/1000, Training Loss: 3.7786152782510534e-05, Test Loss: 4.1671170395874e-05\n",
            "Epoch 660/1000, Training Loss: 3.771196280049041e-05, Test Loss: 4.159849619147389e-05\n",
            "Epoch 661/1000, Training Loss: 3.763820689368362e-05, Test Loss: 4.152616223542827e-05\n",
            "Epoch 662/1000, Training Loss: 3.7564879237299776e-05, Test Loss: 4.1454163664805e-05\n",
            "Epoch 663/1000, Training Loss: 3.7491974114043156e-05, Test Loss: 4.138249572127135e-05\n",
            "Epoch 664/1000, Training Loss: 3.741948591191114e-05, Test Loss: 4.1311153748733754e-05\n",
            "Epoch 665/1000, Training Loss: 3.734740912203686e-05, Test Loss: 4.1240133191048066e-05\n",
            "Epoch 666/1000, Training Loss: 3.727573833658113e-05, Test Loss: 4.116942958976395e-05\n",
            "Epoch 667/1000, Training Loss: 3.7204468246669064e-05, Test Loss: 4.10990385819323e-05\n",
            "Epoch 668/1000, Training Loss: 3.713359364037174e-05, Test Loss: 4.102895589795737e-05\n",
            "Epoch 669/1000, Training Loss: 3.70631094007317e-05, Test Loss: 4.0959177359497476e-05\n",
            "Epoch 670/1000, Training Loss: 3.699301050383065e-05, Test Loss: 4.0889698877411855e-05\n",
            "Epoch 671/1000, Training Loss: 3.692329201689972e-05, Test Loss: 4.0820516449759286e-05\n",
            "Epoch 672/1000, Training Loss: 3.685394909646802e-05, Test Loss: 4.075162615983732e-05\n",
            "Epoch 673/1000, Training Loss: 3.67849769865549e-05, Test Loss: 4.068302417426334e-05\n",
            "Epoch 674/1000, Training Loss: 3.6716371016898e-05, Test Loss: 4.061470674110691e-05\n",
            "Epoch 675/1000, Training Loss: 3.6648126601220445e-05, Test Loss: 4.0546670188058594e-05\n",
            "Epoch 676/1000, Training Loss: 3.658023923553527e-05, Test Loss: 4.047891092063569e-05\n",
            "Epoch 677/1000, Training Loss: 3.651270449648525e-05, Test Loss: 4.0411425420445924e-05\n",
            "Epoch 678/1000, Training Loss: 3.64455180397212e-05, Test Loss: 4.034421024346349e-05\n",
            "Epoch 679/1000, Training Loss: 3.6378675598309294e-05, Test Loss: 4.027726201836993e-05\n",
            "Epoch 680/1000, Training Loss: 3.631217298117994e-05, Test Loss: 4.021057744491332e-05\n",
            "Epoch 681/1000, Training Loss: 3.6246006071601235e-05, Test Loss: 4.014415329231927e-05\n",
            "Epoch 682/1000, Training Loss: 3.61801708256932e-05, Test Loss: 4.007798639772193e-05\n",
            "Epoch 683/1000, Training Loss: 3.611466327096766e-05, Test Loss: 4.00120736646458e-05\n",
            "Epoch 684/1000, Training Loss: 3.60494795049021e-05, Test Loss: 3.9946412061511574e-05\n",
            "Epoch 685/1000, Training Loss: 3.598461569354223e-05, Test Loss: 3.988099862017804e-05\n",
            "Epoch 686/1000, Training Loss: 3.5920068070135786e-05, Test Loss: 3.9815830434519053e-05\n",
            "Epoch 687/1000, Training Loss: 3.585583293379318e-05, Test Loss: 3.975090465903181e-05\n",
            "Epoch 688/1000, Training Loss: 3.579190664817945e-05, Test Loss: 3.968621850746698e-05\n",
            "Epoch 689/1000, Training Loss: 3.572828564023004e-05, Test Loss: 3.9621769251510126e-05\n",
            "Epoch 690/1000, Training Loss: 3.566496639889732e-05, Test Loss: 3.955755421947222e-05\n",
            "Epoch 691/1000, Training Loss: 3.560194547392148e-05, Test Loss: 3.949357079502144e-05\n",
            "Epoch 692/1000, Training Loss: 3.553921947462768e-05, Test Loss: 3.942981641593509e-05\n",
            "Epoch 693/1000, Training Loss: 3.547678506874844e-05, Test Loss: 3.936628857289054e-05\n",
            "Epoch 694/1000, Training Loss: 3.541463898127156e-05, Test Loss: 3.930298480827199e-05\n",
            "Epoch 695/1000, Training Loss: 3.535277799331073e-05, Test Loss: 3.9239902715011616e-05\n",
            "Epoch 696/1000, Training Loss: 3.5291198941001854e-05, Test Loss: 3.9177039935451235e-05\n",
            "Epoch 697/1000, Training Loss: 3.52298987144205e-05, Test Loss: 3.911439416023478e-05\n",
            "Epoch 698/1000, Training Loss: 3.516887425652327e-05, Test Loss: 3.9051963127222563e-05\n",
            "Epoch 699/1000, Training Loss: 3.510812256211077e-05, Test Loss: 3.898974462042945e-05\n",
            "Epoch 700/1000, Training Loss: 3.504764067681139e-05, Test Loss: 3.8927736468991824e-05\n",
            "Epoch 701/1000, Training Loss: 3.4987425696090475e-05, Test Loss: 3.886593654614068e-05\n",
            "Epoch 702/1000, Training Loss: 3.492747476427261e-05, Test Loss: 3.880434276822891e-05\n",
            "Epoch 703/1000, Training Loss: 3.4867785073591616e-05, Test Loss: 3.874295309374908e-05\n",
            "Epoch 704/1000, Training Loss: 3.480835386325565e-05, Test Loss: 3.8681765522390444e-05\n",
            "Epoch 705/1000, Training Loss: 3.474917841853542e-05, Test Loss: 3.862077809411108e-05\n",
            "Epoch 706/1000, Training Loss: 3.469025606986587e-05, Test Loss: 3.855998888823727e-05\n",
            "Epoch 707/1000, Training Loss: 3.4631584191973924e-05, Test Loss: 3.8499396022571126e-05\n",
            "Epoch 708/1000, Training Loss: 3.4573160203016024e-05, Test Loss: 3.84389976525308e-05\n",
            "Epoch 709/1000, Training Loss: 3.451498156374187e-05, Test Loss: 3.837879197030315e-05\n",
            "Epoch 710/1000, Training Loss: 3.445704577666746e-05, Test Loss: 3.831877720401501e-05\n",
            "Epoch 711/1000, Training Loss: 3.439935038527194e-05, Test Loss: 3.825895161692554e-05\n",
            "Epoch 712/1000, Training Loss: 3.4341892973206564e-05, Test Loss: 3.8199313506638765e-05\n",
            "Epoch 713/1000, Training Loss: 3.4284671163521995e-05, Test Loss: 3.8139861204326746e-05\n",
            "Epoch 714/1000, Training Loss: 3.422768261791119e-05, Test Loss: 3.8080593073974643e-05\n",
            "Epoch 715/1000, Training Loss: 3.4170925035966717e-05, Test Loss: 3.8021507511646295e-05\n",
            "Epoch 716/1000, Training Loss: 3.4114396154456176e-05, Test Loss: 3.796260294475428e-05\n",
            "Epoch 717/1000, Training Loss: 3.405809374660805e-05, Test Loss: 3.7903877831365425e-05\n",
            "Epoch 718/1000, Training Loss: 3.400201562141771e-05, Test Loss: 3.784533065949891e-05\n",
            "Epoch 719/1000, Training Loss: 3.394615962296164e-05, Test Loss: 3.778695994645921e-05\n",
            "Epoch 720/1000, Training Loss: 3.3890523629731124e-05, Test Loss: 3.772876423817624e-05\n",
            "Epoch 721/1000, Training Loss: 3.383510555397584e-05, Test Loss: 3.7670742108554875e-05\n",
            "Epoch 722/1000, Training Loss: 3.377990334106239e-05, Test Loss: 3.761289215885152e-05\n",
            "Epoch 723/1000, Training Loss: 3.3724914968846576e-05, Test Loss: 3.755521301705298e-05\n",
            "Epoch 724/1000, Training Loss: 3.367013844705594e-05, Test Loss: 3.749770333727989e-05\n",
            "Epoch 725/1000, Training Loss: 3.361557181668823e-05, Test Loss: 3.744036179919167e-05\n",
            "Epoch 726/1000, Training Loss: 3.356121314941963e-05, Test Loss: 3.7383187107410374e-05\n",
            "Epoch 727/1000, Training Loss: 3.350706054702516e-05, Test Loss: 3.7326177990965026e-05\n",
            "Epoch 728/1000, Training Loss: 3.345311214081253e-05, Test Loss: 3.7269333202736946e-05\n",
            "Epoch 729/1000, Training Loss: 3.3399366091066035e-05, Test Loss: 3.721265151892002e-05\n",
            "Epoch 730/1000, Training Loss: 3.3345820586501655e-05, Test Loss: 3.715613173849733e-05\n",
            "Epoch 731/1000, Training Loss: 3.329247384373436e-05, Test Loss: 3.709977268272734e-05\n",
            "Epoch 732/1000, Training Loss: 3.3239324106754555e-05, Test Loss: 3.704357319463924e-05\n",
            "Epoch 733/1000, Training Loss: 3.318636964641686e-05, Test Loss: 3.6987532138545905e-05\n",
            "Epoch 734/1000, Training Loss: 3.3133608759938254e-05, Test Loss: 3.693164839955954e-05\n",
            "Epoch 735/1000, Training Loss: 3.308103977040571e-05, Test Loss: 3.687592088312217e-05\n",
            "Epoch 736/1000, Training Loss: 3.3028661026294496e-05, Test Loss: 3.68203485145521e-05\n",
            "Epoch 737/1000, Training Loss: 3.2976470900997304e-05, Test Loss: 3.676493023858573e-05\n",
            "Epoch 738/1000, Training Loss: 3.2924467792360266e-05, Test Loss: 3.670966501895121e-05\n",
            "Epoch 739/1000, Training Loss: 3.287265012223051e-05, Test Loss: 3.6654551837928575e-05\n",
            "Epoch 740/1000, Training Loss: 3.282101633601184e-05, Test Loss: 3.659958969593613e-05\n",
            "Epoch 741/1000, Training Loss: 3.276956490222915e-05, Test Loss: 3.65447776111203e-05\n",
            "Epoch 742/1000, Training Loss: 3.271829431210222e-05, Test Loss: 3.649011461895568e-05\n",
            "Epoch 743/1000, Training Loss: 3.266720307912742e-05, Test Loss: 3.643559977185202e-05\n",
            "Epoch 744/1000, Training Loss: 3.261628973866834e-05, Test Loss: 3.638123213877175e-05\n",
            "Epoch 745/1000, Training Loss: 3.256555284755375e-05, Test Loss: 3.6327010804856406e-05\n",
            "Epoch 746/1000, Training Loss: 3.251499098368417e-05, Test Loss: 3.627293487106266e-05\n",
            "Epoch 747/1000, Training Loss: 3.2464602745647704e-05, Test Loss: 3.621900345380124e-05\n",
            "Epoch 748/1000, Training Loss: 3.241438675233955e-05, Test Loss: 3.616521568459123e-05\n",
            "Epoch 749/1000, Training Loss: 3.23643416425938e-05, Test Loss: 3.611157070971516e-05\n",
            "Epoch 750/1000, Training Loss: 3.231446607481871e-05, Test Loss: 3.605806768988811e-05\n",
            "Epoch 751/1000, Training Loss: 3.2264758726642015e-05, Test Loss: 3.600470579992964e-05\n",
            "Epoch 752/1000, Training Loss: 3.221521829456116e-05, Test Loss: 3.595148422844794e-05\n",
            "Epoch 753/1000, Training Loss: 3.216584349360204e-05, Test Loss: 3.58984021775227e-05\n",
            "Epoch 754/1000, Training Loss: 3.211663305698345e-05, Test Loss: 3.5845458862405355e-05\n",
            "Epoch 755/1000, Training Loss: 3.2067585735789125e-05, Test Loss: 3.579265351121549e-05\n",
            "Epoch 756/1000, Training Loss: 3.2018700298644876e-05, Test Loss: 3.5739985364657115e-05\n",
            "Epoch 757/1000, Training Loss: 3.196997553140336e-05, Test Loss: 3.568745367573264e-05\n",
            "Epoch 758/1000, Training Loss: 3.192141023683494e-05, Test Loss: 3.5635057709458475e-05\n",
            "Epoch 759/1000, Training Loss: 3.187300323432321e-05, Test Loss: 3.5582796742598715e-05\n",
            "Epoch 760/1000, Training Loss: 3.1824753359569544e-05, Test Loss: 3.5530670063398234e-05\n",
            "Epoch 761/1000, Training Loss: 3.17766594642994e-05, Test Loss: 3.54786769713243e-05\n",
            "Epoch 762/1000, Training Loss: 3.172872041597746e-05, Test Loss: 3.54268167768102e-05\n",
            "Epoch 763/1000, Training Loss: 3.1680935097528233e-05, Test Loss: 3.5375088801008856e-05\n",
            "Epoch 764/1000, Training Loss: 3.163330240705924e-05, Test Loss: 3.532349237555173e-05\n",
            "Epoch 765/1000, Training Loss: 3.158582125759432e-05, Test Loss: 3.527202684230718e-05\n",
            "Epoch 766/1000, Training Loss: 3.1538490576807166e-05, Test Loss: 3.5220691553154936e-05\n",
            "Epoch 767/1000, Training Loss: 3.149130930676347e-05, Test Loss: 3.5169485869754594e-05\n",
            "Epoch 768/1000, Training Loss: 3.1444276403666955e-05, Test Loss: 3.5118409163330196e-05\n",
            "Epoch 769/1000, Training Loss: 3.1397390837611316e-05, Test Loss: 3.506746081444793e-05\n",
            "Epoch 770/1000, Training Loss: 3.135065159233413e-05, Test Loss: 3.5016640212809664e-05\n",
            "Epoch 771/1000, Training Loss: 3.1304057664979444e-05, Test Loss: 3.496594675704343e-05\n",
            "Epoch 772/1000, Training Loss: 3.1257608065862015e-05, Test Loss: 3.491537985450344e-05\n",
            "Epoch 773/1000, Training Loss: 3.121130181823768e-05, Test Loss: 3.4864938921074445e-05\n",
            "Epoch 774/1000, Training Loss: 3.116513795807748e-05, Test Loss: 3.481462338097299e-05\n",
            "Epoch 775/1000, Training Loss: 3.111911553384684e-05, Test Loss: 3.476443266656997e-05\n",
            "Epoch 776/1000, Training Loss: 3.107323360628781e-05, Test Loss: 3.471436621819279e-05\n",
            "Epoch 777/1000, Training Loss: 3.102749124820684e-05, Test Loss: 3.46644234839593e-05\n",
            "Epoch 778/1000, Training Loss: 3.098188754426585e-05, Test Loss: 3.461460391959308e-05\n",
            "Epoch 779/1000, Training Loss: 3.0936421590777794e-05, Test Loss: 3.456490698825444e-05\n",
            "Epoch 780/1000, Training Loss: 3.089109249550596e-05, Test Loss: 3.451533216037492e-05\n",
            "Epoch 781/1000, Training Loss: 3.084589937746667e-05, Test Loss: 3.446587891349048e-05\n",
            "Epoch 782/1000, Training Loss: 3.080084136673671e-05, Test Loss: 3.441654673208371e-05\n",
            "Epoch 783/1000, Training Loss: 3.075591760426385e-05, Test Loss: 3.436733510742493e-05\n",
            "Epoch 784/1000, Training Loss: 3.0711127241680955e-05, Test Loss: 3.4318243537421253e-05\n",
            "Epoch 785/1000, Training Loss: 3.066646944112403e-05, Test Loss: 3.426927152646711e-05\n",
            "Epoch 786/1000, Training Loss: 3.062194337505392e-05, Test Loss: 3.422041858529506e-05\n",
            "Epoch 787/1000, Training Loss: 3.0577548226079905e-05, Test Loss: 3.4171684230843e-05\n",
            "Epoch 788/1000, Training Loss: 3.053328318678872e-05, Test Loss: 3.412306798610204e-05\n",
            "Epoch 789/1000, Training Loss: 3.04891474595766e-05, Test Loss: 3.407456937998843e-05\n",
            "Epoch 790/1000, Training Loss: 3.0445140256481746e-05, Test Loss: 3.402618794721088e-05\n",
            "Epoch 791/1000, Training Loss: 3.0401260799024438e-05, Test Loss: 3.3977923228134685e-05\n",
            "Epoch 792/1000, Training Loss: 3.0357508318046296e-05, Test Loss: 3.3929774768663233e-05\n",
            "Epoch 793/1000, Training Loss: 3.0313882053555137e-05, Test Loss: 3.3881742120106046e-05\n",
            "Epoch 794/1000, Training Loss: 3.0270381254571292e-05, Test Loss: 3.3833824839062645e-05\n",
            "Epoch 795/1000, Training Loss: 3.0227005178976602e-05, Test Loss: 3.3786022487305656e-05\n",
            "Epoch 796/1000, Training Loss: 3.018375309336907e-05, Test Loss: 3.373833463165687e-05\n",
            "Epoch 797/1000, Training Loss: 3.0140624272916753e-05, Test Loss: 3.369076084388578e-05\n",
            "Epoch 798/1000, Training Loss: 3.0097618001214913e-05, Test Loss: 3.364330070059326e-05\n",
            "Epoch 799/1000, Training Loss: 3.0054733570149724e-05, Test Loss: 3.359595378310175e-05\n",
            "Epoch 800/1000, Training Loss: 3.001197027975889e-05, Test Loss: 3.354871967735485e-05\n",
            "Epoch 801/1000, Training Loss: 2.9969327438098917e-05, Test Loss: 3.350159797381245e-05\n",
            "Epoch 802/1000, Training Loss: 2.9926804361113748e-05, Test Loss: 3.3454588267349636e-05\n",
            "Epoch 803/1000, Training Loss: 2.9884400372504824e-05, Test Loss: 3.3407690157161266e-05\n",
            "Epoch 804/1000, Training Loss: 2.984211480360599e-05, Test Loss: 3.3360903246661926e-05\n",
            "Epoch 805/1000, Training Loss: 2.979994699325774e-05, Test Loss: 3.33142271434011e-05\n",
            "Epoch 806/1000, Training Loss: 2.9757896287686234e-05, Test Loss: 3.326766145895735e-05\n",
            "Epoch 807/1000, Training Loss: 2.971596204038314e-05, Test Loss: 3.3221205808866395e-05\n",
            "Epoch 808/1000, Training Loss: 2.9674143611988564e-05, Test Loss: 3.3174859812520374e-05\n",
            "Epoch 809/1000, Training Loss: 2.9632440370176434e-05, Test Loss: 3.31286230930884e-05\n",
            "Epoch 810/1000, Training Loss: 2.959085168953985e-05, Test Loss: 3.30824952774377e-05\n",
            "Epoch 811/1000, Training Loss: 2.9549376951482084e-05, Test Loss: 3.3036475996041255e-05\n",
            "Epoch 812/1000, Training Loss: 2.9508015544106363e-05, Test Loss: 3.2990564882909465e-05\n",
            "Epoch 813/1000, Training Loss: 2.9466766862109237e-05, Test Loss: 3.294476157551051e-05\n",
            "Epoch 814/1000, Training Loss: 2.9425630306676417e-05, Test Loss: 3.289906571468598e-05\n",
            "Epoch 815/1000, Training Loss: 2.9384605285378834e-05, Test Loss: 3.285347694459242e-05\n",
            "Epoch 816/1000, Training Loss: 2.9343691212072056e-05, Test Loss: 3.28079949126174e-05\n",
            "Epoch 817/1000, Training Loss: 2.9302887506797662e-05, Test Loss: 3.2762619269312874e-05\n",
            "Epoch 818/1000, Training Loss: 2.926219359568533e-05, Test Loss: 3.2717349668328165e-05\n",
            "Epoch 819/1000, Training Loss: 2.9221608910857253e-05, Test Loss: 3.26721857663407e-05\n",
            "Epoch 820/1000, Training Loss: 2.9181132890334117e-05, Test Loss: 3.2627127222993294e-05\n",
            "Epoch 821/1000, Training Loss: 2.9140764977944114e-05, Test Loss: 3.258217370082693e-05\n",
            "Epoch 822/1000, Training Loss: 2.9100504623231817e-05, Test Loss: 3.253732486521989e-05\n",
            "Epoch 823/1000, Training Loss: 2.906035128136914e-05, Test Loss: 3.249258038432826e-05\n",
            "Epoch 824/1000, Training Loss: 2.9020304413068832e-05, Test Loss: 3.2447939929027754e-05\n",
            "Epoch 825/1000, Training Loss: 2.8980363484499397e-05, Test Loss: 3.240340317284666e-05\n",
            "Epoch 826/1000, Training Loss: 2.8940527967199627e-05, Test Loss: 3.235896979192415e-05\n",
            "Epoch 827/1000, Training Loss: 2.890079733799813e-05, Test Loss: 3.231463946494104e-05\n",
            "Epoch 828/1000, Training Loss: 2.886117107893079e-05, Test Loss: 3.227041187307581e-05\n",
            "Epoch 829/1000, Training Loss: 2.882164867716267e-05, Test Loss: 3.22262866999445e-05\n",
            "Epoch 830/1000, Training Loss: 2.8782229624908958e-05, Test Loss: 3.218226363155543e-05\n",
            "Epoch 831/1000, Training Loss: 2.8742913419358712e-05, Test Loss: 3.213834235625519e-05\n",
            "Epoch 832/1000, Training Loss: 2.8703699562600007e-05, Test Loss: 3.2094522564682906e-05\n",
            "Epoch 833/1000, Training Loss: 2.866458756154597e-05, Test Loss: 3.2050803949718456e-05\n",
            "Epoch 834/1000, Training Loss: 2.862557692786227e-05, Test Loss: 3.2007186206437876e-05\n",
            "Epoch 835/1000, Training Loss: 2.8586667177895457e-05, Test Loss: 3.196366903207121e-05\n",
            "Epoch 836/1000, Training Loss: 2.8547857832604084e-05, Test Loss: 3.192025212595156e-05\n",
            "Epoch 837/1000, Training Loss: 2.8509148417488688e-05, Test Loss: 3.1876935189478636e-05\n",
            "Epoch 838/1000, Training Loss: 2.8470538462526375e-05, Test Loss: 3.1833717926068676e-05\n",
            "Epoch 839/1000, Training Loss: 2.8432027502101787e-05, Test Loss: 3.1790600041120746e-05\n",
            "Epoch 840/1000, Training Loss: 2.8393615074944963e-05, Test Loss: 3.1747581241973226e-05\n",
            "Epoch 841/1000, Training Loss: 2.8355300724065365e-05, Test Loss: 3.1704661237866125e-05\n",
            "Epoch 842/1000, Training Loss: 2.8317083996690673e-05, Test Loss: 3.166183973989865e-05\n",
            "Epoch 843/1000, Training Loss: 2.8278964444204043e-05, Test Loss: 3.1619116460996e-05\n",
            "Epoch 844/1000, Training Loss: 2.8240941622084237e-05, Test Loss: 3.157649111587017e-05\n",
            "Epoch 845/1000, Training Loss: 2.8203015089845732e-05, Test Loss: 3.153396342099159e-05\n",
            "Epoch 846/1000, Training Loss: 2.816518441098092e-05, Test Loss: 3.149153309454094e-05\n",
            "Epoch 847/1000, Training Loss: 2.812744915290234e-05, Test Loss: 3.144919985638831e-05\n",
            "Epoch 848/1000, Training Loss: 2.808980888688645e-05, Test Loss: 3.140696342805289e-05\n",
            "Epoch 849/1000, Training Loss: 2.8052263188018388e-05, Test Loss: 3.13648235326756e-05\n",
            "Epoch 850/1000, Training Loss: 2.8014811635137618e-05, Test Loss: 3.132277989498028e-05\n",
            "Epoch 851/1000, Training Loss: 2.7977453810784148e-05, Test Loss: 3.128083224125287e-05\n",
            "Epoch 852/1000, Training Loss: 2.794018930114642e-05, Test Loss: 3.123898029930324e-05\n",
            "Epoch 853/1000, Training Loss: 2.7903017696009758e-05, Test Loss: 3.119722379844077e-05\n",
            "Epoch 854/1000, Training Loss: 2.786593858870556e-05, Test Loss: 3.115556246944493e-05\n",
            "Epoch 855/1000, Training Loss: 2.7828951576061995e-05, Test Loss: 3.1113996044537185e-05\n",
            "Epoch 856/1000, Training Loss: 2.779205625835433e-05, Test Loss: 3.1072524257352004e-05\n",
            "Epoch 857/1000, Training Loss: 2.7755252239257504e-05, Test Loss: 3.1031146842913996e-05\n",
            "Epoch 858/1000, Training Loss: 2.7718539125798662e-05, Test Loss: 3.098986353761239e-05\n",
            "Epoch 859/1000, Training Loss: 2.7681916528310747e-05, Test Loss: 3.094867407917282e-05\n",
            "Epoch 860/1000, Training Loss: 2.7645384060387013e-05, Test Loss: 3.0907578206632354e-05\n",
            "Epoch 861/1000, Training Loss: 2.7608941338836277e-05, Test Loss: 3.086657566032189e-05\n",
            "Epoch 862/1000, Training Loss: 2.7572587983638855e-05, Test Loss: 3.082566618183808e-05\n",
            "Epoch 863/1000, Training Loss: 2.7536323617903544e-05, Test Loss: 3.078484951402035e-05\n",
            "Epoch 864/1000, Training Loss: 2.7500147867824302e-05, Test Loss: 3.0744125400928854e-05\n",
            "Epoch 865/1000, Training Loss: 2.746406036263935e-05, Test Loss: 3.07034935878268e-05\n",
            "Epoch 866/1000, Training Loss: 2.742806073458959e-05, Test Loss: 3.066295382115757e-05\n",
            "Epoch 867/1000, Training Loss: 2.7392148618878778e-05, Test Loss: 3.0622505848519735e-05\n",
            "Epoch 868/1000, Training Loss: 2.7356323653632666e-05, Test Loss: 3.058214941865511e-05\n",
            "Epoch 869/1000, Training Loss: 2.7320585479861694e-05, Test Loss: 3.054188428142278e-05\n",
            "Epoch 870/1000, Training Loss: 2.7284933741421017e-05, Test Loss: 3.0501710187783685e-05\n",
            "Epoch 871/1000, Training Loss: 2.7249368084974416e-05, Test Loss: 3.0461626889780177e-05\n",
            "Epoch 872/1000, Training Loss: 2.7213888159955254e-05, Test Loss: 3.042163414052129e-05\n",
            "Epoch 873/1000, Training Loss: 2.7178493618532025e-05, Test Loss: 3.038173169416337e-05\n",
            "Epoch 874/1000, Training Loss: 2.7143184115571463e-05, Test Loss: 3.034191930588959e-05\n",
            "Epoch 875/1000, Training Loss: 2.7107959308603474e-05, Test Loss: 3.0302196731897766e-05\n",
            "Epoch 876/1000, Training Loss: 2.7072818857787252e-05, Test Loss: 3.026256372938366e-05\n",
            "Epoch 877/1000, Training Loss: 2.7037762425875145e-05, Test Loss: 3.02230200565265e-05\n",
            "Epoch 878/1000, Training Loss: 2.70027896781824e-05, Test Loss: 3.0183565472467152e-05\n",
            "Epoch 879/1000, Training Loss: 2.696790028255116e-05, Test Loss: 3.0144199737299943e-05\n",
            "Epoch 880/1000, Training Loss: 2.6933093909320225e-05, Test Loss: 3.0104922612057274e-05\n",
            "Epoch 881/1000, Training Loss: 2.68983702312926e-05, Test Loss: 3.0065733858690038e-05\n",
            "Epoch 882/1000, Training Loss: 2.6863728923703654e-05, Test Loss: 3.0026633240060174e-05\n",
            "Epoch 883/1000, Training Loss: 2.6829169664190788e-05, Test Loss: 2.9987620519925698e-05\n",
            "Epoch 884/1000, Training Loss: 2.6794692132763474e-05, Test Loss: 2.994869546292423e-05\n",
            "Epoch 885/1000, Training Loss: 2.676029601177331e-05, Test Loss: 2.9909857834563157e-05\n",
            "Epoch 886/1000, Training Loss: 2.6725980985884107e-05, Test Loss: 2.9871107401207234e-05\n",
            "Epoch 887/1000, Training Loss: 2.6691746742044e-05, Test Loss: 2.9832443930065094e-05\n",
            "Epoch 888/1000, Training Loss: 2.665759296945712e-05, Test Loss: 2.9793867189177693e-05\n",
            "Epoch 889/1000, Training Loss: 2.662351935955473e-05, Test Loss: 2.9755376947409595e-05\n",
            "Epoch 890/1000, Training Loss: 2.6589525605969012e-05, Test Loss: 2.9716972974431656e-05\n",
            "Epoch 891/1000, Training Loss: 2.6555611404505277e-05, Test Loss: 2.9678655040716702e-05\n",
            "Epoch 892/1000, Training Loss: 2.65217764531156e-05, Test Loss: 2.9640422917526694e-05\n",
            "Epoch 893/1000, Training Loss: 2.6488020451873926e-05, Test Loss: 2.9602276376897275e-05\n",
            "Epoch 894/1000, Training Loss: 2.6454343102947923e-05, Test Loss: 2.9564215191638055e-05\n",
            "Epoch 895/1000, Training Loss: 2.6420744110576424e-05, Test Loss: 2.952623913531202e-05\n",
            "Epoch 896/1000, Training Loss: 2.638722318104314e-05, Test Loss: 2.9488347982234566e-05\n",
            "Epoch 897/1000, Training Loss: 2.635378002265175e-05, Test Loss: 2.9450541507458724e-05\n",
            "Epoch 898/1000, Training Loss: 2.6320414345703214e-05, Test Loss: 2.941281948676866e-05\n",
            "Epoch 899/1000, Training Loss: 2.6287125862471786e-05, Test Loss: 2.9375181696670302e-05\n",
            "Epoch 900/1000, Training Loss: 2.625391428718063e-05, Test Loss: 2.9337627914382172e-05\n",
            "Epoch 901/1000, Training Loss: 2.6220779335980152e-05, Test Loss: 2.9300157917827494e-05\n",
            "Epoch 902/1000, Training Loss: 2.6187720726924822e-05, Test Loss: 2.9262771485626802e-05\n",
            "Epoch 903/1000, Training Loss: 2.615473817995118e-05, Test Loss: 2.9225468397088703e-05\n",
            "Epoch 904/1000, Training Loss: 2.612183141685595e-05, Test Loss: 2.918824843220503e-05\n",
            "Epoch 905/1000, Training Loss: 2.6089000161274474e-05, Test Loss: 2.915111137163785e-05\n",
            "Epoch 906/1000, Training Loss: 2.6056244138660066e-05, Test Loss: 2.9114056996717634e-05\n",
            "Epoch 907/1000, Training Loss: 2.6023563076262294e-05, Test Loss: 2.9077085089434847e-05\n",
            "Epoch 908/1000, Training Loss: 2.5990956703107123e-05, Test Loss: 2.9040195432430143e-05\n",
            "Epoch 909/1000, Training Loss: 2.595842474997642e-05, Test Loss: 2.900338780899298e-05\n",
            "Epoch 910/1000, Training Loss: 2.592596694938839e-05, Test Loss: 2.8966662003048964e-05\n",
            "Epoch 911/1000, Training Loss: 2.589358303557798e-05, Test Loss: 2.8930017799157336e-05\n",
            "Epoch 912/1000, Training Loss: 2.5861272744477125e-05, Test Loss: 2.8893454982505192e-05\n",
            "Epoch 913/1000, Training Loss: 2.5829035813696504e-05, Test Loss: 2.885697333889922e-05\n",
            "Epoch 914/1000, Training Loss: 2.5796871982506324e-05, Test Loss: 2.882057265476234e-05\n",
            "Epoch 915/1000, Training Loss: 2.5764780991818884e-05, Test Loss: 2.8784252717123206e-05\n",
            "Epoch 916/1000, Training Loss: 2.5732762584168498e-05, Test Loss: 2.8748013313619616e-05\n",
            "Epoch 917/1000, Training Loss: 2.5700816503695782e-05, Test Loss: 2.8711854232482412e-05\n",
            "Epoch 918/1000, Training Loss: 2.5668942496129466e-05, Test Loss: 2.8675775262537715e-05\n",
            "Epoch 919/1000, Training Loss: 2.563714030876821e-05, Test Loss: 2.8639776193200342e-05\n",
            "Epoch 920/1000, Training Loss: 2.5605409690465e-05, Test Loss: 2.860385681446745e-05\n",
            "Epoch 921/1000, Training Loss: 2.557375039160921e-05, Test Loss: 2.8568016916914038e-05\n",
            "Epoch 922/1000, Training Loss: 2.5542162164111028e-05, Test Loss: 2.853225629168754e-05\n",
            "Epoch 923/1000, Training Loss: 2.5510644761384533e-05, Test Loss: 2.8496574730506425e-05\n",
            "Epoch 924/1000, Training Loss: 2.5479197938331545e-05, Test Loss: 2.8460972025653325e-05\n",
            "Epoch 925/1000, Training Loss: 2.5447821451326857e-05, Test Loss: 2.8425447969971186e-05\n",
            "Epoch 926/1000, Training Loss: 2.5416515058202023e-05, Test Loss: 2.8390002356855973e-05\n",
            "Epoch 927/1000, Training Loss: 2.5385278518229467e-05, Test Loss: 2.8354634980261727e-05\n",
            "Epoch 928/1000, Training Loss: 2.5354111592108636e-05, Test Loss: 2.8319345634685075e-05\n",
            "Epoch 929/1000, Training Loss: 2.532301404195031e-05, Test Loss: 2.8284134115168446e-05\n",
            "Epoch 930/1000, Training Loss: 2.5291985631262326e-05, Test Loss: 2.824900021729578e-05\n",
            "Epoch 931/1000, Training Loss: 2.526102612493514e-05, Test Loss: 2.821394373718688e-05\n",
            "Epoch 932/1000, Training Loss: 2.52301352892269e-05, Test Loss: 2.81789644714949e-05\n",
            "Epoch 933/1000, Training Loss: 2.519931289175121e-05, Test Loss: 2.8144062217403823e-05\n",
            "Epoch 934/1000, Training Loss: 2.5168558701460943e-05, Test Loss: 2.8109236772621544e-05\n",
            "Epoch 935/1000, Training Loss: 2.5137872488636965e-05, Test Loss: 2.8074487935382087e-05\n",
            "Epoch 936/1000, Training Loss: 2.510725402487264e-05, Test Loss: 2.8039815504437546e-05\n",
            "Epoch 937/1000, Training Loss: 2.5076703083062468e-05, Test Loss: 2.800521927906011e-05\n",
            "Epoch 938/1000, Training Loss: 2.5046219437388254e-05, Test Loss: 2.7970699059030285e-05\n",
            "Epoch 939/1000, Training Loss: 2.5015802863305782e-05, Test Loss: 2.7936254644644372e-05\n",
            "Epoch 940/1000, Training Loss: 2.4985453137533036e-05, Test Loss: 2.7901885836705346e-05\n",
            "Epoch 941/1000, Training Loss: 2.4955170038037403e-05, Test Loss: 2.7867592436522782e-05\n",
            "Epoch 942/1000, Training Loss: 2.4924953344023147e-05, Test Loss: 2.783337424590651e-05\n",
            "Epoch 943/1000, Training Loss: 2.4894802835919973e-05, Test Loss: 2.7799231067169565e-05\n",
            "Epoch 944/1000, Training Loss: 2.4864718295370308e-05, Test Loss: 2.7765162703120565e-05\n",
            "Epoch 945/1000, Training Loss: 2.483469950521822e-05, Test Loss: 2.7731168957063393e-05\n",
            "Epoch 946/1000, Training Loss: 2.4804746249496952e-05, Test Loss: 2.7697249632795842e-05\n",
            "Epoch 947/1000, Training Loss: 2.4774858313418254e-05, Test Loss: 2.76634045346067e-05\n",
            "Epoch 948/1000, Training Loss: 2.474503548336141e-05, Test Loss: 2.762963346726895e-05\n",
            "Epoch 949/1000, Training Loss: 2.4715277546860606e-05, Test Loss: 2.759593623604822e-05\n",
            "Epoch 950/1000, Training Loss: 2.4685584292595696e-05, Test Loss: 2.7562312646687976e-05\n",
            "Epoch 951/1000, Training Loss: 2.4655955510380122e-05, Test Loss: 2.7528762505417057e-05\n",
            "Epoch 952/1000, Training Loss: 2.462639099115129e-05, Test Loss: 2.7495285618942116e-05\n",
            "Epoch 953/1000, Training Loss: 2.4596890526959208e-05, Test Loss: 2.7461881794447106e-05\n",
            "Epoch 954/1000, Training Loss: 2.4567453910956118e-05, Test Loss: 2.7428550839594355e-05\n",
            "Epoch 955/1000, Training Loss: 2.4538080937387293e-05, Test Loss: 2.7395292562517894e-05\n",
            "Epoch 956/1000, Training Loss: 2.450877140157969e-05, Test Loss: 2.7362106771824075e-05\n",
            "Epoch 957/1000, Training Loss: 2.447952509993256e-05, Test Loss: 2.7328993276590846e-05\n",
            "Epoch 958/1000, Training Loss: 2.4450341829907893e-05, Test Loss: 2.729595188636329e-05\n",
            "Epoch 959/1000, Training Loss: 2.4421221390020447e-05, Test Loss: 2.7262982411153534e-05\n",
            "Epoch 960/1000, Training Loss: 2.439216357982739e-05, Test Loss: 2.7230084661441335e-05\n",
            "Epoch 961/1000, Training Loss: 2.436316819992102e-05, Test Loss: 2.719725844816638e-05\n",
            "Epoch 962/1000, Training Loss: 2.4334235051917072e-05, Test Loss: 2.716450358273259e-05\n",
            "Epoch 963/1000, Training Loss: 2.4305363938446855e-05, Test Loss: 2.7131819877004848e-05\n",
            "Epoch 964/1000, Training Loss: 2.4276554663147923e-05, Test Loss: 2.709920714330576e-05\n",
            "Epoch 965/1000, Training Loss: 2.4247807030655512e-05, Test Loss: 2.7066665194416993e-05\n",
            "Epoch 966/1000, Training Loss: 2.4219120846592942e-05, Test Loss: 2.7034193843574742e-05\n",
            "Epoch 967/1000, Training Loss: 2.4190495917563214e-05, Test Loss: 2.7001792904470324e-05\n",
            "Epoch 968/1000, Training Loss: 2.4161932051141272e-05, Test Loss: 2.696946219125074e-05\n",
            "Epoch 969/1000, Training Loss: 2.4133429055863904e-05, Test Loss: 2.693720151851277e-05\n",
            "Epoch 970/1000, Training Loss: 2.4104986741222685e-05, Test Loss: 2.6905010701306138e-05\n",
            "Epoch 971/1000, Training Loss: 2.4076604917655542e-05, Test Loss: 2.6872889555128112e-05\n",
            "Epoch 972/1000, Training Loss: 2.4048283396538042e-05, Test Loss: 2.684083789592551e-05\n",
            "Epoch 973/1000, Training Loss: 2.4020021990175594e-05, Test Loss: 2.6808855540092087e-05\n",
            "Epoch 974/1000, Training Loss: 2.3991820511796007e-05, Test Loss: 2.6776942304468513e-05\n",
            "Epoch 975/1000, Training Loss: 2.3963678775541122e-05, Test Loss: 2.674509800634015e-05\n",
            "Epoch 976/1000, Training Loss: 2.3935596596458952e-05, Test Loss: 2.671332246343475e-05\n",
            "Epoch 977/1000, Training Loss: 2.3907573790496372e-05, Test Loss: 2.668161549392568e-05\n",
            "Epoch 978/1000, Training Loss: 2.387961017449175e-05, Test Loss: 2.6649976916425456e-05\n",
            "Epoch 979/1000, Training Loss: 2.3851705566166984e-05, Test Loss: 2.6618406549987748e-05\n",
            "Epoch 980/1000, Training Loss: 2.382385978412056e-05, Test Loss: 2.658690421410638e-05\n",
            "Epoch 981/1000, Training Loss: 2.3796072647819994e-05, Test Loss: 2.6555469728715328e-05\n",
            "Epoch 982/1000, Training Loss: 2.3768343977594994e-05, Test Loss: 2.652410291418289e-05\n",
            "Epoch 983/1000, Training Loss: 2.374067359462997e-05, Test Loss: 2.6492803591317803e-05\n",
            "Epoch 984/1000, Training Loss: 2.371306132095758e-05, Test Loss: 2.646157158136131e-05\n",
            "Epoch 985/1000, Training Loss: 2.368550697945065e-05, Test Loss: 2.643040670599164e-05\n",
            "Epoch 986/1000, Training Loss: 2.3658010393817754e-05, Test Loss: 2.6399308787319664e-05\n",
            "Epoch 987/1000, Training Loss: 2.3630571388593693e-05, Test Loss: 2.6368277647890355e-05\n",
            "Epoch 988/1000, Training Loss: 2.3603189789134282e-05, Test Loss: 2.6337313110682258e-05\n",
            "Epoch 989/1000, Training Loss: 2.357586542160999e-05, Test Loss: 2.6306414999103203e-05\n",
            "Epoch 990/1000, Training Loss: 2.354859811299878e-05, Test Loss: 2.6275583136988714e-05\n",
            "Epoch 991/1000, Training Loss: 2.3521387691079798e-05, Test Loss: 2.6244817348610182e-05\n",
            "Epoch 992/1000, Training Loss: 2.3494233984427536e-05, Test Loss: 2.621411745866532e-05\n",
            "Epoch 993/1000, Training Loss: 2.3467136822404962e-05, Test Loss: 2.618348329227828e-05\n",
            "Epoch 994/1000, Training Loss: 2.344009603515745e-05, Test Loss: 2.6152914675004594e-05\n",
            "Epoch 995/1000, Training Loss: 2.3413111453607242e-05, Test Loss: 2.6122411432821357e-05\n",
            "Epoch 996/1000, Training Loss: 2.3386182909446696e-05, Test Loss: 2.60919733921358e-05\n",
            "Epoch 997/1000, Training Loss: 2.3359310235132673e-05, Test Loss: 2.606160037977724e-05\n",
            "Epoch 998/1000, Training Loss: 2.3332493263880553e-05, Test Loss: 2.6031292222999862e-05\n",
            "Epoch 999/1000, Training Loss: 2.3305731829658442e-05, Test Loss: 2.6001048749483502e-05\n",
            "Epoch 1000/1000, Training Loss: 2.327902576718131e-05, Test Loss: 2.5970869787329685e-05\n",
            "Epoch 1/1000, Training Loss: 0.002967649078234947, Test Loss: 0.003340757437156911\n",
            "Epoch 2/1000, Training Loss: 0.002889709834859693, Test Loss: 0.0032939919086140434\n",
            "Epoch 3/1000, Training Loss: 0.002878228428103787, Test Loss: 0.0032927767734193237\n",
            "Epoch 4/1000, Training Loss: 0.0028751191332471748, Test Loss: 0.003293961935330295\n",
            "Epoch 5/1000, Training Loss: 0.0028737284773875126, Test Loss: 0.003294541979840825\n",
            "Epoch 6/1000, Training Loss: 0.0028728101211603517, Test Loss: 0.003294589938804079\n",
            "Epoch 7/1000, Training Loss: 0.0028720685238529625, Test Loss: 0.003294368829047914\n",
            "Epoch 8/1000, Training Loss: 0.0028714173332622817, Test Loss: 0.0032940436824795806\n",
            "Epoch 9/1000, Training Loss: 0.002870823826522395, Test Loss: 0.0032936959381442855\n",
            "Epoch 10/1000, Training Loss: 0.0028702717288381628, Test Loss: 0.0032933596181441193\n",
            "Epoch 11/1000, Training Loss: 0.0028697511892578037, Test Loss: 0.0032930453067217974\n",
            "Epoch 12/1000, Training Loss: 0.0028692555617975735, Test Loss: 0.003292753153742211\n",
            "Epoch 13/1000, Training Loss: 0.002868780091439731, Test Loss: 0.003292479320288162\n",
            "Epoch 14/1000, Training Loss: 0.002868321241188552, Test Loss: 0.003292218955064128\n",
            "Epoch 15/1000, Training Loss: 0.002867876294917056, Test Loss: 0.003291967448346027\n",
            "Epoch 16/1000, Training Loss: 0.0028674431067669203, Test Loss: 0.0032917208733121085\n",
            "Epoch 17/1000, Training Loss: 0.002867019938207056, Test Loss: 0.0032914760668177174\n",
            "Epoch 18/1000, Training Loss: 0.002866605350138783, Test Loss: 0.003291230565710037\n",
            "Epoch 19/1000, Training Loss: 0.0028661981301606842, Test Loss: 0.0032909824975060615\n",
            "Epoch 20/1000, Training Loss: 0.002865797242453554, Test Loss: 0.003290730467719195\n",
            "Epoch 21/1000, Training Loss: 0.0028654017923216146, Test Loss: 0.0032904734597687924\n",
            "Epoch 22/1000, Training Loss: 0.0028650110003352715, Test Loss: 0.0032902107516936483\n",
            "Epoch 23/1000, Training Loss: 0.002864624182869624, Test Loss: 0.0032899418490864875\n",
            "Epoch 24/1000, Training Loss: 0.0028642407370014773, Test Loss: 0.003289666432008673\n",
            "Epoch 25/1000, Training Loss: 0.0028638601284617536, Test Loss: 0.003289384313340446\n",
            "Epoch 26/1000, Training Loss: 0.0028634818817999093, Test Loss: 0.003289095406242791\n",
            "Epoch 27/1000, Training Loss: 0.0028631055722045397, Test Loss: 0.0032887996987844077\n",
            "Epoch 28/1000, Training Loss: 0.0028627308186049104, Test Loss: 0.00328849723416953\n",
            "Epoch 29/1000, Training Loss: 0.0028623572777923033, Test Loss: 0.0032881880953353785\n",
            "Epoch 30/1000, Training Loss: 0.002861984639373223, Test Loss: 0.003287872392959711\n",
            "Epoch 31/1000, Training Loss: 0.00286161262141428, Test Loss: 0.003287550256133589\n",
            "Epoch 32/1000, Training Loss: 0.002861240966670545, Test Loss: 0.003287221825121343\n",
            "Epoch 33/1000, Training Loss: 0.002860869439311312, Test Loss: 0.0032868872457584852\n",
            "Epoch 34/1000, Training Loss: 0.0028604978220730264, Test Loss: 0.0032865466651375674\n",
            "Epoch 35/1000, Training Loss: 0.002860125913780931, Test Loss: 0.003286200228308441\n",
            "Epoch 36/1000, Training Loss: 0.0028597535271900456, Test Loss: 0.0032858480757784396\n",
            "Epoch 37/1000, Training Loss: 0.0028593804871033268, Test Loss: 0.0032854903416438437\n",
            "Epoch 38/1000, Training Loss: 0.0028590066287307414, Test Loss: 0.003285127152219571\n",
            "Epoch 39/1000, Training Loss: 0.0028586317962579007, Test Loss: 0.0032847586250618764\n",
            "Epoch 40/1000, Training Loss: 0.0028582558415970703, Test Loss: 0.003284384868300594\n",
            "Epoch 41/1000, Training Loss: 0.002857878623296911, Test Loss: 0.003284005980214628\n",
            "Epoch 42/1000, Training Loss: 0.0028575000055903743, Test Loss: 0.003283622048997833\n",
            "Epoch 43/1000, Training Loss: 0.0028571198575628553, Test Loss: 0.003283233152673186\n",
            "Epoch 44/1000, Training Loss: 0.0028567380524249673, Test Loss: 0.0032828393591215897\n",
            "Epoch 45/1000, Training Loss: 0.00285635446687634, Test Loss: 0.0032824407261983564\n",
            "Epoch 46/1000, Training Loss: 0.002855968980548603, Test Loss: 0.003282037301915899\n",
            "Epoch 47/1000, Training Loss: 0.002855581475517199, Test Loss: 0.0032816291246753754\n",
            "Epoch 48/1000, Training Loss: 0.002855191835873048, Test Loss: 0.003281216223533508\n",
            "Epoch 49/1000, Training Loss: 0.002854799947346228, Test Loss: 0.0032807986184936085\n",
            "Epoch 50/1000, Training Loss: 0.002854405696974831, Test Loss: 0.003280376320812027\n",
            "Epoch 51/1000, Training Loss: 0.0028540089728130765, Test Loss: 0.0032799493333130814\n",
            "Epoch 52/1000, Training Loss: 0.0028536096636734964, Test Loss: 0.0032795176507069462\n",
            "Epoch 53/1000, Training Loss: 0.002853207658898712, Test Loss: 0.003279081259906211\n",
            "Epoch 54/1000, Training Loss: 0.0028528028481588706, Test Loss: 0.003278640140337718\n",
            "Epoch 55/1000, Training Loss: 0.0028523951212713573, Test Loss: 0.003278194264247095\n",
            "Epoch 56/1000, Training Loss: 0.002851984368039811, Test Loss: 0.0032777435969940354\n",
            "Epoch 57/1000, Training Loss: 0.002851570478109894, Test Loss: 0.0032772880973368497\n",
            "Epoch 58/1000, Training Loss: 0.002851153340839544, Test Loss: 0.0032768277177052637\n",
            "Epoch 59/1000, Training Loss: 0.002850732845181816, Test Loss: 0.003276362404460752\n",
            "Epoch 60/1000, Training Loss: 0.0028503088795785803, Test Loss: 0.003275892098143962\n",
            "Epoch 61/1000, Training Loss: 0.0028498813318636484, Test Loss: 0.003275416733709\n",
            "Epoch 62/1000, Training Loss: 0.002849450089174034, Test Loss: 0.0032749362407445567\n",
            "Epoch 63/1000, Training Loss: 0.002849015037868253, Test Loss: 0.0032744505436819006\n",
            "Epoch 64/1000, Training Loss: 0.002848576063450705, Test Loss: 0.0032739595619899905\n",
            "Epoch 65/1000, Training Loss: 0.002848133050501302, Test Loss: 0.003273463210357945\n",
            "Epoch 66/1000, Training Loss: 0.0028476858826096234, Test Loss: 0.003272961398865213\n",
            "Epoch 67/1000, Training Loss: 0.0028472344423129767, Test Loss: 0.003272454033139835\n",
            "Epoch 68/1000, Training Loss: 0.0028467786110378144, Test Loss: 0.003271941014505227\n",
            "Epoch 69/1000, Training Loss: 0.0028463182690440427, Test Loss: 0.003271422240115902\n",
            "Epoch 70/1000, Training Loss: 0.0028458532953718058, Test Loss: 0.003270897603082616\n",
            "Epoch 71/1000, Training Loss: 0.0028453835677904092, Test Loss: 0.0032703669925873784\n",
            "Epoch 72/1000, Training Loss: 0.0028449089627490577, Test Loss: 0.003269830293988821\n",
            "Epoch 73/1000, Training Loss: 0.0028444293553291523, Test Loss: 0.0032692873889183733\n",
            "Epoch 74/1000, Training Loss: 0.00284394461919792, Test Loss: 0.003268738155367692\n",
            "Epoch 75/1000, Training Loss: 0.0028434546265631677, Test Loss: 0.0032681824677678194\n",
            "Epoch 76/1000, Training Loss: 0.0028429592481289978, Test Loss: 0.003267620197060497\n",
            "Epoch 77/1000, Training Loss: 0.0028424583530523334, Test Loss: 0.0032670512107620432\n",
            "Epoch 78/1000, Training Loss: 0.0028419518089001262, Test Loss: 0.00326647537302022\n",
            "Epoch 79/1000, Training Loss: 0.002841439481607132, Test Loss: 0.0032658925446644965\n",
            "Epoch 80/1000, Training Loss: 0.002840921235434162, Test Loss: 0.0032653025832500497\n",
            "Epoch 81/1000, Training Loss: 0.0028403969329267274, Test Loss: 0.0032647053430958904\n",
            "Epoch 82/1000, Training Loss: 0.0028398664348740104, Test Loss: 0.003264100675317479\n",
            "Epoch 83/1000, Training Loss: 0.0028393296002680824, Test Loss: 0.003263488427854107\n",
            "Epoch 84/1000, Training Loss: 0.002838786286263341, Test Loss: 0.0032628684454914154\n",
            "Epoch 85/1000, Training Loss: 0.002838236348136107, Test Loss: 0.003262240569879312\n",
            "Epoch 86/1000, Training Loss: 0.002837679639244339, Test Loss: 0.0032616046395455966\n",
            "Epoch 87/1000, Training Loss: 0.0028371160109874442, Test Loss: 0.0032609604899055365\n",
            "Epoch 88/1000, Training Loss: 0.002836545312766151, Test Loss: 0.003260307953267679\n",
            "Epoch 89/1000, Training Loss: 0.0028359673919424173, Test Loss: 0.0032596468588361157\n",
            "Epoch 90/1000, Training Loss: 0.002835382093799353, Test Loss: 0.0032589770327094408\n",
            "Epoch 91/1000, Training Loss: 0.002834789261501154, Test Loss: 0.0032582982978766285\n",
            "Epoch 92/1000, Training Loss: 0.0028341887360530103, Test Loss: 0.003257610474210007\n",
            "Epoch 93/1000, Training Loss: 0.0028335803562610016, Test Loss: 0.003256913378455557\n",
            "Epoch 94/1000, Training Loss: 0.0028329639586919474, Test Loss: 0.0032562068242206897\n",
            "Epoch 95/1000, Training Loss: 0.0028323393776332217, Test Loss: 0.003255490621959697\n",
            "Epoch 96/1000, Training Loss: 0.00283170644505252, Test Loss: 0.003254764578957021\n",
            "Epoch 97/1000, Training Loss: 0.002831064990557573, Test Loss: 0.0032540284993085216\n",
            "Epoch 98/1000, Training Loss: 0.0028304148413558095, Test Loss: 0.003253282183900859\n",
            "Epoch 99/1000, Training Loss: 0.0028297558222139594, Test Loss: 0.0032525254303891506\n",
            "Epoch 100/1000, Training Loss: 0.00282908775541761, Test Loss: 0.003251758033173036\n",
            "Epoch 101/1000, Training Loss: 0.0028284104607306983, Test Loss: 0.003250979783371258\n",
            "Epoch 102/1000, Training Loss: 0.0028277237553549594, Test Loss: 0.003250190468794888\n",
            "Epoch 103/1000, Training Loss: 0.0028270274538893282, Test Loss: 0.0032493898739193224\n",
            "Epoch 104/1000, Training Loss: 0.0028263213682892903, Test Loss: 0.0032485777798551218\n",
            "Epoch 105/1000, Training Loss: 0.0028256053078262056, Test Loss: 0.0032477539643178386\n",
            "Epoch 106/1000, Training Loss: 0.0028248790790465937, Test Loss: 0.00324691820159689\n",
            "Epoch 107/1000, Training Loss: 0.002824142485731398, Test Loss: 0.0032460702625235977\n",
            "Epoch 108/1000, Training Loss: 0.002823395328855239, Test Loss: 0.0032452099144384796\n",
            "Epoch 109/1000, Training Loss: 0.002822637406545663, Test Loss: 0.003244336921157866\n",
            "Epoch 110/1000, Training Loss: 0.002821868514042395, Test Loss: 0.00324345104293993\n",
            "Epoch 111/1000, Training Loss: 0.0028210884436566204, Test Loss: 0.003242552036450225\n",
            "Epoch 112/1000, Training Loss: 0.0028202969847302894, Test Loss: 0.00324163965472678\n",
            "Epoch 113/1000, Training Loss: 0.00281949392359548, Test Loss: 0.00324071364714486\n",
            "Epoch 114/1000, Training Loss: 0.0028186790435338273, Test Loss: 0.0032397737593814297\n",
            "Epoch 115/1000, Training Loss: 0.00281785212473603, Test Loss: 0.00323881973337942\n",
            "Epoch 116/1000, Training Loss: 0.00281701294426147, Test Loss: 0.003237851307311848\n",
            "Epoch 117/1000, Training Loss: 0.0028161612759979525, Test Loss: 0.003236868215545884\n",
            "Epoch 118/1000, Training Loss: 0.002815296890621599, Test Loss: 0.0032358701886068845\n",
            "Epoch 119/1000, Training Loss: 0.002814419555556911, Test Loss: 0.0032348569531425253\n",
            "Epoch 120/1000, Training Loss: 0.002813529034937039, Test Loss: 0.003233828231887027\n",
            "Epoch 121/1000, Training Loss: 0.002812625089564278, Test Loss: 0.003232783743625605\n",
            "Epoch 122/1000, Training Loss: 0.00281170747687083, Test Loss: 0.0032317232031591615\n",
            "Epoch 123/1000, Training Loss: 0.0028107759508798546, Test Loss: 0.0032306463212692952\n",
            "Epoch 124/1000, Training Loss: 0.0028098302621668582, Test Loss: 0.0032295528046837234\n",
            "Epoch 125/1000, Training Loss: 0.002808870157821443, Test Loss: 0.0032284423560421265\n",
            "Epoch 126/1000, Training Loss: 0.002807895381409473, Test Loss: 0.00322731467386253\n",
            "Epoch 127/1000, Training Loss: 0.0028069056729356882, Test Loss: 0.00322616945250826\n",
            "Epoch 128/1000, Training Loss: 0.0028059007688068117, Test Loss: 0.0032250063821555523\n",
            "Epoch 129/1000, Training Loss: 0.002804880401795209, Test Loss: 0.0032238251487618622\n",
            "Epoch 130/1000, Training Loss: 0.0028038443010031415, Test Loss: 0.0032226254340349714\n",
            "Epoch 131/1000, Training Loss: 0.0028027921918276676, Test Loss: 0.0032214069154029266\n",
            "Epoch 132/1000, Training Loss: 0.0028017237959262454, Test Loss: 0.003220169265984897\n",
            "Epoch 133/1000, Training Loss: 0.002800638831183111, Test Loss: 0.003218912154563021\n",
            "Epoch 134/1000, Training Loss: 0.0027995370116764722, Test Loss: 0.0032176352455552883\n",
            "Epoch 135/1000, Training Loss: 0.002798418047646597, Test Loss: 0.0032163381989895596\n",
            "Epoch 136/1000, Training Loss: 0.002797281645464863, Test Loss: 0.003215020670478755\n",
            "Epoch 137/1000, Training Loss: 0.002796127507603828, Test Loss: 0.0032136823111973186\n",
            "Epoch 138/1000, Training Loss: 0.0027949553326084096, Test Loss: 0.0032123227678590043\n",
            "Epoch 139/1000, Training Loss: 0.002793764815068235, Test Loss: 0.003210941682696058\n",
            "Epoch 140/1000, Training Loss: 0.002792555645591243, Test Loss: 0.0032095386934398724\n",
            "Epoch 141/1000, Training Loss: 0.0027913275107786318, Test Loss: 0.003208113433303181\n",
            "Epoch 142/1000, Training Loss: 0.0027900800932012097, Test Loss: 0.0032066655309638636\n",
            "Epoch 143/1000, Training Loss: 0.002788813071377255, Test Loss: 0.003205194610550423\n",
            "Epoch 144/1000, Training Loss: 0.0027875261197519625, Test Loss: 0.003203700291629211\n",
            "Epoch 145/1000, Training Loss: 0.002786218908678566, Test Loss: 0.0032021821891934743\n",
            "Epoch 146/1000, Training Loss: 0.0027848911044012293, Test Loss: 0.0032006399136542685\n",
            "Epoch 147/1000, Training Loss: 0.00278354236903979, Test Loss: 0.003199073070833324\n",
            "Epoch 148/1000, Training Loss: 0.0027821723605764583, Test Loss: 0.0031974812619579213\n",
            "Epoch 149/1000, Training Loss: 0.002780780732844551, Test Loss: 0.0031958640836578073\n",
            "Epoch 150/1000, Training Loss: 0.0027793671355193688, Test Loss: 0.0031942211279642726\n",
            "Epoch 151/1000, Training Loss: 0.00277793121411129, Test Loss: 0.0031925519823113647\n",
            "Epoch 152/1000, Training Loss: 0.0027764726099611912, Test Loss: 0.003190856229539354\n",
            "Epoch 153/1000, Training Loss: 0.002774990960238277, Test Loss: 0.0031891334479004535\n",
            "Epoch 154/1000, Training Loss: 0.002773485897940402, Test Loss: 0.0031873832110668517\n",
            "Epoch 155/1000, Training Loss: 0.0027719570518969896, Test Loss: 0.003185605088141092\n",
            "Epoch 156/1000, Training Loss: 0.002770404046774608, Test Loss: 0.0031837986436688242\n",
            "Epoch 157/1000, Training Loss: 0.0027688265030853076, Test Loss: 0.003181963437653952\n",
            "Epoch 158/1000, Training Loss: 0.0027672240371977787, Test Loss: 0.00318009902557618\n",
            "Epoch 159/1000, Training Loss: 0.002765596261351414, Test Loss: 0.003178204958410987\n",
            "Epoch 160/1000, Training Loss: 0.0027639427836733265, Test Loss: 0.00317628078265201\n",
            "Epoch 161/1000, Training Loss: 0.002762263208198408, Test Loss: 0.0031743260403358166\n",
            "Epoch 162/1000, Training Loss: 0.0027605571348924465, Test Loss: 0.0031723402690690687\n",
            "Epoch 163/1000, Training Loss: 0.002758824159678378, Test Loss: 0.0031703230020580135\n",
            "Epoch 164/1000, Training Loss: 0.0027570638744656897, Test Loss: 0.003168273768140274\n",
            "Epoch 165/1000, Training Loss: 0.0027552758671830057, Test Loss: 0.0031661920918188836\n",
            "Epoch 166/1000, Training Loss: 0.0027534597218138645, Test Loss: 0.003164077493298457\n",
            "Epoch 167/1000, Training Loss: 0.0027516150184356968, Test Loss: 0.003161929488523463\n",
            "Epoch 168/1000, Training Loss: 0.0027497413332619857, Test Loss: 0.003159747589218414\n",
            "Epoch 169/1000, Training Loss: 0.00274783823868758, Test Loss: 0.0031575313029299553\n",
            "Epoch 170/1000, Training Loss: 0.0027459053033371318, Test Loss: 0.0031552801330705905\n",
            "Epoch 171/1000, Training Loss: 0.002743942092116575, Test Loss: 0.003152993578963992\n",
            "Epoch 172/1000, Training Loss: 0.0027419481662675927, Test Loss: 0.0031506711358916346\n",
            "Epoch 173/1000, Training Loss: 0.0027399230834249523, Test Loss: 0.003148312295140589\n",
            "Epoch 174/1000, Training Loss: 0.002737866397676609, Test Loss: 0.003145916544052241\n",
            "Epoch 175/1000, Training Loss: 0.002735777659626414, Test Loss: 0.003143483366071666\n",
            "Epoch 176/1000, Training Loss: 0.0027336564164592825, Test Loss: 0.0031410122407974097\n",
            "Epoch 177/1000, Training Loss: 0.002731502212008612, Test Loss: 0.0031385026440313374\n",
            "Epoch 178/1000, Training Loss: 0.002729314586825744, Test Loss: 0.0031359540478282577\n",
            "Epoch 179/1000, Training Loss: 0.0027270930782512075, Test Loss: 0.0031333659205449132\n",
            "Epoch 180/1000, Training Loss: 0.0027248372204874793, Test Loss: 0.003130737726887973\n",
            "Epoch 181/1000, Training Loss: 0.0027225465446729393, Test Loss: 0.003128068927960593\n",
            "Epoch 182/1000, Training Loss: 0.0027202205789566792, Test Loss: 0.0031253589813070807\n",
            "Epoch 183/1000, Training Loss: 0.002717858848573787, Test Loss: 0.003122607340955163\n",
            "Epoch 184/1000, Training Loss: 0.002715460875920681, Test Loss: 0.0031198134574553288\n",
            "Epoch 185/1000, Training Loss: 0.0027130261806300425, Test Loss: 0.0031169767779166725\n",
            "Epoch 186/1000, Training Loss: 0.0027105542796448494, Test Loss: 0.003114096746038625\n",
            "Epoch 187/1000, Training Loss: 0.0027080446872909643, Test Loss: 0.0031111728021379144\n",
            "Epoch 188/1000, Training Loss: 0.0027054969153476995, Test Loss: 0.0031082043831700623\n",
            "Epoch 189/1000, Training Loss: 0.0027029104731157248, Test Loss: 0.003105190922744675\n",
            "Epoch 190/1000, Training Loss: 0.0027002848674816393, Test Loss: 0.003102131851133742\n",
            "Epoch 191/1000, Training Loss: 0.002697619602978475, Test Loss: 0.003099026595272089\n",
            "Epoch 192/1000, Training Loss: 0.0026949141818413745, Test Loss: 0.003095874578749142\n",
            "Epoch 193/1000, Training Loss: 0.0026921681040575807, Test Loss: 0.003092675221791019\n",
            "Epoch 194/1000, Training Loss: 0.002689380867409883, Test Loss: 0.0030894279412319943\n",
            "Epoch 195/1000, Training Loss: 0.0026865519675125593, Test Loss: 0.0030861321504742894\n",
            "Epoch 196/1000, Training Loss: 0.0026836808978388427, Test Loss: 0.0030827872594350976\n",
            "Epoch 197/1000, Training Loss: 0.002680767149738829, Test Loss: 0.0030793926744796933\n",
            "Epoch 198/1000, Training Loss: 0.002677810212446746, Test Loss: 0.0030759477983394253\n",
            "Epoch 199/1000, Training Loss: 0.0026748095730763843, Test Loss: 0.003072452030013323\n",
            "Epoch 200/1000, Training Loss: 0.0026717647166034884, Test Loss: 0.0030689047646520274\n",
            "Epoch 201/1000, Training Loss: 0.002668675125833795, Test Loss: 0.0030653053934226325\n",
            "Epoch 202/1000, Training Loss: 0.002665540281355374, Test Loss: 0.0030616533033530286\n",
            "Epoch 203/1000, Training Loss: 0.0026623596614738783, Test Loss: 0.0030579478771542677\n",
            "Epoch 204/1000, Training Loss: 0.0026591327421291978, Test Loss: 0.0030541884930193595\n",
            "Epoch 205/1000, Training Loss: 0.0026558589967920244, Test Loss: 0.003050374524396949\n",
            "Epoch 206/1000, Training Loss: 0.0026525378963387094, Test Loss: 0.003046505339738163\n",
            "Epoch 207/1000, Training Loss: 0.0026491689089027893, Test Loss: 0.003042580302214941\n",
            "Epoch 208/1000, Training Loss: 0.0026457514997014664, Test Loss: 0.003038598769408077\n",
            "Epoch 209/1000, Training Loss: 0.0026422851308353004, Test Loss: 0.0030345600929631367\n",
            "Epoch 210/1000, Training Loss: 0.002638769261059302, Test Loss: 0.0030304636182123886\n",
            "Epoch 211/1000, Training Loss: 0.002635203345523581, Test Loss: 0.0030263086837608623\n",
            "Epoch 212/1000, Training Loss: 0.002631586835481665, Test Loss: 0.0030220946210345163\n",
            "Epoch 213/1000, Training Loss: 0.0026279191779645405, Test Loss: 0.003017820753788609\n",
            "Epoch 214/1000, Training Loss: 0.0026241998154184643, Test Loss: 0.0030134863975741534\n",
            "Epoch 215/1000, Training Loss: 0.002620428185304566, Test Loss: 0.0030090908591605087\n",
            "Epoch 216/1000, Training Loss: 0.002616603719658197, Test Loss: 0.003004633435911948\n",
            "Epoch 217/1000, Training Loss: 0.002612725844606034, Test Loss: 0.00300011341511618\n",
            "Epoch 218/1000, Training Loss: 0.0026087939798388866, Test Loss: 0.002995530073262734\n",
            "Epoch 219/1000, Training Loss: 0.00260480753803818, Test Loss: 0.0029908826752691083\n",
            "Epoch 220/1000, Training Loss: 0.0026007659242541182, Test Loss: 0.0029861704736526425\n",
            "Epoch 221/1000, Training Loss: 0.0025966685352335, Test Loss: 0.0029813927076460882\n",
            "Epoch 222/1000, Training Loss: 0.002592514758695272, Test Loss: 0.0029765486022548544\n",
            "Epoch 223/1000, Training Loss: 0.0025883039725518803, Test Loss: 0.00297163736725402\n",
            "Epoch 224/1000, Training Loss: 0.002584035544074579, Test Loss: 0.0029666581961232423\n",
            "Epoch 225/1000, Training Loss: 0.0025797088290009395, Test Loss: 0.0029616102649177496\n",
            "Epoch 226/1000, Training Loss: 0.0025753231705828556, Test Loss: 0.002956492731073764\n",
            "Epoch 227/1000, Training Loss: 0.002570877898573484, Test Loss: 0.002951304732146775\n",
            "Epoch 228/1000, Training Loss: 0.0025663723281516638, Test Loss: 0.0029460453844812402\n",
            "Epoch 229/1000, Training Loss: 0.0025618057587825168, Test Loss: 0.002940713781810446\n",
            "Epoch 230/1000, Training Loss: 0.0025571774730130687, Test Loss: 0.0029353089937854625\n",
            "Epoch 231/1000, Training Loss: 0.002552486735201934, Test Loss: 0.002929830064432283\n",
            "Epoch 232/1000, Training Loss: 0.002547732790182308, Test Loss: 0.002924276010536516\n",
            "Epoch 233/1000, Training Loss: 0.002542914861857711, Test Loss: 0.0029186458199552514\n",
            "Epoch 234/1000, Training Loss: 0.0025380321517302147, Test Loss: 0.0029129384498559384\n",
            "Epoch 235/1000, Training Loss: 0.002533083837361112, Test Loss: 0.0029071528248825144\n",
            "Epoch 236/1000, Training Loss: 0.00252806907076431, Test Loss: 0.002901287835249303\n",
            "Epoch 237/1000, Training Loss: 0.0025229869767330222, Test Loss: 0.002895342334763557\n",
            "Epoch 238/1000, Training Loss: 0.002517836651100703, Test Loss: 0.002889315138777953\n",
            "Epoch 239/1000, Training Loss: 0.0025126171589375046, Test Loss: 0.0028832050220747507\n",
            "Epoch 240/1000, Training Loss: 0.002507327532683944, Test Loss: 0.002877010716683787\n",
            "Epoch 241/1000, Training Loss: 0.0025019667702238777, Test Loss: 0.002870730909636971\n",
            "Epoch 242/1000, Training Loss: 0.002496533832899324, Test Loss: 0.0028643642406624373\n",
            "Epoch 243/1000, Training Loss: 0.002491027643470125, Test Loss: 0.002857909299822106\n",
            "Epoch 244/1000, Training Loss: 0.0024854470840219365, Test Loss: 0.002851364625096949\n",
            "Epoch 245/1000, Training Loss: 0.002479790993826547, Test Loss: 0.00284472869992488\n",
            "Epoch 246/1000, Training Loss: 0.0024740581671590534, Test Loss: 0.00283799995069684\n",
            "Epoch 247/1000, Training Loss: 0.0024682473510769624, Test Loss: 0.002831176744217296\n",
            "Epoch 248/1000, Training Loss: 0.0024623572431669003, Test Loss: 0.0028242573851360968\n",
            "Epoch 249/1000, Training Loss: 0.002456386489265166, Test Loss: 0.0028172401133593535\n",
            "Epoch 250/1000, Training Loss: 0.0024503336811590054, Test Loss: 0.002810123101447768\n",
            "Epoch 251/1000, Training Loss: 0.0024441973542760993, Test Loss: 0.0028029044520115937\n",
            "Epoch 252/1000, Training Loss: 0.002437975985370433, Test Loss: 0.0027955821951122606\n",
            "Epoch 253/1000, Training Loss: 0.0024316679902133616, Test Loss: 0.002788154285681513\n",
            "Epoch 254/1000, Training Loss: 0.0024252717212993615, Test Loss: 0.002780618600969745\n",
            "Epoch 255/1000, Training Loss: 0.0024187854655767014, Test Loss: 0.002772972938036141\n",
            "Epoch 256/1000, Training Loss: 0.002412207442213926, Test Loss: 0.002765215011294094\n",
            "Epoch 257/1000, Training Loss: 0.0024055358004138163, Test Loss: 0.0027573424501263315\n",
            "Epoch 258/1000, Training Loss: 0.0023987686172872343, Test Loss: 0.002749352796585105\n",
            "Epoch 259/1000, Training Loss: 0.002391903895799989, Test Loss: 0.002741243503193769\n",
            "Epoch 260/1000, Training Loss: 0.002384939562806706, Test Loss: 0.0027330119308671325\n",
            "Epoch 261/1000, Training Loss: 0.0023778734671864397, Test Loss: 0.002724655346968884\n",
            "Epoch 262/1000, Training Loss: 0.0023707033780956087, Test Loss: 0.002716170923525549\n",
            "Epoch 263/1000, Training Loss: 0.0023634269833547115, Test Loss: 0.0027075557356174658\n",
            "Epoch 264/1000, Training Loss: 0.002356041887986133, Test Loss: 0.0026988067599683944\n",
            "Epoch 265/1000, Training Loss: 0.002348545612921324, Test Loss: 0.0026899208737565373\n",
            "Epoch 266/1000, Training Loss: 0.0023409355938965384, Test Loss: 0.0026808948536709803\n",
            "Epoch 267/1000, Training Loss: 0.002333209180557379, Test Loss: 0.0026717253752387305\n",
            "Epoch 268/1000, Training Loss: 0.002325363635793444, Test Loss: 0.0026624090124489747\n",
            "Epoch 269/1000, Training Loss: 0.0023173961353254568, Test Loss: 0.0026529422377023827\n",
            "Epoch 270/1000, Training Loss: 0.002309303767568517, Test Loss: 0.0026433214221148597\n",
            "Epoch 271/1000, Training Loss: 0.0023010835337962704, Test Loss: 0.0026335428362065675\n",
            "Epoch 272/1000, Training Loss: 0.0022927323486322026, Test Loss: 0.002623602651008618\n",
            "Epoch 273/1000, Training Loss: 0.00228424704089559, Test Loss: 0.00261349693962154\n",
            "Epoch 274/1000, Training Loss: 0.002275624354831165, Test Loss: 0.002603221679261305\n",
            "Epoch 275/1000, Training Loss: 0.0022668609517530643, Test Loss: 0.0025927727538304886\n",
            "Epoch 276/1000, Training Loss: 0.0022579534121352444, Test Loss: 0.002582145957054037\n",
            "Epoch 277/1000, Training Loss: 0.0022488982381822175, Test Loss: 0.002571336996220996\n",
            "Epoch 278/1000, Training Loss: 0.002239691856915656, Test Loss: 0.0025603414965755\n",
            "Epoch 279/1000, Training Loss: 0.00223033062381411, Test Loss: 0.0025491550064022255\n",
            "Epoch 280/1000, Training Loss: 0.0022208108270448225, Test Loss: 0.002537773002853515\n",
            "Epoch 281/1000, Training Loss: 0.002211128692328191, Test Loss: 0.0025261908985670474\n",
            "Epoch 282/1000, Training Loss: 0.002201280388477024, Test Loss: 0.002514404049124742\n",
            "Epoch 283/1000, Training Loss: 0.002191262033654062, Test Loss: 0.00250240776140498\n",
            "Epoch 284/1000, Training Loss: 0.0021810697023923156, Test Loss: 0.0024901973028813537\n",
            "Epoch 285/1000, Training Loss: 0.002170699433423634, Test Loss: 0.0024777679119220054\n",
            "Epoch 286/1000, Training Loss: 0.0021601472383611787, Test Loss: 0.002465114809143702\n",
            "Epoch 287/1000, Training Loss: 0.0021494091112813462, Test Loss: 0.0024522332098744523\n",
            "Epoch 288/1000, Training Loss: 0.0021384810392497994, Test Loss: 0.002439118337777163\n",
            "Epoch 289/1000, Training Loss: 0.002127359013834618, Test Loss: 0.002425765439684617\n",
            "Epoch 290/1000, Training Loss: 0.0021160390436469473, Test Loss: 0.0024121698016925895\n",
            "Epoch 291/1000, Training Loss: 0.0021045171679458323, Test Loss: 0.002398326766553197\n",
            "Epoch 292/1000, Training Loss: 0.0020927894713389603, Test Loss: 0.0023842317524042796\n",
            "Epoch 293/1000, Training Loss: 0.0020808520996046364, Test Loss: 0.002369880272862449\n",
            "Epoch 294/1000, Training Loss: 0.0020687012766524397, Test Loss: 0.00235526795849757\n",
            "Epoch 295/1000, Training Loss: 0.0020563333226303656, Test Loss: 0.002340390579694233\n",
            "Epoch 296/1000, Training Loss: 0.0020437446731749274, Test Loss: 0.002325244070891525\n",
            "Epoch 297/1000, Training Loss: 0.002030931899787426, Test Loss: 0.002309824556175634\n",
            "Epoch 298/1000, Training Loss: 0.0020178917313044943, Test Loss: 0.002294128376180782\n",
            "Epoch 299/1000, Training Loss: 0.0020046210764141153, Test Loss: 0.0022781521162325473\n",
            "Epoch 300/1000, Training Loss: 0.0019911170471495006, Test Loss: 0.0022618926356438055\n",
            "Epoch 301/1000, Training Loss: 0.0019773769832729664, Test Loss: 0.002245347098047772\n",
            "Epoch 302/1000, Training Loss: 0.001963398477440226, Test Loss: 0.0022285130026249425\n",
            "Epoch 303/1000, Training Loss: 0.0019491794010127745, Test Loss: 0.002211388216051705\n",
            "Epoch 304/1000, Training Loss: 0.0019347179303626863, Test Loss: 0.0021939710049685207\n",
            "Epoch 305/1000, Training Loss: 0.0019200125734907172, Test Loss: 0.002176260068735479\n",
            "Epoch 306/1000, Training Loss: 0.0019050621967554757, Test Loss: 0.002158254572213369\n",
            "Epoch 307/1000, Training Loss: 0.0018898660514895934, Test Loss: 0.0021399541782803145\n",
            "Epoch 308/1000, Training Loss: 0.001874423800258605, Test Loss: 0.00212135907976803\n",
            "Epoch 309/1000, Training Loss: 0.0018587355425006668, Test Loss: 0.0021024700304791565\n",
            "Epoch 310/1000, Training Loss: 0.0018428018392708653, Test Loss: 0.0020832883749287625\n",
            "Epoch 311/1000, Training Loss: 0.0018266237368033589, Test Loss: 0.0020638160764398132\n",
            "Epoch 312/1000, Training Loss: 0.0018102027885988042, Test Loss: 0.0020440557432153685\n",
            "Epoch 313/1000, Training Loss: 0.001793541075743625, Test Loss: 0.0020240106520097016\n",
            "Epoch 314/1000, Training Loss: 0.001776641225172426, Test Loss: 0.0020036847690275794\n",
            "Epoch 315/1000, Training Loss: 0.0017595064255952306, Test Loss: 0.0019830827676953536\n",
            "Epoch 316/1000, Training Loss: 0.0017421404408273656, Test Loss: 0.001962210042969893\n",
            "Epoch 317/1000, Training Loss: 0.001724547620281557, Test Loss: 0.0019410727218809628\n",
            "Epoch 318/1000, Training Loss: 0.0017067329064088287, Test Loss: 0.0019196776700396443\n",
            "Epoch 319/1000, Training Loss: 0.0016887018389064603, Test Loss: 0.001898032493888295\n",
            "Epoch 320/1000, Training Loss: 0.0016704605555468777, Test Loss: 0.0018761455385160658\n",
            "Epoch 321/1000, Training Loss: 0.0016520157895202284, Test Loss: 0.0018540258809164622\n",
            "Epoch 322/1000, Training Loss: 0.0016333748632241351, Test Loss: 0.0018316833186187934\n",
            "Epoch 323/1000, Training Loss: 0.0016145456784761921, Test Loss: 0.0018091283536820937\n",
            "Epoch 324/1000, Training Loss: 0.0015955367031668993, Test Loss: 0.0017863721720969721\n",
            "Epoch 325/1000, Training Loss: 0.001576356954411744, Test Loss: 0.0017634266186961748\n",
            "Epoch 326/1000, Training Loss: 0.0015570159783006238, Test Loss: 0.0017403041677276374\n",
            "Epoch 327/1000, Training Loss: 0.0015375238263794508, Test Loss: 0.0017170178892930697\n",
            "Epoch 328/1000, Training Loss: 0.001517891029032386, Test Loss: 0.0016935814118999154\n",
            "Epoch 329/1000, Training Loss: 0.0014981285659629, Test Loss: 0.0016700088814142421\n",
            "Epoch 330/1000, Training Loss: 0.0014782478339978053, Test Loss: 0.0016463149167363117\n",
            "Epoch 331/1000, Training Loss: 0.0014582606124601315, Test Loss: 0.0016225145625490653\n",
            "Epoch 332/1000, Training Loss: 0.0014381790263743996, Test Loss: 0.0015986232395125391\n",
            "Epoch 333/1000, Training Loss: 0.0014180155077815589, Test Loss: 0.0015746566922944009\n",
            "Epoch 334/1000, Training Loss: 0.001397782755450907, Test Loss: 0.001550630935838764\n",
            "Epoch 335/1000, Training Loss: 0.001377493693282777, Test Loss: 0.0015265622002823185\n",
            "Epoch 336/1000, Training Loss: 0.0013571614276993267, Test Loss: 0.0015024668749293395\n",
            "Epoch 337/1000, Training Loss: 0.001336799204321248, Test Loss: 0.0014783614516953112\n",
            "Epoch 338/1000, Training Loss: 0.0013164203642264392, Test Loss: 0.0014542624684233685\n",
            "Epoch 339/1000, Training Loss: 0.0012960383000823737, Test Loss: 0.001430186452468879\n",
            "Epoch 340/1000, Training Loss: 0.001275666412437792, Test Loss: 0.001406149864935325\n",
            "Epoch 341/1000, Training Loss: 0.0012553180664512111, Test Loss: 0.00138216904592975\n",
            "Epoch 342/1000, Training Loss: 0.0012350065493238647, Test Loss: 0.0013582601611885027\n",
            "Epoch 343/1000, Training Loss: 0.0012147450286932565, Test Loss: 0.001334439150403933\n",
            "Epoch 344/1000, Training Loss: 0.0011945465122303396, Test Loss: 0.0013107216775604964\n",
            "Epoch 345/1000, Training Loss: 0.0011744238086688235, Test Loss: 0.0012871230835644271\n",
            "Epoch 346/1000, Training Loss: 0.0011543894904789962, Test Loss: 0.0012636583414249647\n",
            "Epoch 347/1000, Training Loss: 0.0011344558583809916, Test Loss: 0.0012403420142172404\n",
            "Epoch 348/1000, Training Loss: 0.0011146349078736742, Test Loss: 0.0012171882160276677\n",
            "Epoch 349/1000, Training Loss: 0.0010949382979354872, Test Loss: 0.0011942105760523892\n",
            "Epoch 350/1000, Training Loss: 0.0010753773220326338, Test Loss: 0.0011714222059879703\n",
            "Epoch 351/1000, Training Loss: 0.0010559628815482044, Test Loss: 0.001148835670821808\n",
            "Epoch 352/1000, Training Loss: 0.0010367054617236276, Test Loss: 0.0011264629630979004\n",
            "Epoch 353/1000, Training Loss: 0.0010176151101810897, Test Loss: 0.001104315480701884\n",
            "Epoch 354/1000, Training Loss: 0.0009987014180727993, Test Loss: 0.001082404008178201\n",
            "Epoch 355/1000, Training Loss: 0.0009799735038804748, Test Loss: 0.0010607387015622125\n",
            "Epoch 356/1000, Training Loss: 0.0009614399998662446, Test Loss: 0.0010393290766812578\n",
            "Epoch 357/1000, Training Loss: 0.0009431090411548931, Test Loss: 0.001018184000851685\n",
            "Epoch 358/1000, Training Loss: 0.0009249882574070252, Test Loss: 0.0009973116878737105\n",
            "Epoch 359/1000, Training Loss: 0.0009070847670235377, Test Loss: 0.000976719696203098\n",
            "Epoch 360/1000, Training Loss: 0.0008894051738043496, Test Loss: 0.0009564149301583161\n",
            "Epoch 361/1000, Training Loss: 0.0008719555659680836, Test Loss: 0.0009364036440038536\n",
            "Epoch 362/1000, Training Loss: 0.0008547415174254116, Test Loss: 0.0009166914487354076\n",
            "Epoch 363/1000, Training Loss: 0.0008377680911862324, Test Loss: 0.0008972833213801905\n",
            "Epoch 364/1000, Training Loss: 0.0008210398447705665, Test Loss: 0.0008781836166160446\n",
            "Epoch 365/1000, Training Loss: 0.0008045608374845414, Test Loss: 0.0008593960805061278\n",
            "Epoch 366/1000, Training Loss: 0.0007883346394164292, Test Loss: 0.000840923866141709\n",
            "Epoch 367/1000, Training Loss: 0.0007723643420030396, Test Loss: 0.0008227695509837551\n",
            "Epoch 368/1000, Training Loss: 0.0007566525700141312, Test Loss: 0.0008049351556945545\n",
            "Epoch 369/1000, Training Loss: 0.0007412014948014562, Test Loss: 0.0007874221642533148\n",
            "Epoch 370/1000, Training Loss: 0.0007260128486597179, Test Loss: 0.0007702315451541993\n",
            "Epoch 371/1000, Training Loss: 0.0007110879401488308, Test Loss: 0.0007533637734916635\n",
            "Epoch 372/1000, Training Loss: 0.0006964276702302763, Test Loss: 0.0007368188537455788\n",
            "Epoch 373/1000, Training Loss: 0.0006820325490750489, Test Loss: 0.0007205963430877853\n",
            "Epoch 374/1000, Training Loss: 0.0006679027134062197, Test Loss: 0.0007046953750416228\n",
            "Epoch 375/1000, Training Loss: 0.0006540379442457115, Test Loss: 0.0006891146833368405\n",
            "Epoch 376/1000, Training Loss: 0.0006404376849420024, Test Loss: 0.0006738526258136578\n",
            "Epoch 377/1000, Training Loss: 0.0006271010593631641, Test Loss: 0.0006589072082414577\n",
            "Epoch 378/1000, Training Loss: 0.0006140268901478573, Test Loss: 0.0006442761079296684\n",
            "Epoch 379/1000, Training Loss: 0.0006012137169151163, Test Loss: 0.0006299566970201973\n",
            "Epoch 380/1000, Training Loss: 0.0005886598143423588, Test Loss: 0.0006159460653627857\n",
            "Epoch 381/1000, Training Loss: 0.0005763632100295988, Test Loss: 0.000602241042886257\n",
            "Epoch 382/1000, Training Loss: 0.0005643217020761306, Test Loss: 0.0005888382213897849\n",
            "Epoch 383/1000, Training Loss: 0.0005525328763044663, Test Loss: 0.0005757339756893619\n",
            "Epoch 384/1000, Training Loss: 0.0005409941230740979, Test Loss: 0.0005629244840646594\n",
            "Epoch 385/1000, Training Loss: 0.0005297026536356414, Test Loss: 0.0005504057479615289\n",
            "Epoch 386/1000, Training Loss: 0.0005186555159831051, Test Loss: 0.0005381736109142591\n",
            "Epoch 387/1000, Training Loss: 0.0005078496101691828, Test Loss: 0.0005262237766603274\n",
            "Epoch 388/1000, Training Loss: 0.0004972817030549283, Test Loss: 0.0005145518264280999\n",
            "Epoch 389/1000, Training Loss: 0.0004869484424714313, Test Loss: 0.0005031532353852045\n",
            "Epoch 390/1000, Training Loss: 0.0004768463707767003, Test Loss: 0.0004920233882416534\n",
            "Epoch 391/1000, Training Loss: 0.0004669719377962251, Test Loss: 0.000481157594007693\n",
            "Epoch 392/1000, Training Loss: 0.0004573215131405002, Test Loss: 0.0004705510999115884\n",
            "Epoch 393/1000, Training Loss: 0.000447891397897045, Test Loss: 0.00046019910448707096\n",
            "Epoch 394/1000, Training Loss: 0.0004386778356983889, Test Loss: 0.0004500967698442229\n",
            "Epoch 395/1000, Training Loss: 0.0004296770231709389, Test Loss: 0.00044023923314108373\n",
            "Epoch 396/1000, Training Loss: 0.0004208851197726858, Test Loss: 0.00043062161727612633\n",
            "Epoch 397/1000, Training Loss: 0.00041229825703036006, Test Loss: 0.00042123904082426846\n",
            "Epoch 398/1000, Training Loss: 0.0004039125471890327, Test Loss: 0.0004120866272411643\n",
            "Epoch 399/1000, Training Loss: 0.0003957240912889591, Test Loss: 0.0004031595133619992\n",
            "Epoch 400/1000, Training Loss: 0.00038772898668629636, Test Loss: 0.00039445285722244434\n",
            "Epoch 401/1000, Training Loss: 0.00037992333403550954, Test Loss: 0.00038596184523017\n",
            "Epoch 402/1000, Training Loss: 0.0003723032437524588, Test Loss: 0.00037768169871605544\n",
            "Epoch 403/1000, Training Loss: 0.0003648648419779779, Test Loss: 0.00036960767989450033\n",
            "Epoch 404/1000, Training Loss: 0.00035760427606232797, Test Loss: 0.0003617350972623956\n",
            "Epoch 405/1000, Training Loss: 0.0003505177195913492, Test Loss: 0.0003540593104661451\n",
            "Epoch 406/1000, Training Loss: 0.0003436013769754008, Test Loss: 0.0003465757346659405\n",
            "Epoch 407/1000, Training Loss: 0.00033685148762213824, Test Loss: 0.0003392798444259062\n",
            "Epoch 408/1000, Training Loss: 0.0003302643297142556, Test Loss: 0.00033216717715832346\n",
            "Epoch 409/1000, Training Loss: 0.00032383622361299583, Test Loss: 0.00032523333614929375\n",
            "Epoch 410/1000, Training Loss: 0.0003175635349080201, Test Loss: 0.0003184739931925172\n",
            "Epoch 411/1000, Training Loss: 0.0003114426771337904, Test Loss: 0.0003118848908569677\n",
            "Epoch 412/1000, Training Loss: 0.00030547011417220106, Test Loss: 0.00030546184441336186\n",
            "Epoch 413/1000, Training Loss: 0.0002996423623605781, Test Loss: 0.00029920074344328475\n",
            "Epoch 414/1000, Training Loss: 0.0002939559923237043, Test Loss: 0.0002930975531539454\n",
            "Epoch 415/1000, Training Loss: 0.00028840763054779965, Test Loss: 0.00028714831542045406\n",
            "Epoch 416/1000, Training Loss: 0.00028299396071375534, Test Loss: 0.0002813491495764675\n",
            "Epoch 417/1000, Training Loss: 0.00027771172480630984, Test Loss: 0.0002756962529731317\n",
            "Epoch 418/1000, Training Loss: 0.0002725577240150554, Test Loss: 0.00027018590132512287\n",
            "Epoch 419/1000, Training Loss: 0.00026752881944250626, Test Loss: 0.00026481444886159995\n",
            "Epoch 420/1000, Training Loss: 0.00026262193263380427, Test Loss: 0.00025957832829897974\n",
            "Epoch 421/1000, Training Loss: 0.0002578340459417946, Test Loss: 0.00025447405065132404\n",
            "Epoch 422/1000, Training Loss: 0.0002531622027406846, Test Loss: 0.00024949820489336526\n",
            "Epoch 423/1000, Training Loss: 0.0002486035075006151, Test Loss: 0.000244647457490095\n",
            "Epoch 424/1000, Training Loss: 0.00024415512573497345, Test Loss: 0.00023991855180614067\n",
            "Epoch 425/1000, Training Loss: 0.0002398142838314802, Test Loss: 0.00023530830740714682\n",
            "Epoch 426/1000, Training Loss: 0.00023557826877751602, Test Loss: 0.00023081361926466652\n",
            "Epoch 427/1000, Training Loss: 0.00023144442778949087, Test Loss: 0.0002264314568752435\n",
            "Epoch 428/1000, Training Loss: 0.00022741016785549341, Test Loss: 0.0002221588633036217\n",
            "Epoch 429/1000, Training Loss: 0.00022347295519982742, Test Loss: 0.00021799295415932395\n",
            "Epoch 430/1000, Training Loss: 0.000219630314677527, Test Loss: 0.00021393091651513156\n",
            "Epoch 431/1000, Training Loss: 0.00021587982910638688, Test Loss: 0.00020997000777541895\n",
            "Epoch 432/1000, Training Loss: 0.000212219138543536, Test Loss: 0.00020610755450161362\n",
            "Epoch 433/1000, Training Loss: 0.00020864593951310233, Test Loss: 0.00020234095120154962\n",
            "Epoch 434/1000, Training Loss: 0.00020515798419103572, Test Loss: 0.00019866765908888528\n",
            "Epoch 435/1000, Training Loss: 0.00020175307955274063, Test Loss: 0.0001950852048183053\n",
            "Epoch 436/1000, Training Loss: 0.00019842908648872996, Test Loss: 0.00019159117920169105\n",
            "Epoch 437/1000, Training Loss: 0.00019518391889312902, Test Loss: 0.00018818323591005646\n",
            "Epoch 438/1000, Training Loss: 0.00019201554272948535, Test Loss: 0.00018485909016557195\n",
            "Epoch 439/1000, Training Loss: 0.00018892197507797845, Test Loss: 0.00018161651742764772\n",
            "Epoch 440/1000, Training Loss: 0.00018590128316780724, Test Loss: 0.00017845335207667244\n",
            "Epoch 441/1000, Training Loss: 0.0001829515833982042, Test Loss: 0.00017536748609863807\n",
            "Epoch 442/1000, Training Loss: 0.00018007104035125407, Test Loss: 0.00017235686777361254\n",
            "Epoch 443/1000, Training Loss: 0.000177257865799405, Test Loss: 0.00016941950037068824\n",
            "Epoch 444/1000, Training Loss: 0.00017451031771030238, Test Loss: 0.000166553440851767\n",
            "Epoch 445/1000, Training Loss: 0.0001718266992513513, Test Loss: 0.00016375679858630522\n",
            "Epoch 446/1000, Training Loss: 0.00016920535779618059, Test Loss: 0.00016102773407890392\n",
            "Epoch 447/1000, Training Loss: 0.0001666446839349619, Test Loss: 0.00015836445771138426\n",
            "Epoch 448/1000, Training Loss: 0.00016414311049037458, Test Loss: 0.0001557652285008475\n",
            "Epoch 449/1000, Training Loss: 0.000161699111540793, Test Loss: 0.00015322835287497058\n",
            "Epoch 450/1000, Training Loss: 0.00015931120145211333, Test Loss: 0.00015075218346564413\n",
            "Epoch 451/1000, Training Loss: 0.00015697793391951639, Test Loss: 0.00014833511792194587\n",
            "Epoch 452/1000, Training Loss: 0.00015469790102025808, Test Loss: 0.00014597559774322027\n",
            "Epoch 453/1000, Training Loss: 0.0001524697322784971, Test Loss: 0.00014367210713295495\n",
            "Epoch 454/1000, Training Loss: 0.00015029209374303585, Test Loss: 0.00014142317187405048\n",
            "Epoch 455/1000, Training Loss: 0.00014816368707870563, Test Loss: 0.00013922735822587557\n",
            "Epoch 456/1000, Training Loss: 0.0001460832486720701, Test Loss: 0.0001370832718435229\n",
            "Epoch 457/1000, Training Loss: 0.00014404954875200116, Test Loss: 0.0001349895567194945\n",
            "Epoch 458/1000, Training Loss: 0.00014206139052557972, Test Loss: 0.00013294489414800156\n",
            "Epoch 459/1000, Training Loss: 0.00014011760932972544, Test Loss: 0.00013094800171200592\n",
            "Epoch 460/1000, Training Loss: 0.00013821707179886446, Test Loss: 0.00012899763229302257\n",
            "Epoch 461/1000, Training Loss: 0.00013635867504887248, Test Loss: 0.00012709257310367921\n",
            "Epoch 462/1000, Training Loss: 0.00013454134587749064, Test Loss: 0.00012523164474295825\n",
            "Epoch 463/1000, Training Loss: 0.00013276403998134373, Test Loss: 0.00012341370027399965\n",
            "Epoch 464/1000, Training Loss: 0.00013102574118962557, Test Loss: 0.0001216376243242998\n",
            "Epoch 465/1000, Training Loss: 0.00012932546071449616, Test Loss: 0.00011990233220810767\n",
            "Epoch 466/1000, Training Loss: 0.00012766223641818086, Test Loss: 0.00011820676907078149\n",
            "Epoch 467/1000, Training Loss: 0.00012603513209672238, Test Loss: 0.00011654990905484895\n",
            "Epoch 468/1000, Training Loss: 0.0001244432367803032, Test Loss: 0.00011493075448746464\n",
            "Epoch 469/1000, Training Loss: 0.0001228856640500335, Test Loss: 0.00011334833508896288\n",
            "Epoch 470/1000, Training Loss: 0.00012136155137106627, Test Loss: 0.00011180170720217655\n",
            "Epoch 471/1000, Training Loss: 0.00011987005944188032, Test Loss: 0.00011028995304216339\n",
            "Epoch 472/1000, Training Loss: 0.00011841037155954802, Test Loss: 0.00010881217996599026\n",
            "Epoch 473/1000, Training Loss: 0.00011698169300078533, Test Loss: 0.00010736751976218078\n",
            "Epoch 474/1000, Training Loss: 0.00011558325041856923, Test Loss: 0.00010595512795946123\n",
            "Epoch 475/1000, Training Loss: 0.00011421429125409895, Test Loss: 0.00010457418315441056\n",
            "Epoch 476/1000, Training Loss: 0.0001128740831638447, Test Loss: 0.00010322388635761187\n",
            "Epoch 477/1000, Training Loss: 0.00011156191346144564, Test Loss: 0.00010190346035791045\n",
            "Epoch 478/1000, Training Loss: 0.00011027708857418469, Test Loss: 0.00010061214910437052\n",
            "Epoch 479/1000, Training Loss: 0.0001090189335137718, Test Loss: 9.934921710553011e-05\n",
            "Epoch 480/1000, Training Loss: 0.00010778679136117745, Test Loss: 9.811394884555706e-05\n",
            "Epoch 481/1000, Training Loss: 0.00010658002276520731, Test Loss: 9.69056482168811e-05\n",
            "Epoch 482/1000, Training Loss: 0.00010539800545456849, Test Loss: 9.572363796892831e-05\n",
            "Epoch 483/1000, Training Loss: 0.0001042401337631128, Test Loss: 9.456725917253404e-05\n",
            "Epoch 484/1000, Training Loss: 0.0001031058181679945, Test Loss: 9.343587069966024e-05\n",
            "Epoch 485/1000, Training Loss: 0.000101994484840441, Test Loss: 9.232884871801224e-05\n",
            "Epoch 486/1000, Training Loss: 0.00010090557520885542, Test Loss: 9.12455862001737e-05\n",
            "Epoch 487/1000, Training Loss: 9.983854553396018e-05, Test Loss: 9.018549244687854e-05\n",
            "Epoch 488/1000, Training Loss: 9.879286649570475e-05, Test Loss: 8.914799262404677e-05\n",
            "Epoch 489/1000, Training Loss: 9.776802279164451e-05, Test Loss: 8.813252731320589e-05\n",
            "Epoch 490/1000, Training Loss: 9.676351274652115e-05, Test Loss: 8.713855207494788e-05\n",
            "Epoch 491/1000, Training Loss: 9.577884793275407e-05, Test Loss: 8.616553702505274e-05\n",
            "Epoch 492/1000, Training Loss: 9.481355280158579e-05, Test Loss: 8.521296642293825e-05\n",
            "Epoch 493/1000, Training Loss: 9.386716432459084e-05, Test Loss: 8.428033827208603e-05\n",
            "Epoch 494/1000, Training Loss: 9.293923164529535e-05, Test Loss: 8.336716393211174e-05\n",
            "Epoch 495/1000, Training Loss: 9.202931574064108e-05, Test Loss: 8.247296774215393e-05\n",
            "Epoch 496/1000, Training Loss: 9.113698909202667e-05, Test Loss: 8.159728665524575e-05\n",
            "Epoch 497/1000, Training Loss: 9.026183536568911e-05, Test Loss: 8.073966988338127e-05\n",
            "Epoch 498/1000, Training Loss: 8.94034491021563e-05, Test Loss: 7.98996785529395e-05\n",
            "Epoch 499/1000, Training Loss: 8.856143541454205e-05, Test Loss: 7.907688537019379e-05\n",
            "Epoch 500/1000, Training Loss: 8.773540969543407e-05, Test Loss: 7.827087429659687e-05\n",
            "Epoch 501/1000, Training Loss: 8.692499733213888e-05, Test Loss: 7.748124023356051e-05\n",
            "Epoch 502/1000, Training Loss: 8.612983343006541e-05, Test Loss: 7.670758871646299e-05\n",
            "Epoch 503/1000, Training Loss: 8.534956254400048e-05, Test Loss: 7.594953561759063e-05\n",
            "Epoch 504/1000, Training Loss: 8.458383841708806e-05, Test Loss: 7.520670685778568e-05\n",
            "Epoch 505/1000, Training Loss: 8.38323237272557e-05, Test Loss: 7.447873812650026e-05\n",
            "Epoch 506/1000, Training Loss: 8.309468984091829e-05, Test Loss: 7.376527461004733e-05\n",
            "Epoch 507/1000, Training Loss: 8.237061657372952e-05, Test Loss: 7.306597072778271e-05\n",
            "Epoch 508/1000, Training Loss: 8.165979195818811e-05, Test Loss: 7.23804898759885e-05\n",
            "Epoch 509/1000, Training Loss: 8.096191201790271e-05, Test Loss: 7.170850417922633e-05\n",
            "Epoch 510/1000, Training Loss: 8.027668054832585e-05, Test Loss: 7.104969424894582e-05\n",
            "Epoch 511/1000, Training Loss: 7.960380890377376e-05, Test Loss: 7.040374894911995e-05\n",
            "Epoch 512/1000, Training Loss: 7.894301579054574e-05, Test Loss: 6.977036516870355e-05\n",
            "Epoch 513/1000, Training Loss: 7.829402706597425e-05, Test Loss: 6.914924760071432e-05\n",
            "Epoch 514/1000, Training Loss: 7.765657554322337e-05, Test Loss: 6.854010852772362e-05\n",
            "Epoch 515/1000, Training Loss: 7.703040080168569e-05, Test Loss: 6.794266761358335e-05\n",
            "Epoch 516/1000, Training Loss: 7.641524900280017e-05, Test Loss: 6.735665170118952e-05\n",
            "Epoch 517/1000, Training Loss: 7.581087271113787e-05, Test Loss: 6.678179461610028e-05\n",
            "Epoch 518/1000, Training Loss: 7.521703072060407e-05, Test Loss: 6.621783697583779e-05\n",
            "Epoch 519/1000, Training Loss: 7.463348788560917e-05, Test Loss: 6.566452600470594e-05\n",
            "Epoch 520/1000, Training Loss: 7.406001495705765e-05, Test Loss: 6.512161535394703e-05\n",
            "Epoch 521/1000, Training Loss: 7.349638842301617e-05, Test Loss: 6.458886492708549e-05\n",
            "Epoch 522/1000, Training Loss: 7.294239035392696e-05, Test Loss: 6.40660407103016e-05\n",
            "Epoch 523/1000, Training Loss: 7.239780825222812e-05, Test Loss: 6.355291460767828e-05\n",
            "Epoch 524/1000, Training Loss: 7.186243490625597e-05, Test Loss: 6.304926428118376e-05\n",
            "Epoch 525/1000, Training Loss: 7.133606824830174e-05, Test Loss: 6.255487299523841e-05\n",
            "Epoch 526/1000, Training Loss: 7.08185112167011e-05, Test Loss: 6.206952946573612e-05\n",
            "Epoch 527/1000, Training Loss: 7.030957162183193e-05, Test Loss: 6.159302771337412e-05\n",
            "Epoch 528/1000, Training Loss: 6.980906201592295e-05, Test Loss: 6.112516692118272e-05\n",
            "Epoch 529/1000, Training Loss: 6.931679956653803e-05, Test Loss: 6.066575129610744e-05\n",
            "Epoch 530/1000, Training Loss: 6.883260593364631e-05, Test Loss: 6.0214589934533945e-05\n",
            "Epoch 531/1000, Training Loss: 6.83563071501574e-05, Test Loss: 5.977149669162873e-05\n",
            "Epoch 532/1000, Training Loss: 6.788773350584166e-05, Test Loss: 5.9336290054401934e-05\n",
            "Epoch 533/1000, Training Loss: 6.742671943450833e-05, Test Loss: 5.890879301835113e-05\n",
            "Epoch 534/1000, Training Loss: 6.697310340436698e-05, Test Loss: 5.848883296760132e-05\n",
            "Epoch 535/1000, Training Loss: 6.652672781147151e-05, Test Loss: 5.807624155843071e-05\n",
            "Epoch 536/1000, Training Loss: 6.608743887615411e-05, Test Loss: 5.767085460607697e-05\n",
            "Epoch 537/1000, Training Loss: 6.565508654236305e-05, Test Loss: 5.7272511974733804e-05\n",
            "Epoch 538/1000, Training Loss: 6.522952437982326e-05, Test Loss: 5.688105747063618e-05\n",
            "Epoch 539/1000, Training Loss: 6.481060948892444e-05, Test Loss: 5.6496338738142044e-05\n",
            "Epoch 540/1000, Training Loss: 6.439820240827522e-05, Test Loss: 5.6118207158732314e-05\n",
            "Epoch 541/1000, Training Loss: 6.399216702482872e-05, Test Loss: 5.574651775283118e-05\n",
            "Epoch 542/1000, Training Loss: 6.359237048651287e-05, Test Loss: 5.538112908436407e-05\n",
            "Epoch 543/1000, Training Loss: 6.319868311728899e-05, Test Loss: 5.5021903167979614e-05\n",
            "Epoch 544/1000, Training Loss: 6.281097833456889e-05, Test Loss: 5.46687053788528e-05\n",
            "Epoch 545/1000, Training Loss: 6.242913256892412e-05, Test Loss: 5.432140436498997e-05\n",
            "Epoch 546/1000, Training Loss: 6.205302518601332e-05, Test Loss: 5.397987196196933e-05\n",
            "Epoch 547/1000, Training Loss: 6.16825384106677e-05, Test Loss: 5.364398311003555e-05\n",
            "Epoch 548/1000, Training Loss: 6.13175572530751e-05, Test Loss: 5.331361577349176e-05\n",
            "Epoch 549/1000, Training Loss: 6.095796943699612e-05, Test Loss: 5.29886508623107e-05\n",
            "Epoch 550/1000, Training Loss: 6.060366532995362e-05, Test Loss: 5.266897215590849e-05\n",
            "Epoch 551/1000, Training Loss: 6.025453787533967e-05, Test Loss: 5.2354466229004494e-05\n",
            "Epoch 552/1000, Training Loss: 5.991048252638803e-05, Test Loss: 5.20450223795304e-05\n",
            "Epoch 553/1000, Training Loss: 5.957139718194725e-05, Test Loss: 5.174053255849782e-05\n",
            "Epoch 554/1000, Training Loss: 5.923718212401873e-05, Test Loss: 5.1440891301798794e-05\n",
            "Epoch 555/1000, Training Loss: 5.890773995699248e-05, Test Loss: 5.1145995663858495e-05\n",
            "Epoch 556/1000, Training Loss: 5.858297554854062e-05, Test Loss: 5.085574515309974e-05\n",
            "Epoch 557/1000, Training Loss: 5.8262795972117935e-05, Test Loss: 5.0570041669160003e-05\n",
            "Epoch 558/1000, Training Loss: 5.7947110451026695e-05, Test Loss: 5.028878944182071e-05\n",
            "Epoch 559/1000, Training Loss: 5.7635830303994545e-05, Test Loss: 5.0011894971582506e-05\n",
            "Epoch 560/1000, Training Loss: 5.732886889222882e-05, Test Loss: 4.973926697185982e-05\n",
            "Epoch 561/1000, Training Loss: 5.7026141567898916e-05, Test Loss: 4.947081631272724e-05\n",
            "Epoch 562/1000, Training Loss: 5.672756562401238e-05, Test Loss: 4.920645596619116e-05\n",
            "Epoch 563/1000, Training Loss: 5.6433060245635723e-05, Test Loss: 4.894610095293138e-05\n",
            "Epoch 564/1000, Training Loss: 5.614254646243145e-05, Test Loss: 4.8689668290472766e-05\n",
            "Epoch 565/1000, Training Loss: 5.585594710246739e-05, Test Loss: 4.843707694275376e-05\n",
            "Epoch 566/1000, Training Loss: 5.55731867472657e-05, Test Loss: 4.818824777104021e-05\n",
            "Epoch 567/1000, Training Loss: 5.5294191688050886e-05, Test Loss: 4.794310348615572e-05\n",
            "Epoch 568/1000, Training Loss: 5.501888988317107e-05, Test Loss: 4.7701568601985885e-05\n",
            "Epoch 569/1000, Training Loss: 5.474721091665185e-05, Test Loss: 4.7463569390222576e-05\n",
            "Epoch 570/1000, Training Loss: 5.44790859578532e-05, Test Loss: 4.722903383631247e-05\n",
            "Epoch 571/1000, Training Loss: 5.421444772220264e-05, Test Loss: 4.699789159657807e-05\n",
            "Epoch 572/1000, Training Loss: 5.395323043296418e-05, Test Loss: 4.677007395647157e-05\n",
            "Epoch 573/1000, Training Loss: 5.369536978402501e-05, Test Loss: 4.654551378993902e-05\n",
            "Epoch 574/1000, Training Loss: 5.344080290366246e-05, Test Loss: 4.632414551985475e-05\n",
            "Epoch 575/1000, Training Loss: 5.318946831927317e-05, Test Loss: 4.610590507950651e-05\n",
            "Epoch 576/1000, Training Loss: 5.29413059230248e-05, Test Loss: 4.589072987508675e-05\n",
            "Epoch 577/1000, Training Loss: 5.269625693841717e-05, Test Loss: 4.567855874918009e-05\n",
            "Epoch 578/1000, Training Loss: 5.2454263887717555e-05, Test Loss: 4.546933194520022e-05\n",
            "Epoch 579/1000, Training Loss: 5.221527056025248e-05, Test Loss: 4.526299107276936e-05\n",
            "Epoch 580/1000, Training Loss: 5.1979221981530676e-05, Test Loss: 4.505947907399759e-05\n",
            "Epoch 581/1000, Training Loss: 5.174606438316867e-05, Test Loss: 4.4858740190644566e-05\n",
            "Epoch 582/1000, Training Loss: 5.151574517360703e-05, Test Loss: 4.466071993214238e-05\n",
            "Epoch 583/1000, Training Loss: 5.1288212909582726e-05, Test Loss: 4.4465365044446455e-05\n",
            "Epoch 584/1000, Training Loss: 5.106341726834931e-05, Test Loss: 4.427262347970027e-05\n",
            "Epoch 585/1000, Training Loss: 5.084130902061536e-05, Test Loss: 4.4082444366688915e-05\n",
            "Epoch 586/1000, Training Loss: 5.062184000418452e-05, Test Loss: 4.3894777982055016e-05\n",
            "Epoch 587/1000, Training Loss: 5.04049630982785e-05, Test Loss: 4.370957572226563e-05\n",
            "Epoch 588/1000, Training Loss: 5.019063219852174e-05, Test Loss: 4.352679007629887e-05\n",
            "Epoch 589/1000, Training Loss: 4.9978802192573715e-05, Test Loss: 4.3346374599039005e-05\n",
            "Epoch 590/1000, Training Loss: 4.9769428936386603e-05, Test Loss: 4.3168283885356646e-05\n",
            "Epoch 591/1000, Training Loss: 4.956246923107425e-05, Test Loss: 4.299247354485846e-05\n",
            "Epoch 592/1000, Training Loss: 4.935788080037378e-05, Test Loss: 4.281890017727838e-05\n",
            "Epoch 593/1000, Training Loss: 4.91556222686868e-05, Test Loss: 4.2647521348512864e-05\n",
            "Epoch 594/1000, Training Loss: 4.895565313967902e-05, Test Loss: 4.247829556725735e-05\n",
            "Epoch 595/1000, Training Loss: 4.87579337754287e-05, Test Loss: 4.23111822622488e-05\n",
            "Epoch 596/1000, Training Loss: 4.856242537610681e-05, Test Loss: 4.214614176008848e-05\n",
            "Epoch 597/1000, Training Loss: 4.8369089960174134e-05, Test Loss: 4.198313526362959e-05\n",
            "Epoch 598/1000, Training Loss: 4.817789034507945e-05, Test Loss: 4.182212483091388e-05\n",
            "Epoch 599/1000, Training Loss: 4.798879012845088e-05, Test Loss: 4.166307335464845e-05\n",
            "Epoch 600/1000, Training Loss: 4.7801753669761527e-05, Test Loss: 4.150594454219915e-05\n",
            "Epoch 601/1000, Training Loss: 4.761674607245799e-05, Test Loss: 4.1350702896095544e-05\n",
            "Epoch 602/1000, Training Loss: 4.743373316654413e-05, Test Loss: 4.1197313695028875e-05\n",
            "Epoch 603/1000, Training Loss: 4.725268149159862e-05, Test Loss: 4.104574297533006e-05\n",
            "Epoch 604/1000, Training Loss: 4.707355828022264e-05, Test Loss: 4.0895957512912675e-05\n",
            "Epoch 605/1000, Training Loss: 4.68963314419028e-05, Test Loss: 4.074792480567879e-05\n",
            "Epoch 606/1000, Training Loss: 4.672096954728171e-05, Test Loss: 4.060161305636078e-05\n",
            "Epoch 607/1000, Training Loss: 4.654744181281781e-05, Test Loss: 4.0456991155800575e-05\n",
            "Epoch 608/1000, Training Loss: 4.637571808583046e-05, Test Loss: 4.0314028666644044e-05\n",
            "Epoch 609/1000, Training Loss: 4.620576882992225e-05, Test Loss: 4.01726958074491e-05\n",
            "Epoch 610/1000, Training Loss: 4.603756511075572e-05, Test Loss: 4.0032963437186284e-05\n",
            "Epoch 611/1000, Training Loss: 4.587107858219606e-05, Test Loss: 3.989480304013465e-05\n",
            "Epoch 612/1000, Training Loss: 4.5706281472787074e-05, Test Loss: 3.975818671114999e-05\n",
            "Epoch 613/1000, Training Loss: 4.554314657257086e-05, Test Loss: 3.9623087141300784e-05\n",
            "Epoch 614/1000, Training Loss: 4.538164722022698e-05, Test Loss: 3.948947760386433e-05\n",
            "Epoch 615/1000, Training Loss: 4.522175729053529e-05, Test Loss: 3.935733194066967e-05\n",
            "Epoch 616/1000, Training Loss: 4.5063451182147055e-05, Test Loss: 3.9226624548782334e-05\n",
            "Epoch 617/1000, Training Loss: 4.490670380565413e-05, Test Loss: 3.909733036751683e-05\n",
            "Epoch 618/1000, Training Loss: 4.475149057195651e-05, Test Loss: 3.896942486577285e-05\n",
            "Epoch 619/1000, Training Loss: 4.459778738091366e-05, Test Loss: 3.884288402968613e-05\n",
            "Epoch 620/1000, Training Loss: 4.4445570610272634e-05, Test Loss: 3.871768435058091e-05\n",
            "Epoch 621/1000, Training Loss: 4.429481710487031e-05, Test Loss: 3.8593802813223514e-05\n",
            "Epoch 622/1000, Training Loss: 4.414550416609836e-05, Test Loss: 3.8471216884365405e-05\n",
            "Epoch 623/1000, Training Loss: 4.399760954162543e-05, Test Loss: 3.8349904501567376e-05\n",
            "Epoch 624/1000, Training Loss: 4.385111141537158e-05, Test Loss: 3.822984406229827e-05\n",
            "Epoch 625/1000, Training Loss: 4.3705988397724034e-05, Test Loss: 3.811101441330371e-05\n",
            "Epoch 626/1000, Training Loss: 4.3562219515992556e-05, Test Loss: 3.799339484023409e-05\n",
            "Epoch 627/1000, Training Loss: 4.341978420509683e-05, Test Loss: 3.78769650575262e-05\n",
            "Epoch 628/1000, Training Loss: 4.3278662298478885e-05, Test Loss: 3.776170519853642e-05\n",
            "Epoch 629/1000, Training Loss: 4.313883401923513e-05, Test Loss: 3.76475958059104e-05\n",
            "Epoch 630/1000, Training Loss: 4.300027997146247e-05, Test Loss: 3.7534617822190194e-05\n",
            "Epoch 631/1000, Training Loss: 4.286298113181511e-05, Test Loss: 3.7422752580651994e-05\n",
            "Epoch 632/1000, Training Loss: 4.2726918841262507e-05, Test Loss: 3.7311981796363686e-05\n",
            "Epoch 633/1000, Training Loss: 4.259207479704691e-05, Test Loss: 3.720228755746442e-05\n",
            "Epoch 634/1000, Training Loss: 4.245843104483331e-05, Test Loss: 3.7093652316652116e-05\n",
            "Epoch 635/1000, Training Loss: 4.232596997104749e-05, Test Loss: 3.698605888287922e-05\n",
            "Epoch 636/1000, Training Loss: 4.2194674295398214e-05, Test Loss: 3.6879490413250276e-05\n",
            "Epoch 637/1000, Training Loss: 4.206452706357929e-05, Test Loss: 3.677393040511696e-05\n",
            "Epoch 638/1000, Training Loss: 4.1935511640140334e-05, Test Loss: 3.6669362688360164e-05\n",
            "Epoch 639/1000, Training Loss: 4.180761170153792e-05, Test Loss: 3.6565771417863934e-05\n",
            "Epoch 640/1000, Training Loss: 4.168081122933806e-05, Test Loss: 3.646314106616558e-05\n",
            "Epoch 641/1000, Training Loss: 4.155509450359459e-05, Test Loss: 3.636145641628971e-05\n",
            "Epoch 642/1000, Training Loss: 4.143044609637422e-05, Test Loss: 3.626070255474721e-05\n",
            "Epoch 643/1000, Training Loss: 4.1306850865439237e-05, Test Loss: 3.616086486470886e-05\n",
            "Epoch 644/1000, Training Loss: 4.1184293948078996e-05, Test Loss: 3.6061929019339335e-05\n",
            "Epoch 645/1000, Training Loss: 4.106276075508751e-05, Test Loss: 3.596388097529238e-05\n",
            "Epoch 646/1000, Training Loss: 4.094223696488262e-05, Test Loss: 3.586670696636329e-05\n",
            "Epoch 647/1000, Training Loss: 4.082270851776303e-05, Test Loss: 3.5770393497290245e-05\n",
            "Epoch 648/1000, Training Loss: 4.0704161610301505e-05, Test Loss: 3.567492733770636e-05\n",
            "Epoch 649/1000, Training Loss: 4.058658268986915e-05, Test Loss: 3.55802955162359e-05\n",
            "Epoch 650/1000, Training Loss: 4.0469958449287304e-05, Test Loss: 3.5486485314730005e-05\n",
            "Epoch 651/1000, Training Loss: 4.035427582160616e-05, Test Loss: 3.5393484262641785e-05\n",
            "Epoch 652/1000, Training Loss: 4.0239521975004905e-05, Test Loss: 3.53012801315341e-05\n",
            "Epoch 653/1000, Training Loss: 4.012568430780881e-05, Test Loss: 3.520986092971699e-05\n",
            "Epoch 654/1000, Training Loss: 4.0012750443626e-05, Test Loss: 3.511921489701624e-05\n",
            "Epoch 655/1000, Training Loss: 3.990070822659354e-05, Test Loss: 3.502933049966031e-05\n",
            "Epoch 656/1000, Training Loss: 3.9789545716737744e-05, Test Loss: 3.494019642529388e-05\n",
            "Epoch 657/1000, Training Loss: 3.967925118543787e-05, Test Loss: 3.485180157810656e-05\n",
            "Epoch 658/1000, Training Loss: 3.956981311099951e-05, Test Loss: 3.476413507407757e-05\n",
            "Epoch 659/1000, Training Loss: 3.946122017432712e-05, Test Loss: 3.467718623633278e-05\n",
            "Epoch 660/1000, Training Loss: 3.935346125469518e-05, Test Loss: 3.459094459060938e-05\n",
            "Epoch 661/1000, Training Loss: 3.92465254256218e-05, Test Loss: 3.45053998608313e-05\n",
            "Epoch 662/1000, Training Loss: 3.9140401950833215e-05, Test Loss: 3.442054196478707e-05\n",
            "Epoch 663/1000, Training Loss: 3.903508028032116e-05, Test Loss: 3.4336361009906e-05\n",
            "Epoch 664/1000, Training Loss: 3.893055004649345e-05, Test Loss: 3.425284728913867e-05\n",
            "Epoch 665/1000, Training Loss: 3.8826801060409246e-05, Test Loss: 3.416999127693184e-05\n",
            "Epoch 666/1000, Training Loss: 3.8723823308102976e-05, Test Loss: 3.4087783625296916e-05\n",
            "Epoch 667/1000, Training Loss: 3.862160694699182e-05, Test Loss: 3.400621515997101e-05\n",
            "Epoch 668/1000, Training Loss: 3.8520142302364286e-05, Test Loss: 3.392527687667005e-05\n",
            "Epoch 669/1000, Training Loss: 3.841941986394798e-05, Test Loss: 3.384495993742391e-05\n",
            "Epoch 670/1000, Training Loss: 3.831943028255802e-05, Test Loss: 3.37652556670036e-05\n",
            "Epoch 671/1000, Training Loss: 3.8220164366818594e-05, Test Loss: 3.3686155549426234e-05\n",
            "Epoch 672/1000, Training Loss: 3.8121613079961435e-05, Test Loss: 3.36076512245456e-05\n",
            "Epoch 673/1000, Training Loss: 3.802376753669443e-05, Test Loss: 3.3529734484718614e-05\n",
            "Epoch 674/1000, Training Loss: 3.7926619000142656e-05, Test Loss: 3.345239727155117e-05\n",
            "Epoch 675/1000, Training Loss: 3.7830158878856884e-05, Test Loss: 3.337563167271902e-05\n",
            "Epoch 676/1000, Training Loss: 3.773437872389102e-05, Test Loss: 3.329942991886222e-05\n",
            "Epoch 677/1000, Training Loss: 3.7639270225943635e-05, Test Loss: 3.3223784380550756e-05\n",
            "Epoch 678/1000, Training Loss: 3.7544825212566125e-05, Test Loss: 3.314868756532234e-05\n",
            "Epoch 679/1000, Training Loss: 3.745103564542876e-05, Test Loss: 3.3074132114785585e-05\n",
            "Epoch 680/1000, Training Loss: 3.735789361765414e-05, Test Loss: 3.3000110801793145e-05\n",
            "Epoch 681/1000, Training Loss: 3.7265391351204694e-05, Test Loss: 3.29266165276776e-05\n",
            "Epoch 682/1000, Training Loss: 3.717352119433181e-05, Test Loss: 3.285364231955258e-05\n",
            "Epoch 683/1000, Training Loss: 3.708227561908042e-05, Test Loss: 3.278118132767335e-05\n",
            "Epoch 684/1000, Training Loss: 3.699164721884825e-05, Test Loss: 3.270922682286229e-05\n",
            "Epoch 685/1000, Training Loss: 3.690162870600192e-05, Test Loss: 3.263777219398882e-05\n",
            "Epoch 686/1000, Training Loss: 3.681221290954273e-05, Test Loss: 3.256681094551042e-05\n",
            "Epoch 687/1000, Training Loss: 3.672339277282632e-05, Test Loss: 3.249633669506709e-05\n",
            "Epoch 688/1000, Training Loss: 3.663516135133184e-05, Test Loss: 3.242634317113418e-05\n",
            "Epoch 689/1000, Training Loss: 3.6547511810479077e-05, Test Loss: 3.235682421072518e-05\n",
            "Epoch 690/1000, Training Loss: 3.646043742349607e-05, Test Loss: 3.228777375714903e-05\n",
            "Epoch 691/1000, Training Loss: 3.6373931569332185e-05, Test Loss: 3.2219185857818445e-05\n",
            "Epoch 692/1000, Training Loss: 3.628798773061705e-05, Test Loss: 3.21510546621065e-05\n",
            "Epoch 693/1000, Training Loss: 3.6202599491662716e-05, Test Loss: 3.208337441925374e-05\n",
            "Epoch 694/1000, Training Loss: 3.611776053651354e-05, Test Loss: 3.201613947632158e-05\n",
            "Epoch 695/1000, Training Loss: 3.603346464703419e-05, Test Loss: 3.1949344276193425e-05\n",
            "Epoch 696/1000, Training Loss: 3.594970570104193e-05, Test Loss: 3.188298335561765e-05\n",
            "Epoch 697/1000, Training Loss: 3.586647767047774e-05, Test Loss: 3.1817051343299184e-05\n",
            "Epoch 698/1000, Training Loss: 3.578377461961952e-05, Test Loss: 3.175154295803259e-05\n",
            "Epoch 699/1000, Training Loss: 3.5701590703331785e-05, Test Loss: 3.1686453006874154e-05\n",
            "Epoch 700/1000, Training Loss: 3.5619920165354034e-05, Test Loss: 3.162177638336019e-05\n",
            "Epoch 701/1000, Training Loss: 3.553875733662622e-05, Test Loss: 3.155750806576109e-05\n",
            "Epoch 702/1000, Training Loss: 3.545809663365019e-05, Test Loss: 3.149364311537805e-05\n",
            "Epoch 703/1000, Training Loss: 3.537793255688592e-05, Test Loss: 3.143017667487522e-05\n",
            "Epoch 704/1000, Training Loss: 3.529825968918331e-05, Test Loss: 3.13671039666496e-05\n",
            "Epoch 705/1000, Training Loss: 3.521907269424665e-05, Test Loss: 3.130442029124012e-05\n",
            "Epoch 706/1000, Training Loss: 3.5140366315131466e-05, Test Loss: 3.124212102576843e-05\n",
            "Epoch 707/1000, Training Loss: 3.506213537277532e-05, Test Loss: 3.118020162241628e-05\n",
            "Epoch 708/1000, Training Loss: 3.498437476455794e-05, Test Loss: 3.1118657606937636e-05\n",
            "Epoch 709/1000, Training Loss: 3.4907079462893415e-05, Test Loss: 3.105748457720235e-05\n",
            "Epoch 710/1000, Training Loss: 3.483024451385119e-05, Test Loss: 3.0996678201772925e-05\n",
            "Epoch 711/1000, Training Loss: 3.4753865035807095e-05, Test Loss: 3.09362342185124e-05\n",
            "Epoch 712/1000, Training Loss: 3.467793621812284e-05, Test Loss: 3.087614843322426e-05\n",
            "Epoch 713/1000, Training Loss: 3.4602453319853096e-05, Test Loss: 3.0816416718320195e-05\n",
            "Epoch 714/1000, Training Loss: 3.452741166847947e-05, Test Loss: 3.07570350115196e-05\n",
            "Epoch 715/1000, Training Loss: 3.4452806658671895e-05, Test Loss: 3.069799931457645e-05\n",
            "Epoch 716/1000, Training Loss: 3.437863375107625e-05, Test Loss: 3.06393056920344e-05\n",
            "Epoch 717/1000, Training Loss: 3.430488847112581e-05, Test Loss: 3.058095027000992e-05\n",
            "Epoch 718/1000, Training Loss: 3.423156640788001e-05, Test Loss: 3.052292923500055e-05\n",
            "Epoch 719/1000, Training Loss: 3.415866321288409e-05, Test Loss: 3.0465238832721334e-05\n",
            "Epoch 720/1000, Training Loss: 3.408617459905688e-05, Test Loss: 3.0407875366965358e-05\n",
            "Epoch 721/1000, Training Loss: 3.4014096339596955e-05, Test Loss: 3.0350835198489354e-05\n",
            "Epoch 722/1000, Training Loss: 3.394242426691489e-05, Test Loss: 3.0294114743923552e-05\n",
            "Epoch 723/1000, Training Loss: 3.3871154271587057e-05, Test Loss: 3.0237710474707073e-05\n",
            "Epoch 724/1000, Training Loss: 3.380028230132957e-05, Test Loss: 3.0181618916043173e-05\n",
            "Epoch 725/1000, Training Loss: 3.3729804359995455e-05, Test Loss: 3.0125836645879814e-05\n",
            "Epoch 726/1000, Training Loss: 3.3659716506589985e-05, Test Loss: 3.0070360293911654e-05\n",
            "Epoch 727/1000, Training Loss: 3.35900148543088e-05, Test Loss: 3.0015186540602294e-05\n",
            "Epoch 728/1000, Training Loss: 3.352069556959428e-05, Test Loss: 2.9960312116231275e-05\n",
            "Epoch 729/1000, Training Loss: 3.3451754871211175e-05, Test Loss: 2.9905733799956863e-05\n",
            "Epoch 730/1000, Training Loss: 3.338318902934218e-05, Test Loss: 2.9851448418902622e-05\n",
            "Epoch 731/1000, Training Loss: 3.331499436470044e-05, Test Loss: 2.979745284726192e-05\n",
            "Epoch 732/1000, Training Loss: 3.32471672476611e-05, Test Loss: 2.9743744005422747e-05\n",
            "Epoch 733/1000, Training Loss: 3.317970409741084e-05, Test Loss: 2.9690318859110074e-05\n",
            "Epoch 734/1000, Training Loss: 3.3112601381113e-05, Test Loss: 2.963717441854834e-05\n",
            "Epoch 735/1000, Training Loss: 3.304585561309155e-05, Test Loss: 2.9584307737638927e-05\n",
            "Epoch 736/1000, Training Loss: 3.2979463354030475e-05, Test Loss: 2.9531715913158802e-05\n",
            "Epoch 737/1000, Training Loss: 3.291342121018821e-05, Test Loss: 2.9479396083973193e-05\n",
            "Epoch 738/1000, Training Loss: 3.284772583263017e-05, Test Loss: 2.9427345430267328e-05\n",
            "Epoch 739/1000, Training Loss: 3.278237391647601e-05, Test Loss: 2.9375561172792184e-05\n",
            "Epoch 740/1000, Training Loss: 3.271736220015885e-05, Test Loss: 2.9324040572128115e-05\n",
            "Epoch 741/1000, Training Loss: 3.265268746470443e-05, Test Loss: 2.9272780927963226e-05\n",
            "Epoch 742/1000, Training Loss: 3.2588346533021e-05, Test Loss: 2.92217795783876e-05\n",
            "Epoch 743/1000, Training Loss: 3.252433626920324e-05, Test Loss: 2.9171033899200706e-05\n",
            "Epoch 744/1000, Training Loss: 3.2460653577853057e-05, Test Loss: 2.9120541303235507e-05\n",
            "Epoch 745/1000, Training Loss: 3.239729540340938e-05, Test Loss: 2.907029923969605e-05\n",
            "Epoch 746/1000, Training Loss: 3.23342587294959e-05, Test Loss: 2.9020305193508374e-05\n",
            "Epoch 747/1000, Training Loss: 3.2271540578277444e-05, Test Loss: 2.8970556684684466e-05\n",
            "Epoch 748/1000, Training Loss: 3.22091380098309e-05, Test Loss: 2.892105126770131e-05\n",
            "Epoch 749/1000, Training Loss: 3.214704812152943e-05, Test Loss: 2.8871786530892593e-05\n",
            "Epoch 750/1000, Training Loss: 3.2085268047436523e-05, Test Loss: 2.8822760095850816e-05\n",
            "Epoch 751/1000, Training Loss: 3.2023794957713475e-05, Test Loss: 2.877396961684443e-05\n",
            "Epoch 752/1000, Training Loss: 3.196262605803757e-05, Test Loss: 2.872541278024624e-05\n",
            "Epoch 753/1000, Training Loss: 3.190175858903202e-05, Test Loss: 2.867708730397214e-05\n",
            "Epoch 754/1000, Training Loss: 3.1841189825706886e-05, Test Loss: 2.8628990936934326e-05\n",
            "Epoch 755/1000, Training Loss: 3.178091707690959e-05, Test Loss: 2.858112145850252e-05\n",
            "Epoch 756/1000, Training Loss: 3.172093768478813e-05, Test Loss: 2.8533476677979392e-05\n",
            "Epoch 757/1000, Training Loss: 3.166124902426351e-05, Test Loss: 2.8486054434084415e-05\n",
            "Epoch 758/1000, Training Loss: 3.1601848502511014e-05, Test Loss: 2.8438852594448632e-05\n",
            "Epoch 759/1000, Training Loss: 3.1542733558454056e-05, Test Loss: 2.839186905512157e-05\n",
            "Epoch 760/1000, Training Loss: 3.1483901662265315e-05, Test Loss: 2.8345101740085642e-05\n",
            "Epoch 761/1000, Training Loss: 3.14253503148782e-05, Test Loss: 2.82985486007823e-05\n",
            "Epoch 762/1000, Training Loss: 3.136707704750921e-05, Test Loss: 2.825220761564702e-05\n",
            "Epoch 763/1000, Training Loss: 3.130907942118567e-05, Test Loss: 2.8206076789653372e-05\n",
            "Epoch 764/1000, Training Loss: 3.125135502628573e-05, Test Loss: 2.8160154153867197e-05\n",
            "Epoch 765/1000, Training Loss: 3.119390148208517e-05, Test Loss: 2.81144377650091e-05\n",
            "Epoch 766/1000, Training Loss: 3.113671643631356e-05, Test Loss: 2.806892570502634e-05\n",
            "Epoch 767/1000, Training Loss: 3.1079797564718796e-05, Test Loss: 2.802361608067238e-05\n",
            "Epoch 768/1000, Training Loss: 3.102314257063926e-05, Test Loss: 2.7978507023095918e-05\n",
            "Epoch 769/1000, Training Loss: 3.096674918458294e-05, Test Loss: 2.7933596687436797e-05\n",
            "Epoch 770/1000, Training Loss: 3.0910615163817794e-05, Test Loss: 2.7888883252432586e-05\n",
            "Epoch 771/1000, Training Loss: 3.08547382919659e-05, Test Loss: 2.784436492002871e-05\n",
            "Epoch 772/1000, Training Loss: 3.079911637860648e-05, Test Loss: 2.7800039915001027e-05\n",
            "Epoch 773/1000, Training Loss: 3.074374725888763e-05, Test Loss: 2.7755906484582534e-05\n",
            "Epoch 774/1000, Training Loss: 3.068862879314288e-05, Test Loss: 2.77119628980983e-05\n",
            "Epoch 775/1000, Training Loss: 3.063375886651607e-05, Test Loss: 2.766820744660932e-05\n",
            "Epoch 776/1000, Training Loss: 3.057913538859348e-05, Test Loss: 2.7624638442559723e-05\n",
            "Epoch 777/1000, Training Loss: 3.0524756293041716e-05, Test Loss: 2.7581254219434887e-05\n",
            "Epoch 778/1000, Training Loss: 3.0470619537252102e-05, Test Loss: 2.7538053131424102e-05\n",
            "Epoch 779/1000, Training Loss: 3.041672310199286e-05, Test Loss: 2.7495033553089245e-05\n",
            "Epoch 780/1000, Training Loss: 3.0363064991064678e-05, Test Loss: 2.745219387904189e-05\n",
            "Epoch 781/1000, Training Loss: 3.03096432309675e-05, Test Loss: 2.7409532523624672e-05\n",
            "Epoch 782/1000, Training Loss: 3.025645587056796e-05, Test Loss: 2.736704792060062e-05\n",
            "Epoch 783/1000, Training Loss: 3.0203500980775353e-05, Test Loss: 2.7324738522847008e-05\n",
            "Epoch 784/1000, Training Loss: 3.015077665422437e-05, Test Loss: 2.728260280205598e-05\n",
            "Epoch 785/1000, Training Loss: 3.0098281004960602e-05, Test Loss: 2.724063924844068e-05\n",
            "Epoch 786/1000, Training Loss: 3.0046012168134135e-05, Test Loss: 2.7198846370446458e-05\n",
            "Epoch 787/1000, Training Loss: 2.9993968299698075e-05, Test Loss: 2.7157222694469467e-05\n",
            "Epoch 788/1000, Training Loss: 2.9942147576111537e-05, Test Loss: 2.7115766764577914e-05\n",
            "Epoch 789/1000, Training Loss: 2.989054819404837e-05, Test Loss: 2.7074477142240312e-05\n",
            "Epoch 790/1000, Training Loss: 2.9839168370110524e-05, Test Loss: 2.7033352406059345e-05\n",
            "Epoch 791/1000, Training Loss: 2.9788006340548434e-05, Test Loss: 2.6992391151508707e-05\n",
            "Epoch 792/1000, Training Loss: 2.973706036098276e-05, Test Loss: 2.6951591990677673e-05\n",
            "Epoch 793/1000, Training Loss: 2.9686328706134848e-05, Test Loss: 2.691095355201762e-05\n",
            "Epoch 794/1000, Training Loss: 2.963580966955849e-05, Test Loss: 2.687047448009579e-05\n",
            "Epoch 795/1000, Training Loss: 2.9585501563378706e-05, Test Loss: 2.6830153435351642e-05\n",
            "Epoch 796/1000, Training Loss: 2.9535402718034272e-05, Test Loss: 2.678998909385943e-05\n",
            "Epoch 797/1000, Training Loss: 2.9485511482023957e-05, Test Loss: 2.674998014709337e-05\n",
            "Epoch 798/1000, Training Loss: 2.9435826221658176e-05, Test Loss: 2.6710125301699755e-05\n",
            "Epoch 799/1000, Training Loss: 2.938634532081393e-05, Test Loss: 2.6670423279270416e-05\n",
            "Epoch 800/1000, Training Loss: 2.9337067180695435e-05, Test Loss: 2.663087281612183e-05\n",
            "Epoch 801/1000, Training Loss: 2.928799021959661e-05, Test Loss: 2.6591472663078965e-05\n",
            "Epoch 802/1000, Training Loss: 2.923911287267036e-05, Test Loss: 2.6552221585262115e-05\n",
            "Epoch 803/1000, Training Loss: 2.9190433591698282e-05, Test Loss: 2.6513118361876708e-05\n",
            "Epoch 804/1000, Training Loss: 2.914195084486798e-05, Test Loss: 2.6474161786009246e-05\n",
            "Epoch 805/1000, Training Loss: 2.909366311655067e-05, Test Loss: 2.6435350664425144e-05\n",
            "Epoch 806/1000, Training Loss: 2.9045568907086127e-05, Test Loss: 2.639668381737091e-05\n",
            "Epoch 807/1000, Training Loss: 2.8997666732567634e-05, Test Loss: 2.6358160078380792e-05\n",
            "Epoch 808/1000, Training Loss: 2.8949955124632868e-05, Test Loss: 2.6319778294083275e-05\n",
            "Epoch 809/1000, Training Loss: 2.890243263025695e-05, Test Loss: 2.6281537324017587e-05\n",
            "Epoch 810/1000, Training Loss: 2.885509781155022e-05, Test Loss: 2.6243436040445934e-05\n",
            "Epoch 811/1000, Training Loss: 2.880794924555845e-05, Test Loss: 2.6205473328175046e-05\n",
            "Epoch 812/1000, Training Loss: 2.8760985524066613e-05, Test Loss: 2.6167648084377856e-05\n",
            "Epoch 813/1000, Training Loss: 2.8714205253405008e-05, Test Loss: 2.6129959218419507e-05\n",
            "Epoch 814/1000, Training Loss: 2.8667607054260018e-05, Test Loss: 2.609240565168569e-05\n",
            "Epoch 815/1000, Training Loss: 2.862118956148756e-05, Test Loss: 2.605498631741401e-05\n",
            "Epoch 816/1000, Training Loss: 2.8574951423928765e-05, Test Loss: 2.6017700160530392e-05\n",
            "Epoch 817/1000, Training Loss: 2.8528891304229067e-05, Test Loss: 2.5980546137483685e-05\n",
            "Epoch 818/1000, Training Loss: 2.8483007878660362e-05, Test Loss: 2.5943523216089604e-05\n",
            "Epoch 819/1000, Training Loss: 2.8437299836946418e-05, Test Loss: 2.590663037537065e-05\n",
            "Epoch 820/1000, Training Loss: 2.8391765882089793e-05, Test Loss: 2.5869866605404478e-05\n",
            "Epoch 821/1000, Training Loss: 2.8346404730203168e-05, Test Loss: 2.583323090717142e-05\n",
            "Epoch 822/1000, Training Loss: 2.8301215110341885e-05, Test Loss: 2.5796722292405314e-05\n",
            "Epoch 823/1000, Training Loss: 2.8256195764340904e-05, Test Loss: 2.5760339783448496e-05\n",
            "Epoch 824/1000, Training Loss: 2.8211345446651632e-05, Test Loss: 2.572408241310663e-05\n",
            "Epoch 825/1000, Training Loss: 2.8166662924184366e-05, Test Loss: 2.568794922450884e-05\n",
            "Epoch 826/1000, Training Loss: 2.8122146976151057e-05, Test Loss: 2.5651939270967928e-05\n",
            "Epoch 827/1000, Training Loss: 2.8077796393911327e-05, Test Loss: 2.5616051615843703e-05\n",
            "Epoch 828/1000, Training Loss: 2.8033609980821374e-05, Test Loss: 2.5580285332409588e-05\n",
            "Epoch 829/1000, Training Loss: 2.7989586552084195e-05, Test Loss: 2.554463950372055e-05\n",
            "Epoch 830/1000, Training Loss: 2.7945724934602708e-05, Test Loss: 2.550911322248261e-05\n",
            "Epoch 831/1000, Training Loss: 2.7902023966835327e-05, Test Loss: 2.547370559092577e-05\n",
            "Epoch 832/1000, Training Loss: 2.7858482498653346e-05, Test Loss: 2.543841572067929e-05\n",
            "Epoch 833/1000, Training Loss: 2.7815099391200816e-05, Test Loss: 2.540324273264684e-05\n",
            "Epoch 834/1000, Training Loss: 2.7771873516756634e-05, Test Loss: 2.536818575688644e-05\n",
            "Epoch 835/1000, Training Loss: 2.7728803758597828e-05, Test Loss: 2.5333243932491312e-05\n",
            "Epoch 836/1000, Training Loss: 2.768588901086735e-05, Test Loss: 2.5298416407471507e-05\n",
            "Epoch 837/1000, Training Loss: 2.764312817843985e-05, Test Loss: 2.5263702338639363e-05\n",
            "Epoch 838/1000, Training Loss: 2.7600520176794047e-05, Test Loss: 2.522910089149538e-05\n",
            "Epoch 839/1000, Training Loss: 2.7558063931883018e-05, Test Loss: 2.5194611240117582e-05\n",
            "Epoch 840/1000, Training Loss: 2.7515758380009707e-05, Test Loss: 2.516023256705073e-05\n",
            "Epoch 841/1000, Training Loss: 2.7473602467701754e-05, Test Loss: 2.5125964063198186e-05\n",
            "Epoch 842/1000, Training Loss: 2.7431595151589942e-05, Test Loss: 2.509180492771676e-05\n",
            "Epoch 843/1000, Training Loss: 2.738973539828777e-05, Test Loss: 2.5057754367910895e-05\n",
            "Epoch 844/1000, Training Loss: 2.734802218427253e-05, Test Loss: 2.5023811599130106e-05\n",
            "Epoch 845/1000, Training Loss: 2.730645449576833e-05, Test Loss: 2.498997584466876e-05\n",
            "Epoch 846/1000, Training Loss: 2.7265031328632062e-05, Test Loss: 2.4956246335664482e-05\n",
            "Epoch 847/1000, Training Loss: 2.722375168823787e-05, Test Loss: 2.492262231100145e-05\n",
            "Epoch 848/1000, Training Loss: 2.7182614589367965e-05, Test Loss: 2.4889103017213492e-05\n",
            "Epoch 849/1000, Training Loss: 2.7141619056100337e-05, Test Loss: 2.4855687708389396e-05\n",
            "Epoch 850/1000, Training Loss: 2.7100764121701187e-05, Test Loss: 2.482237564607858e-05\n",
            "Epoch 851/1000, Training Loss: 2.7060048828518355e-05, Test Loss: 2.4789166099199867e-05\n",
            "Epoch 852/1000, Training Loss: 2.701947222787479e-05, Test Loss: 2.4756058343950566e-05\n",
            "Epoch 853/1000, Training Loss: 2.6979033379966184e-05, Test Loss: 2.4723051663717277e-05\n",
            "Epoch 854/1000, Training Loss: 2.6938731353757024e-05, Test Loss: 2.4690145348987958e-05\n",
            "Epoch 855/1000, Training Loss: 2.6898565226880622e-05, Test Loss: 2.465733869726574e-05\n",
            "Epoch 856/1000, Training Loss: 2.6858534085539764e-05, Test Loss: 2.4624631012983134e-05\n",
            "Epoch 857/1000, Training Loss: 2.6818637024407944e-05, Test Loss: 2.4592021607419448e-05\n",
            "Epoch 858/1000, Training Loss: 2.6778873146533554e-05, Test Loss: 2.455950979861705e-05\n",
            "Epoch 859/1000, Training Loss: 2.6739241563243884e-05, Test Loss: 2.4527094911300384e-05\n",
            "Epoch 860/1000, Training Loss: 2.669974139405139e-05, Test Loss: 2.4494776276796153e-05\n",
            "Epoch 861/1000, Training Loss: 2.666037176656179e-05, Test Loss: 2.446255323295421e-05\n",
            "Epoch 862/1000, Training Loss: 2.6621131816381766e-05, Test Loss: 2.4430425124070226e-05\n",
            "Epoch 863/1000, Training Loss: 2.658202068702895e-05, Test Loss: 2.439839130080928e-05\n",
            "Epoch 864/1000, Training Loss: 2.654303752984446e-05, Test Loss: 2.436645112012931e-05\n",
            "Epoch 865/1000, Training Loss: 2.650418150390378e-05, Test Loss: 2.4334603945208563e-05\n",
            "Epoch 866/1000, Training Loss: 2.6465451775931208e-05, Test Loss: 2.430284914537139e-05\n",
            "Epoch 867/1000, Training Loss: 2.6426847520214833e-05, Test Loss: 2.42711860960169e-05\n",
            "Epoch 868/1000, Training Loss: 2.6388367918521867e-05, Test Loss: 2.4239614178546606e-05\n",
            "Epoch 869/1000, Training Loss: 2.6350012160017137e-05, Test Loss: 2.4208132780296834e-05\n",
            "Epoch 870/1000, Training Loss: 2.631177944117985e-05, Test Loss: 2.4176741294467196e-05\n",
            "Epoch 871/1000, Training Loss: 2.627366896572438e-05, Test Loss: 2.4145439120054646e-05\n",
            "Epoch 872/1000, Training Loss: 2.623567994452e-05, Test Loss: 2.411422566178532e-05\n",
            "Epoch 873/1000, Training Loss: 2.619781159551283e-05, Test Loss: 2.408310033004951e-05\n",
            "Epoch 874/1000, Training Loss: 2.6160063143648554e-05, Test Loss: 2.4052062540836787e-05\n",
            "Epoch 875/1000, Training Loss: 2.612243382079566e-05, Test Loss: 2.4021111715669805e-05\n",
            "Epoch 876/1000, Training Loss: 2.6084922865671472e-05, Test Loss: 2.399024728154435e-05\n",
            "Epoch 877/1000, Training Loss: 2.604752952376594e-05, Test Loss: 2.3959468670864933e-05\n",
            "Epoch 878/1000, Training Loss: 2.6010253047270116e-05, Test Loss: 2.3928775321383584e-05\n",
            "Epoch 879/1000, Training Loss: 2.5973092695002983e-05, Test Loss: 2.389816667614052e-05\n",
            "Epoch 880/1000, Training Loss: 2.5936047732340785e-05, Test Loss: 2.3867642183403426e-05\n",
            "Epoch 881/1000, Training Loss: 2.589911743114583e-05, Test Loss: 2.3837201296609205e-05\n",
            "Epoch 882/1000, Training Loss: 2.5862301069698263e-05, Test Loss: 2.380684347430532e-05\n",
            "Epoch 883/1000, Training Loss: 2.5825597932625986e-05, Test Loss: 2.377656818009429e-05\n",
            "Epoch 884/1000, Training Loss: 2.578900731083838e-05, Test Loss: 2.374637488257471e-05\n",
            "Epoch 885/1000, Training Loss: 2.5752528501458766e-05, Test Loss: 2.3716263055288877e-05\n",
            "Epoch 886/1000, Training Loss: 2.5716160807759065e-05, Test Loss: 2.3686232176665023e-05\n",
            "Epoch 887/1000, Training Loss: 2.5679903539094535e-05, Test Loss: 2.3656281729965137e-05\n",
            "Epoch 888/1000, Training Loss: 2.5643756010839473e-05, Test Loss: 2.3626411203230887e-05\n",
            "Epoch 889/1000, Training Loss: 2.5607717544323943e-05, Test Loss: 2.3596620089231236e-05\n",
            "Epoch 890/1000, Training Loss: 2.557178746677222e-05, Test Loss: 2.3566907885411077e-05\n",
            "Epoch 891/1000, Training Loss: 2.5535965111239036e-05, Test Loss: 2.353727409383918e-05\n",
            "Epoch 892/1000, Training Loss: 2.5500249816550608e-05, Test Loss: 2.350771822115772e-05\n",
            "Epoch 893/1000, Training Loss: 2.5464640927243525e-05, Test Loss: 2.3478239778534255e-05\n",
            "Epoch 894/1000, Training Loss: 2.5429137793505745e-05, Test Loss: 2.3448838281610295e-05\n",
            "Epoch 895/1000, Training Loss: 2.539373977111753e-05, Test Loss: 2.3419513250454426e-05\n",
            "Epoch 896/1000, Training Loss: 2.535844622139433e-05, Test Loss: 2.3390264209513715e-05\n",
            "Epoch 897/1000, Training Loss: 2.5323256511129044e-05, Test Loss: 2.3361090687567125e-05\n",
            "Epoch 898/1000, Training Loss: 2.528817001253504e-05, Test Loss: 2.3331992217678812e-05\n",
            "Epoch 899/1000, Training Loss: 2.52531861031921e-05, Test Loss: 2.3302968337152187e-05\n",
            "Epoch 900/1000, Training Loss: 2.5218304165990028e-05, Test Loss: 2.3274018587484228e-05\n",
            "Epoch 901/1000, Training Loss: 2.5183523589075328e-05, Test Loss: 2.324514251432145e-05\n",
            "Epoch 902/1000, Training Loss: 2.5148843765795593e-05, Test Loss: 2.3216339667415335e-05\n",
            "Epoch 903/1000, Training Loss: 2.5114264094649243e-05, Test Loss: 2.31876096005789e-05\n",
            "Epoch 904/1000, Training Loss: 2.5079783979231426e-05, Test Loss: 2.315895187164423e-05\n",
            "Epoch 905/1000, Training Loss: 2.504540282818244e-05, Test Loss: 2.3130366042419144e-05\n",
            "Epoch 906/1000, Training Loss: 2.5011120055137586e-05, Test Loss: 2.310185167864553e-05\n",
            "Epoch 907/1000, Training Loss: 2.4976935078675914e-05, Test Loss: 2.307340834995927e-05\n",
            "Epoch 908/1000, Training Loss: 2.4942847322270483e-05, Test Loss: 2.3045035629847604e-05\n",
            "Epoch 909/1000, Training Loss: 2.4908856214240068e-05, Test Loss: 2.3016733095610345e-05\n",
            "Epoch 910/1000, Training Loss: 2.487496118769898e-05, Test Loss: 2.298850032831918e-05\n",
            "Epoch 911/1000, Training Loss: 2.4841161680511127e-05, Test Loss: 2.296033691277851e-05\n",
            "Epoch 912/1000, Training Loss: 2.480745713524045e-05, Test Loss: 2.2932242437488044e-05\n",
            "Epoch 913/1000, Training Loss: 2.4773846999105728e-05, Test Loss: 2.2904216494602183e-05\n",
            "Epoch 914/1000, Training Loss: 2.4740330723933384e-05, Test Loss: 2.287625867989428e-05\n",
            "Epoch 915/1000, Training Loss: 2.4706907766112103e-05, Test Loss: 2.284836859271878e-05\n",
            "Epoch 916/1000, Training Loss: 2.4673577586547796e-05, Test Loss: 2.282054583597369e-05\n",
            "Epoch 917/1000, Training Loss: 2.4640339650618884e-05, Test Loss: 2.279279001606492e-05\n",
            "Epoch 918/1000, Training Loss: 2.4607193428131625e-05, Test Loss: 2.2765100742870812e-05\n",
            "Epoch 919/1000, Training Loss: 2.4574138393277512e-05, Test Loss: 2.2737477629705266e-05\n",
            "Epoch 920/1000, Training Loss: 2.4541174024589672e-05, Test Loss: 2.2709920293284198e-05\n",
            "Epoch 921/1000, Training Loss: 2.4508299804900337e-05, Test Loss: 2.268242835368974e-05\n",
            "Epoch 922/1000, Training Loss: 2.4475515221298793e-05, Test Loss: 2.2655001434336874e-05\n",
            "Epoch 923/1000, Training Loss: 2.4442819765090076e-05, Test Loss: 2.2627639161939692e-05\n",
            "Epoch 924/1000, Training Loss: 2.4410212931753442e-05, Test Loss: 2.2600341166477126e-05\n",
            "Epoch 925/1000, Training Loss: 2.4377694220902505e-05, Test Loss: 2.2573107081161496e-05\n",
            "Epoch 926/1000, Training Loss: 2.4345263136244766e-05, Test Loss: 2.2545936542404736e-05\n",
            "Epoch 927/1000, Training Loss: 2.4312919185541447e-05, Test Loss: 2.251882918978681e-05\n",
            "Epoch 928/1000, Training Loss: 2.428066188056922e-05, Test Loss: 2.2491784666024148e-05\n",
            "Epoch 929/1000, Training Loss: 2.424849073708111e-05, Test Loss: 2.246480261693783e-05\n",
            "Epoch 930/1000, Training Loss: 2.4216405274767914e-05, Test Loss: 2.243788269142243e-05\n",
            "Epoch 931/1000, Training Loss: 2.4184405017220768e-05, Test Loss: 2.2411024541416197e-05\n",
            "Epoch 932/1000, Training Loss: 2.4152489491893957e-05, Test Loss: 2.2384227821871145e-05\n",
            "Epoch 933/1000, Training Loss: 2.412065823006748e-05, Test Loss: 2.2357492190721193e-05\n",
            "Epoch 934/1000, Training Loss: 2.408891076681059e-05, Test Loss: 2.2330817308853908e-05\n",
            "Epoch 935/1000, Training Loss: 2.4057246640945498e-05, Test Loss: 2.230420284008266e-05\n",
            "Epoch 936/1000, Training Loss: 2.4025665395012728e-05, Test Loss: 2.227764845111527e-05\n",
            "Epoch 937/1000, Training Loss: 2.3994166575234337e-05, Test Loss: 2.2251153811526786e-05\n",
            "Epoch 938/1000, Training Loss: 2.396274973147993e-05, Test Loss: 2.222471859373179e-05\n",
            "Epoch 939/1000, Training Loss: 2.393141441723188e-05, Test Loss: 2.219834247295557e-05\n",
            "Epoch 940/1000, Training Loss: 2.3900160189551777e-05, Test Loss: 2.2172025127207363e-05\n",
            "Epoch 941/1000, Training Loss: 2.386898660904531e-05, Test Loss: 2.214576623725247e-05\n",
            "Epoch 942/1000, Training Loss: 2.3837893239830567e-05, Test Loss: 2.211956548658655e-05\n",
            "Epoch 943/1000, Training Loss: 2.3806879649503793e-05, Test Loss: 2.2093422561407523e-05\n",
            "Epoch 944/1000, Training Loss: 2.377594540910746e-05, Test Loss: 2.2067337150590484e-05\n",
            "Epoch 945/1000, Training Loss: 2.374509009309787e-05, Test Loss: 2.2041308945661718e-05\n",
            "Epoch 946/1000, Training Loss: 2.3714313279313068e-05, Test Loss: 2.201533764077241e-05\n",
            "Epoch 947/1000, Training Loss: 2.3683614548942042e-05, Test Loss: 2.1989422932673616e-05\n",
            "Epoch 948/1000, Training Loss: 2.3652993486492194e-05, Test Loss: 2.196356452069132e-05\n",
            "Epoch 949/1000, Training Loss: 2.3622449679760058e-05, Test Loss: 2.1937762106701636e-05\n",
            "Epoch 950/1000, Training Loss: 2.3591982719799245e-05, Test Loss: 2.1912015395106457e-05\n",
            "Epoch 951/1000, Training Loss: 2.356159220089189e-05, Test Loss: 2.1886324092808715e-05\n",
            "Epoch 952/1000, Training Loss: 2.3531277720517558e-05, Test Loss: 2.186068790918835e-05\n",
            "Epoch 953/1000, Training Loss: 2.350103887932451e-05, Test Loss: 2.183510655608038e-05\n",
            "Epoch 954/1000, Training Loss: 2.3470875281099855e-05, Test Loss: 2.180957974774927e-05\n",
            "Epoch 955/1000, Training Loss: 2.3440786532741453e-05, Test Loss: 2.1784107200866724e-05\n",
            "Epoch 956/1000, Training Loss: 2.341077224422874e-05, Test Loss: 2.1758688634489134e-05\n",
            "Epoch 957/1000, Training Loss: 2.3380832028594927e-05, Test Loss: 2.173332377003433e-05\n",
            "Epoch 958/1000, Training Loss: 2.3350965501898715e-05, Test Loss: 2.170801233125972e-05\n",
            "Epoch 959/1000, Training Loss: 2.3321172283196948e-05, Test Loss: 2.168275404424004e-05\n",
            "Epoch 960/1000, Training Loss: 2.329145199451748e-05, Test Loss: 2.1657548637344973e-05\n",
            "Epoch 961/1000, Training Loss: 2.326180426083132e-05, Test Loss: 2.16323958412185e-05\n",
            "Epoch 962/1000, Training Loss: 2.3232228710026446e-05, Test Loss: 2.1607295388755975e-05\n",
            "Epoch 963/1000, Training Loss: 2.3202724972881646e-05, Test Loss: 2.1582247015085352e-05\n",
            "Epoch 964/1000, Training Loss: 2.3173292683040144e-05, Test Loss: 2.155725045754392e-05\n",
            "Epoch 965/1000, Training Loss: 2.3143931476983194e-05, Test Loss: 2.1532305455658468e-05\n",
            "Epoch 966/1000, Training Loss: 2.3114640994005242e-05, Test Loss: 2.1507411751125236e-05\n",
            "Epoch 967/1000, Training Loss: 2.308542087618797e-05, Test Loss: 2.148256908778933e-05\n",
            "Epoch 968/1000, Training Loss: 2.305627076837554e-05, Test Loss: 2.145777721162435e-05\n",
            "Epoch 969/1000, Training Loss: 2.302719031814964e-05, Test Loss: 2.143303587071303e-05\n",
            "Epoch 970/1000, Training Loss: 2.299817917580509e-05, Test Loss: 2.1408344815227616e-05\n",
            "Epoch 971/1000, Training Loss: 2.2969236994325216e-05, Test Loss: 2.1383703797410557e-05\n",
            "Epoch 972/1000, Training Loss: 2.294036342935837e-05, Test Loss: 2.1359112571554393e-05\n",
            "Epoch 973/1000, Training Loss: 2.2911558139194057e-05, Test Loss: 2.1334570893984423e-05\n",
            "Epoch 974/1000, Training Loss: 2.2882820784738592e-05, Test Loss: 2.1310078523038472e-05\n",
            "Epoch 975/1000, Training Loss: 2.285415102949313e-05, Test Loss: 2.128563521904937e-05\n",
            "Epoch 976/1000, Training Loss: 2.2825548539529516e-05, Test Loss: 2.126124074432574e-05\n",
            "Epoch 977/1000, Training Loss: 2.2797012983467947e-05, Test Loss: 2.1236894863134478e-05\n",
            "Epoch 978/1000, Training Loss: 2.2768544032454248e-05, Test Loss: 2.1212597341682465e-05\n",
            "Epoch 979/1000, Training Loss: 2.2740141360137943e-05, Test Loss: 2.118834794809867e-05\n",
            "Epoch 980/1000, Training Loss: 2.2711804642649964e-05, Test Loss: 2.1164146452416993e-05\n",
            "Epoch 981/1000, Training Loss: 2.2683533558580025e-05, Test Loss: 2.113999262655866e-05\n",
            "Epoch 982/1000, Training Loss: 2.2655327788956523e-05, Test Loss: 2.11158862443142e-05\n",
            "Epoch 983/1000, Training Loss: 2.262718701722369e-05, Test Loss: 2.109182708132763e-05\n",
            "Epoch 984/1000, Training Loss: 2.2599110929221143e-05, Test Loss: 2.1067814915079356e-05\n",
            "Epoch 985/1000, Training Loss: 2.257109921316264e-05, Test Loss: 2.1043849524868767e-05\n",
            "Epoch 986/1000, Training Loss: 2.2543151559615465e-05, Test Loss: 2.101993069179827e-05\n",
            "Epoch 987/1000, Training Loss: 2.2515267661479603e-05, Test Loss: 2.099605819875681e-05\n",
            "Epoch 988/1000, Training Loss: 2.2487447213968003e-05, Test Loss: 2.097223183040351e-05\n",
            "Epoch 989/1000, Training Loss: 2.245968991458566e-05, Test Loss: 2.0948451373152386e-05\n",
            "Epoch 990/1000, Training Loss: 2.2431995463110247e-05, Test Loss: 2.092471661515548e-05\n",
            "Epoch 991/1000, Training Loss: 2.2404363561571908e-05, Test Loss: 2.0901027346287515e-05\n",
            "Epoch 992/1000, Training Loss: 2.237679391423415e-05, Test Loss: 2.087738335813095e-05\n",
            "Epoch 993/1000, Training Loss: 2.234928622757471e-05, Test Loss: 2.085378444396024e-05\n",
            "Epoch 994/1000, Training Loss: 2.2321840210265432e-05, Test Loss: 2.0830230398726145e-05\n",
            "Epoch 995/1000, Training Loss: 2.229445557315443e-05, Test Loss: 2.0806721019042035e-05\n",
            "Epoch 996/1000, Training Loss: 2.2267132029246455e-05, Test Loss: 2.0783256103166917e-05\n",
            "Epoch 997/1000, Training Loss: 2.223986929368541e-05, Test Loss: 2.0759835450992904e-05\n",
            "Epoch 998/1000, Training Loss: 2.2212667083734534e-05, Test Loss: 2.0736458864028777e-05\n",
            "Epoch 999/1000, Training Loss: 2.2185525118759574e-05, Test Loss: 2.0713126145387345e-05\n",
            "Epoch 1000/1000, Training Loss: 2.2158443120209675e-05, Test Loss: 2.0689837099769633e-05\n",
            "Epoch 1/1000, Training Loss: 0.003542294156801248, Test Loss: 0.0027713446282567607\n",
            "Epoch 2/1000, Training Loss: 0.003442145669692712, Test Loss: 0.0026532005299589575\n",
            "Epoch 3/1000, Training Loss: 0.0034158155586581107, Test Loss: 0.0026289475418862225\n",
            "Epoch 4/1000, Training Loss: 0.003401961684172886, Test Loss: 0.002621564033890021\n",
            "Epoch 5/1000, Training Loss: 0.003392886606391855, Test Loss: 0.002619716873837326\n",
            "Epoch 6/1000, Training Loss: 0.003386576518461352, Test Loss: 0.0026201508091396597\n",
            "Epoch 7/1000, Training Loss: 0.003382096750744035, Test Loss: 0.0026215194393242507\n",
            "Epoch 8/1000, Training Loss: 0.003378859197287399, Test Loss: 0.0026231721373764093\n",
            "Epoch 9/1000, Training Loss: 0.003376462160018212, Test Loss: 0.002624789137127776\n",
            "Epoch 10/1000, Training Loss: 0.0033746301563657355, Test Loss: 0.0026262244561216807\n",
            "Epoch 11/1000, Training Loss: 0.0033731763367662805, Test Loss: 0.002627423977530756\n",
            "Epoch 12/1000, Training Loss: 0.0033719752125417513, Test Loss: 0.002628380449247549\n",
            "Epoch 13/1000, Training Loss: 0.0033709430312758725, Test Loss: 0.0026291089275344965\n",
            "Epoch 14/1000, Training Loss: 0.003370024080391776, Test Loss: 0.0026296336969652296\n",
            "Epoch 15/1000, Training Loss: 0.0033691813476066862, Test Loss: 0.002629981526240407\n",
            "Epoch 16/1000, Training Loss: 0.003368390242575745, Test Loss: 0.0026301783554707825\n",
            "Epoch 17/1000, Training Loss: 0.003367634410164881, Test Loss: 0.0026302478103727295\n",
            "Epoch 18/1000, Training Loss: 0.0033669029503437895, Test Loss: 0.0026302106709374822\n",
            "Epoch 19/1000, Training Loss: 0.00336618857589568, Test Loss: 0.002630084824382703\n",
            "Epoch 20/1000, Training Loss: 0.0033654863926021873, Test Loss: 0.002629885450314142\n",
            "Epoch 21/1000, Training Loss: 0.0033647930917246414, Test Loss: 0.0026296253038657637\n",
            "Epoch 22/1000, Training Loss: 0.003364106415347039, Test Loss: 0.002629315026389085\n",
            "Epoch 23/1000, Training Loss: 0.0033634248022785494, Test Loss: 0.002628963447950585\n",
            "Epoch 24/1000, Training Loss: 0.003362747153462232, Test Loss: 0.0026285778647929985\n",
            "Epoch 25/1000, Training Loss: 0.0033620726765087507, Test Loss: 0.0026281642851612185\n",
            "Epoch 26/1000, Training Loss: 0.0033614007826385027, Test Loss: 0.0026277276423477304\n",
            "Epoch 27/1000, Training Loss: 0.003360731018348205, Test Loss: 0.00262727197659356\n",
            "Epoch 28/1000, Training Loss: 0.003360063020091122, Test Loss: 0.0026268005887678148\n",
            "Epoch 29/1000, Training Loss: 0.003359396484212297, Test Loss: 0.0026263161692082542\n",
            "Epoch 30/1000, Training Loss: 0.003358731146996575, Test Loss: 0.00262582090511726\n",
            "Epoch 31/1000, Training Loss: 0.0033580667714204902, Test Loss: 0.002625316569690861\n",
            "Epoch 32/1000, Training Loss: 0.0033574031383476668, Test Loss: 0.002624804595837941\n",
            "Epoch 33/1000, Training Loss: 0.0033567400406689274, Test Loss: 0.0026242861369934188\n",
            "Epoch 34/1000, Training Loss: 0.003356077279393323, Test Loss: 0.002623762117181219\n",
            "Epoch 35/1000, Training Loss: 0.003355414661031225, Test Loss: 0.002623233272160443\n",
            "Epoch 36/1000, Training Loss: 0.0033547519958328177, Test Loss: 0.002622700183199556\n",
            "Epoch 37/1000, Training Loss: 0.0033540890965926625, Test Loss: 0.0026221633047714453\n",
            "Epoch 38/1000, Training Loss: 0.0033534257778287434, Test Loss: 0.002621622987245565\n",
            "Epoch 39/1000, Training Loss: 0.0033527618552091653, Test Loss: 0.002621079495469481\n",
            "Epoch 40/1000, Training Loss: 0.003352097145142619, Test Loss: 0.0026205330239771857\n",
            "Epoch 41/1000, Training Loss: 0.003351431464477149, Test Loss: 0.0026199837094320892\n",
            "Epoch 42/1000, Training Loss: 0.003350764630270598, Test Loss: 0.0026194316408047493\n",
            "Epoch 43/1000, Training Loss: 0.0033500964596085265, Test Loss: 0.0026188768676961454\n",
            "Epoch 44/1000, Training Loss: 0.0033494267694536566, Test Loss: 0.00261831940714343\n",
            "Epoch 45/1000, Training Loss: 0.0033487553765162927, Test Loss: 0.002617759249184306\n",
            "Epoch 46/1000, Training Loss: 0.003348082097138785, Test Loss: 0.0026171963614061637\n",
            "Epoch 47/1000, Training Loss: 0.003347406747189431, Test Loss: 0.0026166306926649844\n",
            "Epoch 48/1000, Training Loss: 0.0033467291419627776, Test Loss: 0.00261606217612535\n",
            "Epoch 49/1000, Training Loss: 0.0033460490960843044, Test Loss: 0.0026154907317452875\n",
            "Epoch 50/1000, Training Loss: 0.0033453664234181324, Test Loss: 0.002614916268307047\n",
            "Epoch 51/1000, Training Loss: 0.0033446809369768495, Test Loss: 0.002614338685076471\n",
            "Epoch 52/1000, Training Loss: 0.003343992448832822, Test Loss: 0.0026137578731584322\n",
            "Epoch 53/1000, Training Loss: 0.003343300770030561, Test Loss: 0.002613173716603493\n",
            "Epoch 54/1000, Training Loss: 0.003342605710499836, Test Loss: 0.0026125860933108535\n",
            "Epoch 55/1000, Training Loss: 0.0033419070789692893, Test Loss: 0.00261199487576436\n",
            "Epoch 56/1000, Training Loss: 0.003341204682880409, Test Loss: 0.0026113999316316567\n",
            "Epoch 57/1000, Training Loss: 0.003340498328301694, Test Loss: 0.0026108011242510303\n",
            "Epoch 58/1000, Training Loss: 0.0033397878198429187, Test Loss: 0.0026101983130260437\n",
            "Epoch 59/1000, Training Loss: 0.0033390729605694103, Test Loss: 0.0026095913537443273\n",
            "Epoch 60/1000, Training Loss: 0.0033383535519162467, Test Loss: 0.002608980098833978\n",
            "Epoch 61/1000, Training Loss: 0.0033376293936023286, Test Loss: 0.002608364397568514\n",
            "Epoch 62/1000, Training Loss: 0.0033369002835442517, Test Loss: 0.002607744096229371\n",
            "Epoch 63/1000, Training Loss: 0.003336166017769945, Test Loss: 0.002607119038233297\n",
            "Epoch 64/1000, Training Loss: 0.003335426390332021, Test Loss: 0.0026064890642306456\n",
            "Epoch 65/1000, Training Loss: 0.003334681193220812, Test Loss: 0.002605854012179514\n",
            "Epoch 66/1000, Training Loss: 0.0033339302162770533, Test Loss: 0.002605213717399792\n",
            "Epoch 67/1000, Training Loss: 0.0033331732471041887, Test Loss: 0.0026045680126104177\n",
            "Epoch 68/1000, Training Loss: 0.0033324100709802838, Test Loss: 0.0026039167279525953\n",
            "Epoch 69/1000, Training Loss: 0.0033316404707695226, Test Loss: 0.0026032596910012273\n",
            "Epoch 70/1000, Training Loss: 0.0033308642268332824, Test Loss: 0.0026025967267664153\n",
            "Epoch 71/1000, Training Loss: 0.003330081116940771, Test Loss: 0.002601927657686577\n",
            "Epoch 72/1000, Training Loss: 0.0033292909161792322, Test Loss: 0.0026012523036144258\n",
            "Epoch 73/1000, Training Loss: 0.0033284933968637143, Test Loss: 0.002600570481796921\n",
            "Epoch 74/1000, Training Loss: 0.0033276883284464056, Test Loss: 0.0025998820068500197\n",
            "Epoch 75/1000, Training Loss: 0.0033268754774255505, Test Loss: 0.0025991866907290116\n",
            "Epoch 76/1000, Training Loss: 0.0033260546072539565, Test Loss: 0.0025984843426950298\n",
            "Epoch 77/1000, Training Loss: 0.003325225478247096, Test Loss: 0.0025977747692782823\n",
            "Epoch 78/1000, Training Loss: 0.003324387847490848, Test Loss: 0.0025970577742384445\n",
            "Epoch 79/1000, Training Loss: 0.0033235414687488836, Test Loss: 0.0025963331585225945\n",
            "Epoch 80/1000, Training Loss: 0.00332268609236972, Test Loss: 0.0025956007202210273\n",
            "Epoch 81/1000, Training Loss: 0.003321821465193496, Test Loss: 0.00259486025452123\n",
            "Epoch 82/1000, Training Loss: 0.003320947330458487, Test Loss: 0.00259411155366028\n",
            "Epoch 83/1000, Training Loss: 0.003320063427707394, Test Loss: 0.002593354406875873\n",
            "Epoch 84/1000, Training Loss: 0.0033191694926934722, Test Loss: 0.0025925886003562103\n",
            "Epoch 85/1000, Training Loss: 0.003318265257286522, Test Loss: 0.0025918139171888983\n",
            "Epoch 86/1000, Training Loss: 0.003317350449378816, Test Loss: 0.00259103013730904\n",
            "Epoch 87/1000, Training Loss: 0.0033164247927909957, Test Loss: 0.002590237037446686\n",
            "Epoch 88/1000, Training Loss: 0.003315488007178021, Test Loss: 0.0025894343910737683\n",
            "Epoch 89/1000, Training Loss: 0.003314539807935216, Test Loss: 0.0025886219683506706\n",
            "Epoch 90/1000, Training Loss: 0.0033135799061045, Test Loss: 0.002587799536072559\n",
            "Epoch 91/1000, Training Loss: 0.0033126080082808602, Test Loss: 0.002586966857615596\n",
            "Epoch 92/1000, Training Loss: 0.0033116238165191558, Test Loss: 0.0025861236928831828\n",
            "Epoch 93/1000, Training Loss: 0.003310627028241339, Test Loss: 0.002585269798252325\n",
            "Epoch 94/1000, Training Loss: 0.0033096173361441684, Test Loss: 0.0025844049265202596\n",
            "Epoch 95/1000, Training Loss: 0.003308594428107524, Test Loss: 0.002583528826851445\n",
            "Epoch 96/1000, Training Loss: 0.003307557987103411, Test Loss: 0.002582641244725068\n",
            "Epoch 97/1000, Training Loss: 0.0033065076911057657, Test Loss: 0.0025817419218831313\n",
            "Epoch 98/1000, Training Loss: 0.0033054432130011635, Test Loss: 0.0025808305962792977\n",
            "Epoch 99/1000, Training Loss: 0.003304364220500554, Test Loss: 0.0025799070020285746\n",
            "Epoch 100/1000, Training Loss: 0.0033032703760521344, Test Loss: 0.0025789708693579765\n",
            "Epoch 101/1000, Training Loss: 0.0033021613367554925, Test Loss: 0.0025780219245582826\n",
            "Epoch 102/1000, Training Loss: 0.0033010367542771514, Test Loss: 0.0025770598899370217\n",
            "Epoch 103/1000, Training Loss: 0.0032998962747676497, Test Loss: 0.0025760844837728076\n",
            "Epoch 104/1000, Training Loss: 0.0032987395387802966, Test Loss: 0.0025750954202711455\n",
            "Epoch 105/1000, Training Loss: 0.0032975661811917656, Test Loss: 0.0025740924095218613\n",
            "Epoch 106/1000, Training Loss: 0.003296375831124661, Test Loss: 0.0025730751574582662\n",
            "Epoch 107/1000, Training Loss: 0.0032951681118722283, Test Loss: 0.002572043365818213\n",
            "Epoch 108/1000, Training Loss: 0.003293942640825371, Test Loss: 0.0025709967321071473\n",
            "Epoch 109/1000, Training Loss: 0.0032926990294021496, Test Loss: 0.0025699349495633583\n",
            "Epoch 110/1000, Training Loss: 0.0032914368829799244, Test Loss: 0.0025688577071254805\n",
            "Epoch 111/1000, Training Loss: 0.0032901558008303433, Test Loss: 0.002567764689402488\n",
            "Epoch 112/1000, Training Loss: 0.0032888553760573452, Test Loss: 0.0025666555766462556\n",
            "Epoch 113/1000, Training Loss: 0.0032875351955383753, Test Loss: 0.002565530044726862\n",
            "Epoch 114/1000, Training Loss: 0.003286194839869016, Test Loss: 0.002564387765110805\n",
            "Epoch 115/1000, Training Loss: 0.0032848338833112245, Test Loss: 0.0025632284048422425\n",
            "Epoch 116/1000, Training Loss: 0.0032834518937453873, Test Loss: 0.002562051626527451\n",
            "Epoch 117/1000, Training Loss: 0.0032820484326264027, Test Loss: 0.0025608570883226224\n",
            "Epoch 118/1000, Training Loss: 0.003280623054944004, Test Loss: 0.0025596444439251894\n",
            "Epoch 119/1000, Training Loss: 0.0032791753091875454, Test Loss: 0.0025584133425688234\n",
            "Epoch 120/1000, Training Loss: 0.0032777047373154576, Test Loss: 0.0025571634290222516\n",
            "Epoch 121/1000, Training Loss: 0.0032762108747296215, Test Loss: 0.0025558943435920805\n",
            "Epoch 122/1000, Training Loss: 0.00327469325025486, Test Loss: 0.0025546057221297654\n",
            "Epoch 123/1000, Training Loss: 0.003273151386123806, Test Loss: 0.0025532971960428915\n",
            "Epoch 124/1000, Training Loss: 0.0032715847979673455, Test Loss: 0.002551968392310936\n",
            "Epoch 125/1000, Training Loss: 0.003269992994810902, Test Loss: 0.002550618933505668\n",
            "Epoch 126/1000, Training Loss: 0.0032683754790767666, Test Loss: 0.0025492484378163353\n",
            "Epoch 127/1000, Training Loss: 0.0032667317465927374, Test Loss: 0.00254785651907982\n",
            "Epoch 128/1000, Training Loss: 0.0032650612866072706, Test Loss: 0.002546442786815902\n",
            "Epoch 129/1000, Training Loss: 0.0032633635818114038, Test Loss: 0.0025450068462677922\n",
            "Epoch 130/1000, Training Loss: 0.0032616381083676814, Test Loss: 0.0025435482984481114\n",
            "Epoch 131/1000, Training Loss: 0.0032598843359462897, Test Loss: 0.002542066740190426\n",
            "Epoch 132/1000, Training Loss: 0.0032581017277686676, Test Loss: 0.0025405617642065494\n",
            "Epoch 133/1000, Training Loss: 0.0032562897406587855, Test Loss: 0.002539032959149699\n",
            "Epoch 134/1000, Training Loss: 0.003254447825102345, Test Loss: 0.0025374799096837125\n",
            "Epoch 135/1000, Training Loss: 0.003252575425314095, Test Loss: 0.0025359021965584176\n",
            "Epoch 136/1000, Training Loss: 0.0032506719793134863, Test Loss: 0.0025342993966913363\n",
            "Epoch 137/1000, Training Loss: 0.0032487369190088836, Test Loss: 0.002532671083255827\n",
            "Epoch 138/1000, Training Loss: 0.0032467696702905154, Test Loss: 0.0025310168257758375\n",
            "Epoch 139/1000, Training Loss: 0.003244769653132382, Test Loss: 0.0025293361902273513\n",
            "Epoch 140/1000, Training Loss: 0.0032427362817032844, Test Loss: 0.0025276287391466966\n",
            "Epoch 141/1000, Training Loss: 0.0032406689644871667, Test Loss: 0.002525894031745811\n",
            "Epoch 142/1000, Training Loss: 0.003238567104412935, Test Loss: 0.0025241316240345894\n",
            "Epoch 143/1000, Training Loss: 0.0032364300989939144, Test Loss: 0.0025223410689504165\n",
            "Epoch 144/1000, Training Loss: 0.0032342573404770727, Test Loss: 0.0025205219164950083\n",
            "Epoch 145/1000, Training Loss: 0.0032320482160021645, Test Loss: 0.0025186737138786393\n",
            "Epoch 146/1000, Training Loss: 0.0032298021077708985, Test Loss: 0.0025167960056718657\n",
            "Epoch 147/1000, Training Loss: 0.003227518393226223, Test Loss: 0.002514888333964817\n",
            "Epoch 148/1000, Training Loss: 0.0032251964452418356, Test Loss: 0.0025129502385341585\n",
            "Epoch 149/1000, Training Loss: 0.0032228356323219533, Test Loss: 0.0025109812570177654\n",
            "Epoch 150/1000, Training Loss: 0.003220435318811406, Test Loss: 0.002508980925097207\n",
            "Epoch 151/1000, Training Loss: 0.00321799486511607, Test Loss: 0.0025069487766880695\n",
            "Epoch 152/1000, Training Loss: 0.0032155136279336284, Test Loss: 0.0025048843441381865\n",
            "Epoch 153/1000, Training Loss: 0.0032129909604946415, Test Loss: 0.002502787158433798\n",
            "Epoch 154/1000, Training Loss: 0.003210426212813856, Test Loss: 0.002500656749413666\n",
            "Epoch 155/1000, Training Loss: 0.0032078187319516593, Test Loss: 0.0024984926459911856\n",
            "Epoch 156/1000, Training Loss: 0.003205167862285564, Test Loss: 0.002496294376384453\n",
            "Epoch 157/1000, Training Loss: 0.0032024729457915406, Test Loss: 0.002494061468354322\n",
            "Epoch 158/1000, Training Loss: 0.0031997333223349996, Test Loss: 0.002491793449450383\n",
            "Epoch 159/1000, Training Loss: 0.0031969483299711643, Test Loss: 0.0024894898472648678\n",
            "Epoch 160/1000, Training Loss: 0.003194117305254536, Test Loss: 0.0024871501896943575\n",
            "Epoch 161/1000, Training Loss: 0.003191239583557088, Test Loss: 0.002484774005209284\n",
            "Epoch 162/1000, Training Loss: 0.003188314499394773, Test Loss: 0.0024823608231310653\n",
            "Epoch 163/1000, Training Loss: 0.003185341386761868, Test Loss: 0.0024799101739167682\n",
            "Epoch 164/1000, Training Loss: 0.0031823195794725935, Test Loss: 0.0024774215894511615\n",
            "Epoch 165/1000, Training Loss: 0.00317924841150939, Test Loss: 0.0024748946033459392\n",
            "Epoch 166/1000, Training Loss: 0.0031761272173771414, Test Loss: 0.0024723287512459307\n",
            "Epoch 167/1000, Training Loss: 0.003172955332462539, Test Loss: 0.002469723571142043\n",
            "Epoch 168/1000, Training Loss: 0.003169732093397699, Test Loss: 0.002467078603690622\n",
            "Epoch 169/1000, Training Loss: 0.003166456838427029, Test Loss: 0.002464393392538929\n",
            "Epoch 170/1000, Training Loss: 0.0031631289077762347, Test Loss: 0.0024616674846563414\n",
            "Epoch 171/1000, Training Loss: 0.003159747644022233, Test Loss: 0.002458900430670848\n",
            "Epoch 172/1000, Training Loss: 0.003156312392462618, Test Loss: 0.002456091785210359\n",
            "Epoch 173/1000, Training Loss: 0.0031528225014831623, Test Loss: 0.0024532411072482854\n",
            "Epoch 174/1000, Training Loss: 0.0031492773229217295, Test Loss: 0.0024503479604527736\n",
            "Epoch 175/1000, Training Loss: 0.003145676212426778, Test Loss: 0.0024474119135389294\n",
            "Epoch 176/1000, Training Loss: 0.0031420185298084912, Test Loss: 0.0024444325406232626\n",
            "Epoch 177/1000, Training Loss: 0.0031383036393803788, Test Loss: 0.0024414094215795194\n",
            "Epoch 178/1000, Training Loss: 0.0031345309102890216, Test Loss: 0.002438342142394996\n",
            "Epoch 179/1000, Training Loss: 0.0031306997168294294, Test Loss: 0.002435230295526282\n",
            "Epoch 180/1000, Training Loss: 0.003126809438743262, Test Loss: 0.002432073480253358\n",
            "Epoch 181/1000, Training Loss: 0.003122859461496965, Test Loss: 0.002428871303030779\n",
            "Epoch 182/1000, Training Loss: 0.003118849176536641, Test Loss: 0.0024256233778346447\n",
            "Epoch 183/1000, Training Loss: 0.003114777981516246, Test Loss: 0.002422329326503882\n",
            "Epoch 184/1000, Training Loss: 0.0031106452804954376, Test Loss: 0.0024189887790742456\n",
            "Epoch 185/1000, Training Loss: 0.0031064504841031983, Test Loss: 0.002415601374103347\n",
            "Epoch 186/1000, Training Loss: 0.0031021930096630356, Test Loss: 0.002412166758984808\n",
            "Epoch 187/1000, Training Loss: 0.00309787228127538, Test Loss: 0.002408684590249581\n",
            "Epoch 188/1000, Training Loss: 0.003093487729852454, Test Loss: 0.002405154533852235\n",
            "Epoch 189/1000, Training Loss: 0.0030890387931006924, Test Loss: 0.0024015762654398896\n",
            "Epoch 190/1000, Training Loss: 0.003084524915445472, Test Loss: 0.0023979494706013148\n",
            "Epoch 191/1000, Training Loss: 0.003079945547892676, Test Loss: 0.0023942738450935127\n",
            "Epoch 192/1000, Training Loss: 0.003075300147821333, Test Loss: 0.002390549095042927\n",
            "Epoch 193/1000, Training Loss: 0.003070588178701342, Test Loss: 0.0023867749371182505\n",
            "Epoch 194/1000, Training Loss: 0.0030658091097300042, Test Loss: 0.0023829510986715656\n",
            "Epoch 195/1000, Training Loss: 0.0030609624153808907, Test Loss: 0.002379077317844413\n",
            "Epoch 196/1000, Training Loss: 0.0030560475748583056, Test Loss: 0.0023751533436351014\n",
            "Epoch 197/1000, Training Loss: 0.0030510640714504467, Test Loss: 0.0023711789359234224\n",
            "Epoch 198/1000, Training Loss: 0.0030460113917741166, Test Loss: 0.0023671538654486805\n",
            "Epoch 199/1000, Training Loss: 0.0030408890249037468, Test Loss: 0.002363077913736716\n",
            "Epoch 200/1000, Training Loss: 0.0030356964613772884, Test Loss: 0.0023589508729714086\n",
            "Epoch 201/1000, Training Loss: 0.003030433192071481, Test Loss: 0.0023547725458058655\n",
            "Epoch 202/1000, Training Loss: 0.0030250987069388906, Test Loss: 0.002350542745108277\n",
            "Epoch 203/1000, Training Loss: 0.0030196924935991165, Test Loss: 0.002346261293637198\n",
            "Epoch 204/1000, Training Loss: 0.003014214035776552, Test Loss: 0.0023419280236407097\n",
            "Epoch 205/1000, Training Loss: 0.0030086628115771593, Test Loss: 0.002337542776373707\n",
            "Epoch 206/1000, Training Loss: 0.003003038291596799, Test Loss: 0.0023331054015272704\n",
            "Epoch 207/1000, Training Loss: 0.002997339936853888, Test Loss: 0.0023286157565638134\n",
            "Epoch 208/1000, Training Loss: 0.0029915671965392676, Test Loss: 0.0023240737059514144\n",
            "Epoch 209/1000, Training Loss: 0.002985719505576575, Test Loss: 0.002319479120290481\n",
            "Epoch 210/1000, Training Loss: 0.0029797962819866517, Test Loss: 0.0023148318753255736\n",
            "Epoch 211/1000, Training Loss: 0.0029737969240500174, Test Loss: 0.002310131850834946\n",
            "Epoch 212/1000, Training Loss: 0.0029677208072618633, Test Loss: 0.0023053789293900214\n",
            "Epoch 213/1000, Training Loss: 0.0029615672810746367, Test Loss: 0.002300572994976739\n",
            "Epoch 214/1000, Training Loss: 0.0029553356654238717, Test Loss: 0.0022957139314703496\n",
            "Epoch 215/1000, Training Loss: 0.0029490252470336675, Test Loss: 0.0022908016209549327\n",
            "Epoch 216/1000, Training Loss: 0.0029426352754989764, Test Loss: 0.0022858359418784966\n",
            "Epoch 217/1000, Training Loss: 0.0029361649591427763, Test Loss: 0.002280816767034261\n",
            "Epoch 218/1000, Training Loss: 0.002929613460647068, Test Loss: 0.002275743961358228\n",
            "Epoch 219/1000, Training Loss: 0.0029229798924577366, Test Loss: 0.0022706173795328556\n",
            "Epoch 220/1000, Training Loss: 0.0029162633119643633, Test Loss: 0.002265436863386189\n",
            "Epoch 221/1000, Training Loss: 0.0029094627164572584, Test Loss: 0.0022602022390754203\n",
            "Epoch 222/1000, Training Loss: 0.002902577037865209, Test Loss: 0.0022549133140433984\n",
            "Epoch 223/1000, Training Loss: 0.0028956051372787716, Test Loss: 0.0022495698737361996\n",
            "Epoch 224/1000, Training Loss: 0.002888545799265264, Test Loss: 0.0022441716780693863\n",
            "Epoch 225/1000, Training Loss: 0.0028813977259830813, Test Loss: 0.002238718457630198\n",
            "Epoch 226/1000, Training Loss: 0.0028741595311044155, Test Loss: 0.002233209909602383\n",
            "Epoch 227/1000, Training Loss: 0.0028668297335569847, Test Loss: 0.002227645693400013\n",
            "Epoch 228/1000, Training Loss: 0.0028594067510969937, Test Loss: 0.0022220254259961696\n",
            "Epoch 229/1000, Training Loss: 0.002851888893727078, Test Loss: 0.0022163486769319323\n",
            "Epoch 230/1000, Training Loss: 0.002844274356974709, Test Loss: 0.002210614962990824\n",
            "Epoch 231/1000, Training Loss: 0.0028365612150481534, Test Loss: 0.0022048237425234666\n",
            "Epoch 232/1000, Training Loss: 0.0028287474138887526, Test Loss: 0.00219897440940693\n",
            "Epoch 233/1000, Training Loss: 0.002820830764140005, Test Loss: 0.0021930662866231645\n",
            "Epoch 234/1000, Training Loss: 0.002812808934055604, Test Loss: 0.0021870986194407133\n",
            "Epoch 235/1000, Training Loss: 0.0028046794423702893, Test Loss: 0.0021810705681840634\n",
            "Epoch 236/1000, Training Loss: 0.002796439651159015, Test Loss: 0.002174981200575135\n",
            "Epoch 237/1000, Training Loss: 0.002788086758711611, Test Loss: 0.0021688294836318543\n",
            "Epoch 238/1000, Training Loss: 0.0027796177924517586, Test Loss: 0.002162614275109414\n",
            "Epoch 239/1000, Training Loss: 0.0027710296019306502, Test Loss: 0.0021563343144707123\n",
            "Epoch 240/1000, Training Loss: 0.00276231885192732, Test Loss: 0.0021499882133737087\n",
            "Epoch 241/1000, Training Loss: 0.002753482015689107, Test Loss: 0.0021435744456651225\n",
            "Epoch 242/1000, Training Loss: 0.0027445153683471964, Test Loss: 0.002137091336871854\n",
            "Epoch 243/1000, Training Loss: 0.0027354149805436114, Test Loss: 0.00213053705318416\n",
            "Epoch 244/1000, Training Loss: 0.0027261767123073852, Test Loss: 0.0021239095899276738\n",
            "Epoch 245/1000, Training Loss: 0.0027167962072189604, Test Loss: 0.0021172067595251567\n",
            "Epoch 246/1000, Training Loss: 0.0027072688869031218, Test Loss: 0.0021104261789533165\n",
            "Epoch 247/1000, Training Loss: 0.0026975899458920125, Test Loss: 0.0021035652567053093\n",
            "Epoch 248/1000, Training Loss: 0.002687754346900917, Test Loss: 0.0020966211792756533\n",
            "Epoch 249/1000, Training Loss: 0.0026777568165607286, Test Loss: 0.0020895908971914393\n",
            "Epoch 250/1000, Training Loss: 0.0026675918416520917, Test Loss: 0.002082471110621749\n",
            "Epoch 251/1000, Training Loss: 0.002657253665887468, Test Loss: 0.0020752582546066096\n",
            "Epoch 252/1000, Training Loss: 0.002646736287288562, Test Loss: 0.0020679484839571578\n",
            "Epoch 253/1000, Training Loss: 0.0026360334562078925, Test Loss: 0.002060537657890466\n",
            "Epoch 254/1000, Training Loss: 0.002625138674044829, Test Loss: 0.0020530213244757045\n",
            "Epoch 255/1000, Training Loss: 0.0026140451927080845, Test Loss: 0.002045394704982733\n",
            "Epoch 256/1000, Training Loss: 0.002602746014878795, Test Loss: 0.002037652678240359\n",
            "Epoch 257/1000, Training Loss: 0.0025912338951307474, Test Loss: 0.002029789765129084\n",
            "Epoch 258/1000, Training Loss: 0.002579501341967389, Test Loss: 0.002021800113352382\n",
            "Epoch 259/1000, Training Loss: 0.0025675406208390517, Test Loss: 0.002013677482651436\n",
            "Epoch 260/1000, Training Loss: 0.0025553437582085, Test Loss: 0.0020054152306508604\n",
            "Epoch 261/1000, Training Loss: 0.0025429025467387113, Test Loss: 0.001997006299547266\n",
            "Epoch 262/1000, Training Loss: 0.002530208551684029, Test Loss: 0.00198844320387873\n",
            "Epoch 263/1000, Training Loss: 0.002517253118574616, Test Loss: 0.001979718019641199\n",
            "Epoch 264/1000, Training Loss: 0.0025040273822949954, Test Loss: 0.0019708223750478835\n",
            "Epoch 265/1000, Training Loss: 0.0024905222776705756, Test Loss: 0.001961747443259695\n",
            "Epoch 266/1000, Training Loss: 0.0024767285516918555, Test Loss: 0.001952483937448926\n",
            "Epoch 267/1000, Training Loss: 0.0024626367775249353, Test Loss: 0.0019430221085948397\n",
            "Epoch 268/1000, Training Loss: 0.002448237370479298, Test Loss: 0.0019333517464486974\n",
            "Epoch 269/1000, Training Loss: 0.0024335206061300925, Test Loss: 0.0019234621841472174\n",
            "Epoch 270/1000, Training Loss: 0.0024184766408225657, Test Loss: 0.0019133423069976126\n",
            "Epoch 271/1000, Training Loss: 0.0024030955348213036, Test Loss: 0.0019029805660043572\n",
            "Epoch 272/1000, Training Loss: 0.002387367278406581, Test Loss: 0.0018923649967575995\n",
            "Epoch 273/1000, Training Loss: 0.002371281821264404, Test Loss: 0.0018814832443554172\n",
            "Epoch 274/1000, Training Loss: 0.0023548291055657273, Test Loss: 0.0018703225950865653\n",
            "Epoch 275/1000, Training Loss: 0.002337999103183001, Test Loss: 0.0018588700156559296\n",
            "Epoch 276/1000, Training Loss: 0.0023207818575478024, Test Loss: 0.0018471122007900278\n",
            "Epoch 277/1000, Training Loss: 0.002303167530710383, Test Loss: 0.0018350356301124681\n",
            "Epoch 278/1000, Training Loss: 0.0022851464562179654, Test Loss: 0.0018226266352256216\n",
            "Epoch 279/1000, Training Loss: 0.0022667091984806837, Test Loss: 0.0018098714779702782\n",
            "Epoch 280/1000, Training Loss: 0.0022478466193378097, Test Loss: 0.0017967564408534697\n",
            "Epoch 281/1000, Training Loss: 0.0022285499525669336, Test Loss: 0.0017832679306273682\n",
            "Epoch 282/1000, Training Loss: 0.0022088108870882707, Test Loss: 0.0017693925959590886\n",
            "Epoch 283/1000, Training Loss: 0.0021886216595968485, Test Loss: 0.0017551174600393322\n",
            "Epoch 284/1000, Training Loss: 0.0021679751572971835, Test Loss: 0.0017404300688222516\n",
            "Epoch 285/1000, Training Loss: 0.0021468650313067757, Test Loss: 0.0017253186553527289\n",
            "Epoch 286/1000, Training Loss: 0.0021252858211239013, Test Loss: 0.001709772320302208\n",
            "Epoch 287/1000, Training Loss: 0.0021032330903087036, Test Loss: 0.0016937812283821646\n",
            "Epoch 288/1000, Training Loss: 0.002080703573191684, Test Loss: 0.001677336819718718\n",
            "Epoch 289/1000, Training Loss: 0.0020576953319891883, Test Loss: 0.0016604320345407638\n",
            "Epoch 290/1000, Training Loss: 0.002034207923163624, Test Loss: 0.001643061548652708\n",
            "Epoch 291/1000, Training Loss: 0.0020102425712147007, Test Loss: 0.0016252220161386375\n",
            "Epoch 292/1000, Training Loss: 0.001985802347333114, Test Loss: 0.0016069123146003966\n",
            "Epoch 293/1000, Training Loss: 0.0019608923495068697, Test Loss: 0.0015881337870108003\n",
            "Epoch 294/1000, Training Loss: 0.0019355198797741577, Test Loss: 0.001568890473031867\n",
            "Epoch 295/1000, Training Loss: 0.0019096946134119625, Test Loss: 0.0015491893214981587\n",
            "Epoch 296/1000, Training Loss: 0.001883428754000784, Test Loss: 0.001529040374813022\n",
            "Epoch 297/1000, Training Loss: 0.001856737167592134, Test Loss: 0.0015084569153839338\n",
            "Epoch 298/1000, Training Loss: 0.0018296374887187012, Test Loss: 0.0014874555640735725\n",
            "Epoch 299/1000, Training Loss: 0.0018021501908231088, Test Loss: 0.001466056321097366\n",
            "Epoch 300/1000, Training Loss: 0.0017742986139314832, Test Loss: 0.001444282540960246\n",
            "Epoch 301/1000, Training Loss: 0.0017461089431340935, Test Loss: 0.0014221608349464837\n",
            "Epoch 302/1000, Training Loss: 0.001717610132697425, Test Loss: 0.0013997208973381228\n",
            "Epoch 303/1000, Training Loss: 0.0016888337724135635, Test Loss: 0.0013769952548336498\n",
            "Epoch 304/1000, Training Loss: 0.0016598138950317909, Test Loss: 0.0013540189423736173\n",
            "Epoch 305/1000, Training Loss: 0.0016305867261942847, Test Loss: 0.001330829112479904\n",
            "Epoch 306/1000, Training Loss: 0.0016011903810391956, Test Loss: 0.0013074645889511252\n",
            "Epoch 307/1000, Training Loss: 0.0015716645143295787, Test Loss: 0.0012839653789852655\n",
            "Epoch 308/1000, Training Loss: 0.0015420499333893278, Test Loss: 0.0012603721602084442\n",
            "Epoch 309/1000, Training Loss: 0.0015123881850634365, Test Loss: 0.0012367257604367727\n",
            "Epoch 310/1000, Training Loss: 0.0014827211291963815, Test Loss: 0.0012130666481578042\n",
            "Epoch 311/1000, Training Loss: 0.0014530905116295154, Test Loss: 0.0011894344506876834\n",
            "Epoch 312/1000, Training Loss: 0.0014235375494247315, Test Loss: 0.0011658675148651374\n",
            "Epoch 313/1000, Training Loss: 0.0013941025399763624, Test Loss: 0.0011424025222136847\n",
            "Epoch 314/1000, Training Loss: 0.0013648245039974095, Test Loss: 0.0011190741670388431\n",
            "Epoch 315/1000, Training Loss: 0.0013357408702327884, Test Loss: 0.0010959149022530005\n",
            "Epoch 316/1000, Training Loss: 0.0013068872073625564, Test Loss: 0.0010729547541476348\n",
            "Epoch 317/1000, Training Loss: 0.0012782970061151946, Test Loss: 0.0010502212041214108\n",
            "Epoch 318/1000, Training Loss: 0.00125000151229623, Test Loss: 0.001027739132711815\n",
            "Epoch 319/1000, Training Loss: 0.001222029609395454, Test Loss: 0.0010055308192760907\n",
            "Epoch 320/1000, Training Loss: 0.00119440774776352, Test Loss: 0.0009836159893551796\n",
            "Epoch 321/1000, Training Loss: 0.0011671599160953156, Test Loss: 0.0009620119010984933\n",
            "Epoch 322/1000, Training Loss: 0.0011403076501280815, Test Loss: 0.0009407334620457783\n",
            "Epoch 323/1000, Training Loss: 0.0011138700730278768, Test Loss: 0.0009197933679450918\n",
            "Epoch 324/1000, Training Loss: 0.0010878639618448578, Test Loss: 0.0008992022560106554\n",
            "Epoch 325/1000, Training Loss: 0.0010623038345985102, Test Loss: 0.0008789688659714964\n",
            "Epoch 326/1000, Training Loss: 0.0010372020529376938, Test Loss: 0.0008591002033245442\n",
            "Epoch 327/1000, Training Loss: 0.001012568935838127, Test Loss: 0.0008396017002949549\n",
            "Epoch 328/1000, Training Loss: 0.0009884128803919249, Test Loss: 0.0008204773710541429\n",
            "Epoch 329/1000, Training Loss: 0.0009647404863598527, Test Loss: 0.000801729958704529\n",
            "Epoch 330/1000, Training Loss: 0.0009415566817592879, Test Loss: 0.0007833610723802281\n",
            "Epoch 331/1000, Training Loss: 0.0009188648473222628, Test Loss: 0.0007653713135207623\n",
            "Epoch 332/1000, Training Loss: 0.0008966669381615141, Test Loss: 0.0007477603909484353\n",
            "Epoch 333/1000, Training Loss: 0.0008749636014194698, Test Loss: 0.0007305272248257529\n",
            "Epoch 334/1000, Training Loss: 0.0008537542890429406, Test Loss: 0.0007136700398991433\n",
            "Epoch 335/1000, Training Loss: 0.0008330373651275085, Test Loss: 0.0006971864486646516\n",
            "Epoch 336/1000, Training Loss: 0.0008128102075149948, Test Loss: 0.000681073525236194\n",
            "Epoch 337/1000, Training Loss: 0.0007930693035127278, Test Loss: 0.0006653278707741743\n",
            "Epoch 338/1000, Training Loss: 0.000773810339741859, Test Loss: 0.0006499456713563877\n",
            "Epoch 339/1000, Training Loss: 0.0007550282862217325, Test Loss: 0.000634922749157725\n",
            "Epoch 340/1000, Training Loss: 0.0007367174748663325, Test Loss: 0.0006202546077622648\n",
            "Epoch 341/1000, Training Loss: 0.0007188716726128314, Test Loss: 0.0006059364723695943\n",
            "Epoch 342/1000, Training Loss: 0.000701484149428174, Test Loss: 0.0005919633255853021\n",
            "Epoch 343/1000, Training Loss: 0.0006845477414515095, Test Loss: 0.0005783299394086785\n",
            "Epoch 344/1000, Training Loss: 0.0006680549095326383, Test Loss: 0.0005650309039537415\n",
            "Epoch 345/1000, Training Loss: 0.000651997793422334, Test Loss: 0.0005520606533655946\n",
            "Epoch 346/1000, Training Loss: 0.0006363682618620438, Test Loss: 0.0005394134893252063\n",
            "Epoch 347/1000, Training Loss: 0.0006211579588095679, Test Loss: 0.0005270836024729744\n",
            "Epoch 348/1000, Training Loss: 0.0006063583460256921, Test Loss: 0.0005150650920259675\n",
            "Epoch 349/1000, Training Loss: 0.0005919607422344991, Test Loss: 0.0005033519838151285\n",
            "Epoch 350/1000, Training Loss: 0.00057795635905857, Test Loss: 0.0004919382469273555\n",
            "Epoch 351/1000, Training Loss: 0.0005643363339192129, Test Loss: 0.00048081780910239226\n",
            "Epoch 352/1000, Training Loss: 0.0005510917600817155, Test Loss: 0.00046998457100554765\n",
            "Epoch 353/1000, Training Loss: 0.0005382137140164112, Test Loss: 0.0004594324194737581\n",
            "Epoch 354/1000, Training Loss: 0.0005256932802377937, Test Loss: 0.0004491552398134969\n",
            "Epoch 355/1000, Training Loss: 0.0005135215737763467, Test Loss: 0.0004391469272143076\n",
            "Epoch 356/1000, Training Loss: 0.000501689760430642, Test Loss: 0.00042940139733018003\n",
            "Epoch 357/1000, Training Loss: 0.0004901890749407762, Test Loss: 0.00041991259607236273\n",
            "Epoch 358/1000, Training Loss: 0.00047901083721811627, Test Loss: 0.000410674508650819\n",
            "Epoch 359/1000, Training Loss: 0.0004681464667605948, Test Loss: 0.0004016811678970288\n",
            "Epoch 360/1000, Training Loss: 0.00045758749537727405, Test Loss: 0.00039292666189770113\n",
            "Epoch 361/1000, Training Loss: 0.00044732557834057724, Test Loss: 0.00038440514096697636\n",
            "Epoch 362/1000, Training Loss: 0.0004373525040793931, Test Loss: 0.0003761108239834926\n",
            "Epoch 363/1000, Training Loss: 0.00042766020252113545, Test Loss: 0.0003680380041180498\n",
            "Epoch 364/1000, Training Loss: 0.00041824075218587067, Test Loss: 0.00036018105397742577\n",
            "Epoch 365/1000, Training Loss: 0.0004090863861305823, Test Loss: 0.0003525344301898501\n",
            "Epoch 366/1000, Training Loss: 0.0004001894968367897, Test Loss: 0.00034509267745783464\n",
            "Epoch 367/1000, Training Loss: 0.0003915426401298571, Test Loss: 0.00033785043210420306\n",
            "Epoch 368/1000, Training Loss: 0.00038313853821352555, Test Loss: 0.00033080242513731635\n",
            "Epoch 369/1000, Training Loss: 0.0003749700818984912, Test Loss: 0.0003239434848615768\n",
            "Epoch 370/1000, Training Loss: 0.00036703033209928765, Test Loss: 0.0003172685390593098\n",
            "Epoch 371/1000, Training Loss: 0.0003593125206690613, Test Loss: 0.00031077261676989916\n",
            "Epoch 372/1000, Training Loss: 0.0003518100506375878, Test Loss: 0.0003044508496919103\n",
            "Epoch 373/1000, Training Loss: 0.0003445164959134738, Test Loss: 0.0002982984732334742\n",
            "Epoch 374/1000, Training Loss: 0.00033742560050739883, Test Loss: 0.00029231082723574426\n",
            "Epoch 375/1000, Training Loss: 0.00033053127732923776, Test Loss: 0.0002864833563936298\n",
            "Epoch 376/1000, Training Loss: 0.00032382760660808766, Test Loss: 0.00028081161039731\n",
            "Epoch 377/1000, Training Loss: 0.0003173088339805399, Test Loss: 0.0002752912438172562\n",
            "Epoch 378/1000, Training Loss: 0.00031096936828902945, Test Loss: 0.0002699180157546227\n",
            "Epoch 379/1000, Training Loss: 0.0003048037791288012, Test Loss: 0.00026468778927798894\n",
            "Epoch 380/1000, Training Loss: 0.000298806794178806, Test Loss: 0.00025959653066641805\n",
            "Epoch 381/1000, Training Loss: 0.0002929732963489497, Test Loss: 0.000254640308477917\n",
            "Epoch 382/1000, Training Loss: 0.00028729832077325004, Test Loss: 0.00024981529246129066\n",
            "Epoch 383/1000, Training Loss: 0.00028177705167581514, Test Loss: 0.0002451177523284053\n",
            "Epoch 384/1000, Training Loss: 0.0002764048191341298, Test Loss: 0.00024054405640289096\n",
            "Epoch 385/1000, Training Loss: 0.0002711770957617994, Test Loss: 0.0002360906701602866\n",
            "Epoch 386/1000, Training Loss: 0.0002660894933307259, Test Loss: 0.0002317541546736379\n",
            "Epoch 387/1000, Training Loss: 0.00026113775935076065, Test Loss: 0.00022753116497765044\n",
            "Epoch 388/1000, Training Loss: 0.00025631777362293903, Test Loss: 0.00022341844836350016\n",
            "Epoch 389/1000, Training Loss: 0.00025162554478075735, Test Loss: 0.0002194128426155569\n",
            "Epoch 390/1000, Training Loss: 0.00024705720683230775, Test Loss: 0.00021551127420037168\n",
            "Epoch 391/1000, Training Loss: 0.00024260901571467855, Test Loss: 0.00021171075641747327\n",
            "Epoch 392/1000, Training Loss: 0.00023827734587065445, Test Loss: 0.0002080083875207171\n",
            "Epoch 393/1000, Training Loss: 0.00023405868685652184, Test Loss: 0.0002044013488181651\n",
            "Epoch 394/1000, Training Loss: 0.00022994963998869384, Test Loss: 0.0002008869027578093\n",
            "Epoch 395/1000, Training Loss: 0.00022594691503577193, Test Loss: 0.00019746239100570405\n",
            "Epoch 396/1000, Training Loss: 0.00022204732696182075, Test Loss: 0.00019412523252252949\n",
            "Epoch 397/1000, Training Loss: 0.00021824779272567858, Test Loss: 0.00019087292164393\n",
            "Epoch 398/1000, Training Loss: 0.00021454532814045045, Test Loss: 0.00018770302616950005\n",
            "Epoch 399/1000, Training Loss: 0.00021093704479656388, Test Loss: 0.0001846131854647092\n",
            "Epoch 400/1000, Training Loss: 0.00020742014705115962, Test Loss: 0.0001816011085796012\n",
            "Epoch 401/1000, Training Loss: 0.00020399192908606247, Test Loss: 0.00017866457238769321\n",
            "Epoch 402/1000, Training Loss: 0.00020064977203598309, Test Loss: 0.00017580141974800914\n",
            "Epoch 403/1000, Training Loss: 0.00019739114118823457, Test Loss: 0.00017300955769288862\n",
            "Epoch 404/1000, Training Loss: 0.00019421358325479466, Test Loss: 0.00017028695564379981\n",
            "Epoch 405/1000, Training Loss: 0.0001911147237171898, Test Loss: 0.0001676316436571036\n",
            "Epoch 406/1000, Training Loss: 0.00018809226424436928, Test Loss: 0.00016504171070139435\n",
            "Epoch 407/1000, Training Loss: 0.0001851439801834433, Test Loss: 0.00016251530296780187\n",
            "Epoch 408/1000, Training Loss: 0.0001822677181229081, Test Loss: 0.0001600506222143515\n",
            "Epoch 409/1000, Training Loss: 0.00017946139352780028, Test Loss: 0.00015764592414531817\n",
            "Epoch 410/1000, Training Loss: 0.00017672298844598116, Test Loss: 0.00015529951682624954\n",
            "Epoch 411/1000, Training Loss: 0.00017405054928462704, Test Loss: 0.00015300975913518642\n",
            "Epoch 412/1000, Training Loss: 0.00017144218465584848, Test Loss: 0.000150775059250427\n",
            "Epoch 413/1000, Training Loss: 0.00016889606329025843, Test Loss: 0.00014859387317504914\n",
            "Epoch 414/1000, Training Loss: 0.00016641041201717922, Test Loss: 0.0001464647032982533\n",
            "Epoch 415/1000, Training Loss: 0.0001639835138101403, Test Loss: 0.00014438609699349164\n",
            "Epoch 416/1000, Training Loss: 0.00016161370589620055, Test Loss: 0.00014235664525321756\n",
            "Epoch 417/1000, Training Loss: 0.00015929937792763275, Test Loss: 0.00014037498136002766\n",
            "Epoch 418/1000, Training Loss: 0.00015703897021441217, Test Loss: 0.00013843977959386518\n",
            "Epoch 419/1000, Training Loss: 0.00015483097201597135, Test Loss: 0.00013654975397488754\n",
            "Epoch 420/1000, Training Loss: 0.00015267391989061098, Test Loss: 0.0001347036570415256\n",
            "Epoch 421/1000, Training Loss: 0.0001505663961010066, Test Loss: 0.00013290027866325258\n",
            "Epoch 422/1000, Training Loss: 0.00014850702707419, Test Loss: 0.00013113844488747331\n",
            "Epoch 423/1000, Training Loss: 0.00014649448191441495, Test Loss: 0.00012941701681994382\n",
            "Epoch 424/1000, Training Loss: 0.00014452747096732342, Test Loss: 0.0001277348895380928\n",
            "Epoch 425/1000, Training Loss: 0.00014260474443385067, Test Loss: 0.00012609099103658708\n",
            "Epoch 426/1000, Training Loss: 0.00014072509103229297, Test Loss: 0.00012448428120445402\n",
            "Epoch 427/1000, Training Loss: 0.00013888733670701728, Test Loss: 0.00012291375083306637\n",
            "Epoch 428/1000, Training Loss: 0.00013709034338230504, Test Loss: 0.00012137842065427601\n",
            "Epoch 429/1000, Training Loss: 0.00013533300775984017, Test Loss: 0.00011987734040797963\n",
            "Epoch 430/1000, Training Loss: 0.00013361426015840697, Test Loss: 0.00011840958793839332\n",
            "Epoch 431/1000, Training Loss: 0.00013193306339435187, Test Loss: 0.00011697426831829192\n",
            "Epoch 432/1000, Training Loss: 0.00013028841170144268, Test Loss: 0.00011557051300050455\n",
            "Epoch 433/1000, Training Loss: 0.00012867932968876496, Test Loss: 0.00011419747899592133\n",
            "Epoch 434/1000, Training Loss: 0.00012710487133533327, Test Loss: 0.00011285434807730057\n",
            "Epoch 435/1000, Training Loss: 0.00012556411902014353, Test Loss: 0.00011154032600815905\n",
            "Epoch 436/1000, Training Loss: 0.00012405618258640603, Test Loss: 0.00011025464179602322\n",
            "Epoch 437/1000, Training Loss: 0.00012258019843876396, Test Loss: 0.00010899654696935934\n",
            "Epoch 438/1000, Training Loss: 0.00012113532867230756, Test Loss: 0.00010776531487748166\n",
            "Epoch 439/1000, Training Loss: 0.00011972076023226013, Test Loss: 0.0001065602400127693\n",
            "Epoch 440/1000, Training Loss: 0.00011833570410321643, Test Loss: 0.00010538063735450955\n",
            "Epoch 441/1000, Training Loss: 0.00011697939452687736, Test Loss: 0.00010422584173373383\n",
            "Epoch 442/1000, Training Loss: 0.00011565108824724852, Test Loss: 0.0001030952072183919\n",
            "Epoch 443/1000, Training Loss: 0.00011435006378229711, Test Loss: 0.0001019881065182467\n",
            "Epoch 444/1000, Training Loss: 0.00011307562072110486, Test Loss: 0.00010090393040887281\n",
            "Epoch 445/1000, Training Loss: 0.00011182707904559849, Test Loss: 9.984208717416902e-05\n",
            "Epoch 446/1000, Training Loss: 0.00011060377847592493, Test Loss: 9.880200206678062e-05\n",
            "Epoch 447/1000, Training Loss: 0.00010940507783865092, Test Loss: 9.778311678590107e-05\n",
            "Epoch 448/1000, Training Loss: 0.00010823035445690866, Test Loss: 9.678488897186078e-05\n",
            "Epoch 449/1000, Training Loss: 0.0001070790035617045, Test Loss: 9.58067917169939e-05\n",
            "Epoch 450/1000, Training Loss: 0.00010595043772361032, Test Loss: 9.484831309224275e-05\n",
            "Epoch 451/1000, Training Loss: 0.00010484408630407532, Test Loss: 9.390895568900234e-05\n",
            "Epoch 452/1000, Training Loss: 0.00010375939492565158, Test Loss: 9.298823617569589e-05\n",
            "Epoch 453/1000, Training Loss: 0.00010269582496043366, Test Loss: 9.208568486862329e-05\n",
            "Epoch 454/1000, Training Loss: 0.00010165285303604593, Test Loss: 9.120084531660256e-05\n",
            "Epoch 455/1000, Training Loss: 0.00010062997055852376, Test Loss: 9.033327389895039e-05\n",
            "Epoch 456/1000, Training Loss: 9.96266832514814e-05, Test Loss: 8.948253943637923e-05\n",
            "Epoch 457/1000, Training Loss: 9.864251071095769e-05, Test Loss: 8.864822281436458e-05\n",
            "Epoch 458/1000, Training Loss: 9.767698597538385e-05, Test Loss: 8.782991661860185e-05\n",
            "Epoch 459/1000, Training Loss: 9.672965511009372e-05, Test Loss: 8.702722478211633e-05\n",
            "Epoch 460/1000, Training Loss: 9.580007680587966e-05, Test Loss: 8.623976224368131e-05\n",
            "Epoch 461/1000, Training Loss: 9.488782199104529e-05, Test Loss: 8.546715461713683e-05\n",
            "Epoch 462/1000, Training Loss: 9.399247345649714e-05, Test Loss: 8.47090378712706e-05\n",
            "Epoch 463/1000, Training Loss: 9.31136254933837e-05, Test Loss: 8.396505801990523e-05\n",
            "Epoch 464/1000, Training Loss: 9.225088354283376e-05, Test Loss: 8.323487082184487e-05\n",
            "Epoch 465/1000, Training Loss: 9.14038638573493e-05, Test Loss: 8.251814149036906e-05\n",
            "Epoch 466/1000, Training Loss: 9.057219317344458e-05, Test Loss: 8.181454441194693e-05\n",
            "Epoch 467/1000, Training Loss: 8.975550839511473e-05, Test Loss: 8.112376287386834e-05\n",
            "Epoch 468/1000, Training Loss: 8.895345628775628e-05, Test Loss: 8.044548880050413e-05\n",
            "Epoch 469/1000, Training Loss: 8.8165693182158e-05, Test Loss: 7.977942249789555e-05\n",
            "Epoch 470/1000, Training Loss: 8.739188468819995e-05, Test Loss: 7.91252724064099e-05\n",
            "Epoch 471/1000, Training Loss: 8.6631705417925e-05, Test Loss: 7.848275486118824e-05\n",
            "Epoch 472/1000, Training Loss: 8.588483871764329e-05, Test Loss: 7.785159386013239e-05\n",
            "Epoch 473/1000, Training Loss: 8.515097640874339e-05, Test Loss: 7.723152083916979e-05\n",
            "Epoch 474/1000, Training Loss: 8.442981853691689e-05, Test Loss: 7.6622274454577e-05\n",
            "Epoch 475/1000, Training Loss: 8.372107312949028e-05, Test Loss: 7.602360037210797e-05\n",
            "Epoch 476/1000, Training Loss: 8.302445596057925e-05, Test Loss: 7.543525106271601e-05\n",
            "Epoch 477/1000, Training Loss: 8.23396903237896e-05, Test Loss: 7.485698560464018e-05\n",
            "Epoch 478/1000, Training Loss: 8.166650681221157e-05, Test Loss: 7.428856949166326e-05\n",
            "Epoch 479/1000, Training Loss: 8.100464310544612e-05, Test Loss: 7.372977444732556e-05\n",
            "Epoch 480/1000, Training Loss: 8.03538437634103e-05, Test Loss: 7.318037824489646e-05\n",
            "Epoch 481/1000, Training Loss: 7.971386002670427e-05, Test Loss: 7.264016453292757e-05\n",
            "Epoch 482/1000, Training Loss: 7.90844496233e-05, Test Loss: 7.210892266619657e-05\n",
            "Epoch 483/1000, Training Loss: 7.846537658133471e-05, Test Loss: 7.158644754185633e-05\n",
            "Epoch 484/1000, Training Loss: 7.785641104781594e-05, Test Loss: 7.107253944064828e-05\n",
            "Epoch 485/1000, Training Loss: 7.725732911300852e-05, Test Loss: 7.056700387297782e-05\n",
            "Epoch 486/1000, Training Loss: 7.666791264033024e-05, Test Loss: 7.00696514297184e-05\n",
            "Epoch 487/1000, Training Loss: 7.608794910155821e-05, Test Loss: 6.958029763758184e-05\n",
            "Epoch 488/1000, Training Loss: 7.551723141717572e-05, Test Loss: 6.909876281890638e-05\n",
            "Epoch 489/1000, Training Loss: 7.495555780167722e-05, Test Loss: 6.862487195572778e-05\n",
            "Epoch 490/1000, Training Loss: 7.440273161365628e-05, Test Loss: 6.815845455797113e-05\n",
            "Epoch 491/1000, Training Loss: 7.385856121054307e-05, Test Loss: 6.769934453566591e-05\n",
            "Epoch 492/1000, Training Loss: 7.332285980780193e-05, Test Loss: 6.724738007501617e-05\n",
            "Epoch 493/1000, Training Loss: 7.279544534246368e-05, Test Loss: 6.680240351823348e-05\n",
            "Epoch 494/1000, Training Loss: 7.227614034083749e-05, Test Loss: 6.636426124699136e-05\n",
            "Epoch 495/1000, Training Loss: 7.176477179026578e-05, Test Loss: 6.593280356938953e-05\n",
            "Epoch 496/1000, Training Loss: 7.126117101478875e-05, Test Loss: 6.550788461032594e-05\n",
            "Epoch 497/1000, Training Loss: 7.076517355458782e-05, Test Loss: 6.508936220514894e-05\n",
            "Epoch 498/1000, Training Loss: 7.027661904907971e-05, Test Loss: 6.467709779649652e-05\n",
            "Epoch 499/1000, Training Loss: 6.979535112355272e-05, Test Loss: 6.427095633422475e-05\n",
            "Epoch 500/1000, Training Loss: 6.932121727921675e-05, Test Loss: 6.387080617831388e-05\n",
            "Epoch 501/1000, Training Loss: 6.885406878655953e-05, Test Loss: 6.347651900466725e-05\n",
            "Epoch 502/1000, Training Loss: 6.839376058190594e-05, Test Loss: 6.308796971371002e-05\n",
            "Epoch 503/1000, Training Loss: 6.794015116706498e-05, Test Loss: 6.270503634169294e-05\n",
            "Epoch 504/1000, Training Loss: 6.749310251197928e-05, Test Loss: 6.232759997462203e-05\n",
            "Epoch 505/1000, Training Loss: 6.705247996026662e-05, Test Loss: 6.195554466473245e-05\n",
            "Epoch 506/1000, Training Loss: 6.66181521375662e-05, Test Loss: 6.158875734941717e-05\n",
            "Epoch 507/1000, Training Loss: 6.618999086260164e-05, Test Loss: 6.122712777254211e-05\n",
            "Epoch 508/1000, Training Loss: 6.576787106086646e-05, Test Loss: 6.0870548408070676e-05\n",
            "Epoch 509/1000, Training Loss: 6.535167068085312e-05, Test Loss: 6.051891438591877e-05\n",
            "Epoch 510/1000, Training Loss: 6.494127061275128e-05, Test Loss: 6.0172123419985555e-05\n",
            "Epoch 511/1000, Training Loss: 6.453655460951419e-05, Test Loss: 5.983007573827273e-05\n",
            "Epoch 512/1000, Training Loss: 6.41374092102449e-05, Test Loss: 5.949267401504082e-05\n",
            "Epoch 513/1000, Training Loss: 6.374372366581393e-05, Test Loss: 5.9159823304941116e-05\n",
            "Epoch 514/1000, Training Loss: 6.335538986663575e-05, Test Loss: 5.88314309790415e-05\n",
            "Epoch 515/1000, Training Loss: 6.297230227254682e-05, Test Loss: 5.850740666271753e-05\n",
            "Epoch 516/1000, Training Loss: 6.259435784471137e-05, Test Loss: 5.818766217532592e-05\n",
            "Epoch 517/1000, Training Loss: 6.222145597949282e-05, Test Loss: 5.787211147161859e-05\n",
            "Epoch 518/1000, Training Loss: 6.185349844422852e-05, Test Loss: 5.756067058483864e-05\n",
            "Epoch 519/1000, Training Loss: 6.149038931485352e-05, Test Loss: 5.72532575714485e-05\n",
            "Epoch 520/1000, Training Loss: 6.113203491530709e-05, Test Loss: 5.694979245743854e-05\n",
            "Epoch 521/1000, Training Loss: 6.077834375867739e-05, Test Loss: 5.66501971861708e-05\n",
            "Epoch 522/1000, Training Loss: 6.0429226490019285e-05, Test Loss: 5.635439556770757e-05\n",
            "Epoch 523/1000, Training Loss: 6.008459583080241e-05, Test Loss: 5.6062313229577177e-05\n",
            "Epoch 524/1000, Training Loss: 5.974436652493927e-05, Test Loss: 5.57738775689495e-05\n",
            "Epoch 525/1000, Training Loss: 5.9408455286332226e-05, Test Loss: 5.548901770614627e-05\n",
            "Epoch 526/1000, Training Loss: 5.907678074790994e-05, Test Loss: 5.5207664439481105e-05\n",
            "Epoch 527/1000, Training Loss: 5.874926341209809e-05, Test Loss: 5.492975020136643e-05\n",
            "Epoch 528/1000, Training Loss: 5.842582560268426e-05, Test Loss: 5.4655209015656275e-05\n",
            "Epoch 529/1000, Training Loss: 5.8106391418027306e-05, Test Loss: 5.438397645618647e-05\n",
            "Epoch 530/1000, Training Loss: 5.779088668558243e-05, Test Loss: 5.411598960648144e-05\n",
            "Epoch 531/1000, Training Loss: 5.7479238917693893e-05, Test Loss: 5.38511870205785e-05\n",
            "Epoch 532/1000, Training Loss: 5.7171377268614344e-05, Test Loss: 5.3589508684954054e-05\n",
            "Epoch 533/1000, Training Loss: 5.686723249272748e-05, Test Loss: 5.3330895981509735e-05\n",
            "Epoch 534/1000, Training Loss: 5.656673690391849e-05, Test Loss: 5.3075291651579795e-05\n",
            "Epoch 535/1000, Training Loss: 5.626982433607666e-05, Test Loss: 5.282263976094724e-05\n",
            "Epoch 536/1000, Training Loss: 5.5976430104683206e-05, Test Loss: 5.257288566582067e-05\n",
            "Epoch 537/1000, Training Loss: 5.5686490969459534e-05, Test Loss: 5.232597597975447e-05\n",
            "Epoch 538/1000, Training Loss: 5.539994509803613e-05, Test Loss: 5.2081858541480955e-05\n",
            "Epoch 539/1000, Training Loss: 5.5116732030625535e-05, Test Loss: 5.184048238362702e-05\n",
            "Epoch 540/1000, Training Loss: 5.4836792645651685e-05, Test Loss: 5.160179770228928e-05\n",
            "Epoch 541/1000, Training Loss: 5.456006912632607e-05, Test Loss: 5.1365755827446814e-05\n",
            "Epoch 542/1000, Training Loss: 5.4286504928126466e-05, Test Loss: 5.113230919417688e-05\n",
            "Epoch 543/1000, Training Loss: 5.401604474715959e-05, Test Loss: 5.090141131466019e-05\n",
            "Epoch 544/1000, Training Loss: 5.374863448938928e-05, Test Loss: 5.067301675095023e-05\n",
            "Epoch 545/1000, Training Loss: 5.3484221240685126e-05, Test Loss: 5.044708108848157e-05\n",
            "Epoch 546/1000, Training Loss: 5.322275323768508e-05, Test Loss: 5.0223560910292866e-05\n",
            "Epoch 547/1000, Training Loss: 5.29641798394435e-05, Test Loss: 5.00024137719575e-05\n",
            "Epoch 548/1000, Training Loss: 5.2708451499834994e-05, Test Loss: 4.9783598177181413e-05\n",
            "Epoch 549/1000, Training Loss: 5.245551974070682e-05, Test Loss: 4.956707355406606e-05\n",
            "Epoch 550/1000, Training Loss: 5.2205337125738464e-05, Test Loss: 4.935280023200915e-05\n",
            "Epoch 551/1000, Training Loss: 5.1957857235005844e-05, Test Loss: 4.9140739419224275e-05\n",
            "Epoch 552/1000, Training Loss: 5.17130346402221e-05, Test Loss: 4.893085318086546e-05\n",
            "Epoch 553/1000, Training Loss: 5.147082488063359e-05, Test Loss: 4.87231044177365e-05\n",
            "Epoch 554/1000, Training Loss: 5.123118443955457e-05, Test Loss: 4.851745684556906e-05\n",
            "Epoch 555/1000, Training Loss: 5.099407072152614e-05, Test Loss: 4.831387497485613e-05\n",
            "Epoch 556/1000, Training Loss: 5.075944203007488e-05, Test Loss: 4.811232409121616e-05\n",
            "Epoch 557/1000, Training Loss: 5.0527257546058616e-05, Test Loss: 4.79127702362866e-05\n",
            "Epoch 558/1000, Training Loss: 5.029747730658002e-05, Test Loss: 4.7715180189117506e-05\n",
            "Epoch 559/1000, Training Loss: 5.007006218445466e-05, Test Loss: 4.751952144806116e-05\n",
            "Epoch 560/1000, Training Loss: 4.9844973868215866e-05, Test Loss: 4.7325762213142224e-05\n",
            "Epoch 561/1000, Training Loss: 4.9622174842640576e-05, Test Loss: 4.713387136888545e-05\n",
            "Epoch 562/1000, Training Loss: 4.940162836978632e-05, Test Loss: 4.694381846760398e-05\n",
            "Epoch 563/1000, Training Loss: 4.918329847051742e-05, Test Loss: 4.675557371312333e-05\n",
            "Epoch 564/1000, Training Loss: 4.896714990651043e-05, Test Loss: 4.656910794493052e-05\n",
            "Epoch 565/1000, Training Loss: 4.8753148162730076e-05, Test Loss: 4.6384392622736875e-05\n",
            "Epoch 566/1000, Training Loss: 4.854125943035313e-05, Test Loss: 4.620139981145529e-05\n",
            "Epoch 567/1000, Training Loss: 4.833145059013661e-05, Test Loss: 4.6020102166554096e-05\n",
            "Epoch 568/1000, Training Loss: 4.812368919621143e-05, Test Loss: 4.584047291980366e-05\n",
            "Epoch 569/1000, Training Loss: 4.7917943460293094e-05, Test Loss: 4.566248586538773e-05\n",
            "Epoch 570/1000, Training Loss: 4.7714182236298505e-05, Test Loss: 4.548611534637785e-05\n",
            "Epoch 571/1000, Training Loss: 4.751237500535315e-05, Test Loss: 4.5311336241556334e-05\n",
            "Epoch 572/1000, Training Loss: 4.7312491861185675e-05, Test Loss: 4.5138123952584586e-05\n",
            "Epoch 573/1000, Training Loss: 4.71145034958898e-05, Test Loss: 4.4966454391495685e-05\n",
            "Epoch 574/1000, Training Loss: 4.691838118605249e-05, Test Loss: 4.479630396851473e-05\n",
            "Epoch 575/1000, Training Loss: 4.672409677923136e-05, Test Loss: 4.4627649580190627e-05\n",
            "Epoch 576/1000, Training Loss: 4.653162268077417e-05, Test Loss: 4.446046859783327e-05\n",
            "Epoch 577/1000, Training Loss: 4.634093184097639e-05, Test Loss: 4.429473885624689e-05\n",
            "Epoch 578/1000, Training Loss: 4.615199774255601e-05, Test Loss: 4.413043864275148e-05\n",
            "Epoch 579/1000, Training Loss: 4.5964794388448606e-05, Test Loss: 4.3967546686485145e-05\n",
            "Epoch 580/1000, Training Loss: 4.577929628990676e-05, Test Loss: 4.380604214798094e-05\n",
            "Epoch 581/1000, Training Loss: 4.559547845490057e-05, Test Loss: 4.364590460900707e-05\n",
            "Epoch 582/1000, Training Loss: 4.541331637680426e-05, Test Loss: 4.348711406266511e-05\n",
            "Epoch 583/1000, Training Loss: 4.523278602337005e-05, Test Loss: 4.332965090374338e-05\n",
            "Epoch 584/1000, Training Loss: 4.505386382597396e-05, Test Loss: 4.31734959193111e-05\n",
            "Epoch 585/1000, Training Loss: 4.487652666912997e-05, Test Loss: 4.301863027955101e-05\n",
            "Epoch 586/1000, Training Loss: 4.4700751880265706e-05, Test Loss: 4.286503552882594e-05\n",
            "Epoch 587/1000, Training Loss: 4.452651721975062e-05, Test Loss: 4.2712693576968557e-05\n",
            "Epoch 588/1000, Training Loss: 4.43538008711715e-05, Test Loss: 4.2561586690792134e-05\n",
            "Epoch 589/1000, Training Loss: 4.418258143184839e-05, Test Loss: 4.24116974858135e-05\n",
            "Epoch 590/1000, Training Loss: 4.401283790358267e-05, Test Loss: 4.2263008918184006e-05\n",
            "Epoch 591/1000, Training Loss: 4.384454968363387e-05, Test Loss: 4.211550427682385e-05\n",
            "Epoch 592/1000, Training Loss: 4.367769655592173e-05, Test Loss: 4.1969167175752805e-05\n",
            "Epoch 593/1000, Training Loss: 4.351225868243537e-05, Test Loss: 4.182398154661117e-05\n",
            "Epoch 594/1000, Training Loss: 4.3348216594862304e-05, Test Loss: 4.167993163137112e-05\n",
            "Epoch 595/1000, Training Loss: 4.318555118641515e-05, Test Loss: 4.153700197522485e-05\n",
            "Epoch 596/1000, Training Loss: 4.302424370386079e-05, Test Loss: 4.1395177419652625e-05\n",
            "Epoch 597/1000, Training Loss: 4.2864275739742835e-05, Test Loss: 4.125444309566453e-05\n",
            "Epoch 598/1000, Training Loss: 4.270562922479347e-05, Test Loss: 4.111478441720596e-05\n",
            "Epoch 599/1000, Training Loss: 4.2548286420526294e-05, Test Loss: 4.0976187074728086e-05\n",
            "Epoch 600/1000, Training Loss: 4.239222991201431e-05, Test Loss: 4.083863702891804e-05\n",
            "Epoch 601/1000, Training Loss: 4.223744260083669e-05, Test Loss: 4.0702120504585515e-05\n",
            "Epoch 602/1000, Training Loss: 4.2083907698198475e-05, Test Loss: 4.0566623984695535e-05\n",
            "Epoch 603/1000, Training Loss: 4.193160871821553e-05, Test Loss: 4.043213420455105e-05\n",
            "Epoch 604/1000, Training Loss: 4.178052947136057e-05, Test Loss: 4.0298638146118955e-05\n",
            "Epoch 605/1000, Training Loss: 4.163065405806673e-05, Test Loss: 4.016612303249645e-05\n",
            "Epoch 606/1000, Training Loss: 4.148196686248279e-05, Test Loss: 4.0034576322508485e-05\n",
            "Epoch 607/1000, Training Loss: 4.133445254638108e-05, Test Loss: 3.9903985705442366e-05\n",
            "Epoch 608/1000, Training Loss: 4.1188096043206326e-05, Test Loss: 3.977433909590681e-05\n",
            "Epoch 609/1000, Training Loss: 4.104288255227044e-05, Test Loss: 3.964562462882123e-05\n",
            "Epoch 610/1000, Training Loss: 4.0898797533081026e-05, Test Loss: 3.951783065452468e-05\n",
            "Epoch 611/1000, Training Loss: 4.075582669980824e-05, Test Loss: 3.9390945734001576e-05\n",
            "Epoch 612/1000, Training Loss: 4.061395601588029e-05, Test Loss: 3.9264958634226845e-05\n",
            "Epoch 613/1000, Training Loss: 4.047317168870903e-05, Test Loss: 3.9139858323624075e-05\n",
            "Epoch 614/1000, Training Loss: 4.0333460164536306e-05, Test Loss: 3.901563396763257e-05\n",
            "Epoch 615/1000, Training Loss: 4.019480812340623e-05, Test Loss: 3.8892274924380346e-05\n",
            "Epoch 616/1000, Training Loss: 4.0057202474251835e-05, Test Loss: 3.8769770740464074e-05\n",
            "Epoch 617/1000, Training Loss: 3.99206303500986e-05, Test Loss: 3.864811114682904e-05\n",
            "Epoch 618/1000, Training Loss: 3.9785079103382325e-05, Test Loss: 3.852728605475259e-05\n",
            "Epoch 619/1000, Training Loss: 3.965053630137272e-05, Test Loss: 3.8407285551916124e-05\n",
            "Epoch 620/1000, Training Loss: 3.95169897217062e-05, Test Loss: 3.82880998985801e-05\n",
            "Epoch 621/1000, Training Loss: 3.9384427348023215e-05, Test Loss: 3.8169719523844274e-05\n",
            "Epoch 622/1000, Training Loss: 3.925283736570499e-05, Test Loss: 3.805213502200378e-05\n",
            "Epoch 623/1000, Training Loss: 3.912220815771128e-05, Test Loss: 3.7935337148986445e-05\n",
            "Epoch 624/1000, Training Loss: 3.89925283005142e-05, Test Loss: 3.781931681887855e-05\n",
            "Epoch 625/1000, Training Loss: 3.8863786560125155e-05, Test Loss: 3.770406510053423e-05\n",
            "Epoch 626/1000, Training Loss: 3.8735971888214704e-05, Test Loss: 3.758957321426155e-05\n",
            "Epoch 627/1000, Training Loss: 3.8609073418320986e-05, Test Loss: 3.747583252859332e-05\n",
            "Epoch 628/1000, Training Loss: 3.848308046214567e-05, Test Loss: 3.7362834557128197e-05\n",
            "Epoch 629/1000, Training Loss: 3.8357982505936634e-05, Test Loss: 3.725057095545361e-05\n",
            "Epoch 630/1000, Training Loss: 3.823376920695017e-05, Test Loss: 3.713903351813544e-05\n",
            "Epoch 631/1000, Training Loss: 3.8110430389998786e-05, Test Loss: 3.702821417578331e-05\n",
            "Epoch 632/1000, Training Loss: 3.798795604407287e-05, Test Loss: 3.691810499218297e-05\n",
            "Epoch 633/1000, Training Loss: 3.786633631904254e-05, Test Loss: 3.680869816149646e-05\n",
            "Epoch 634/1000, Training Loss: 3.7745561522435556e-05, Test Loss: 3.6699986005530427e-05\n",
            "Epoch 635/1000, Training Loss: 3.7625622116285096e-05, Test Loss: 3.6591960971065413e-05\n",
            "Epoch 636/1000, Training Loss: 3.7506508714051166e-05, Test Loss: 3.648461562725057e-05\n",
            "Epoch 637/1000, Training Loss: 3.7388212077611645e-05, Test Loss: 3.637794266305714e-05\n",
            "Epoch 638/1000, Training Loss: 3.727072311432247e-05, Test Loss: 3.627193488479574e-05\n",
            "Epoch 639/1000, Training Loss: 3.715403287414042e-05, Test Loss: 3.6166585213685846e-05\n",
            "Epoch 640/1000, Training Loss: 3.703813254681466e-05, Test Loss: 3.606188668348901e-05\n",
            "Epoch 641/1000, Training Loss: 3.6923013459139346e-05, Test Loss: 3.5957832438189555e-05\n",
            "Epoch 642/1000, Training Loss: 3.680866707227007e-05, Test Loss: 3.585441572973656e-05\n",
            "Epoch 643/1000, Training Loss: 3.669508497909631e-05, Test Loss: 3.575162991583421e-05\n",
            "Epoch 644/1000, Training Loss: 3.6582258901676536e-05, Test Loss: 3.564946845778381e-05\n",
            "Epoch 645/1000, Training Loss: 3.647018068872895e-05, Test Loss: 3.554792491837706e-05\n",
            "Epoch 646/1000, Training Loss: 3.635884231317952e-05, Test Loss: 3.5446992959837345e-05\n",
            "Epoch 647/1000, Training Loss: 3.6248235869761096e-05, Test Loss: 3.534666634181018e-05\n",
            "Epoch 648/1000, Training Loss: 3.6138353572669825e-05, Test Loss: 3.5246938919397974e-05\n",
            "Epoch 649/1000, Training Loss: 3.6029187753271904e-05, Test Loss: 3.514780464124063e-05\n",
            "Epoch 650/1000, Training Loss: 3.592073085785954e-05, Test Loss: 3.504925754764087e-05\n",
            "Epoch 651/1000, Training Loss: 3.5812975445459474e-05, Test Loss: 3.4951291768732524e-05\n",
            "Epoch 652/1000, Training Loss: 3.570591418568694e-05, Test Loss: 3.4853901522693086e-05\n",
            "Epoch 653/1000, Training Loss: 3.559953985664891e-05, Test Loss: 3.47570811139926e-05\n",
            "Epoch 654/1000, Training Loss: 3.5493845342891465e-05, Test Loss: 3.4660824931684774e-05\n",
            "Epoch 655/1000, Training Loss: 3.538882363339368e-05, Test Loss: 3.4565127447739196e-05\n",
            "Epoch 656/1000, Training Loss: 3.5284467819605025e-05, Test Loss: 3.446998321540913e-05\n",
            "Epoch 657/1000, Training Loss: 3.518077109352543e-05, Test Loss: 3.43753868676387e-05\n",
            "Epoch 658/1000, Training Loss: 3.5077726745826844e-05, Test Loss: 3.4281333115502216e-05\n",
            "Epoch 659/1000, Training Loss: 3.4975328164015164e-05, Test Loss: 3.4187816746684976e-05\n",
            "Epoch 660/1000, Training Loss: 3.487356883063463e-05, Test Loss: 3.409483262399277e-05\n",
            "Epoch 661/1000, Training Loss: 3.477244232150813e-05, Test Loss: 3.400237568389859e-05\n",
            "Epoch 662/1000, Training Loss: 3.467194230401584e-05, Test Loss: 3.3910440935120765e-05\n",
            "Epoch 663/1000, Training Loss: 3.4572062535413554e-05, Test Loss: 3.381902345723523e-05\n",
            "Epoch 664/1000, Training Loss: 3.447279686118345e-05, Test Loss: 3.372811839931558e-05\n",
            "Epoch 665/1000, Training Loss: 3.4374139213424723e-05, Test Loss: 3.3637720978607026e-05\n",
            "Epoch 666/1000, Training Loss: 3.427608360927406e-05, Test Loss: 3.3547826479227445e-05\n",
            "Epoch 667/1000, Training Loss: 3.417862414936225e-05, Test Loss: 3.345843025090078e-05\n",
            "Epoch 668/1000, Training Loss: 3.408175501630576e-05, Test Loss: 3.3369527707715584e-05\n",
            "Epoch 669/1000, Training Loss: 3.398547047322585e-05, Test Loss: 3.328111432691269e-05\n",
            "Epoch 670/1000, Training Loss: 3.388976486230268e-05, Test Loss: 3.319318564770149e-05\n",
            "Epoch 671/1000, Training Loss: 3.379463260335865e-05, Test Loss: 3.310573727009745e-05\n",
            "Epoch 672/1000, Training Loss: 3.3700068192472474e-05, Test Loss: 3.301876485379355e-05\n",
            "Epoch 673/1000, Training Loss: 3.3606066200622325e-05, Test Loss: 3.2932264117049495e-05\n",
            "Epoch 674/1000, Training Loss: 3.3512621272356674e-05, Test Loss: 3.2846230835608403e-05\n",
            "Epoch 675/1000, Training Loss: 3.34197281244945e-05, Test Loss: 3.2760660841637595e-05\n",
            "Epoch 676/1000, Training Loss: 3.3327381544851514e-05, Test Loss: 3.2675550022694716e-05\n",
            "Epoch 677/1000, Training Loss: 3.323557639099437e-05, Test Loss: 3.2590894320709297e-05\n",
            "Epoch 678/1000, Training Loss: 3.31443075890195e-05, Test Loss: 3.2506689730998014e-05\n",
            "Epoch 679/1000, Training Loss: 3.305357013235877e-05, Test Loss: 3.242293230129108e-05\n",
            "Epoch 680/1000, Training Loss: 3.2963359080609474e-05, Test Loss: 3.233961813078712e-05\n",
            "Epoch 681/1000, Training Loss: 3.2873669558386865e-05, Test Loss: 3.2256743369223935e-05\n",
            "Epoch 682/1000, Training Loss: 3.278449675420453e-05, Test Loss: 3.2174304215970686e-05\n",
            "Epoch 683/1000, Training Loss: 3.269583591937381e-05, Test Loss: 3.209229691914503e-05\n",
            "Epoch 684/1000, Training Loss: 3.260768236692824e-05, Test Loss: 3.20107177747416e-05\n",
            "Epoch 685/1000, Training Loss: 3.252003147056852e-05, Test Loss: 3.1929563125782e-05\n",
            "Epoch 686/1000, Training Loss: 3.243287866363216e-05, Test Loss: 3.184882936148891e-05\n",
            "Epoch 687/1000, Training Loss: 3.2346219438079644e-05, Test Loss: 3.176851291646824e-05\n",
            "Epoch 688/1000, Training Loss: 3.226004934350578e-05, Test Loss: 3.1688610269917695e-05\n",
            "Epoch 689/1000, Training Loss: 3.2174363986168545e-05, Test Loss: 3.1609117944846505e-05\n",
            "Epoch 690/1000, Training Loss: 3.2089159028037704e-05, Test Loss: 3.153003250731462e-05\n",
            "Epoch 691/1000, Training Loss: 3.2004430185865026e-05, Test Loss: 3.145135056568893e-05\n",
            "Epoch 692/1000, Training Loss: 3.192017323026999e-05, Test Loss: 3.137306876991161e-05\n",
            "Epoch 693/1000, Training Loss: 3.18363839848471e-05, Test Loss: 3.129518381078929e-05\n",
            "Epoch 694/1000, Training Loss: 3.1753058325289144e-05, Test Loss: 3.1217692419292634e-05\n",
            "Epoch 695/1000, Training Loss: 3.167019217852908e-05, Test Loss: 3.11405913658757e-05\n",
            "Epoch 696/1000, Training Loss: 3.158778152189775e-05, Test Loss: 3.1063877459803016e-05\n",
            "Epoch 697/1000, Training Loss: 3.150582238230095e-05, Test Loss: 3.098754754849705e-05\n",
            "Epoch 698/1000, Training Loss: 3.142431083541058e-05, Test Loss: 3.0911598516899476e-05\n",
            "Epoch 699/1000, Training Loss: 3.134324300487292e-05, Test Loss: 3.083602728683951e-05\n",
            "Epoch 700/1000, Training Loss: 3.1262615061532345e-05, Test Loss: 3.076083081641993e-05\n",
            "Epoch 701/1000, Training Loss: 3.118242322267059e-05, Test Loss: 3.068600609941983e-05\n",
            "Epoch 702/1000, Training Loss: 3.11026637512608e-05, Test Loss: 3.0611550164701315e-05\n",
            "Epoch 703/1000, Training Loss: 3.1023332955236845e-05, Test Loss: 3.0537460075635395e-05\n",
            "Epoch 704/1000, Training Loss: 3.094442718677616e-05, Test Loss: 3.04637329295366e-05\n",
            "Epoch 705/1000, Training Loss: 3.086594284159785e-05, Test Loss: 3.0390365857109682e-05\n",
            "Epoch 706/1000, Training Loss: 3.078787635827271e-05, Test Loss: 3.0317356021910352e-05\n",
            "Epoch 707/1000, Training Loss: 3.0710224217548366e-05, Test Loss: 3.0244700619811797e-05\n",
            "Epoch 708/1000, Training Loss: 3.0632982941686806e-05, Test Loss: 3.0172396878487114e-05\n",
            "Epoch 709/1000, Training Loss: 3.055614909381489e-05, Test Loss: 3.010044205690282e-05\n",
            "Epoch 710/1000, Training Loss: 3.047971927728794e-05, Test Loss: 3.002883344481694e-05\n",
            "Epoch 711/1000, Training Loss: 3.0403690135063734e-05, Test Loss: 2.995756836229268e-05\n",
            "Epoch 712/1000, Training Loss: 3.0328058349091124e-05, Test Loss: 2.9886644159221564e-05\n",
            "Epoch 713/1000, Training Loss: 3.025282063970823e-05, Test Loss: 2.9816058214853247e-05\n",
            "Epoch 714/1000, Training Loss: 3.0177973765054196e-05, Test Loss: 2.9745807937337283e-05\n",
            "Epoch 715/1000, Training Loss: 3.010351452049008e-05, Test Loss: 2.9675890763274285e-05\n",
            "Epoch 716/1000, Training Loss: 3.002943973803169e-05, Test Loss: 2.9606304157274885e-05\n",
            "Epoch 717/1000, Training Loss: 2.995574628579435e-05, Test Loss: 2.9537045611527686e-05\n",
            "Epoch 718/1000, Training Loss: 2.988243106744561e-05, Test Loss: 2.946811264537731e-05\n",
            "Epoch 719/1000, Training Loss: 2.980949102167099e-05, Test Loss: 2.9399502804909034e-05\n",
            "Epoch 720/1000, Training Loss: 2.9736923121647985e-05, Test Loss: 2.9331213662544452e-05\n",
            "Epoch 721/1000, Training Loss: 2.9664724374529848e-05, Test Loss: 2.9263242816640175e-05\n",
            "Epoch 722/1000, Training Loss: 2.9592891820940146e-05, Test Loss: 2.9195587891100483e-05\n",
            "Epoch 723/1000, Training Loss: 2.9521422534477033e-05, Test Loss: 2.912824653499295e-05\n",
            "Epoch 724/1000, Training Loss: 2.945031362122541e-05, Test Loss: 2.9061216422173953e-05\n",
            "Epoch 725/1000, Training Loss: 2.9379562219277042e-05, Test Loss: 2.8994495250922175e-05\n",
            "Epoch 726/1000, Training Loss: 2.9309165498264236e-05, Test Loss: 2.892808074357603e-05\n",
            "Epoch 727/1000, Training Loss: 2.9239120658897206e-05, Test Loss: 2.886197064618281e-05\n",
            "Epoch 728/1000, Training Loss: 2.9169424932511807e-05, Test Loss: 2.8796162728150697e-05\n",
            "Epoch 729/1000, Training Loss: 2.9100075580625596e-05, Test Loss: 2.87306547819082e-05\n",
            "Epoch 730/1000, Training Loss: 2.903106989450281e-05, Test Loss: 2.8665444622573196e-05\n",
            "Epoch 731/1000, Training Loss: 2.896240519472625e-05, Test Loss: 2.8600530087622686e-05\n",
            "Epoch 732/1000, Training Loss: 2.8894078830776272e-05, Test Loss: 2.853590903657882e-05\n",
            "Epoch 733/1000, Training Loss: 2.8826088180618076e-05, Test Loss: 2.8471579350685074e-05\n",
            "Epoch 734/1000, Training Loss: 2.8758430650298396e-05, Test Loss: 2.840753893260641e-05\n",
            "Epoch 735/1000, Training Loss: 2.869110367354687e-05, Test Loss: 2.834378570612262e-05\n",
            "Epoch 736/1000, Training Loss: 2.8624104711383738e-05, Test Loss: 2.828031761583249e-05\n",
            "Epoch 737/1000, Training Loss: 2.855743125173832e-05, Test Loss: 2.8217132626861646e-05\n",
            "Epoch 738/1000, Training Loss: 2.8491080809071547e-05, Test Loss: 2.8154228724579497e-05\n",
            "Epoch 739/1000, Training Loss: 2.842505092400579e-05, Test Loss: 2.8091603914315507e-05\n",
            "Epoch 740/1000, Training Loss: 2.835933916295998e-05, Test Loss: 2.8029256221087108e-05\n",
            "Epoch 741/1000, Training Loss: 2.8293943117794197e-05, Test Loss: 2.7967183689326237e-05\n",
            "Epoch 742/1000, Training Loss: 2.822886040545874e-05, Test Loss: 2.7905384382620342e-05\n",
            "Epoch 743/1000, Training Loss: 2.8164088667648876e-05, Test Loss: 2.784385638344564e-05\n",
            "Epoch 744/1000, Training Loss: 2.8099625570465034e-05, Test Loss: 2.7782597792916042e-05\n",
            "Epoch 745/1000, Training Loss: 2.8035468804082226e-05, Test Loss: 2.7721606730531767e-05\n",
            "Epoch 746/1000, Training Loss: 2.797161608242154e-05, Test Loss: 2.76608813339328e-05\n",
            "Epoch 747/1000, Training Loss: 2.790806514282867e-05, Test Loss: 2.760041975865693e-05\n",
            "Epoch 748/1000, Training Loss: 2.78448137457601e-05, Test Loss: 2.7540220177905314e-05\n",
            "Epoch 749/1000, Training Loss: 2.7781859674469426e-05, Test Loss: 2.7480280782306037e-05\n",
            "Epoch 750/1000, Training Loss: 2.7719200734705957e-05, Test Loss: 2.7420599779688732e-05\n",
            "Epoch 751/1000, Training Loss: 2.765683475441156e-05, Test Loss: 2.7361175394856907e-05\n",
            "Epoch 752/1000, Training Loss: 2.7594759583428894e-05, Test Loss: 2.7302005869370715e-05\n",
            "Epoch 753/1000, Training Loss: 2.753297309320955e-05, Test Loss: 2.7243089461328614e-05\n",
            "Epoch 754/1000, Training Loss: 2.7471473176529617e-05, Test Loss: 2.7184424445153736e-05\n",
            "Epoch 755/1000, Training Loss: 2.7410257747211148e-05, Test Loss: 2.71260091113879e-05\n",
            "Epoch 756/1000, Training Loss: 2.734932473984507e-05, Test Loss: 2.7067841766482293e-05\n",
            "Epoch 757/1000, Training Loss: 2.728867210952169e-05, Test Loss: 2.700992073260073e-05\n",
            "Epoch 758/1000, Training Loss: 2.7228297831563076e-05, Test Loss: 2.6952244347416065e-05\n",
            "Epoch 759/1000, Training Loss: 2.7168199901263353e-05, Test Loss: 2.6894810963919136e-05\n",
            "Epoch 760/1000, Training Loss: 2.710837633362952e-05, Test Loss: 2.6837618950226058e-05\n",
            "Epoch 761/1000, Training Loss: 2.7048825163129617e-05, Test Loss: 2.678066668939162e-05\n",
            "Epoch 762/1000, Training Loss: 2.6989544443442886e-05, Test Loss: 2.672395257922265e-05\n",
            "Epoch 763/1000, Training Loss: 2.69305322472147e-05, Test Loss: 2.666747503209725e-05\n",
            "Epoch 764/1000, Training Loss: 2.6871786665816365e-05, Test Loss: 2.6611232474784726e-05\n",
            "Epoch 765/1000, Training Loss: 2.681330580910843e-05, Test Loss: 2.655522334827308e-05\n",
            "Epoch 766/1000, Training Loss: 2.6755087805206388e-05, Test Loss: 2.649944610759466e-05\n",
            "Epoch 767/1000, Training Loss: 2.66971308002521e-05, Test Loss: 2.6443899221654655e-05\n",
            "Epoch 768/1000, Training Loss: 2.6639432958187958e-05, Test Loss: 2.6388581173067467e-05\n",
            "Epoch 769/1000, Training Loss: 2.6581992460535356e-05, Test Loss: 2.6333490457991278e-05\n",
            "Epoch 770/1000, Training Loss: 2.652480750617498e-05, Test Loss: 2.6278625585965387e-05\n",
            "Epoch 771/1000, Training Loss: 2.646787631113336e-05, Test Loss: 2.6223985079754756e-05\n",
            "Epoch 772/1000, Training Loss: 2.641119710837002e-05, Test Loss: 2.6169567475190333e-05\n",
            "Epoch 773/1000, Training Loss: 2.6354768147570493e-05, Test Loss: 2.611537132101798e-05\n",
            "Epoch 774/1000, Training Loss: 2.6298587694939956e-05, Test Loss: 2.606139517874567e-05\n",
            "Epoch 775/1000, Training Loss: 2.624265403300262e-05, Test Loss: 2.6007637622496493e-05\n",
            "Epoch 776/1000, Training Loss: 2.6186965460402732e-05, Test Loss: 2.5954097238862842e-05\n",
            "Epoch 777/1000, Training Loss: 2.6131520291709413e-05, Test Loss: 2.5900772626759327e-05\n",
            "Epoch 778/1000, Training Loss: 2.6076316857223516e-05, Test Loss: 2.5847662397285292e-05\n",
            "Epoch 779/1000, Training Loss: 2.6021353502789528e-05, Test Loss: 2.579476517358168e-05\n",
            "Epoch 780/1000, Training Loss: 2.5966628589608043e-05, Test Loss: 2.5742079590699427e-05\n",
            "Epoch 781/1000, Training Loss: 2.591214049405224e-05, Test Loss: 2.5689604295457154e-05\n",
            "Epoch 782/1000, Training Loss: 2.585788760748741e-05, Test Loss: 2.5637337946314408e-05\n",
            "Epoch 783/1000, Training Loss: 2.580386833609243e-05, Test Loss: 2.5585279213239247e-05\n",
            "Epoch 784/1000, Training Loss: 2.5750081100685018e-05, Test Loss: 2.553342677757803e-05\n",
            "Epoch 785/1000, Training Loss: 2.5696524336548754e-05, Test Loss: 2.5481779331930263e-05\n",
            "Epoch 786/1000, Training Loss: 2.564319649326299e-05, Test Loss: 2.5430335580022838e-05\n",
            "Epoch 787/1000, Training Loss: 2.5590096034535187e-05, Test Loss: 2.5379094236588885e-05\n",
            "Epoch 788/1000, Training Loss: 2.5537221438036588e-05, Test Loss: 2.5328054027243808e-05\n",
            "Epoch 789/1000, Training Loss: 2.5484571195239198e-05, Test Loss: 2.5277213688370515e-05\n",
            "Epoch 790/1000, Training Loss: 2.54321438112553e-05, Test Loss: 2.5226571966996974e-05\n",
            "Epoch 791/1000, Training Loss: 2.537993780468035e-05, Test Loss: 2.5176127620685152e-05\n",
            "Epoch 792/1000, Training Loss: 2.532795170743738e-05, Test Loss: 2.5125879417414416e-05\n",
            "Epoch 793/1000, Training Loss: 2.5276184064624345e-05, Test Loss: 2.5075826135469915e-05\n",
            "Epoch 794/1000, Training Loss: 2.5224633434362205e-05, Test Loss: 2.502596656333307e-05\n",
            "Epoch 795/1000, Training Loss: 2.5173298387647325e-05, Test Loss: 2.4976299499572227e-05\n",
            "Epoch 796/1000, Training Loss: 2.5122177508204412e-05, Test Loss: 2.492682375273461e-05\n",
            "Epoch 797/1000, Training Loss: 2.507126939234279e-05, Test Loss: 2.4877538141241268e-05\n",
            "Epoch 798/1000, Training Loss: 2.5020572648812908e-05, Test Loss: 2.482844149328273e-05\n",
            "Epoch 799/1000, Training Loss: 2.4970085898667224e-05, Test Loss: 2.4779532646716126e-05\n",
            "Epoch 800/1000, Training Loss: 2.4919807775122043e-05, Test Loss: 2.4730810448966196e-05\n",
            "Epoch 801/1000, Training Loss: 2.486973692342062e-05, Test Loss: 2.4682273756921827e-05\n",
            "Epoch 802/1000, Training Loss: 2.481987200069935e-05, Test Loss: 2.4633921436840077e-05\n",
            "Epoch 803/1000, Training Loss: 2.4770211675855204e-05, Test Loss: 2.4585752364248995e-05\n",
            "Epoch 804/1000, Training Loss: 2.4720754629415007e-05, Test Loss: 2.4537765423850938e-05\n",
            "Epoch 805/1000, Training Loss: 2.4671499553407793e-05, Test Loss: 2.448995950943118e-05\n",
            "Epoch 806/1000, Training Loss: 2.462244515123667e-05, Test Loss: 2.444233352376156e-05\n",
            "Epoch 807/1000, Training Loss: 2.457359013755459e-05, Test Loss: 2.4394886378510544e-05\n",
            "Epoch 808/1000, Training Loss: 2.4524933238141056e-05, Test Loss: 2.4347616994153254e-05\n",
            "Epoch 809/1000, Training Loss: 2.447647318977935e-05, Test Loss: 2.430052429988204e-05\n",
            "Epoch 810/1000, Training Loss: 2.4428208740138293e-05, Test Loss: 2.42536072335166e-05\n",
            "Epoch 811/1000, Training Loss: 2.438013864765277e-05, Test Loss: 2.4206864741421175e-05\n",
            "Epoch 812/1000, Training Loss: 2.4332261681408005e-05, Test Loss: 2.4160295778415705e-05\n",
            "Epoch 813/1000, Training Loss: 2.4284576621023522e-05, Test Loss: 2.4113899307692826e-05\n",
            "Epoch 814/1000, Training Loss: 2.4237082256540316e-05, Test Loss: 2.4067674300735313e-05\n",
            "Epoch 815/1000, Training Loss: 2.4189777388309045e-05, Test Loss: 2.4021619737232015e-05\n",
            "Epoch 816/1000, Training Loss: 2.4142660826879576e-05, Test Loss: 2.397573460499829e-05\n",
            "Epoch 817/1000, Training Loss: 2.409573139289115e-05, Test Loss: 2.393001789989609e-05\n",
            "Epoch 818/1000, Training Loss: 2.4048987916966272e-05, Test Loss: 2.3884468625754057e-05\n",
            "Epoch 819/1000, Training Loss: 2.400242923960391e-05, Test Loss: 2.3839085794290607e-05\n",
            "Epoch 820/1000, Training Loss: 2.395605421107504e-05, Test Loss: 2.3793868425035497e-05\n",
            "Epoch 821/1000, Training Loss: 2.390986169131966e-05, Test Loss: 2.3748815545256135e-05\n",
            "Epoch 822/1000, Training Loss: 2.3863850549845207e-05, Test Loss: 2.3703926189881456e-05\n",
            "Epoch 823/1000, Training Loss: 2.381801966562492e-05, Test Loss: 2.3659199401429205e-05\n",
            "Epoch 824/1000, Training Loss: 2.3772367926999965e-05, Test Loss: 2.3614634229930125e-05\n",
            "Epoch 825/1000, Training Loss: 2.3726894231581174e-05, Test Loss: 2.357022973285765e-05\n",
            "Epoch 826/1000, Training Loss: 2.3681597486152346e-05, Test Loss: 2.3525984975057984e-05\n",
            "Epoch 827/1000, Training Loss: 2.3636476606574775e-05, Test Loss: 2.3481899028678235e-05\n",
            "Epoch 828/1000, Training Loss: 2.3591530517694108e-05, Test Loss: 2.343797097309766e-05\n",
            "Epoch 829/1000, Training Loss: 2.3546758153246042e-05, Test Loss: 2.3394199894857543e-05\n",
            "Epoch 830/1000, Training Loss: 2.350215845576583e-05, Test Loss: 2.335058488759613e-05\n",
            "Epoch 831/1000, Training Loss: 2.345773037649754e-05, Test Loss: 2.330712505197911e-05\n",
            "Epoch 832/1000, Training Loss: 2.3413472875304693e-05, Test Loss: 2.3263819495637322e-05\n",
            "Epoch 833/1000, Training Loss: 2.3369384920581828e-05, Test Loss: 2.3220667333096624e-05\n",
            "Epoch 834/1000, Training Loss: 2.3325465489167995e-05, Test Loss: 2.317766768571723e-05\n",
            "Epoch 835/1000, Training Loss: 2.3281713566260157e-05, Test Loss: 2.3134819681628815e-05\n",
            "Epoch 836/1000, Training Loss: 2.323812814532881e-05, Test Loss: 2.309212245566776e-05\n",
            "Epoch 837/1000, Training Loss: 2.3194708228035112e-05, Test Loss: 2.3049575149314533e-05\n",
            "Epoch 838/1000, Training Loss: 2.3151452824145652e-05, Test Loss: 2.3007176910632897e-05\n",
            "Epoch 839/1000, Training Loss: 2.310836095145303e-05, Test Loss: 2.2964926894209185e-05\n",
            "Epoch 840/1000, Training Loss: 2.3065431635694743e-05, Test Loss: 2.2922824261091515e-05\n",
            "Epoch 841/1000, Training Loss: 2.3022663910472375e-05, Test Loss: 2.2880868178731818e-05\n",
            "Epoch 842/1000, Training Loss: 2.2980056817174433e-05, Test Loss: 2.2839057820927133e-05\n",
            "Epoch 843/1000, Training Loss: 2.2937609404897944e-05, Test Loss: 2.2797392367760555e-05\n",
            "Epoch 844/1000, Training Loss: 2.2895320730371532e-05, Test Loss: 2.2755871005545373e-05\n",
            "Epoch 845/1000, Training Loss: 2.2853189857880363e-05, Test Loss: 2.2714492926767905e-05\n",
            "Epoch 846/1000, Training Loss: 2.281121585918997e-05, Test Loss: 2.2673257330030417e-05\n",
            "Epoch 847/1000, Training Loss: 2.276939781347383e-05, Test Loss: 2.2632163419998683e-05\n",
            "Epoch 848/1000, Training Loss: 2.2727734807238735e-05, Test Loss: 2.2591210407345316e-05\n",
            "Epoch 849/1000, Training Loss: 2.268622593425444e-05, Test Loss: 2.2550397508695708e-05\n",
            "Epoch 850/1000, Training Loss: 2.264487029548071e-05, Test Loss: 2.2509723946576502e-05\n",
            "Epoch 851/1000, Training Loss: 2.260366699899756e-05, Test Loss: 2.2469188949359683e-05\n",
            "Epoch 852/1000, Training Loss: 2.256261515993641e-05, Test Loss: 2.242879175121378e-05\n",
            "Epoch 853/1000, Training Loss: 2.2521713900409925e-05, Test Loss: 2.2388531592049798e-05\n",
            "Epoch 854/1000, Training Loss: 2.2480962349444655e-05, Test Loss: 2.234840771747031e-05\n",
            "Epoch 855/1000, Training Loss: 2.2440359642914437e-05, Test Loss: 2.2308419378721936e-05\n",
            "Epoch 856/1000, Training Loss: 2.2399904923473147e-05, Test Loss: 2.226856583263981e-05\n",
            "Epoch 857/1000, Training Loss: 2.2359597340489986e-05, Test Loss: 2.2228846341604626e-05\n",
            "Epoch 858/1000, Training Loss: 2.231943604998425e-05, Test Loss: 2.218926017348777e-05\n",
            "Epoch 859/1000, Training Loss: 2.227942021456133e-05, Test Loss: 2.2149806601608836e-05\n",
            "Epoch 860/1000, Training Loss: 2.2239549003349673e-05, Test Loss: 2.2110484904682962e-05\n",
            "Epoch 861/1000, Training Loss: 2.219982159193817e-05, Test Loss: 2.2071294366776542e-05\n",
            "Epoch 862/1000, Training Loss: 2.216023716231433e-05, Test Loss: 2.2032234277257907e-05\n",
            "Epoch 863/1000, Training Loss: 2.212079490280328e-05, Test Loss: 2.199330393075378e-05\n",
            "Epoch 864/1000, Training Loss: 2.208149400800703e-05, Test Loss: 2.1954502627101158e-05\n",
            "Epoch 865/1000, Training Loss: 2.2042333678745137e-05, Test Loss: 2.1915829671304504e-05\n",
            "Epoch 866/1000, Training Loss: 2.2003313121996003e-05, Test Loss: 2.1877284373488157e-05\n",
            "Epoch 867/1000, Training Loss: 2.1964431550838384e-05, Test Loss: 2.183886604885329e-05\n",
            "Epoch 868/1000, Training Loss: 2.1925688184393e-05, Test Loss: 2.1800574017635414e-05\n",
            "Epoch 869/1000, Training Loss: 2.1887082247766783e-05, Test Loss: 2.1762407605057497e-05\n",
            "Epoch 870/1000, Training Loss: 2.184861297199569e-05, Test Loss: 2.1724366141291067e-05\n",
            "Epoch 871/1000, Training Loss: 2.1810279593989582e-05, Test Loss: 2.1686448961410118e-05\n",
            "Epoch 872/1000, Training Loss: 2.177208135647654e-05, Test Loss: 2.164865540535128e-05\n",
            "Epoch 873/1000, Training Loss: 2.1734017507949052e-05, Test Loss: 2.1610984817871487e-05\n",
            "Epoch 874/1000, Training Loss: 2.169608730260966e-05, Test Loss: 2.1573436548505276e-05\n",
            "Epoch 875/1000, Training Loss: 2.1658290000318262e-05, Test Loss: 2.1536009951527356e-05\n",
            "Epoch 876/1000, Training Loss: 2.162062486653906e-05, Test Loss: 2.149870438590895e-05\n",
            "Epoch 877/1000, Training Loss: 2.1583091172288696e-05, Test Loss: 2.146151921527865e-05\n",
            "Epoch 878/1000, Training Loss: 2.1545688194085104e-05, Test Loss: 2.142445380788377e-05\n",
            "Epoch 879/1000, Training Loss: 2.150841521389598e-05, Test Loss: 2.1387507536550494e-05\n",
            "Epoch 880/1000, Training Loss: 2.1471271519089084e-05, Test Loss: 2.1350679778645077e-05\n",
            "Epoch 881/1000, Training Loss: 2.1434256402381628e-05, Test Loss: 2.1313969916034973e-05\n",
            "Epoch 882/1000, Training Loss: 2.1397369161792557e-05, Test Loss: 2.1277377335051703e-05\n",
            "Epoch 883/1000, Training Loss: 2.1360609100592614e-05, Test Loss: 2.1240901426453152e-05\n",
            "Epoch 884/1000, Training Loss: 2.1323975527255813e-05, Test Loss: 2.1204541585385024e-05\n",
            "Epoch 885/1000, Training Loss: 2.1287467755413512e-05, Test Loss: 2.1168297211345393e-05\n",
            "Epoch 886/1000, Training Loss: 2.1251085103806245e-05, Test Loss: 2.1132167708146344e-05\n",
            "Epoch 887/1000, Training Loss: 2.121482689623704e-05, Test Loss: 2.109615248388101e-05\n",
            "Epoch 888/1000, Training Loss: 2.117869246152574e-05, Test Loss: 2.1060250950883898e-05\n",
            "Epoch 889/1000, Training Loss: 2.1142681133462958e-05, Test Loss: 2.1024462525697454e-05\n",
            "Epoch 890/1000, Training Loss: 2.110679225076598e-05, Test Loss: 2.0988786629036834e-05\n",
            "Epoch 891/1000, Training Loss: 2.107102515703283e-05, Test Loss: 2.0953222685755067e-05\n",
            "Epoch 892/1000, Training Loss: 2.103537920069903e-05, Test Loss: 2.091777012480756e-05\n",
            "Epoch 893/1000, Training Loss: 2.0999853734993846e-05, Test Loss: 2.088242837921855e-05\n",
            "Epoch 894/1000, Training Loss: 2.096444811789698e-05, Test Loss: 2.0847196886047724e-05\n",
            "Epoch 895/1000, Training Loss: 2.0929161712096004e-05, Test Loss: 2.08120750863575e-05\n",
            "Epoch 896/1000, Training Loss: 2.0893993884944124e-05, Test Loss: 2.0777062425176378e-05\n",
            "Epoch 897/1000, Training Loss: 2.0858944008417696e-05, Test Loss: 2.0742158351470614e-05\n",
            "Epoch 898/1000, Training Loss: 2.0824011459076017e-05, Test Loss: 2.0707362318108645e-05\n",
            "Epoch 899/1000, Training Loss: 2.0789195618019037e-05, Test Loss: 2.067267378182819e-05\n",
            "Epoch 900/1000, Training Loss: 2.0754495870847914e-05, Test Loss: 2.06380922032075e-05\n",
            "Epoch 901/1000, Training Loss: 2.0719911607624948e-05, Test Loss: 2.0603617046631244e-05\n",
            "Epoch 902/1000, Training Loss: 2.0685442222833625e-05, Test Loss: 2.056924778026004e-05\n",
            "Epoch 903/1000, Training Loss: 2.0651087115338383e-05, Test Loss: 2.0534983875998766e-05\n",
            "Epoch 904/1000, Training Loss: 2.0616845688348055e-05, Test Loss: 2.0500824809465855e-05\n",
            "Epoch 905/1000, Training Loss: 2.0582717349375434e-05, Test Loss: 2.0466770059963553e-05\n",
            "Epoch 906/1000, Training Loss: 2.054870151019976e-05, Test Loss: 2.04328191104478e-05\n",
            "Epoch 907/1000, Training Loss: 2.0514797586829377e-05, Test Loss: 2.0398971447496894e-05\n",
            "Epoch 908/1000, Training Loss: 2.048100499946505e-05, Test Loss: 2.0365226561282595e-05\n",
            "Epoch 909/1000, Training Loss: 2.0447323172461725e-05, Test Loss: 2.033158394554028e-05\n",
            "Epoch 910/1000, Training Loss: 2.0413751534292994e-05, Test Loss: 2.0298043097542744e-05\n",
            "Epoch 911/1000, Training Loss: 2.0380289517514805e-05, Test Loss: 2.026460351806605e-05\n",
            "Epoch 912/1000, Training Loss: 2.0346936558729642e-05, Test Loss: 2.0231264711365275e-05\n",
            "Epoch 913/1000, Training Loss: 2.0313692098551324e-05, Test Loss: 2.0198026185144695e-05\n",
            "Epoch 914/1000, Training Loss: 2.0280555581570135e-05, Test Loss: 2.0164887450529386e-05\n",
            "Epoch 915/1000, Training Loss: 2.0247526456317645e-05, Test Loss: 2.013184802203788e-05\n",
            "Epoch 916/1000, Training Loss: 2.021460417523246e-05, Test Loss: 2.0098907417553884e-05\n",
            "Epoch 917/1000, Training Loss: 2.0181788194627013e-05, Test Loss: 2.006606515830038e-05\n",
            "Epoch 918/1000, Training Loss: 2.014907797465361e-05, Test Loss: 2.0033320768811533e-05\n",
            "Epoch 919/1000, Training Loss: 2.011647297927065e-05, Test Loss: 2.000067377690643e-05\n",
            "Epoch 920/1000, Training Loss: 2.0083972676209993e-05, Test Loss: 1.9968123713660297e-05\n",
            "Epoch 921/1000, Training Loss: 2.0051576536944865e-05, Test Loss: 1.9935670113381534e-05\n",
            "Epoch 922/1000, Training Loss: 2.0019284036657793e-05, Test Loss: 1.9903312513583827e-05\n",
            "Epoch 923/1000, Training Loss: 1.998709465420644e-05, Test Loss: 1.9871050454958896e-05\n",
            "Epoch 924/1000, Training Loss: 1.995500787209531e-05, Test Loss: 1.983888348135482e-05\n",
            "Epoch 925/1000, Training Loss: 1.9923023176441915e-05, Test Loss: 1.9806811139745964e-05\n",
            "Epoch 926/1000, Training Loss: 1.9891140056946533e-05, Test Loss: 1.9774832980211148e-05\n",
            "Epoch 927/1000, Training Loss: 1.9859358006861817e-05, Test Loss: 1.9742948555906302e-05\n",
            "Epoch 928/1000, Training Loss: 1.9827676522962534e-05, Test Loss: 1.971115742304183e-05\n",
            "Epoch 929/1000, Training Loss: 1.9796095105514987e-05, Test Loss: 1.967945914085663e-05\n",
            "Epoch 930/1000, Training Loss: 1.9764613258247033e-05, Test Loss: 1.9647853271593918e-05\n",
            "Epoch 931/1000, Training Loss: 1.9733230488319507e-05, Test Loss: 1.9616339380477534e-05\n",
            "Epoch 932/1000, Training Loss: 1.9701946306296568e-05, Test Loss: 1.958491703568823e-05\n",
            "Epoch 933/1000, Training Loss: 1.967076022611629e-05, Test Loss: 1.955358580833897e-05\n",
            "Epoch 934/1000, Training Loss: 1.9639671765062992e-05, Test Loss: 1.9522345272453054e-05\n",
            "Epoch 935/1000, Training Loss: 1.96086804437378e-05, Test Loss: 1.9491195004938202e-05\n",
            "Epoch 936/1000, Training Loss: 1.957778578603203e-05, Test Loss: 1.9460134585565803e-05\n",
            "Epoch 937/1000, Training Loss: 1.954698731909766e-05, Test Loss: 1.9429163596947787e-05\n",
            "Epoch 938/1000, Training Loss: 1.9516284573321915e-05, Test Loss: 1.9398281624513504e-05\n",
            "Epoch 939/1000, Training Loss: 1.9485677082297266e-05, Test Loss: 1.9367488256486568e-05\n",
            "Epoch 940/1000, Training Loss: 1.945516438279694e-05, Test Loss: 1.933678308386297e-05\n",
            "Epoch 941/1000, Training Loss: 1.9424746014747673e-05, Test Loss: 1.9306165700389995e-05\n",
            "Epoch 942/1000, Training Loss: 1.939442152120174e-05, Test Loss: 1.9275635702542233e-05\n",
            "Epoch 943/1000, Training Loss: 1.9364190448312634e-05, Test Loss: 1.9245192689502635e-05\n",
            "Epoch 944/1000, Training Loss: 1.9334052345308177e-05, Test Loss: 1.921483626313821e-05\n",
            "Epoch 945/1000, Training Loss: 1.930400676446482e-05, Test Loss: 1.918456602797987e-05\n",
            "Epoch 946/1000, Training Loss: 1.9274053261082775e-05, Test Loss: 1.915438159120225e-05\n",
            "Epoch 947/1000, Training Loss: 1.9244191393459794e-05, Test Loss: 1.9124282562600026e-05\n",
            "Epoch 948/1000, Training Loss: 1.921442072286687e-05, Test Loss: 1.9094268554569916e-05\n",
            "Epoch 949/1000, Training Loss: 1.9184740813523815e-05, Test Loss: 1.906433918208911e-05\n",
            "Epoch 950/1000, Training Loss: 1.9155151232573782e-05, Test Loss: 1.9034494062693563e-05\n",
            "Epoch 951/1000, Training Loss: 1.9125651550060755e-05, Test Loss: 1.900473281645978e-05\n",
            "Epoch 952/1000, Training Loss: 1.909624133890295e-05, Test Loss: 1.8975055065982807e-05\n",
            "Epoch 953/1000, Training Loss: 1.90669201748712e-05, Test Loss: 1.8945460436356833e-05\n",
            "Epoch 954/1000, Training Loss: 1.9037687636564555e-05, Test Loss: 1.8915948555156562e-05\n",
            "Epoch 955/1000, Training Loss: 1.900854330538705e-05, Test Loss: 1.8886519052417256e-05\n",
            "Epoch 956/1000, Training Loss: 1.8979486765524473e-05, Test Loss: 1.8857171560612243e-05\n",
            "Epoch 957/1000, Training Loss: 1.895051760392159e-05, Test Loss: 1.8827905714640024e-05\n",
            "Epoch 958/1000, Training Loss: 1.8921635410259393e-05, Test Loss: 1.8798721151797046e-05\n",
            "Epoch 959/1000, Training Loss: 1.889283977693228e-05, Test Loss: 1.8769617511765693e-05\n",
            "Epoch 960/1000, Training Loss: 1.886413029902658e-05, Test Loss: 1.874059443659034e-05\n",
            "Epoch 961/1000, Training Loss: 1.8835506574297787e-05, Test Loss: 1.8711651570664306e-05\n",
            "Epoch 962/1000, Training Loss: 1.88069682031493e-05, Test Loss: 1.8682788560707016e-05\n",
            "Epoch 963/1000, Training Loss: 1.8778514788610356e-05, Test Loss: 1.865400505574455e-05\n",
            "Epoch 964/1000, Training Loss: 1.8750145936314474e-05, Test Loss: 1.8625300707097706e-05\n",
            "Epoch 965/1000, Training Loss: 1.8721861254478633e-05, Test Loss: 1.8596675168355275e-05\n",
            "Epoch 966/1000, Training Loss: 1.869366035388195e-05, Test Loss: 1.856812809536603e-05\n",
            "Epoch 967/1000, Training Loss: 1.8665542847845205e-05, Test Loss: 1.853965914621191e-05\n",
            "Epoch 968/1000, Training Loss: 1.863750835220942e-05, Test Loss: 1.8511267981195495e-05\n",
            "Epoch 969/1000, Training Loss: 1.8609556485316348e-05, Test Loss: 1.848295426282246e-05\n",
            "Epoch 970/1000, Training Loss: 1.858168686798792e-05, Test Loss: 1.845471765578221e-05\n",
            "Epoch 971/1000, Training Loss: 1.8553899123505504e-05, Test Loss: 1.8426557826932008e-05\n",
            "Epoch 972/1000, Training Loss: 1.8526192877591016e-05, Test Loss: 1.8398474445280676e-05\n",
            "Epoch 973/1000, Training Loss: 1.849856775838627e-05, Test Loss: 1.8370467181970385e-05\n",
            "Epoch 974/1000, Training Loss: 1.8471023396434428e-05, Test Loss: 1.8342535710258927e-05\n",
            "Epoch 975/1000, Training Loss: 1.8443559424659296e-05, Test Loss: 1.8314679705506537e-05\n",
            "Epoch 976/1000, Training Loss: 1.8416175478348077e-05, Test Loss: 1.828689884515694e-05\n",
            "Epoch 977/1000, Training Loss: 1.8388871195130567e-05, Test Loss: 1.825919280872162e-05\n",
            "Epoch 978/1000, Training Loss: 1.836164621496118e-05, Test Loss: 1.8231561277763235e-05\n",
            "Epoch 979/1000, Training Loss: 1.83345001801004e-05, Test Loss: 1.820400393588042e-05\n",
            "Epoch 980/1000, Training Loss: 1.8307432735095844e-05, Test Loss: 1.8176520468691122e-05\n",
            "Epoch 981/1000, Training Loss: 1.8280443526764687e-05, Test Loss: 1.8149110563819044e-05\n",
            "Epoch 982/1000, Training Loss: 1.8253532204174603e-05, Test Loss: 1.8121773910872312e-05\n",
            "Epoch 983/1000, Training Loss: 1.8226698418626343e-05, Test Loss: 1.8094510201434774e-05\n",
            "Epoch 984/1000, Training Loss: 1.819994182363612e-05, Test Loss: 1.8067319129047158e-05\n",
            "Epoch 985/1000, Training Loss: 1.8173262074917462e-05, Test Loss: 1.804020038919092e-05\n",
            "Epoch 986/1000, Training Loss: 1.814665883036404e-05, Test Loss: 1.80131536792771e-05\n",
            "Epoch 987/1000, Training Loss: 1.8120131750032155e-05, Test Loss: 1.7986178698625298e-05\n",
            "Epoch 988/1000, Training Loss: 1.809368049612399e-05, Test Loss: 1.7959275148454814e-05\n",
            "Epoch 989/1000, Training Loss: 1.8067304732969783e-05, Test Loss: 1.793244273186579e-05\n",
            "Epoch 990/1000, Training Loss: 1.804100412701188e-05, Test Loss: 1.7905681153826647e-05\n",
            "Epoch 991/1000, Training Loss: 1.8014778346787796e-05, Test Loss: 1.787899012115818e-05\n",
            "Epoch 992/1000, Training Loss: 1.7988627062913047e-05, Test Loss: 1.7852369342521422e-05\n",
            "Epoch 993/1000, Training Loss: 1.7962549948065465e-05, Test Loss: 1.782581852840018e-05\n",
            "Epoch 994/1000, Training Loss: 1.793654667696881e-05, Test Loss: 1.7799337391089374e-05\n",
            "Epoch 995/1000, Training Loss: 1.791061692637617e-05, Test Loss: 1.7772925644679963e-05\n",
            "Epoch 996/1000, Training Loss: 1.7884760375054655e-05, Test Loss: 1.7746583005044448e-05\n",
            "Epoch 997/1000, Training Loss: 1.785897670376914e-05, Test Loss: 1.7720309189825076e-05\n",
            "Epoch 998/1000, Training Loss: 1.783326559526709e-05, Test Loss: 1.769410391841828e-05\n",
            "Epoch 999/1000, Training Loss: 1.7807626734262037e-05, Test Loss: 1.7667966911961497e-05\n",
            "Epoch 1000/1000, Training Loss: 1.7782059807419376e-05, Test Loss: 1.764189789331898e-05\n",
            "Epoch 1/1000, Training Loss: 0.003550889798982072, Test Loss: 0.004317433715504763\n",
            "Epoch 2/1000, Training Loss: 0.00350397930167744, Test Loss: 0.004295304922437326\n",
            "Epoch 3/1000, Training Loss: 0.0034975704081211955, Test Loss: 0.004298796740212717\n",
            "Epoch 4/1000, Training Loss: 0.003494954799629768, Test Loss: 0.0043015281258285834\n",
            "Epoch 5/1000, Training Loss: 0.0034932374215590023, Test Loss: 0.0043032272752627585\n",
            "Epoch 6/1000, Training Loss: 0.003491935696435082, Test Loss: 0.004304333648329208\n",
            "Epoch 7/1000, Training Loss: 0.003490875386805169, Test Loss: 0.004305064243488363\n",
            "Epoch 8/1000, Training Loss: 0.0034899619047586285, Test Loss: 0.004305521102544962\n",
            "Epoch 9/1000, Training Loss: 0.0034891393725832143, Test Loss: 0.00430576387812754\n",
            "Epoch 10/1000, Training Loss: 0.003488374529636548, Test Loss: 0.004305835660670807\n",
            "Epoch 11/1000, Training Loss: 0.0034876474374067714, Test Loss: 0.004305770678642553\n",
            "Epoch 12/1000, Training Loss: 0.003486945898145365, Test Loss: 0.004305596583270887\n",
            "Epoch 13/1000, Training Loss: 0.0034862621877262533, Test Loss: 0.004305335464493174\n",
            "Epoch 14/1000, Training Loss: 0.003485591184374137, Test Loss: 0.004305004679758558\n",
            "Epoch 15/1000, Training Loss: 0.003484929308357665, Test Loss: 0.0043046176687196605\n",
            "Epoch 16/1000, Training Loss: 0.003484273921584412, Test Loss: 0.004304184721103925\n",
            "Epoch 17/1000, Training Loss: 0.003483622984600121, Test Loss: 0.004303713653014452\n",
            "Epoch 18/1000, Training Loss: 0.0034829748569487682, Test Loss: 0.0043032103715344334\n",
            "Epoch 19/1000, Training Loss: 0.0034823281775448285, Test Loss: 0.0043026793284186285\n",
            "Epoch 20/1000, Training Loss: 0.0034816817901145293, Test Loss: 0.0043021238750023056\n",
            "Epoch 21/1000, Training Loss: 0.0034810346944654453, Test Loss: 0.004301546534619993\n",
            "Epoch 22/1000, Training Loss: 0.0034803860129567475, Test Loss: 0.004300949208900979\n",
            "Epoch 23/1000, Training Loss: 0.0034797349662490803, Test Loss: 0.004300333332486746\n",
            "Epoch 24/1000, Training Loss: 0.00347908085498425, Test Loss: 0.0042996999882660555\n",
            "Epoch 25/1000, Training Loss: 0.003478423045454107, Test Loss: 0.0042990499927881495\n",
            "Epoch 26/1000, Training Loss: 0.003477760958096143, Test Loss: 0.004298383959368869\n",
            "Epoch 27/1000, Training Loss: 0.003477094058088768, Test Loss: 0.004297702344631113\n",
            "Epoch 28/1000, Training Loss: 0.0034764218475679056, Test Loss: 0.004297005482810614\n",
            "Epoch 29/1000, Training Loss: 0.003475743859132903, Test Loss: 0.00429629361106368\n",
            "Epoch 30/1000, Training Loss: 0.0034750596503994135, Test Loss: 0.004295566888178731\n",
            "Epoch 31/1000, Training Loss: 0.0034743687994145274, Test Loss: 0.0042948254084643435\n",
            "Epoch 32/1000, Training Loss: 0.0034736709007886237, Test Loss: 0.004294069212116205\n",
            "Epoch 33/1000, Training Loss: 0.003472965562426411, Test Loss: 0.0042932982930165954\n",
            "Epoch 34/1000, Training Loss: 0.0034722524027607283, Test Loss: 0.004292512604662191\n",
            "Epoch 35/1000, Training Loss: 0.0034715310484090666, Test Loss: 0.004291712064726648\n",
            "Epoch 36/1000, Training Loss: 0.0034708011321859956, Test Loss: 0.00429089655862554\n",
            "Epoch 37/1000, Training Loss: 0.0034700622914154177, Test Loss: 0.004290065942349911\n",
            "Epoch 38/1000, Training Loss: 0.0034693141664955463, Test Loss: 0.004289220044760958\n",
            "Epoch 39/1000, Training Loss: 0.0034685563996769355, Test Loss: 0.004288358669484643\n",
            "Epoch 40/1000, Training Loss: 0.0034677886340201626, Test Loss: 0.004287481596506448\n",
            "Epoch 41/1000, Training Loss: 0.003467010512505027, Test Loss: 0.004286588583538304\n",
            "Epoch 42/1000, Training Loss: 0.003466221677267559, Test Loss: 0.004285679367209716\n",
            "Epoch 43/1000, Training Loss: 0.0034654217689448928, Test Loss: 0.004284753664120602\n",
            "Epoch 44/1000, Training Loss: 0.0034646104261111823, Test Loss: 0.004283811171783004\n",
            "Epoch 45/1000, Training Loss: 0.0034637872847904294, Test Loss: 0.004282851569471527\n",
            "Epoch 46/1000, Training Loss: 0.0034629519780343026, Test Loss: 0.0042818745189970656\n",
            "Epoch 47/1000, Training Loss: 0.003462104135554941, Test Loss: 0.004280879665414728\n",
            "Epoch 48/1000, Training Loss: 0.0034612433834043024, Test Loss: 0.004279866637674199\n",
            "Epoch 49/1000, Training Loss: 0.0034603693436929717, Test Loss: 0.004278835049218995\n",
            "Epoch 50/1000, Training Loss: 0.0034594816343424613, Test Loss: 0.004277784498539692\n",
            "Epoch 51/1000, Training Loss: 0.003458579868865974, Test Loss: 0.004276714569685383\n",
            "Epoch 52/1000, Training Loss: 0.0034576636561734174, Test Loss: 0.004275624832736861\n",
            "Epoch 53/1000, Training Loss: 0.0034567326003970946, Test Loss: 0.004274514844244655\n",
            "Epoch 54/1000, Training Loss: 0.00345578630073509, Test Loss: 0.004273384147634643\n",
            "Epoch 55/1000, Training Loss: 0.0034548243513098176, Test Loss: 0.00427223227358371\n",
            "Epoch 56/1000, Training Loss: 0.0034538463410396, Test Loss: 0.004271058740367759\n",
            "Epoch 57/1000, Training Loss: 0.003452851853521489, Test Loss: 0.004269863054184171\n",
            "Epoch 58/1000, Training Loss: 0.0034518404669238024, Test Loss: 0.004268644709450715\n",
            "Epoch 59/1000, Training Loss: 0.0034508117538870954, Test Loss: 0.004267403189082739\n",
            "Epoch 60/1000, Training Loss: 0.0034497652814324906, Test Loss: 0.004266137964750436\n",
            "Epoch 61/1000, Training Loss: 0.003448700610876426, Test Loss: 0.004264848497117769\n",
            "Epoch 62/1000, Training Loss: 0.0034476172977510665, Test Loss: 0.004263534236064665\n",
            "Epoch 63/1000, Training Loss: 0.0034465148917297017, Test Loss: 0.004262194620893885\n",
            "Epoch 64/1000, Training Loss: 0.0034453929365565743, Test Loss: 0.0042608290805239675\n",
            "Epoch 65/1000, Training Loss: 0.0034442509699806776, Test Loss: 0.0042594370336695\n",
            "Epoch 66/1000, Training Loss: 0.0034430885236931075, Test Loss: 0.004258017889009918\n",
            "Epoch 67/1000, Training Loss: 0.003441905123267644, Test Loss: 0.0042565710453479665\n",
            "Epoch 68/1000, Training Loss: 0.0034407002881042892, Test Loss: 0.004255095891758853\n",
            "Epoch 69/1000, Training Loss: 0.003439473531375521, Test Loss: 0.004253591807731042\n",
            "Epoch 70/1000, Training Loss: 0.0034382243599750933, Test Loss: 0.004252058163299669\n",
            "Epoch 71/1000, Training Loss: 0.0034369522744692326, Test Loss: 0.004250494319173336\n",
            "Epoch 72/1000, Training Loss: 0.0034356567690501305, Test Loss: 0.004248899626855135\n",
            "Epoch 73/1000, Training Loss: 0.0034343373314916605, Test Loss: 0.00424727342875865\n",
            "Epoch 74/1000, Training Loss: 0.003432993443107281, Test Loss: 0.0042456150583196\n",
            "Epoch 75/1000, Training Loss: 0.003431624578710131, Test Loss: 0.004243923840103815\n",
            "Epoch 76/1000, Training Loss: 0.0034302302065753274, Test Loss: 0.00424219908991216\n",
            "Epoch 77/1000, Training Loss: 0.003428809788404547, Test Loss: 0.004240440114883019\n",
            "Epoch 78/1000, Training Loss: 0.003427362779292953, Test Loss: 0.0042386462135929025\n",
            "Epoch 79/1000, Training Loss: 0.003425888627698607, Test Loss: 0.0042368166761557555\n",
            "Epoch 80/1000, Training Loss: 0.003424386775414503, Test Loss: 0.004234950784321499\n",
            "Epoch 81/1000, Training Loss: 0.003422856657543406, Test Loss: 0.004233047811574349\n",
            "Epoch 82/1000, Training Loss: 0.003421297702475703, Test Loss: 0.0042311070232314545\n",
            "Epoch 83/1000, Training Loss: 0.003419709331870502, Test Loss: 0.004229127676542397\n",
            "Epoch 84/1000, Training Loss: 0.003418090960640253, Test Loss: 0.004227109020790065\n",
            "Epoch 85/1000, Training Loss: 0.0034164419969391867, Test Loss: 0.004225050297393482\n",
            "Epoch 86/1000, Training Loss: 0.003414761842155899, Test Loss: 0.0042229507400131485\n",
            "Epoch 87/1000, Training Loss: 0.0034130498909104388, Test Loss: 0.0042208095746594345\n",
            "Epoch 88/1000, Training Loss: 0.003411305531056295, Test Loss: 0.004218626019804682\n",
            "Epoch 89/1000, Training Loss: 0.0034095281436876835, Test Loss: 0.004216399286499563\n",
            "Epoch 90/1000, Training Loss: 0.0034077171031525934, Test Loss: 0.004214128578494372\n",
            "Epoch 91/1000, Training Loss: 0.003405871777072053, Test Loss: 0.0042118130923658945\n",
            "Epoch 92/1000, Training Loss: 0.003403991526366122, Test Loss: 0.004209452017650519\n",
            "Epoch 93/1000, Training Loss: 0.0034020757052871196, Test Loss: 0.004207044536984332\n",
            "Epoch 94/1000, Training Loss: 0.0034001236614606464, Test Loss: 0.004204589826250895\n",
            "Epoch 95/1000, Training Loss: 0.0033981347359349624, Test Loss: 0.004202087054737494\n",
            "Epoch 96/1000, Training Loss: 0.0033961082632393068, Test Loss: 0.004199535385300599\n",
            "Epoch 97/1000, Training Loss: 0.0033940435714517685, Test Loss: 0.004196933974541405\n",
            "Epoch 98/1000, Training Loss: 0.003391939982277328, Test Loss: 0.004194281972992229\n",
            "Epoch 99/1000, Training Loss: 0.0033897968111367143, Test Loss: 0.004191578525314657\n",
            "Epoch 100/1000, Training Loss: 0.0033876133672667095, Test Loss: 0.0041888227705103006\n",
            "Epoch 101/1000, Training Loss: 0.003385388953832582, Test Loss: 0.004186013842145054\n",
            "Epoch 102/1000, Training Loss: 0.0033831228680532843, Test Loss: 0.004183150868587785\n",
            "Epoch 103/1000, Training Loss: 0.003380814401340117, Test Loss: 0.004180232973264369\n",
            "Epoch 104/1000, Training Loss: 0.0033784628394494943, Test Loss: 0.004177259274927993\n",
            "Epoch 105/1000, Training Loss: 0.0033760674626505094, Test Loss: 0.004174228887946705\n",
            "Epoch 106/1000, Training Loss: 0.003373627545907938, Test Loss: 0.004171140922609128\n",
            "Epoch 107/1000, Training Loss: 0.00337114235908135, Test Loss: 0.004167994485449291\n",
            "Epoch 108/1000, Training Loss: 0.0033686111671409574, Test Loss: 0.004164788679591531\n",
            "Epoch 109/1000, Training Loss: 0.00336603323040084, Test Loss: 0.004161522605116382\n",
            "Epoch 110/1000, Training Loss: 0.0033634078047701387, Test Loss: 0.0041581953594483885\n",
            "Epoch 111/1000, Training Loss: 0.003360734142022817, Test Loss: 0.0041548060377667095\n",
            "Epoch 112/1000, Training Loss: 0.0033580114900865305, Test Loss: 0.004151353733439426\n",
            "Epoch 113/1000, Training Loss: 0.00335523909335114, Test Loss: 0.004147837538482363\n",
            "Epoch 114/1000, Training Loss: 0.0033524161929973577, Test Loss: 0.004144256544043222\n",
            "Epoch 115/1000, Training Loss: 0.0033495420273459676, Test Loss: 0.004140609840911826\n",
            "Epoch 116/1000, Training Loss: 0.0033466158322280302, Test Loss: 0.004136896520057126\n",
            "Epoch 117/1000, Training Loss: 0.0033436368413764296, Test Loss: 0.004133115673191651\n",
            "Epoch 118/1000, Training Loss: 0.003340604286839046, Test Loss: 0.004129266393363973\n",
            "Epoch 119/1000, Training Loss: 0.003337517399413821, Test Loss: 0.004125347775579693\n",
            "Epoch 120/1000, Training Loss: 0.0033343754091058574, Test Loss: 0.0041213589174513565\n",
            "Epoch 121/1000, Training Loss: 0.003331177545606683, Test Loss: 0.00411729891987764\n",
            "Epoch 122/1000, Training Loss: 0.003327923038795689, Test Loss: 0.004113166887752024\n",
            "Epoch 123/1000, Training Loss: 0.0033246111192636874, Test Loss: 0.004108961930701071\n",
            "Epoch 124/1000, Training Loss: 0.003321241018858441, Test Loss: 0.004104683163852284\n",
            "Epoch 125/1000, Training Loss: 0.0033178119712519243, Test Loss: 0.00410032970863143\n",
            "Epoch 126/1000, Training Loss: 0.003314323212528958, Test Loss: 0.004095900693589002\n",
            "Epoch 127/1000, Training Loss: 0.0033107739817967606, Test Loss: 0.004091395255255397\n",
            "Epoch 128/1000, Training Loss: 0.0033071635218148445, Test Loss: 0.004086812539024196\n",
            "Epoch 129/1000, Training Loss: 0.003303491079644541, Test Loss: 0.004082151700062733\n",
            "Epoch 130/1000, Training Loss: 0.00329975590731732, Test Loss: 0.004077411904248962\n",
            "Epoch 131/1000, Training Loss: 0.003295957262520922, Test Loss: 0.004072592329133448\n",
            "Epoch 132/1000, Training Loss: 0.0032920944093021622, Test Loss: 0.0040676921649250115\n",
            "Epoch 133/1000, Training Loss: 0.00328816661878511, Test Loss: 0.004062710615498403\n",
            "Epoch 134/1000, Training Loss: 0.0032841731699031905, Test Loss: 0.004057646899422074\n",
            "Epoch 135/1000, Training Loss: 0.0032801133501435464, Test Loss: 0.00405250025100387\n",
            "Epoch 136/1000, Training Loss: 0.0032759864563018416, Test Loss: 0.004047269921352173\n",
            "Epoch 137/1000, Training Loss: 0.003271791795245469, Test Loss: 0.004041955179449746\n",
            "Epoch 138/1000, Training Loss: 0.0032675286846829323, Test Loss: 0.0040365553132371765\n",
            "Epoch 139/1000, Training Loss: 0.0032631964539369517, Test Loss: 0.004031069630702551\n",
            "Epoch 140/1000, Training Loss: 0.0032587944447186133, Test Loss: 0.0040254974609735825\n",
            "Epoch 141/1000, Training Loss: 0.0032543220118996605, Test Loss: 0.004019838155408084\n",
            "Epoch 142/1000, Training Loss: 0.00324977852427978, Test Loss: 0.004014091088678337\n",
            "Epoch 143/1000, Training Loss: 0.00324516336534548, Test Loss: 0.004008255659844432\n",
            "Epoch 144/1000, Training Loss: 0.0032404759340169276, Test Loss: 0.004002331293411372\n",
            "Epoch 145/1000, Training Loss: 0.0032357156453788147, Test Loss: 0.003996317440364231\n",
            "Epoch 146/1000, Training Loss: 0.0032308819313910915, Test Loss: 0.0039902135791752506\n",
            "Epoch 147/1000, Training Loss: 0.0032259742415751214, Test Loss: 0.00398401921677637\n",
            "Epoch 148/1000, Training Loss: 0.003220992043670527, Test Loss: 0.003977733889490218\n",
            "Epoch 149/1000, Training Loss: 0.003215934824257767, Test Loss: 0.0039713571639121365\n",
            "Epoch 150/1000, Training Loss: 0.0032108020893411543, Test Loss: 0.0039648886377354085\n",
            "Epoch 151/1000, Training Loss: 0.0032055933648868005, Test Loss: 0.003958327940511359\n",
            "Epoch 152/1000, Training Loss: 0.003200308197309699, Test Loss: 0.003951674734335622\n",
            "Epoch 153/1000, Training Loss: 0.003194946153903886, Test Loss: 0.003944928714451382\n",
            "Epoch 154/1000, Training Loss: 0.0031895068232093847, Test Loss: 0.003938089609759999\n",
            "Epoch 155/1000, Training Loss: 0.0031839898153094037, Test Loss: 0.003931157183229029\n",
            "Epoch 156/1000, Training Loss: 0.0031783947620510344, Test Loss: 0.003924131232187231\n",
            "Epoch 157/1000, Training Loss: 0.0031727213171824898, Test Loss: 0.00391701158849584\n",
            "Epoch 158/1000, Training Loss: 0.0031669691563997525, Test Loss: 0.003909798118584986\n",
            "Epoch 159/1000, Training Loss: 0.003161137977295333, Test Loss: 0.0039024907233439084\n",
            "Epoch 160/1000, Training Loss: 0.0031552274992017247, Test Loss: 0.003895089337853311\n",
            "Epoch 161/1000, Training Loss: 0.003149237462922014, Test Loss: 0.0038875939309480044\n",
            "Epoch 162/1000, Training Loss: 0.0031431676303400936, Test Loss: 0.0038800045045978542\n",
            "Epoch 163/1000, Training Loss: 0.003137017783902828, Test Loss: 0.0038723210930948614\n",
            "Epoch 164/1000, Training Loss: 0.003130787725966614, Test Loss: 0.003864543762034329\n",
            "Epoch 165/1000, Training Loss: 0.00312447727800076, Test Loss: 0.0038566726070779255\n",
            "Epoch 166/1000, Training Loss: 0.003118086279640282, Test Loss: 0.003848707752486691\n",
            "Epoch 167/1000, Training Loss: 0.0031116145875808277, Test Loss: 0.0038406493494121995\n",
            "Epoch 168/1000, Training Loss: 0.0031050620743086534, Test Loss: 0.003832497573934376\n",
            "Epoch 169/1000, Training Loss: 0.0030984286266588644, Test Loss: 0.0038242526248348273\n",
            "Epoch 170/1000, Training Loss: 0.0030917141441954205, Test Loss: 0.0038159147210950757\n",
            "Epoch 171/1000, Training Loss: 0.0030849185374067917, Test Loss: 0.0038074840991096383\n",
            "Epoch 172/1000, Training Loss: 0.0030780417257115977, Test Loss: 0.0037989610096045984\n",
            "Epoch 173/1000, Training Loss: 0.003071083635269048, Test Loss: 0.00379034571425312\n",
            "Epoch 174/1000, Training Loss: 0.0030640441965895148, Test Loss: 0.0037816384819802725\n",
            "Epoch 175/1000, Training Loss: 0.0030569233419412566, Test Loss: 0.0037728395849505746\n",
            "Epoch 176/1000, Training Loss: 0.003049721002549875, Test Loss: 0.0037639492942327683\n",
            "Epoch 177/1000, Training Loss: 0.0030424371055878673, Test Loss: 0.0037549678751376448\n",
            "Epoch 178/1000, Training Loss: 0.003035071570952363, Test Loss: 0.003745895582226028\n",
            "Epoch 179/1000, Training Loss: 0.003027624307829946, Test Loss: 0.0037367326539855766\n",
            "Epoch 180/1000, Training Loss: 0.003020095211048298, Test Loss: 0.0037274793071765005\n",
            "Epoch 181/1000, Training Loss: 0.0030124841572152727, Test Loss: 0.003718135730848027\n",
            "Epoch 182/1000, Training Loss: 0.003004791000646908, Test Loss: 0.0037087020800291364\n",
            "Epoch 183/1000, Training Loss: 0.0029970155690868146, Test Loss: 0.0036991784690988292\n",
            "Epoch 184/1000, Training Loss: 0.0029891576592202883, Test Loss: 0.0036895649648431288\n",
            "Epoch 185/1000, Training Loss: 0.0029812170319874296, Test Loss: 0.0036798615792077542\n",
            "Epoch 186/1000, Training Loss: 0.002973193407700524, Test Loss: 0.0036700682617574676\n",
            "Epoch 187/1000, Training Loss: 0.002965086460971753, Test Loss: 0.0036601848918548204\n",
            "Epoch 188/1000, Training Loss: 0.0029568958154583525, Test Loss: 0.0036502112705730926\n",
            "Epoch 189/1000, Training Loss: 0.0029486210384330257, Test Loss: 0.003640147112359953\n",
            "Epoch 190/1000, Training Loss: 0.002940261635188397, Test Loss: 0.003629992036470232\n",
            "Epoch 191/1000, Training Loss: 0.0029318170432849943, Test Loss: 0.003619745558187978\n",
            "Epoch 192/1000, Training Loss: 0.002923286626652993, Test Loss: 0.003609407079859485\n",
            "Epoch 193/1000, Training Loss: 0.002914669669558663, Test Loss: 0.0035989758817607104\n",
            "Epoch 194/1000, Training Loss: 0.0029059653704470606, Test Loss: 0.003588451112823741\n",
            "Epoch 195/1000, Training Loss: 0.0028971728356730745, Test Loss: 0.003577831781248333\n",
            "Epoch 196/1000, Training Loss: 0.0028882910731334336, Test Loss: 0.0035671167450256123\n",
            "Epoch 197/1000, Training Loss: 0.0028793189858127427, Test Loss: 0.003556304702401956\n",
            "Epoch 198/1000, Training Loss: 0.002870255365256977, Test Loss: 0.003545394182311834\n",
            "Epoch 199/1000, Training Loss: 0.002861098884988246, Test Loss: 0.0035343835348088876\n",
            "Epoch 200/1000, Training Loss: 0.0028518480938749032, Test Loss: 0.0035232709215249304\n",
            "Epoch 201/1000, Training Loss: 0.002842501409471378, Test Loss: 0.0035120543061866166\n",
            "Epoch 202/1000, Training Loss: 0.002833057111342409, Test Loss: 0.003500731445219581\n",
            "Epoch 203/1000, Training Loss: 0.0028235133343865754, Test Loss: 0.0034892998784694846\n",
            "Epoch 204/1000, Training Loss: 0.002813868062174384, Test Loss: 0.003477756920069116\n",
            "Epoch 205/1000, Training Loss: 0.0028041191203165595, Test Loss: 0.0034660996494800273\n",
            "Epoch 206/1000, Training Loss: 0.00279426416987858, Test Loss: 0.003454324902736587\n",
            "Epoch 207/1000, Training Loss: 0.002784300700858153, Test Loss: 0.003442429263919507\n",
            "Epoch 208/1000, Training Loss: 0.0027742260257430152, Test Loss: 0.0034304090568851005\n",
            "Epoch 209/1000, Training Loss: 0.002764037273167386, Test Loss: 0.0034182603372757063\n",
            "Epoch 210/1000, Training Loss: 0.0027537313816866048, Test Loss: 0.0034059788848359936\n",
            "Epoch 211/1000, Training Loss: 0.0027433050936908772, Test Loss: 0.0033935601960591766\n",
            "Epoch 212/1000, Training Loss: 0.002732754949480895, Test Loss: 0.003380999477186789\n",
            "Epoch 213/1000, Training Loss: 0.0027220772815302434, Test Loss: 0.0033682916375855536\n",
            "Epoch 214/1000, Training Loss: 0.0027112682089621165, Test Loss: 0.003355431283525049\n",
            "Epoch 215/1000, Training Loss: 0.0027003236322709497, Test Loss: 0.0033424127123806787\n",
            "Epoch 216/1000, Training Loss: 0.0026892392283232158, Test Loss: 0.003329229907287661\n",
            "Epoch 217/1000, Training Loss: 0.00267801044567587, Test Loss: 0.0033158765322738658\n",
            "Epoch 218/1000, Training Loss: 0.0026666325002557994, Test Loss: 0.003302345927902026\n",
            "Epoch 219/1000, Training Loss: 0.002655100371449175, Test Loss: 0.003288631107455767\n",
            "Epoch 220/1000, Training Loss: 0.002643408798655987, Test Loss: 0.003274724753708557\n",
            "Epoch 221/1000, Training Loss: 0.0026315522783720516, Test Loss: 0.0032606192163209947\n",
            "Epoch 222/1000, Training Loss: 0.0026195250618687273, Test Loss: 0.003246306509918953\n",
            "Epoch 223/1000, Training Loss: 0.0026073211535492568, Test Loss: 0.0032317783129142247\n",
            "Epoch 224/1000, Training Loss: 0.00259493431007025, Test Loss: 0.003217025967139638\n",
            "Epoch 225/1000, Training Loss: 0.002582358040327115, Test Loss: 0.003202040478382726\n",
            "Epoch 226/1000, Training Loss: 0.002569585606413467, Test Loss: 0.0031868125179160857\n",
            "Epoch 227/1000, Training Loss: 0.0025566100256763542, Test Loss: 0.003171332425138165\n",
            "Epoch 228/1000, Training Loss: 0.0025434240740017013, Test Loss: 0.0031555902114559827\n",
            "Epoch 229/1000, Training Loss: 0.0025300202904773887, Test Loss: 0.0031395755655606967\n",
            "Epoch 230/1000, Training Loss: 0.0025163909835948122, Test Loss: 0.0031232778602682397\n",
            "Epoch 231/1000, Training Loss: 0.0025025282391633267, Test Loss: 0.0031066861611202283\n",
            "Epoch 232/1000, Training Loss: 0.0024884239301255257, Test Loss: 0.003089789236964763\n",
            "Epoch 233/1000, Training Loss: 0.0024740697284744016, Test Loss: 0.003072575572762352\n",
            "Epoch 234/1000, Training Loss: 0.002459457119485925, Test Loss: 0.0030550333848886435\n",
            "Epoch 235/1000, Training Loss: 0.0024445774184918027, Test Loss: 0.0030371506392323955\n",
            "Epoch 236/1000, Training Loss: 0.0024294217904269524, Test Loss: 0.00301891507241353\n",
            "Epoch 237/1000, Training Loss: 0.0024139812723937386, Test Loss: 0.003000314216471534\n",
            "Epoch 238/1000, Training Loss: 0.0023982467994898998, Test Loss: 0.002981335427397745\n",
            "Epoch 239/1000, Training Loss: 0.0023822092341485275, Test Loss: 0.002961965917905562\n",
            "Epoch 240/1000, Training Loss: 0.002365859399235857, Test Loss: 0.002942192794848548\n",
            "Epoch 241/1000, Training Loss: 0.002349188115145169, Test Loss: 0.0029220031017069925\n",
            "Epoch 242/1000, Training Loss: 0.0023321862411119503, Test Loss: 0.0029013838665665577\n",
            "Epoch 243/1000, Training Loss: 0.0023148447209560704, Test Loss: 0.0028803221560072517\n",
            "Epoch 244/1000, Training Loss: 0.0022971546334298438, Test Loss: 0.0028588051353046623\n",
            "Epoch 245/1000, Training Loss: 0.0022791072473162554, Test Loss: 0.0028368201353168093\n",
            "Epoch 246/1000, Training Loss: 0.0022606940813780963, Test Loss: 0.0028143547263870623\n",
            "Epoch 247/1000, Training Loss: 0.0022419069692060905, Test Loss: 0.0027913967995346033\n",
            "Epoch 248/1000, Training Loss: 0.0022227381289514504, Test Loss: 0.0027679346551272065\n",
            "Epoch 249/1000, Training Loss: 0.002203180237855651, Test Loss: 0.0027439570991353327\n",
            "Epoch 250/1000, Training Loss: 0.002183226511407206, Test Loss: 0.0027194535469503654\n",
            "Epoch 251/1000, Training Loss: 0.0021628707868622315, Test Loss: 0.0026944141346131917\n",
            "Epoch 252/1000, Training Loss: 0.0021421076107627475, Test Loss: 0.0026688298371413954\n",
            "Epoch 253/1000, Training Loss: 0.002120932329975056, Test Loss: 0.002642692593465812\n",
            "Epoch 254/1000, Training Loss: 0.0020993411856511128, Test Loss: 0.002615995437290718\n",
            "Epoch 255/1000, Training Loss: 0.0020773314093901233, Test Loss: 0.002588732632979809\n",
            "Epoch 256/1000, Training Loss: 0.002054901320747632, Test Loss: 0.002560899815345285\n",
            "Epoch 257/1000, Training Loss: 0.002032050425107555, Test Loss: 0.002532494131985027\n",
            "Epoch 258/1000, Training Loss: 0.002008779510801491, Test Loss: 0.002503514386578259\n",
            "Epoch 259/1000, Training Loss: 0.001985090744232798, Test Loss: 0.002473961181320459\n",
            "Epoch 260/1000, Training Loss: 0.001960987761643466, Test Loss: 0.002443837056460808\n",
            "Epoch 261/1000, Training Loss: 0.0019364757560537702, Test Loss: 0.0024131466247087566\n",
            "Epoch 262/1000, Training Loss: 0.0019115615578123568, Test Loss: 0.0023818966981089375\n",
            "Epoch 263/1000, Training Loss: 0.001886253707121451, Test Loss: 0.002350096404854173\n",
            "Epoch 264/1000, Training Loss: 0.0018605625168533505, Test Loss: 0.002317757293423497\n",
            "Epoch 265/1000, Training Loss: 0.0018345001239538094, Test Loss: 0.00228489342140286\n",
            "Epoch 266/1000, Training Loss: 0.0018080805277401646, Test Loss: 0.002251521426377583\n",
            "Epoch 267/1000, Training Loss: 0.0017813196134504627, Test Loss: 0.0022176605763815425\n",
            "Epoch 268/1000, Training Loss: 0.001754235159488316, Test Loss: 0.0021833327975522314\n",
            "Epoch 269/1000, Training Loss: 0.0017268468269395658, Test Loss: 0.002148562676872949\n",
            "Epoch 270/1000, Training Loss: 0.001699176130113496, Test Loss: 0.0021133774381826838\n",
            "Epoch 271/1000, Training Loss: 0.0016712463870848556, Test Loss: 0.0020778068899966837\n",
            "Epoch 272/1000, Training Loss: 0.001643082649483563, Test Loss: 0.002041883344100503\n",
            "Epoch 273/1000, Training Loss: 0.0016147116110960457, Test Loss: 0.0020056415043505495\n",
            "Epoch 274/1000, Training Loss: 0.0015861614952030546, Test Loss: 0.0019691183256259318\n",
            "Epoch 275/1000, Training Loss: 0.0015574619209790292, Test Loss: 0.0019323528434198405\n",
            "Epoch 276/1000, Training Loss: 0.0015286437497114413, Test Loss: 0.001895385975124119\n",
            "Epoch 277/1000, Training Loss: 0.0014997389120554676, Test Loss: 0.0018582602946361337\n",
            "Epoch 278/1000, Training Loss: 0.0014707802180087559, Test Loss: 0.001821019782491748\n",
            "Epoch 279/1000, Training Loss: 0.0014418011517583424, Test Loss: 0.001783709554289021\n",
            "Epoch 280/1000, Training Loss: 0.0014128356540007336, Test Loss: 0.0017463755707010325\n",
            "Epoch 281/1000, Training Loss: 0.0013839178947480848, Test Loss: 0.0017090643328680699\n",
            "Epoch 282/1000, Training Loss: 0.001355082039989168, Test Loss: 0.0016718225673931935\n",
            "Epoch 283/1000, Training Loss: 0.0013263620158536322, Test Loss: 0.0016346969055233054\n",
            "Epoch 284/1000, Training Loss: 0.0012977912741144624, Test Loss: 0.0015977335613631847\n",
            "Epoch 285/1000, Training Loss: 0.0012694025629405984, Test Loss: 0.0015609780141239982\n",
            "Epoch 286/1000, Training Loss: 0.0012412277067695928, Test Loss: 0.0015244746994364383\n",
            "Epoch 287/1000, Training Loss: 0.001213297399002728, Test Loss: 0.00148826671464861\n",
            "Epoch 288/1000, Training Loss: 0.0011856410109346743, Test Loss: 0.0014523955427750046\n",
            "Epoch 289/1000, Training Loss: 0.0011582864199242695, Test Loss: 0.0014169007993646637\n",
            "Epoch 290/1000, Training Loss: 0.00113125985930823, Test Loss: 0.0013818200060232257\n",
            "Epoch 291/1000, Training Loss: 0.0011045857919768634, Test Loss: 0.001347188393671162\n",
            "Epoch 292/1000, Training Loss: 0.001078286808896191, Test Loss: 0.001313038737873769\n",
            "Epoch 293/1000, Training Loss: 0.001052383553203479, Test Loss: 0.0012794012277682432\n",
            "Epoch 294/1000, Training Loss: 0.001026894669852382, Test Loss: 0.0012463033692742375\n",
            "Epoch 295/1000, Training Loss: 0.0010018367801681348, Test Loss: 0.0012137699224435987\n",
            "Epoch 296/1000, Training Loss: 0.0009772244801169389, Test Loss: 0.0011818228720173153\n",
            "Epoch 297/1000, Training Loss: 0.0009530703606172188, Test Loss: 0.0011504814295456337\n",
            "Epoch 298/1000, Training Loss: 0.0009293850478371046, Test Loss: 0.0011197620648155028\n",
            "Epoch 299/1000, Training Loss: 0.0009061772611403846, Test Loss: 0.0010896785638372297\n",
            "Epoch 300/1000, Training Loss: 0.0008834538861633009, Test Loss: 0.0010602421102790362\n",
            "Epoch 301/1000, Training Loss: 0.0008612200604228962, Test Loss: 0.0010314613870068153\n",
            "Epoch 302/1000, Training Loss: 0.0008394792688653135, Test Loss: 0.0010033426942815437\n",
            "Epoch 303/1000, Training Loss: 0.0008182334468474159, Test Loss: 0.0009758900811777773\n",
            "Epoch 304/1000, Training Loss: 0.0007974830881930741, Test Loss: 0.0009491054868978037\n",
            "Epoch 305/1000, Training Loss: 0.0007772273561613886, Test Loss: 0.0009229888888492774\n",
            "Epoch 306/1000, Training Loss: 0.0007574641953931535, Test Loss: 0.0008975384546102915\n",
            "Epoch 307/1000, Training Loss: 0.0007381904431501213, Test Loss: 0.0008727506952053001\n",
            "Epoch 308/1000, Training Loss: 0.0007194019384166409, Test Loss: 0.0008486206174403198\n",
            "Epoch 309/1000, Training Loss: 0.0007010936276853508, Test Loss: 0.0008251418733807996\n",
            "Epoch 310/1000, Training Loss: 0.0006832596664888851, Test Loss: 0.0008023069053864776\n",
            "Epoch 311/1000, Training Loss: 0.000665893515962922, Test Loss: 0.0007801070854347555\n",
            "Epoch 312/1000, Training Loss: 0.0006489880339275295, Test Loss: 0.0007585328477588525\n",
            "Epoch 313/1000, Training Loss: 0.0006325355601518673, Test Loss: 0.0007375738140945095\n",
            "Epoch 314/1000, Training Loss: 0.0006165279956209967, Test Loss: 0.0007172189110661189\n",
            "Epoch 315/1000, Training Loss: 0.0006009568757527484, Test Loss: 0.0006974564794482982\n",
            "Epoch 316/1000, Training Loss: 0.0005858134376190556, Test Loss: 0.0006782743752128791\n",
            "Epoch 317/1000, Training Loss: 0.000571088681311015, Test Loss: 0.0006596600624148093\n",
            "Epoch 318/1000, Training Loss: 0.0005567734256527406, Test Loss: 0.0006416006980863953\n",
            "Epoch 319/1000, Training Loss: 0.000542858358517977, Test Loss: 0.0006240832093996378\n",
            "Epoch 320/1000, Training Loss: 0.000529334082037698, Test Loss: 0.0006070943634246743\n",
            "Epoch 321/1000, Training Loss: 0.0005161911530088957, Test Loss: 0.0005906208298609123\n",
            "Epoch 322/1000, Training Loss: 0.0005034201188266988, Test Loss: 0.000574649237149883\n",
            "Epoch 323/1000, Training Loss: 0.0004910115492654225, Test Loss: 0.0005591662223971893\n",
            "Epoch 324/1000, Training Loss: 0.00047895606443141165, Test Loss: 0.0005441584755385457\n",
            "Epoch 325/1000, Training Loss: 0.0004672443592025282, Test Loss: 0.0005296127781831557\n",
            "Epoch 326/1000, Training Loss: 0.00045586722445759676, Test Loss: 0.0005155160375592526\n",
            "Epoch 327/1000, Training Loss: 0.00044481556538480266, Test Loss: 0.0005018553159728663\n",
            "Epoch 328/1000, Training Loss: 0.0004340804171420839, Test Loss: 0.0004886178561732607\n",
            "Epoch 329/1000, Training Loss: 0.0004236529581254266, Test Loss: 0.00047579110299831066\n",
            "Epoch 330/1000, Training Loss: 0.00041352452108354835, Test Loss: 0.00046336272165122726\n",
            "Epoch 331/1000, Training Loss: 0.0004036866022997757, Test Loss: 0.0004513206129372616\n",
            "Epoch 332/1000, Training Loss: 0.00039413086904479866, Test Loss: 0.000439652925766123\n",
            "Epoch 333/1000, Training Loss: 0.0003848491654871747, Test Loss: 0.0004283480672029405\n",
            "Epoch 334/1000, Training Loss: 0.00037583351723257747, Test Loss: 0.0004173947103285401\n",
            "Epoch 335/1000, Training Loss: 0.00036707613464762853, Test Loss: 0.0004067818001483571\n",
            "Epoch 336/1000, Training Loss: 0.0003585694151099296, Test Loss: 0.00039649855776893007\n",
            "Epoch 337/1000, Training Loss: 0.00035030594431271296, Test Loss: 0.0003865344830417742\n",
            "Epoch 338/1000, Training Loss: 0.00034227849674015514, Test Loss: 0.0003768793558562919\n",
            "Epoch 339/1000, Training Loss: 0.00033448003541810753, Test Loss: 0.0003675232362465805\n",
            "Epoch 340/1000, Training Loss: 0.0003269037110345088, Test Loss: 0.0003584564634614156\n",
            "Epoch 341/1000, Training Loss: 0.00031954286051419096, Test Loss: 0.00034966965413216195\n",
            "Epoch 342/1000, Training Loss: 0.0003123910051240257, Test Loss: 0.00034115369966014814\n",
            "Epoch 343/1000, Training Loss: 0.00030544184817642346, Test Loss: 0.0003328997629328026\n",
            "Epoch 344/1000, Training Loss: 0.00029868927239192064, Test Loss: 0.00032489927446670535\n",
            "Epoch 345/1000, Training Loss: 0.00029212733697502844, Test Loss: 0.00031714392806548116\n",
            "Epoch 346/1000, Training Loss: 0.00028575027445156746, Test Loss: 0.00030962567607118206\n",
            "Epoch 347/1000, Training Loss: 0.00027955248731035155, Test Loss: 0.00030233672427939644\n",
            "Epoch 348/1000, Training Loss: 0.0002735285444871789, Test Loss: 0.00029526952658056563\n",
            "Epoch 349/1000, Training Loss: 0.00026767317772477805, Test Loss: 0.0002884167793831385\n",
            "Epoch 350/1000, Training Loss: 0.00026198127783836115, Test Loss: 0.00028177141586782997\n",
            "Epoch 351/1000, Training Loss: 0.0002564478909129098, Test Loss: 0.0002753266001165746\n",
            "Epoch 352/1000, Training Loss: 0.000251068214455174, Test Loss: 0.0002690757211546775\n",
            "Epoch 353/1000, Training Loss: 0.0002458375935204372, Test Loss: 0.0002630123869399504\n",
            "Epoch 354/1000, Training Loss: 0.00024075151683161615, Test Loss: 0.0002571304183285579\n",
            "Epoch 355/1000, Training Loss: 0.00023580561290593402, Test Loss: 0.0002514238430434562\n",
            "Epoch 356/1000, Training Loss: 0.00023099564620235202, Test Loss: 0.00024588688966797374\n",
            "Epoch 357/1000, Training Loss: 0.00022631751330116605, Test Loss: 0.00024051398168409424\n",
            "Epoch 358/1000, Training Loss: 0.00022176723912547373, Test Loss: 0.0002352997315722334\n",
            "Epoch 359/1000, Training Loss: 0.00021734097321282332, Test Loss: 0.0002302389349868967\n",
            "Epoch 360/1000, Training Loss: 0.00021303498604405255, Test Loss: 0.0002253265650204899\n",
            "Epoch 361/1000, Training Loss: 0.0002088456654351397, Test Loss: 0.00022055776656551465\n",
            "Epoch 362/1000, Training Loss: 0.00020476951299692656, Test Loss: 0.00021592785078377642\n",
            "Epoch 363/1000, Training Loss: 0.00020080314066661797, Test Loss: 0.00021143228968958398\n",
            "Epoch 364/1000, Training Loss: 0.00019694326731422462, Test Loss: 0.00020706671085266278\n",
            "Epoch 365/1000, Training Loss: 0.00019318671542636778, Test Loss: 0.0002028268922252349\n",
            "Epoch 366/1000, Training Loss: 0.0001895304078692396, Test Loss: 0.00019870875709663692\n",
            "Epoch 367/1000, Training Loss: 0.00018597136473202046, Test Loss: 0.0001947083691779726\n",
            "Epoch 368/1000, Training Loss: 0.00018250670025153788, Test Loss: 0.00019082192781840978\n",
            "Epoch 369/1000, Training Loss: 0.00017913361981854495, Test Loss: 0.00018704576335402582\n",
            "Epoch 370/1000, Training Loss: 0.00017584941706564325, Test Loss: 0.0001833763325894837\n",
            "Epoch 371/1000, Training Loss: 0.00017265147103655192, Test Loss: 0.0001798102144122235\n",
            "Epoch 372/1000, Training Loss: 0.00016953724343617593, Test Loss: 0.0001763441055384427\n",
            "Epoch 373/1000, Training Loss: 0.00016650427596066157, Test Loss: 0.0001729748163896503\n",
            "Epoch 374/1000, Training Loss: 0.00016355018770646435, Test Loss: 0.00016969926709828816\n",
            "Epoch 375/1000, Training Loss: 0.00016067267265727127, Test Loss: 0.00016651448364056153\n",
            "Epoch 376/1000, Training Loss: 0.00015786949724747544, Test Loss: 0.00016341759409441931\n",
            "Epoch 377/1000, Training Loss: 0.000155138498000801, Test Loss: 0.00016040582502033272\n",
            "Epoch 378/1000, Training Loss: 0.00015247757924255402, Test Loss: 0.0001574764979624294\n",
            "Epoch 379/1000, Training Loss: 0.00014988471088394985, Test Loss: 0.00015462702606735182\n",
            "Epoch 380/1000, Training Loss: 0.00014735792627683733, Test Loss: 0.00015185491081808178\n",
            "Epoch 381/1000, Training Loss: 0.00014489532013716393, Test Loss: 0.0001491577388799335\n",
            "Epoch 382/1000, Training Loss: 0.00014249504653545233, Test Loss: 0.00014653317905581357\n",
            "Epoch 383/1000, Training Loss: 0.00014015531695252663, Test Loss: 0.00014397897934777945\n",
            "Epoch 384/1000, Training Loss: 0.00013787439839878053, Test Loss: 0.0001414929641219986\n",
            "Epoch 385/1000, Training Loss: 0.00013565061159519, Test Loss: 0.00013907303137407077\n",
            "Epoch 386/1000, Training Loss: 0.0001334823292143433, Test Loss: 0.00013671715009177848\n",
            "Epoch 387/1000, Training Loss: 0.00013136797417973814, Test Loss: 0.00013442335771227446\n",
            "Epoch 388/1000, Training Loss: 0.00012930601802162723, Test Loss: 0.00013218975767081167\n",
            "Epoch 389/1000, Training Loss: 0.0001272949792877045, Test Loss: 0.00013001451703805911\n",
            "Epoch 390/1000, Training Loss: 0.00012533342200694478, Test Loss: 0.00012789586424317574\n",
            "Epoch 391/1000, Training Loss: 0.00012341995420495511, Test Loss: 0.00012583208687979943\n",
            "Epoch 392/1000, Training Loss: 0.00012155322646921795, Test Loss: 0.00012382152959220625\n",
            "Epoch 393/1000, Training Loss: 0.00011973193056261353, Test Loss: 0.00012186259203888751\n",
            "Epoch 394/1000, Training Loss: 0.00011795479808370313, Test Loss: 0.00011995372693093729\n",
            "Epoch 395/1000, Training Loss: 0.00011622059917220662, Test Loss: 0.00011809343814259895\n",
            "Epoch 396/1000, Training Loss: 0.00011452814125824242, Test Loss: 0.00011628027889150342\n",
            "Epoch 397/1000, Training Loss: 0.00011287626785383607, Test Loss: 0.00011451284998607374\n",
            "Epoch 398/1000, Training Loss: 0.00011126385738532671, Test Loss: 0.00011278979813774548\n",
            "Epoch 399/1000, Training Loss: 0.00010968982206527951, Test Loss: 0.00011110981433563894\n",
            "Epoch 400/1000, Training Loss: 0.00010815310680258119, Test Loss: 0.00010947163228143493\n",
            "Epoch 401/1000, Training Loss: 0.0001066526881494321, Test Loss: 0.00010787402688226137\n",
            "Epoch 402/1000, Training Loss: 0.00010518757328396488, Test Loss: 0.00010631581279944964\n",
            "Epoch 403/1000, Training Loss: 0.00010375679902729259, Test Loss: 0.00010479584305110819\n",
            "Epoch 404/1000, Training Loss: 0.0001023594308937854, Test Loss: 0.00010331300766651328\n",
            "Epoch 405/1000, Training Loss: 0.00010099456217345796, Test Loss: 0.000101866232390396\n",
            "Epoch 406/1000, Training Loss: 9.966131304533693e-05, Test Loss: 0.00010045447743525231\n",
            "Epoch 407/1000, Training Loss: 9.835882972075851e-05, Test Loss: 9.907673627987382e-05\n",
            "Epoch 408/1000, Training Loss: 9.708628361554764e-05, Test Loss: 9.773203451236064e-05\n",
            "Epoch 409/1000, Training Loss: 9.584287055007964e-05, Test Loss: 9.641942871594758e-05\n",
            "Epoch 410/1000, Training Loss: 9.462780997626214e-05, Test Loss: 9.513800539600178e-05\n",
            "Epoch 411/1000, Training Loss: 9.344034423049385e-05, Test Loss: 9.388687994665743e-05\n",
            "Epoch 412/1000, Training Loss: 9.227973781169744e-05, Test Loss: 9.26651956555521e-05\n",
            "Epoch 413/1000, Training Loss: 9.114527668355914e-05, Test Loss: 9.147212274524014e-05\n",
            "Epoch 414/1000, Training Loss: 9.003626760012602e-05, Test Loss: 9.030685744986888e-05\n",
            "Epoch 415/1000, Training Loss: 8.89520374539438e-05, Test Loss: 8.916862112578184e-05\n",
            "Epoch 416/1000, Training Loss: 8.789193264595857e-05, Test Loss: 8.805665939473815e-05\n",
            "Epoch 417/1000, Training Loss: 8.685531847641254e-05, Test Loss: 8.697024131852249e-05\n",
            "Epoch 418/1000, Training Loss: 8.584157855601366e-05, Test Loss: 8.590865860372726e-05\n",
            "Epoch 419/1000, Training Loss: 8.485011423665895e-05, Test Loss: 8.48712248355609e-05\n",
            "Epoch 420/1000, Training Loss: 8.388034406104246e-05, Test Loss: 8.385727473956884e-05\n",
            "Epoch 421/1000, Training Loss: 8.29317032304826e-05, Test Loss: 8.286616347020143e-05\n",
            "Epoch 422/1000, Training Loss: 8.200364309033743e-05, Test Loss: 8.189726592519527e-05\n",
            "Epoch 423/1000, Training Loss: 8.10956306323922e-05, Test Loss: 8.094997608478819e-05\n",
            "Epoch 424/1000, Training Loss: 8.020714801363915e-05, Test Loss: 8.002370637481579e-05\n",
            "Epoch 425/1000, Training Loss: 7.933769209086567e-05, Test Loss: 7.91178870527701e-05\n",
            "Epoch 426/1000, Training Loss: 7.84867739705187e-05, Test Loss: 7.823196561595611e-05\n",
            "Epoch 427/1000, Training Loss: 7.765391857330162e-05, Test Loss: 7.736540623089381e-05\n",
            "Epoch 428/1000, Training Loss: 7.683866421300336e-05, Test Loss: 7.65176891831558e-05\n",
            "Epoch 429/1000, Training Loss: 7.604056218905977e-05, Test Loss: 7.568831034686436e-05\n",
            "Epoch 430/1000, Training Loss: 7.525917639238442e-05, Test Loss: 7.487678067310673e-05\n",
            "Epoch 431/1000, Training Loss: 7.449408292399999e-05, Test Loss: 7.408262569653626e-05\n",
            "Epoch 432/1000, Training Loss: 7.374486972603123e-05, Test Loss: 7.330538505947412e-05\n",
            "Epoch 433/1000, Training Loss: 7.301113622464312e-05, Test Loss: 7.254461205286161e-05\n",
            "Epoch 434/1000, Training Loss: 7.229249298450565e-05, Test Loss: 7.179987317340697e-05\n",
            "Epoch 435/1000, Training Loss: 7.158856137439153e-05, Test Loss: 7.107074769632354e-05\n",
            "Epoch 436/1000, Training Loss: 7.08989732435264e-05, Test Loss: 7.035682726307155e-05\n",
            "Epoch 437/1000, Training Loss: 7.022337060832908e-05, Test Loss: 6.965771548354065e-05\n",
            "Epoch 438/1000, Training Loss: 6.956140534917655e-05, Test Loss: 6.897302755212099e-05\n",
            "Epoch 439/1000, Training Loss: 6.891273891686385e-05, Test Loss: 6.830238987715808e-05\n",
            "Epoch 440/1000, Training Loss: 6.827704204842145e-05, Test Loss: 6.76454397232731e-05\n",
            "Epoch 441/1000, Training Loss: 6.765399449197721e-05, Test Loss: 6.700182486608081e-05\n",
            "Epoch 442/1000, Training Loss: 6.704328474035e-05, Test Loss: 6.637120325883509e-05\n",
            "Epoch 443/1000, Training Loss: 6.644460977309025e-05, Test Loss: 6.575324271056381e-05\n",
            "Epoch 444/1000, Training Loss: 6.585767480667618e-05, Test Loss: 6.514762057526809e-05\n",
            "Epoch 445/1000, Training Loss: 6.528219305258426e-05, Test Loss: 6.455402345176011e-05\n",
            "Epoch 446/1000, Training Loss: 6.471788548298172e-05, Test Loss: 6.397214689377239e-05\n",
            "Epoch 447/1000, Training Loss: 6.416448060378124e-05, Test Loss: 6.340169512993903e-05\n",
            "Epoch 448/1000, Training Loss: 6.362171423480348e-05, Test Loss: 6.284238079329135e-05\n",
            "Epoch 449/1000, Training Loss: 6.308932929682203e-05, Test Loss: 6.229392465992547e-05\n",
            "Epoch 450/1000, Training Loss: 6.25670756052501e-05, Test Loss: 6.175605539649534e-05\n",
            "Epoch 451/1000, Training Loss: 6.205470967025857e-05, Test Loss: 6.122850931621928e-05\n",
            "Epoch 452/1000, Training Loss: 6.15519945030985e-05, Test Loss: 6.07110301430798e-05\n",
            "Epoch 453/1000, Training Loss: 6.10586994284366e-05, Test Loss: 6.020336878393066e-05\n",
            "Epoch 454/1000, Training Loss: 6.0574599902493254e-05, Test Loss: 5.970528310820962e-05\n",
            "Epoch 455/1000, Training Loss: 6.0099477336797484e-05, Test Loss: 5.9216537734998785e-05\n",
            "Epoch 456/1000, Training Loss: 5.9633118927372266e-05, Test Loss: 5.873690382715407e-05\n",
            "Epoch 457/1000, Training Loss: 5.917531748917043e-05, Test Loss: 5.826615889225686e-05\n",
            "Epoch 458/1000, Training Loss: 5.8725871295592015e-05, Test Loss: 5.7804086590141095e-05\n",
            "Epoch 459/1000, Training Loss: 5.828458392291012e-05, Test Loss: 5.735047654675795e-05\n",
            "Epoch 460/1000, Training Loss: 5.785126409945369e-05, Test Loss: 5.6905124174157696e-05\n",
            "Epoch 461/1000, Training Loss: 5.742572555938203e-05, Test Loss: 5.646783049636643e-05\n",
            "Epoch 462/1000, Training Loss: 5.70077869009133e-05, Test Loss: 5.6038401980952256e-05\n",
            "Epoch 463/1000, Training Loss: 5.6597271448849564e-05, Test Loss: 5.5616650376073524e-05\n",
            "Epoch 464/1000, Training Loss: 5.619400712126778e-05, Test Loss: 5.520239255282188e-05\n",
            "Epoch 465/1000, Training Loss: 5.5797826300241474e-05, Test Loss: 5.479545035267386e-05\n",
            "Epoch 466/1000, Training Loss: 5.5408565706456325e-05, Test Loss: 5.4395650439863174e-05\n",
            "Epoch 467/1000, Training Loss: 5.5026066277603283e-05, Test Loss: 5.400282415851473e-05\n",
            "Epoch 468/1000, Training Loss: 5.465017305041988e-05, Test Loss: 5.361680739436054e-05\n",
            "Epoch 469/1000, Training Loss: 5.4280735046269255e-05, Test Loss: 5.323744044089234e-05\n",
            "Epoch 470/1000, Training Loss: 5.391760516013569e-05, Test Loss: 5.286456786978025e-05\n",
            "Epoch 471/1000, Training Loss: 5.3560640052939586e-05, Test Loss: 5.2498038405428554e-05\n",
            "Epoch 472/1000, Training Loss: 5.320970004704915e-05, Test Loss: 5.213770480350666e-05\n",
            "Epoch 473/1000, Training Loss: 5.286464902490759e-05, Test Loss: 5.1783423733337225e-05\n",
            "Epoch 474/1000, Training Loss: 5.2525354330655165e-05, Test Loss: 5.143505566399168e-05\n",
            "Epoch 475/1000, Training Loss: 5.21916866746696e-05, Test Loss: 5.1092464753979914e-05\n",
            "Epoch 476/1000, Training Loss: 5.186352004092286e-05, Test Loss: 5.0755518744405786e-05\n",
            "Epoch 477/1000, Training Loss: 5.154073159706737e-05, Test Loss: 5.042408885546665e-05\n",
            "Epoch 478/1000, Training Loss: 5.122320160716544e-05, Test Loss: 5.0098049686194314e-05\n",
            "Epoch 479/1000, Training Loss: 5.091081334697892e-05, Test Loss: 4.977727911731168e-05\n",
            "Epoch 480/1000, Training Loss: 5.0603453021745425e-05, Test Loss: 4.946165821712207e-05\n",
            "Epoch 481/1000, Training Loss: 5.0301009686347446e-05, Test Loss: 4.915107115030493e-05\n",
            "Epoch 482/1000, Training Loss: 5.0003375167814874e-05, Test Loss: 4.884540508953461e-05\n",
            "Epoch 483/1000, Training Loss: 4.97104439900866e-05, Test Loss: 4.854455012983433e-05\n",
            "Epoch 484/1000, Training Loss: 4.942211330094976e-05, Test Loss: 4.8248399205550584e-05\n",
            "Epoch 485/1000, Training Loss: 4.913828280110159e-05, Test Loss: 4.79568480098882e-05\n",
            "Epoch 486/1000, Training Loss: 4.885885467526222e-05, Test Loss: 4.7669794916898806e-05\n",
            "Epoch 487/1000, Training Loss: 4.85837335252752e-05, Test Loss: 4.7387140905855125e-05\n",
            "Epoch 488/1000, Training Loss: 4.831282630513603e-05, Test Loss: 4.710878948792952e-05\n",
            "Epoch 489/1000, Training Loss: 4.8046042257889395e-05, Test Loss: 4.6834646635095884e-05\n",
            "Epoch 490/1000, Training Loss: 4.778329285433594e-05, Test Loss: 4.6564620711188317e-05\n",
            "Epoch 491/1000, Training Loss: 4.752449173349462e-05, Test Loss: 4.629862240504177e-05\n",
            "Epoch 492/1000, Training Loss: 4.726955464476772e-05, Test Loss: 4.60365646656495e-05\n",
            "Epoch 493/1000, Training Loss: 4.701839939175492e-05, Test Loss: 4.577836263927166e-05\n",
            "Epoch 494/1000, Training Loss: 4.677094577766601e-05, Test Loss: 4.552393360842873e-05\n",
            "Epoch 495/1000, Training Loss: 4.652711555228719e-05, Test Loss: 4.527319693272056e-05\n",
            "Epoch 496/1000, Training Loss: 4.628683236044779e-05, Test Loss: 4.502607399141469e-05\n",
            "Epoch 497/1000, Training Loss: 4.605002169194813e-05, Test Loss: 4.478248812774156e-05\n",
            "Epoch 498/1000, Training Loss: 4.581661083290129e-05, Test Loss: 4.4542364594848e-05\n",
            "Epoch 499/1000, Training Loss: 4.558652881844739e-05, Test Loss: 4.4305630503352345e-05\n",
            "Epoch 500/1000, Training Loss: 4.535970638679537e-05, Test Loss: 4.407221477044578e-05\n",
            "Epoch 501/1000, Training Loss: 4.5136075934560034e-05, Test Loss: 4.384204807050312e-05\n",
            "Epoch 502/1000, Training Loss: 4.4915571473347755e-05, Test Loss: 4.36150627871425e-05\n",
            "Epoch 503/1000, Training Loss: 4.469812858755868e-05, Test Loss: 4.339119296669323e-05\n",
            "Epoch 504/1000, Training Loss: 4.448368439336652e-05, Test Loss: 4.317037427302969e-05\n",
            "Epoch 505/1000, Training Loss: 4.4272177498842026e-05, Test Loss: 4.29525439437238e-05\n",
            "Epoch 506/1000, Training Loss: 4.406354796518664e-05, Test Loss: 4.27376407474748e-05\n",
            "Epoch 507/1000, Training Loss: 4.385773726904022e-05, Test Loss: 4.25256049427819e-05\n",
            "Epoch 508/1000, Training Loss: 4.365468826583724e-05, Test Loss: 4.2316378237814576e-05\n",
            "Epoch 509/1000, Training Loss: 4.345434515417519e-05, Test Loss: 4.210990375144857e-05\n",
            "Epoch 510/1000, Training Loss: 4.325665344116482e-05, Test Loss: 4.190612597542088e-05\n",
            "Epoch 511/1000, Training Loss: 4.30615599087403e-05, Test Loss: 4.170499073758748e-05\n",
            "Epoch 512/1000, Training Loss: 4.286901258089199e-05, Test Loss: 4.150644516622998e-05\n",
            "Epoch 513/1000, Training Loss: 4.2678960691801135e-05, Test Loss: 4.131043765539048e-05\n",
            "Epoch 514/1000, Training Loss: 4.249135465484862e-05, Test Loss: 4.1116917831202206e-05\n",
            "Epoch 515/1000, Training Loss: 4.2306146032472164e-05, Test Loss: 4.0925836519180557e-05\n",
            "Epoch 516/1000, Training Loss: 4.212328750684502e-05, Test Loss: 4.07371457124482e-05\n",
            "Epoch 517/1000, Training Loss: 4.1942732851356374e-05, Test Loss: 4.055079854086618e-05\n",
            "Epoch 518/1000, Training Loss: 4.176443690286717e-05, Test Loss: 4.036674924104121e-05\n",
            "Epoch 519/1000, Training Loss: 4.158835553472014e-05, Test Loss: 4.01849531271859e-05\n",
            "Epoch 520/1000, Training Loss: 4.1414445630482256e-05, Test Loss: 4.000536656280276e-05\n",
            "Epoch 521/1000, Training Loss: 4.1242665058398e-05, Test Loss: 3.982794693316544e-05\n",
            "Epoch 522/1000, Training Loss: 4.107297264653273e-05, Test Loss: 3.965265261858032e-05\n",
            "Epoch 523/1000, Training Loss: 4.090532815858818e-05, Test Loss: 3.947944296839982e-05\n",
            "Epoch 524/1000, Training Loss: 4.0739692270369166e-05, Test Loss: 3.930827827576397e-05\n",
            "Epoch 525/1000, Training Loss: 4.0576026546881026e-05, Test Loss: 3.913911975304597e-05\n",
            "Epoch 526/1000, Training Loss: 4.041429342004641e-05, Test Loss: 3.89719295079972e-05\n",
            "Epoch 527/1000, Training Loss: 4.0254456167014404e-05, Test Loss: 3.8806670520542445e-05\n",
            "Epoch 528/1000, Training Loss: 4.009647888905442e-05, Test Loss: 3.864330662023336e-05\n",
            "Epoch 529/1000, Training Loss: 3.9940326491010667e-05, Test Loss: 3.848180246432454e-05\n",
            "Epoch 530/1000, Training Loss: 3.978596466130659e-05, Test Loss: 3.832212351645778e-05\n",
            "Epoch 531/1000, Training Loss: 3.96333598524812e-05, Test Loss: 3.816423602593929e-05\n",
            "Epoch 532/1000, Training Loss: 3.948247926224233e-05, Test Loss: 3.800810700758757e-05\n",
            "Epoch 533/1000, Training Loss: 3.9333290815023184e-05, Test Loss: 3.785370422213964e-05\n",
            "Epoch 534/1000, Training Loss: 3.9185763144028926e-05, Test Loss: 3.7700996157196236e-05\n",
            "Epoch 535/1000, Training Loss: 3.903986557375353e-05, Test Loss: 3.754995200868857e-05\n",
            "Epoch 536/1000, Training Loss: 3.8895568102965135e-05, Test Loss: 3.740054166286005e-05\n",
            "Epoch 537/1000, Training Loss: 3.8752841388133744e-05, Test Loss: 3.725273567873567e-05\n",
            "Epoch 538/1000, Training Loss: 3.8611656727301216e-05, Test Loss: 3.71065052710761e-05\n",
            "Epoch 539/1000, Training Loss: 3.8471986044368455e-05, Test Loss: 3.696182229378984e-05\n",
            "Epoch 540/1000, Training Loss: 3.833380187380156e-05, Test Loss: 3.681865922380823e-05\n",
            "Epoch 541/1000, Training Loss: 3.819707734573439e-05, Test Loss: 3.667698914538948e-05\n",
            "Epoch 542/1000, Training Loss: 3.806178617146209e-05, Test Loss: 3.653678573485279e-05\n",
            "Epoch 543/1000, Training Loss: 3.792790262931577e-05, Test Loss: 3.639802324573044e-05\n",
            "Epoch 544/1000, Training Loss: 3.779540155090066e-05, Test Loss: 3.626067649431256e-05\n",
            "Epoch 545/1000, Training Loss: 3.766425830769763e-05, Test Loss: 3.612472084559211e-05\n",
            "Epoch 546/1000, Training Loss: 3.753444879801072e-05, Test Loss: 3.599013219958306e-05\n",
            "Epoch 547/1000, Training Loss: 3.740594943425439e-05, Test Loss: 3.585688697801033e-05\n",
            "Epoch 548/1000, Training Loss: 3.7278737130568205e-05, Test Loss: 3.572496211135494e-05\n",
            "Epoch 549/1000, Training Loss: 3.71527892907567e-05, Test Loss: 3.559433502624849e-05\n",
            "Epoch 550/1000, Training Loss: 3.7028083796535146e-05, Test Loss: 3.5464983633205206e-05\n",
            "Epoch 551/1000, Training Loss: 3.6904598996083316e-05, Test Loss: 3.533688631468263e-05\n",
            "Epoch 552/1000, Training Loss: 3.6782313692892864e-05, Test Loss: 3.521002191345982e-05\n",
            "Epoch 553/1000, Training Loss: 3.666120713490096e-05, Test Loss: 3.508436972132853e-05\n",
            "Epoch 554/1000, Training Loss: 3.654125900390389e-05, Test Loss: 3.4959909468082524e-05\n",
            "Epoch 555/1000, Training Loss: 3.6422449405241703e-05, Test Loss: 3.4836621310799816e-05\n",
            "Epoch 556/1000, Training Loss: 3.630475885774866e-05, Test Loss: 3.471448582341137e-05\n",
            "Epoch 557/1000, Training Loss: 3.61881682839588e-05, Test Loss: 3.459348398654435e-05\n",
            "Epoch 558/1000, Training Loss: 3.6072659000563405e-05, Test Loss: 3.4473597177634986e-05\n",
            "Epoch 559/1000, Training Loss: 3.5958212709111145e-05, Test Loss: 3.435480716130375e-05\n",
            "Epoch 560/1000, Training Loss: 3.5844811486945834e-05, Test Loss: 3.423709607998203e-05\n",
            "Epoch 561/1000, Training Loss: 3.573243777837366e-05, Test Loss: 3.41204464447863e-05\n",
            "Epoch 562/1000, Training Loss: 3.562107438605652e-05, Test Loss: 3.400484112663459e-05\n",
            "Epoch 563/1000, Training Loss: 3.551070446262223e-05, Test Loss: 3.389026334759426e-05\n",
            "Epoch 564/1000, Training Loss: 3.5401311502488255e-05, Test Loss: 3.3776696672455087e-05\n",
            "Epoch 565/1000, Training Loss: 3.5292879333891455e-05, Test Loss: 3.366412500052701e-05\n",
            "Epoch 566/1000, Training Loss: 3.5185392111121604e-05, Test Loss: 3.3552532557648954e-05\n",
            "Epoch 567/1000, Training Loss: 3.5078834306947955e-05, Test Loss: 3.344190388840811e-05\n",
            "Epoch 568/1000, Training Loss: 3.4973190705238465e-05, Test Loss: 3.333222384856103e-05\n",
            "Epoch 569/1000, Training Loss: 3.4868446393764684e-05, Test Loss: 3.3223477597651684e-05\n",
            "Epoch 570/1000, Training Loss: 3.476458675718739e-05, Test Loss: 3.311565059182364e-05\n",
            "Epoch 571/1000, Training Loss: 3.4661597470217864e-05, Test Loss: 3.3008728576815605e-05\n",
            "Epoch 572/1000, Training Loss: 3.455946449095211e-05, Test Loss: 3.290269758114116e-05\n",
            "Epoch 573/1000, Training Loss: 3.4458174054369754e-05, Test Loss: 3.279754390944305e-05\n",
            "Epoch 574/1000, Training Loss: 3.435771266599868e-05, Test Loss: 3.269325413602082e-05\n",
            "Epoch 575/1000, Training Loss: 3.425806709573496e-05, Test Loss: 3.258981509852455e-05\n",
            "Epoch 576/1000, Training Loss: 3.415922437181928e-05, Test Loss: 3.2487213891812454e-05\n",
            "Epoch 577/1000, Training Loss: 3.406117177496089e-05, Test Loss: 3.238543786196291e-05\n",
            "Epoch 578/1000, Training Loss: 3.396389683261206e-05, Test Loss: 3.228447460044609e-05\n",
            "Epoch 579/1000, Training Loss: 3.386738731338065e-05, Test Loss: 3.2184311938440555e-05\n",
            "Epoch 580/1000, Training Loss: 3.37716312215832e-05, Test Loss: 3.208493794129663e-05\n",
            "Epoch 581/1000, Training Loss: 3.3676616791933196e-05, Test Loss: 3.198634090314328e-05\n",
            "Epoch 582/1000, Training Loss: 3.358233248436179e-05, Test Loss: 3.1888509341629885e-05\n",
            "Epoch 583/1000, Training Loss: 3.3488766978965204e-05, Test Loss: 3.179143199280682e-05\n",
            "Epoch 584/1000, Training Loss: 3.3395909171077536e-05, Test Loss: 3.1695097806131155e-05\n",
            "Epoch 585/1000, Training Loss: 3.330374816646498e-05, Test Loss: 3.1599495939602225e-05\n",
            "Epoch 586/1000, Training Loss: 3.321227327663991e-05, Test Loss: 3.1504615755022446e-05\n",
            "Epoch 587/1000, Training Loss: 3.3121474014289446e-05, Test Loss: 3.141044681337598e-05\n",
            "Epoch 588/1000, Training Loss: 3.303134008881557e-05, Test Loss: 3.1316978870324705e-05\n",
            "Epoch 589/1000, Training Loss: 3.294186140198662e-05, Test Loss: 3.122420187182034e-05\n",
            "Epoch 590/1000, Training Loss: 3.285302804369521e-05, Test Loss: 3.113210594982631e-05\n",
            "Epoch 591/1000, Training Loss: 3.276483028781841e-05, Test Loss: 3.1040681418146275e-05\n",
            "Epoch 592/1000, Training Loss: 3.267725858818296e-05, Test Loss: 3.0949918768361715e-05\n",
            "Epoch 593/1000, Training Loss: 3.259030357462517e-05, Test Loss: 3.085980866586779e-05\n",
            "Epoch 594/1000, Training Loss: 3.250395604915053e-05, Test Loss: 3.077034194601151e-05\n",
            "Epoch 595/1000, Training Loss: 3.2418206982185406e-05, Test Loss: 3.068150961032695e-05\n",
            "Epoch 596/1000, Training Loss: 3.233304750891955e-05, Test Loss: 3.0593302822862594e-05\n",
            "Epoch 597/1000, Training Loss: 3.224846892574124e-05, Test Loss: 3.050571290660639e-05\n",
            "Epoch 598/1000, Training Loss: 3.216446268675429e-05, Test Loss: 3.0418731339993288e-05\n",
            "Epoch 599/1000, Training Loss: 3.208102040038324e-05, Test Loss: 3.0332349753503557e-05\n",
            "Epoch 600/1000, Training Loss: 3.19981338260611e-05, Test Loss: 3.0246559926348598e-05\n",
            "Epoch 601/1000, Training Loss: 3.191579487099525e-05, Test Loss: 3.0161353783233975e-05\n",
            "Epoch 602/1000, Training Loss: 3.183399558701411e-05, Test Loss: 3.007672339120941e-05\n",
            "Epoch 603/1000, Training Loss: 3.1752728167489254e-05, Test Loss: 2.999266095659091e-05\n",
            "Epoch 604/1000, Training Loss: 3.1671984944332144e-05, Test Loss: 2.9909158821965875e-05\n",
            "Epoch 605/1000, Training Loss: 3.159175838506276e-05, Test Loss: 2.9826209463265653e-05\n",
            "Epoch 606/1000, Training Loss: 3.1512041089951136e-05, Test Loss: 2.9743805486916936e-05\n",
            "Epoch 607/1000, Training Loss: 3.143282578922624e-05, Test Loss: 2.9661939627061972e-05\n",
            "Epoch 608/1000, Training Loss: 3.135410534035253e-05, Test Loss: 2.9580604742844412e-05\n",
            "Epoch 609/1000, Training Loss: 3.127587272537188e-05, Test Loss: 2.9499793815766125e-05\n",
            "Epoch 610/1000, Training Loss: 3.119812104831055e-05, Test Loss: 2.9419499947107473e-05\n",
            "Epoch 611/1000, Training Loss: 3.112084353264862e-05, Test Loss: 2.9339716355413354e-05\n",
            "Epoch 612/1000, Training Loss: 3.104403351884807e-05, Test Loss: 2.9260436374037864e-05\n",
            "Epoch 613/1000, Training Loss: 3.096768446194337e-05, Test Loss: 2.91816534487518e-05\n",
            "Epoch 614/1000, Training Loss: 3.0891789929187335e-05, Test Loss: 2.9103361135407927e-05\n",
            "Epoch 615/1000, Training Loss: 3.081634359775517e-05, Test Loss: 2.902555309766572e-05\n",
            "Epoch 616/1000, Training Loss: 3.074133925250259e-05, Test Loss: 2.8948223104767182e-05\n",
            "Epoch 617/1000, Training Loss: 3.0666770783777984e-05, Test Loss: 2.8871365029373693e-05\n",
            "Epoch 618/1000, Training Loss: 3.059263218528715e-05, Test Loss: 2.8794972845450572e-05\n",
            "Epoch 619/1000, Training Loss: 3.051891755200871e-05, Test Loss: 2.8719040626205068e-05\n",
            "Epoch 620/1000, Training Loss: 3.04456210781592e-05, Test Loss: 2.8643562542076018e-05\n",
            "Epoch 621/1000, Training Loss: 3.0372737055206947e-05, Test Loss: 2.8568532858768928e-05\n",
            "Epoch 622/1000, Training Loss: 3.0300259869934512e-05, Test Loss: 2.8493945935345512e-05\n",
            "Epoch 623/1000, Training Loss: 3.0228184002544727e-05, Test Loss: 2.8419796222351235e-05\n",
            "Epoch 624/1000, Training Loss: 3.0156504024814206e-05, Test Loss: 2.8346078259996088e-05\n",
            "Epoch 625/1000, Training Loss: 3.00852145982899e-05, Test Loss: 2.8272786676374755e-05\n",
            "Epoch 626/1000, Training Loss: 3.001431047252839e-05, Test Loss: 2.8199916185731338e-05\n",
            "Epoch 627/1000, Training Loss: 2.9943786483376957e-05, Test Loss: 2.8127461586766968e-05\n",
            "Epoch 628/1000, Training Loss: 2.987363755129629e-05, Test Loss: 2.805541776098659e-05\n",
            "Epoch 629/1000, Training Loss: 2.980385867972239e-05, Test Loss: 2.7983779671089463e-05\n",
            "Epoch 630/1000, Training Loss: 2.9734444953467094e-05, Test Loss: 2.7912542359392094e-05\n",
            "Epoch 631/1000, Training Loss: 2.9665391537157068e-05, Test Loss: 2.784170094629686e-05\n",
            "Epoch 632/1000, Training Loss: 2.9596693673709062e-05, Test Loss: 2.777125062878973e-05\n",
            "Epoch 633/1000, Training Loss: 2.9528346682842294e-05, Test Loss: 2.7701186678981787e-05\n",
            "Epoch 634/1000, Training Loss: 2.9460345959624805e-05, Test Loss: 2.763150444267931e-05\n",
            "Epoch 635/1000, Training Loss: 2.939268697305475e-05, Test Loss: 2.7562199337991932e-05\n",
            "Epoch 636/1000, Training Loss: 2.9325365264675844e-05, Test Loss: 2.749326685397463e-05\n",
            "Epoch 637/1000, Training Loss: 2.9258376447223833e-05, Test Loss: 2.742470254929761e-05\n",
            "Epoch 638/1000, Training Loss: 2.919171620330657e-05, Test Loss: 2.7356502050954443e-05\n",
            "Epoch 639/1000, Training Loss: 2.912538028411421e-05, Test Loss: 2.7288661052996634e-05\n",
            "Epoch 640/1000, Training Loss: 2.905936450815959e-05, Test Loss: 2.7221175315297465e-05\n",
            "Epoch 641/1000, Training Loss: 2.8993664760048936e-05, Test Loss: 2.7154040662350807e-05\n",
            "Epoch 642/1000, Training Loss: 2.8928276989281346e-05, Test Loss: 2.7087252982093258e-05\n",
            "Epoch 643/1000, Training Loss: 2.8863197209075507e-05, Test Loss: 2.7020808224755052e-05\n",
            "Epoch 644/1000, Training Loss: 2.8798421495225273e-05, Test Loss: 2.6954702401741364e-05\n",
            "Epoch 645/1000, Training Loss: 2.873394598498089e-05, Test Loss: 2.6888931584536655e-05\n",
            "Epoch 646/1000, Training Loss: 2.8669766875957565e-05, Test Loss: 2.682349190363637e-05\n",
            "Epoch 647/1000, Training Loss: 2.8605880425068494e-05, Test Loss: 2.6758379547503598e-05\n",
            "Epoch 648/1000, Training Loss: 2.8542282947483245e-05, Test Loss: 2.6693590761551664e-05\n",
            "Epoch 649/1000, Training Loss: 2.8478970815610326e-05, Test Loss: 2.662912184714722e-05\n",
            "Epoch 650/1000, Training Loss: 2.841594045810413e-05, Test Loss: 2.6564969160640723e-05\n",
            "Epoch 651/1000, Training Loss: 2.8353188358893845e-05, Test Loss: 2.6501129112417717e-05\n",
            "Epoch 652/1000, Training Loss: 2.829071105623598e-05, Test Loss: 2.643759816597204e-05\n",
            "Epoch 653/1000, Training Loss: 2.8228505141788324e-05, Test Loss: 2.637437283700028e-05\n",
            "Epoch 654/1000, Training Loss: 2.8166567259705423e-05, Test Loss: 2.6311449692520928e-05\n",
            "Epoch 655/1000, Training Loss: 2.8104894105755725e-05, Test Loss: 2.624882535000848e-05\n",
            "Epoch 656/1000, Training Loss: 2.8043482426457932e-05, Test Loss: 2.618649647655335e-05\n",
            "Epoch 657/1000, Training Loss: 2.7982329018238213e-05, Test Loss: 2.6124459788034793e-05\n",
            "Epoch 658/1000, Training Loss: 2.792143072660699e-05, Test Loss: 2.6062712048320912e-05\n",
            "Epoch 659/1000, Training Loss: 2.7860784445353687e-05, Test Loss: 2.600125006847945e-05\n",
            "Epoch 660/1000, Training Loss: 2.780038711576124e-05, Test Loss: 2.594007070601203e-05\n",
            "Epoch 661/1000, Training Loss: 2.7740235725838075e-05, Test Loss: 2.5879170864103218e-05\n",
            "Epoch 662/1000, Training Loss: 2.7680327309566677e-05, Test Loss: 2.5818547490888125e-05\n",
            "Epoch 663/1000, Training Loss: 2.762065894617201e-05, Test Loss: 2.575819757873635e-05\n",
            "Epoch 664/1000, Training Loss: 2.7561227759404028e-05, Test Loss: 2.5698118163553395e-05\n",
            "Epoch 665/1000, Training Loss: 2.750203091683748e-05, Test Loss: 2.563830632409423e-05\n",
            "Epoch 666/1000, Training Loss: 2.7443065629187663e-05, Test Loss: 2.5578759181297066e-05\n",
            "Epoch 667/1000, Training Loss: 2.7384329149642777e-05, Test Loss: 2.5519473897629703e-05\n",
            "Epoch 668/1000, Training Loss: 2.7325818773209002e-05, Test Loss: 2.54604476764518e-05\n",
            "Epoch 669/1000, Training Loss: 2.7267531836072694e-05, Test Loss: 2.5401677761389454e-05\n",
            "Epoch 670/1000, Training Loss: 2.7209465714975576e-05, Test Loss: 2.534316143572472e-05\n",
            "Epoch 671/1000, Training Loss: 2.715161782660483e-05, Test Loss: 2.528489602180315e-05\n",
            "Epoch 672/1000, Training Loss: 2.7093985626996113e-05, Test Loss: 2.522687888044514e-05\n",
            "Epoch 673/1000, Training Loss: 2.703656661095053e-05, Test Loss: 2.5169107410379096e-05\n",
            "Epoch 674/1000, Training Loss: 2.6979358311464314e-05, Test Loss: 2.5111579047683243e-05\n",
            "Epoch 675/1000, Training Loss: 2.692235829917081e-05, Test Loss: 2.505429126523965e-05\n",
            "Epoch 676/1000, Training Loss: 2.6865564181796138e-05, Test Loss: 2.4997241572201683e-05\n",
            "Epoch 677/1000, Training Loss: 2.6808973603625226e-05, Test Loss: 2.4940427513471074e-05\n",
            "Epoch 678/1000, Training Loss: 2.67525842449806e-05, Test Loss: 2.488384666918803e-05\n",
            "Epoch 679/1000, Training Loss: 2.6696393821712346e-05, Test Loss: 2.482749665423267e-05\n",
            "Epoch 680/1000, Training Loss: 2.6640400084698372e-05, Test Loss: 2.477137511773466e-05\n",
            "Epoch 681/1000, Training Loss: 2.658460081935779e-05, Test Loss: 2.4715479742596556e-05\n",
            "Epoch 682/1000, Training Loss: 2.6528993845173023e-05, Test Loss: 2.4659808245025353e-05\n",
            "Epoch 683/1000, Training Loss: 2.647357701522128e-05, Test Loss: 2.4604358374073896e-05\n",
            "Epoch 684/1000, Training Loss: 2.641834821571892e-05, Test Loss: 2.4549127911192714e-05\n",
            "Epoch 685/1000, Training Loss: 2.6363305365573124e-05, Test Loss: 2.4494114669792172e-05\n",
            "Epoch 686/1000, Training Loss: 2.630844641594432e-05, Test Loss: 2.4439316494810453e-05\n",
            "Epoch 687/1000, Training Loss: 2.62537693498164e-05, Test Loss: 2.438473126229412e-05\n",
            "Epoch 688/1000, Training Loss: 2.6199272181578807e-05, Test Loss: 2.4330356878984886e-05\n",
            "Epoch 689/1000, Training Loss: 2.614495295661381e-05, Test Loss: 2.4276191281916758e-05\n",
            "Epoch 690/1000, Training Loss: 2.609080975089489e-05, Test Loss: 2.4222232438020084e-05\n",
            "Epoch 691/1000, Training Loss: 2.6036840670592343e-05, Test Loss: 2.4168478343731507e-05\n",
            "Epoch 692/1000, Training Loss: 2.5983043851687383e-05, Test Loss: 2.41149270246185e-05\n",
            "Epoch 693/1000, Training Loss: 2.592941745959293e-05, Test Loss: 2.4061576535002348e-05\n",
            "Epoch 694/1000, Training Loss: 2.5875959688783796e-05, Test Loss: 2.4008424957595224e-05\n",
            "Epoch 695/1000, Training Loss: 2.5822668762433033e-05, Test Loss: 2.3955470403142034e-05\n",
            "Epoch 696/1000, Training Loss: 2.5769542932055742e-05, Test Loss: 2.39027110100695e-05\n",
            "Epoch 697/1000, Training Loss: 2.571658047716033e-05, Test Loss: 2.3850144944141264e-05\n",
            "Epoch 698/1000, Training Loss: 2.5663779704905565e-05, Test Loss: 2.3797770398119423e-05\n",
            "Epoch 699/1000, Training Loss: 2.5611138949765454e-05, Test Loss: 2.3745585591434397e-05\n",
            "Epoch 700/1000, Training Loss: 2.5558656573199694e-05, Test Loss: 2.3693588769859047e-05\n",
            "Epoch 701/1000, Training Loss: 2.550633096333025e-05, Test Loss: 2.3641778205188935e-05\n",
            "Epoch 702/1000, Training Loss: 2.5454160534624792e-05, Test Loss: 2.3590152194928223e-05\n",
            "Epoch 703/1000, Training Loss: 2.5402143727585072e-05, Test Loss: 2.3538709061984113e-05\n",
            "Epoch 704/1000, Training Loss: 2.535027900844185e-05, Test Loss: 2.3487447154361675e-05\n",
            "Epoch 705/1000, Training Loss: 2.529856486885423e-05, Test Loss: 2.3436364844867643e-05\n",
            "Epoch 706/1000, Training Loss: 2.5246999825616344e-05, Test Loss: 2.3385460530819877e-05\n",
            "Epoch 707/1000, Training Loss: 2.5195582420366094e-05, Test Loss: 2.3334732633757843e-05\n",
            "Epoch 708/1000, Training Loss: 2.514431121930232e-05, Test Loss: 2.3284179599162673e-05\n",
            "Epoch 709/1000, Training Loss: 2.5093184812903982e-05, Test Loss: 2.3233799896178837e-05\n",
            "Epoch 710/1000, Training Loss: 2.5042201815656426e-05, Test Loss: 2.3183592017340812e-05\n",
            "Epoch 711/1000, Training Loss: 2.4991360865780476e-05, Test Loss: 2.3133554478305288e-05\n",
            "Epoch 712/1000, Training Loss: 2.4940660624966547e-05, Test Loss: 2.308368581758714e-05\n",
            "Epoch 713/1000, Training Loss: 2.4890099778113296e-05, Test Loss: 2.3033984596297747e-05\n",
            "Epoch 714/1000, Training Loss: 2.4839677033070246e-05, Test Loss: 2.298444939789204e-05\n",
            "Epoch 715/1000, Training Loss: 2.47893911203843e-05, Test Loss: 2.2935078827913692e-05\n",
            "Epoch 716/1000, Training Loss: 2.473924079305003e-05, Test Loss: 2.2885871513746802e-05\n",
            "Epoch 717/1000, Training Loss: 2.4689224826264383e-05, Test Loss: 2.2836826104373743e-05\n",
            "Epoch 718/1000, Training Loss: 2.463934201718368e-05, Test Loss: 2.2787941270130375e-05\n",
            "Epoch 719/1000, Training Loss: 2.4589591184685842e-05, Test Loss: 2.2739215702471655e-05\n",
            "Epoch 720/1000, Training Loss: 2.453997116913398e-05, Test Loss: 2.2690648113732788e-05\n",
            "Epoch 721/1000, Training Loss: 2.4490480832145795e-05, Test Loss: 2.264223723690377e-05\n",
            "Epoch 722/1000, Training Loss: 2.4441119056362932e-05, Test Loss: 2.2593981825395662e-05\n",
            "Epoch 723/1000, Training Loss: 2.4391884745226545e-05, Test Loss: 2.2545880652817517e-05\n",
            "Epoch 724/1000, Training Loss: 2.434277682275317e-05, Test Loss: 2.2497932512754974e-05\n",
            "Epoch 725/1000, Training Loss: 2.4293794233315186e-05, Test Loss: 2.2450136218548204e-05\n",
            "Epoch 726/1000, Training Loss: 2.4244935941423183e-05, Test Loss: 2.2402490603076532e-05\n",
            "Epoch 727/1000, Training Loss: 2.419620093151117e-05, Test Loss: 2.2354994518543605e-05\n",
            "Epoch 728/1000, Training Loss: 2.41475882077236e-05, Test Loss: 2.230764683626564e-05\n",
            "Epoch 729/1000, Training Loss: 2.4099096793705962e-05, Test Loss: 2.226044644646159e-05\n",
            "Epoch 730/1000, Training Loss: 2.4050725732397102e-05, Test Loss: 2.2213392258044906e-05\n",
            "Epoch 731/1000, Training Loss: 2.400247408582308e-05, Test Loss: 2.2166483198420557e-05\n",
            "Epoch 732/1000, Training Loss: 2.395434093489468e-05, Test Loss: 2.211971821328088e-05\n",
            "Epoch 733/1000, Training Loss: 2.3906325379205338e-05, Test Loss: 2.2073096266404445e-05\n",
            "Epoch 734/1000, Training Loss: 2.385842653683244e-05, Test Loss: 2.2026616339458463e-05\n",
            "Epoch 735/1000, Training Loss: 2.3810643544139118e-05, Test Loss: 2.1980277431800128e-05\n",
            "Epoch 736/1000, Training Loss: 2.3762975555579353e-05, Test Loss: 2.1934078560283133e-05\n",
            "Epoch 737/1000, Training Loss: 2.3715421743503086e-05, Test Loss: 2.188801875906352e-05\n",
            "Epoch 738/1000, Training Loss: 2.3667981297964807e-05, Test Loss: 2.1842097079407923e-05\n",
            "Epoch 739/1000, Training Loss: 2.362065342653176e-05, Test Loss: 2.1796312589503963e-05\n",
            "Epoch 740/1000, Training Loss: 2.3573437354095566e-05, Test Loss: 2.1750664374271096e-05\n",
            "Epoch 741/1000, Training Loss: 2.3526332322683774e-05, Test Loss: 2.170515153517523e-05\n",
            "Epoch 742/1000, Training Loss: 2.3479337591273394e-05, Test Loss: 2.1659773190040758e-05\n",
            "Epoch 743/1000, Training Loss: 2.34324524356065e-05, Test Loss: 2.161452847286931e-05\n",
            "Epoch 744/1000, Training Loss: 2.3385676148004645e-05, Test Loss: 2.1569416533654187e-05\n",
            "Epoch 745/1000, Training Loss: 2.33390080371876e-05, Test Loss: 2.1524436538200857e-05\n",
            "Epoch 746/1000, Training Loss: 2.329244742809061e-05, Test Loss: 2.1479587667945914e-05\n",
            "Epoch 747/1000, Training Loss: 2.3245993661684833e-05, Test Loss: 2.143486911977824e-05\n",
            "Epoch 748/1000, Training Loss: 2.319964609479604e-05, Test Loss: 2.1390280105858975e-05\n",
            "Epoch 749/1000, Training Loss: 2.3153404099927193e-05, Test Loss: 2.1345819853448778e-05\n",
            "Epoch 750/1000, Training Loss: 2.3107267065080656e-05, Test Loss: 2.1301487604727924e-05\n",
            "Epoch 751/1000, Training Loss: 2.306123439358097e-05, Test Loss: 2.1257282616623534e-05\n",
            "Epoch 752/1000, Training Loss: 2.301530550389807e-05, Test Loss: 2.1213204160634753e-05\n",
            "Epoch 753/1000, Training Loss: 2.2969479829472874e-05, Test Loss: 2.11692515226603e-05\n",
            "Epoch 754/1000, Training Loss: 2.2923756818542285e-05, Test Loss: 2.1125424002824785e-05\n",
            "Epoch 755/1000, Training Loss: 2.2878135933965165e-05, Test Loss: 2.108172091531037e-05\n",
            "Epoch 756/1000, Training Loss: 2.283261665304866e-05, Test Loss: 2.103814158818293e-05\n",
            "Epoch 757/1000, Training Loss: 2.2787198467376208e-05, Test Loss: 2.09946853632241e-05\n",
            "Epoch 758/1000, Training Loss: 2.2741880882634493e-05, Test Loss: 2.0951351595761403e-05\n",
            "Epoch 759/1000, Training Loss: 2.2696663418442537e-05, Test Loss: 2.0908139654500645e-05\n",
            "Epoch 760/1000, Training Loss: 2.2651545608179863e-05, Test Loss: 2.0865048921357624e-05\n",
            "Epoch 761/1000, Training Loss: 2.2606526998816537e-05, Test Loss: 2.0822078791290947e-05\n",
            "Epoch 762/1000, Training Loss: 2.2561607150742178e-05, Test Loss: 2.0779228672136293e-05\n",
            "Epoch 763/1000, Training Loss: 2.2516785637597014e-05, Test Loss: 2.073649798444034e-05\n",
            "Epoch 764/1000, Training Loss: 2.2472062046101507e-05, Test Loss: 2.0693886161295243e-05\n",
            "Epoch 765/1000, Training Loss: 2.2427435975888014e-05, Test Loss: 2.0651392648174143e-05\n",
            "Epoch 766/1000, Training Loss: 2.238290703933126e-05, Test Loss: 2.0609016902766322e-05\n",
            "Epoch 767/1000, Training Loss: 2.2338474861380918e-05, Test Loss: 2.0566758394815358e-05\n",
            "Epoch 768/1000, Training Loss: 2.229413907939317e-05, Test Loss: 2.0524616605954015e-05\n",
            "Epoch 769/1000, Training Loss: 2.2249899342962413e-05, Test Loss: 2.0482591029543027e-05\n",
            "Epoch 770/1000, Training Loss: 2.220575531375425e-05, Test Loss: 2.0440681170508187e-05\n",
            "Epoch 771/1000, Training Loss: 2.2161706665337934e-05, Test Loss: 2.039888654517822e-05\n",
            "Epoch 772/1000, Training Loss: 2.2117753083019256e-05, Test Loss: 2.0357206681124806e-05\n",
            "Epoch 773/1000, Training Loss: 2.2073894263674247e-05, Test Loss: 2.0315641117001103e-05\n",
            "Epoch 774/1000, Training Loss: 2.2030129915582508e-05, Test Loss: 2.027418940238122e-05\n",
            "Epoch 775/1000, Training Loss: 2.19864597582598e-05, Test Loss: 2.02328510976e-05\n",
            "Epoch 776/1000, Training Loss: 2.194288352229332e-05, Test Loss: 2.019162577359455e-05\n",
            "Epoch 777/1000, Training Loss: 2.1899400949175257e-05, Test Loss: 2.0150513011744026e-05\n",
            "Epoch 778/1000, Training Loss: 2.1856011791136303e-05, Test Loss: 2.010951240371074e-05\n",
            "Epoch 779/1000, Training Loss: 2.1812715810981595e-05, Test Loss: 2.006862355128367e-05\n",
            "Epoch 780/1000, Training Loss: 2.1769512781923878e-05, Test Loss: 2.0027846066218292e-05\n",
            "Epoch 781/1000, Training Loss: 2.172640248741876e-05, Test Loss: 1.998717957008045e-05\n",
            "Epoch 782/1000, Training Loss: 2.1683384721000577e-05, Test Loss: 1.994662369408888e-05\n",
            "Epoch 783/1000, Training Loss: 2.164045928611657e-05, Test Loss: 1.9906178078958784e-05\n",
            "Epoch 784/1000, Training Loss: 2.1597625995962725e-05, Test Loss: 1.9865842374743948e-05\n",
            "Epoch 785/1000, Training Loss: 2.1554884673319238e-05, Test Loss: 1.9825616240683903e-05\n",
            "Epoch 786/1000, Training Loss: 2.1512235150386556e-05, Test Loss: 1.9785499345045633e-05\n",
            "Epoch 787/1000, Training Loss: 2.1469677268621154e-05, Test Loss: 1.9745491364969585e-05\n",
            "Epoch 788/1000, Training Loss: 2.1427210878571793e-05, Test Loss: 1.970559198631501e-05\n",
            "Epoch 789/1000, Training Loss: 2.1384835839716427e-05, Test Loss: 1.966580090350657e-05\n",
            "Epoch 790/1000, Training Loss: 2.13425520202983e-05, Test Loss: 1.9626117819378035e-05\n",
            "Epoch 791/1000, Training Loss: 2.1300359297163262e-05, Test Loss: 1.958654244502269e-05\n",
            "Epoch 792/1000, Training Loss: 2.1258257555596955e-05, Test Loss: 1.9547074499636973e-05\n",
            "Epoch 793/1000, Training Loss: 2.121624668916226e-05, Test Loss: 1.950771371037027e-05\n",
            "Epoch 794/1000, Training Loss: 2.1174326599536753e-05, Test Loss: 1.946845981217156e-05\n",
            "Epoch 795/1000, Training Loss: 2.1132497196351232e-05, Test Loss: 1.9429312547639428e-05\n",
            "Epoch 796/1000, Training Loss: 2.109075839702773e-05, Test Loss: 1.939027166687011e-05\n",
            "Epoch 797/1000, Training Loss: 2.1049110126618695e-05, Test Loss: 1.935133692730668e-05\n",
            "Epoch 798/1000, Training Loss: 2.10075523176452e-05, Test Loss: 1.9312508093590775e-05\n",
            "Epoch 799/1000, Training Loss: 2.0966084909937126e-05, Test Loss: 1.9273784937411093e-05\n",
            "Epoch 800/1000, Training Loss: 2.0924707850472256e-05, Test Loss: 1.923516723735648e-05\n",
            "Epoch 801/1000, Training Loss: 2.088342109321709e-05, Test Loss: 1.9196654778766814e-05\n",
            "Epoch 802/1000, Training Loss: 2.0842224598966647e-05, Test Loss: 1.9158247353584645e-05\n",
            "Epoch 803/1000, Training Loss: 2.0801118335186013e-05, Test Loss: 1.9119944760208926e-05\n",
            "Epoch 804/1000, Training Loss: 2.076010227585153e-05, Test Loss: 1.9081746803349103e-05\n",
            "Epoch 805/1000, Training Loss: 2.0719176401292807e-05, Test Loss: 1.9043653293877956e-05\n",
            "Epoch 806/1000, Training Loss: 2.067834069803487e-05, Test Loss: 1.9005664048687042e-05\n",
            "Epoch 807/1000, Training Loss: 2.063759515864149e-05, Test Loss: 1.896777889054211e-05\n",
            "Epoch 808/1000, Training Loss: 2.059693978155812e-05, Test Loss: 1.8929997647939495e-05\n",
            "Epoch 809/1000, Training Loss: 2.0556374570956132e-05, Test Loss: 1.8892320154962333e-05\n",
            "Epoch 810/1000, Training Loss: 2.051589953657746e-05, Test Loss: 1.885474625113863e-05\n",
            "Epoch 811/1000, Training Loss: 2.047551469357949e-05, Test Loss: 1.881727578129974e-05\n",
            "Epoch 812/1000, Training Loss: 2.043522006238096e-05, Test Loss: 1.877990859543866e-05\n",
            "Epoch 813/1000, Training Loss: 2.0395015668508873e-05, Test Loss: 1.874264454857004e-05\n",
            "Epoch 814/1000, Training Loss: 2.035490154244422e-05, Test Loss: 1.870548350059156e-05\n",
            "Epoch 815/1000, Training Loss: 2.0314877719471406e-05, Test Loss: 1.866842531614408e-05\n",
            "Epoch 816/1000, Training Loss: 2.0274944239525625e-05, Test Loss: 1.8631469864474807e-05\n",
            "Epoch 817/1000, Training Loss: 2.0235101147042905e-05, Test Loss: 1.859461701930104e-05\n",
            "Epoch 818/1000, Training Loss: 2.0195348490809397e-05, Test Loss: 1.8557866658671628e-05\n",
            "Epoch 819/1000, Training Loss: 2.015568632381251e-05, Test Loss: 1.852121866483504e-05\n",
            "Epoch 820/1000, Training Loss: 2.011611470309261e-05, Test Loss: 1.8484672924102103e-05\n",
            "Epoch 821/1000, Training Loss: 2.007663368959524e-05, Test Loss: 1.8448229326715644e-05\n",
            "Epoch 822/1000, Training Loss: 2.003724334802481e-05, Test Loss: 1.841188776671456e-05\n",
            "Epoch 823/1000, Training Loss: 1.9997943746698603e-05, Test Loss: 1.837564814180682e-05\n",
            "Epoch 824/1000, Training Loss: 1.995873495740175e-05, Test Loss: 1.8339510353234674e-05\n",
            "Epoch 825/1000, Training Loss: 1.9919617055243555e-05, Test Loss: 1.8303474305648563e-05\n",
            "Epoch 826/1000, Training Loss: 1.9880590118514204e-05, Test Loss: 1.8267539906976924e-05\n",
            "Epoch 827/1000, Training Loss: 1.984165422854384e-05, Test Loss: 1.8231707068299905e-05\n",
            "Epoch 828/1000, Training Loss: 1.9802809469560135e-05, Test Loss: 1.8195975703722377e-05\n",
            "Epoch 829/1000, Training Loss: 1.9764055928549046e-05, Test Loss: 1.8160345730248137e-05\n",
            "Epoch 830/1000, Training Loss: 1.9725393695116402e-05, Test Loss: 1.8124817067658047e-05\n",
            "Epoch 831/1000, Training Loss: 1.968682286134956e-05, Test Loss: 1.808938963838532e-05\n",
            "Epoch 832/1000, Training Loss: 1.9648343521680935e-05, Test Loss: 1.8054063367393987e-05\n",
            "Epoch 833/1000, Training Loss: 1.9609955772752665e-05, Test Loss: 1.8018838182058444e-05\n",
            "Epoch 834/1000, Training Loss: 1.957165971328205e-05, Test Loss: 1.7983714012044206e-05\n",
            "Epoch 835/1000, Training Loss: 1.9533455443928428e-05, Test Loss: 1.7948690789189375e-05\n",
            "Epoch 836/1000, Training Loss: 1.9495343067161382e-05, Test Loss: 1.7913768447386913e-05\n",
            "Epoch 837/1000, Training Loss: 1.9457322687129875e-05, Test Loss: 1.78789469224712e-05\n",
            "Epoch 838/1000, Training Loss: 1.941939440953298e-05, Test Loss: 1.7844226152100218e-05\n",
            "Epoch 839/1000, Training Loss: 1.9381558341491175e-05, Test Loss: 1.780960607564446e-05\n",
            "Epoch 840/1000, Training Loss: 1.934381459142035e-05, Test Loss: 1.7775086634074506e-05\n",
            "Epoch 841/1000, Training Loss: 1.930616326890551e-05, Test Loss: 1.774066776985044e-05\n",
            "Epoch 842/1000, Training Loss: 1.9268604484576166e-05, Test Loss: 1.7706349426810792e-05\n",
            "Epoch 843/1000, Training Loss: 1.9231138349985462e-05, Test Loss: 1.7672131550067266e-05\n",
            "Epoch 844/1000, Training Loss: 1.919376497748588e-05, Test Loss: 1.76380140858954e-05\n",
            "Epoch 845/1000, Training Loss: 1.915648448011137e-05, Test Loss: 1.7603996981631424e-05\n",
            "Epoch 846/1000, Training Loss: 1.9119296971457727e-05, Test Loss: 1.757008018556642e-05\n",
            "Epoch 847/1000, Training Loss: 1.9082202565565634e-05, Test Loss: 1.7536263646844376e-05\n",
            "Epoch 848/1000, Training Loss: 1.9045201376804654e-05, Test Loss: 1.750254731536208e-05\n",
            "Epoch 849/1000, Training Loss: 1.9008293519759152e-05, Test Loss: 1.7468931141667463e-05\n",
            "Epoch 850/1000, Training Loss: 1.8971479109115406e-05, Test Loss: 1.743541507686197e-05\n",
            "Epoch 851/1000, Training Loss: 1.8934758259550667e-05, Test Loss: 1.7401999072504968e-05\n",
            "Epoch 852/1000, Training Loss: 1.889813108562269e-05, Test Loss: 1.7368683080515613e-05\n",
            "Epoch 853/1000, Training Loss: 1.8861597701662325e-05, Test Loss: 1.733546705308122e-05\n",
            "Epoch 854/1000, Training Loss: 1.8825158221666248e-05, Test Loss: 1.730235094256349e-05\n",
            "Epoch 855/1000, Training Loss: 1.8788812759192227e-05, Test Loss: 1.7269334701408018e-05\n",
            "Epoch 856/1000, Training Loss: 1.8752561427255633e-05, Test Loss: 1.7236418282054242e-05\n",
            "Epoch 857/1000, Training Loss: 1.8716404338227398e-05, Test Loss: 1.7203601636848008e-05\n",
            "Epoch 858/1000, Training Loss: 1.8680341603733694e-05, Test Loss: 1.7170884717953208e-05\n",
            "Epoch 859/1000, Training Loss: 1.8644373334557574e-05, Test Loss: 1.713826747727024e-05\n",
            "Epoch 860/1000, Training Loss: 1.8608499640541677e-05, Test Loss: 1.7105749866348773e-05\n",
            "Epoch 861/1000, Training Loss: 1.857272063049323e-05, Test Loss: 1.70733318363081e-05\n",
            "Epoch 862/1000, Training Loss: 1.8537036412089993e-05, Test Loss: 1.7041013337755825e-05\n",
            "Epoch 863/1000, Training Loss: 1.850144709178837e-05, Test Loss: 1.7008794320708957e-05\n",
            "Epoch 864/1000, Training Loss: 1.846595277473276e-05, Test Loss: 1.6976674734516426e-05\n",
            "Epoch 865/1000, Training Loss: 1.8430553564667695e-05, Test Loss: 1.6944654527784242e-05\n",
            "Epoch 866/1000, Training Loss: 1.839524956385003e-05, Test Loss: 1.6912733648299894e-05\n",
            "Epoch 867/1000, Training Loss: 1.8360040872964068e-05, Test Loss: 1.688091204296012e-05\n",
            "Epoch 868/1000, Training Loss: 1.832492759103843e-05, Test Loss: 1.6849189657700722e-05\n",
            "Epoch 869/1000, Training Loss: 1.828990981536334e-05, Test Loss: 1.6817566437425436e-05\n",
            "Epoch 870/1000, Training Loss: 1.8254987641411025e-05, Test Loss: 1.678604232593922e-05\n",
            "Epoch 871/1000, Training Loss: 1.82201611627574e-05, Test Loss: 1.6754617265881223e-05\n",
            "Epoch 872/1000, Training Loss: 1.818543047100543e-05, Test Loss: 1.6723291198659985e-05\n",
            "Epoch 873/1000, Training Loss: 1.8150795655709565e-05, Test Loss: 1.669206406439023e-05\n",
            "Epoch 874/1000, Training Loss: 1.811625680430387e-05, Test Loss: 1.6660935801830865e-05\n",
            "Epoch 875/1000, Training Loss: 1.8081814002029337e-05, Test Loss: 1.6629906348325912e-05\n",
            "Epoch 876/1000, Training Loss: 1.8047467331864787e-05, Test Loss: 1.6598975639744205e-05\n",
            "Epoch 877/1000, Training Loss: 1.8013216874458986e-05, Test Loss: 1.656814361042442e-05\n",
            "Epoch 878/1000, Training Loss: 1.797906270806396e-05, Test Loss: 1.6537410193116884e-05\n",
            "Epoch 879/1000, Training Loss: 1.794500490847145e-05, Test Loss: 1.6506775318933855e-05\n",
            "Epoch 880/1000, Training Loss: 1.7911043548949167e-05, Test Loss: 1.647623891729233e-05\n",
            "Epoch 881/1000, Training Loss: 1.787717870018031e-05, Test Loss: 1.6445800915867213e-05\n",
            "Epoch 882/1000, Training Loss: 1.784341043020409e-05, Test Loss: 1.641546124053926e-05\n",
            "Epoch 883/1000, Training Loss: 1.780973880435919e-05, Test Loss: 1.638521981534994e-05\n",
            "Epoch 884/1000, Training Loss: 1.77761638852265e-05, Test Loss: 1.635507656245422e-05\n",
            "Epoch 885/1000, Training Loss: 1.7742685732575924e-05, Test Loss: 1.6325031402073674e-05\n",
            "Epoch 886/1000, Training Loss: 1.7709304403314705e-05, Test Loss: 1.6295084252459287e-05\n",
            "Epoch 887/1000, Training Loss: 1.7676019951435267e-05, Test Loss: 1.6265235029843045e-05\n",
            "Epoch 888/1000, Training Loss: 1.7642832427967422e-05, Test Loss: 1.6235483648402302e-05\n",
            "Epoch 889/1000, Training Loss: 1.760974188093097e-05, Test Loss: 1.620583002022024e-05\n",
            "Epoch 890/1000, Training Loss: 1.7576748355290008e-05, Test Loss: 1.6176274055248356e-05\n",
            "Epoch 891/1000, Training Loss: 1.754385189291e-05, Test Loss: 1.6146815661270975e-05\n",
            "Epoch 892/1000, Training Loss: 1.7511052532514043e-05, Test Loss: 1.6117454743871893e-05\n",
            "Epoch 893/1000, Training Loss: 1.7478350309644432e-05, Test Loss: 1.6088191206401426e-05\n",
            "Epoch 894/1000, Training Loss: 1.7445745256622463e-05, Test Loss: 1.605902494994489e-05\n",
            "Epoch 895/1000, Training Loss: 1.741323740251162e-05, Test Loss: 1.602995587329334e-05\n",
            "Epoch 896/1000, Training Loss: 1.7380826773082647e-05, Test Loss: 1.6000983872915335e-05\n",
            "Epoch 897/1000, Training Loss: 1.734851339077941e-05, Test Loss: 1.5972108842929985e-05\n",
            "Epoch 898/1000, Training Loss: 1.7316297274686392e-05, Test Loss: 1.5943330675081274e-05\n",
            "Epoch 899/1000, Training Loss: 1.728417844049886e-05, Test Loss: 1.5914649258714957e-05\n",
            "Epoch 900/1000, Training Loss: 1.7252156900493097e-05, Test Loss: 1.5886064480754808e-05\n",
            "Epoch 901/1000, Training Loss: 1.7220232663499965e-05, Test Loss: 1.5857576225683123e-05\n",
            "Epoch 902/1000, Training Loss: 1.718840573487803e-05, Test Loss: 1.5829184375517936e-05\n",
            "Epoch 903/1000, Training Loss: 1.7156676116490177e-05, Test Loss: 1.580088880979736e-05\n",
            "Epoch 904/1000, Training Loss: 1.712504380668067e-05, Test Loss: 1.577268940556182e-05\n",
            "Epoch 905/1000, Training Loss: 1.7093508800254082e-05, Test Loss: 1.5744586037337012e-05\n",
            "Epoch 906/1000, Training Loss: 1.706207108845499e-05, Test Loss: 1.5716578577119696e-05\n",
            "Epoch 907/1000, Training Loss: 1.703073065895126e-05, Test Loss: 1.5688666894366197e-05\n",
            "Epoch 908/1000, Training Loss: 1.6999487495816028e-05, Test Loss: 1.5660850855977775e-05\n",
            "Epoch 909/1000, Training Loss: 1.6968341579513163e-05, Test Loss: 1.5633130326291714e-05\n",
            "Epoch 910/1000, Training Loss: 1.6937292886883946e-05, Test Loss: 1.5605505167071925e-05\n",
            "Epoch 911/1000, Training Loss: 1.690634139113413e-05, Test Loss: 1.5577975237499257e-05\n",
            "Epoch 912/1000, Training Loss: 1.687548706182341e-05, Test Loss: 1.5550540394166107e-05\n",
            "Epoch 913/1000, Training Loss: 1.6844729864855904e-05, Test Loss: 1.5523200491069236e-05\n",
            "Epoch 914/1000, Training Loss: 1.6814069762472626e-05, Test Loss: 1.5495955379607423e-05\n",
            "Epoch 915/1000, Training Loss: 1.6783506713244025e-05, Test Loss: 1.5468804908573907e-05\n",
            "Epoch 916/1000, Training Loss: 1.6753040672065278e-05, Test Loss: 1.5441748924158776e-05\n",
            "Epoch 917/1000, Training Loss: 1.672267159015222e-05, Test Loss: 1.5414787269944526e-05\n",
            "Epoch 918/1000, Training Loss: 1.6692399415038426e-05, Test Loss: 1.5387919786907094e-05\n",
            "Epoch 919/1000, Training Loss: 1.666222409057425e-05, Test Loss: 1.5361146313417207e-05\n",
            "Epoch 920/1000, Training Loss: 1.6632145556926463e-05, Test Loss: 1.533446668524123e-05\n",
            "Epoch 921/1000, Training Loss: 1.6602163750579927e-05, Test Loss: 1.5307880735546466e-05\n",
            "Epoch 922/1000, Training Loss: 1.6572278604339357e-05, Test Loss: 1.528138829490376e-05\n",
            "Epoch 923/1000, Training Loss: 1.6542490047333593e-05, Test Loss: 1.5254989191292097e-05\n",
            "Epoch 924/1000, Training Loss: 1.6512798005020056e-05, Test Loss: 1.5228683250107483e-05\n",
            "Epoch 925/1000, Training Loss: 1.6483202399190782e-05, Test Loss: 1.5202470294166442e-05\n",
            "Epoch 926/1000, Training Loss: 1.645370314798054e-05, Test Loss: 1.5176350143718507e-05\n",
            "Epoch 927/1000, Training Loss: 1.64243001658738e-05, Test Loss: 1.5150322616451576e-05\n",
            "Epoch 928/1000, Training Loss: 1.6394993363714808e-05, Test Loss: 1.5124387527503795e-05\n",
            "Epoch 929/1000, Training Loss: 1.636578264871866e-05, Test Loss: 1.5098544689474461e-05\n",
            "Epoch 930/1000, Training Loss: 1.6336667924482267e-05, Test Loss: 1.5072793912436198e-05\n",
            "Epoch 931/1000, Training Loss: 1.6307649090997234e-05, Test Loss: 1.504713500394539e-05\n",
            "Epoch 932/1000, Training Loss: 1.6278726044663758e-05, Test Loss: 1.502156776906036e-05\n",
            "Epoch 933/1000, Training Loss: 1.6249898678305295e-05, Test Loss: 1.499609201035013e-05\n",
            "Epoch 934/1000, Training Loss: 1.6221166881184638e-05, Test Loss: 1.4970707527914697e-05\n",
            "Epoch 935/1000, Training Loss: 1.619253053902019e-05, Test Loss: 1.494541411939824e-05\n",
            "Epoch 936/1000, Training Loss: 1.6163989534003603e-05, Test Loss: 1.492021158000597e-05\n",
            "Epoch 937/1000, Training Loss: 1.6135543744819075e-05, Test Loss: 1.4895099702524191e-05\n",
            "Epoch 938/1000, Training Loss: 1.6107193046662098e-05, Test Loss: 1.4870078277335297e-05\n",
            "Epoch 939/1000, Training Loss: 1.60789373112602e-05, Test Loss: 1.4845147092438027e-05\n",
            "Epoch 940/1000, Training Loss: 1.6050776406894638e-05, Test Loss: 1.4820305933468944e-05\n",
            "Epoch 941/1000, Training Loss: 1.602271019842192e-05, Test Loss: 1.4795554583720786e-05\n",
            "Epoch 942/1000, Training Loss: 1.5994738547296958e-05, Test Loss: 1.4770892824164887e-05\n",
            "Epoch 943/1000, Training Loss: 1.596686131159727e-05, Test Loss: 1.4746320433470935e-05\n",
            "Epoch 944/1000, Training Loss: 1.5939078346046675e-05, Test Loss: 1.4721837188032066e-05\n",
            "Epoch 945/1000, Training Loss: 1.5911389502041403e-05, Test Loss: 1.4697442861985262e-05\n",
            "Epoch 946/1000, Training Loss: 1.5883794627675692e-05, Test Loss: 1.467313722723729e-05\n",
            "Epoch 947/1000, Training Loss: 1.5856293567768675e-05, Test Loss: 1.464892005348661e-05\n",
            "Epoch 948/1000, Training Loss: 1.582888616389224e-05, Test Loss: 1.4624791108250246e-05\n",
            "Epoch 949/1000, Training Loss: 1.5801572254398324e-05, Test Loss: 1.4600750156886423e-05\n",
            "Epoch 950/1000, Training Loss: 1.5774351674448638e-05, Test Loss: 1.4576796962622399e-05\n",
            "Epoch 951/1000, Training Loss: 1.5747224256043504e-05, Test Loss: 1.4552931286580502e-05\n",
            "Epoch 952/1000, Training Loss: 1.572018982805218e-05, Test Loss: 1.4529152887803967e-05\n",
            "Epoch 953/1000, Training Loss: 1.569324821624357e-05, Test Loss: 1.450546152328422e-05\n",
            "Epoch 954/1000, Training Loss: 1.566639924331676e-05, Test Loss: 1.4481856947988133e-05\n",
            "Epoch 955/1000, Training Loss: 1.5639642728934318e-05, Test Loss: 1.4458338914888417e-05\n",
            "Epoch 956/1000, Training Loss: 1.5612978489752987e-05, Test Loss: 1.4434907174988986e-05\n",
            "Epoch 957/1000, Training Loss: 1.5586406339457585e-05, Test Loss: 1.4411561477355456e-05\n",
            "Epoch 958/1000, Training Loss: 1.555992608879387e-05, Test Loss: 1.4388301569144145e-05\n",
            "Epoch 959/1000, Training Loss: 1.553353754560214e-05, Test Loss: 1.4365127195630736e-05\n",
            "Epoch 960/1000, Training Loss: 1.550724051485229e-05, Test Loss: 1.4342038100241916e-05\n",
            "Epoch 961/1000, Training Loss: 1.5481034798677747e-05, Test Loss: 1.4319034024583958e-05\n",
            "Epoch 962/1000, Training Loss: 1.545492019641058e-05, Test Loss: 1.4296114708473738e-05\n",
            "Epoch 963/1000, Training Loss: 1.542889650461778e-05, Test Loss: 1.4273279889969807e-05\n",
            "Epoch 964/1000, Training Loss: 1.540296351713666e-05, Test Loss: 1.4250529305404419e-05\n",
            "Epoch 965/1000, Training Loss: 1.537712102511062e-05, Test Loss: 1.42278626894115e-05\n",
            "Epoch 966/1000, Training Loss: 1.5351368817026716e-05, Test Loss: 1.4205279774962297e-05\n",
            "Epoch 967/1000, Training Loss: 1.532570667875179e-05, Test Loss: 1.4182780293394879e-05\n",
            "Epoch 968/1000, Training Loss: 1.5300134393570063e-05, Test Loss: 1.4160363974446313e-05\n",
            "Epoch 969/1000, Training Loss: 1.5274651742220342e-05, Test Loss: 1.4138030546285288e-05\n",
            "Epoch 970/1000, Training Loss: 1.5249258502934576e-05, Test Loss: 1.4115779735544703e-05\n",
            "Epoch 971/1000, Training Loss: 1.5223954451474864e-05, Test Loss: 1.4093611267354117e-05\n",
            "Epoch 972/1000, Training Loss: 1.519873936117234e-05, Test Loss: 1.4071524865370995e-05\n",
            "Epoch 973/1000, Training Loss: 1.5173613002965512e-05, Test Loss: 1.4049520251816115e-05\n",
            "Epoch 974/1000, Training Loss: 1.514857514543912e-05, Test Loss: 1.4027597147504623e-05\n",
            "Epoch 975/1000, Training Loss: 1.512362555486257e-05, Test Loss: 1.4005755271879693e-05\n",
            "Epoch 976/1000, Training Loss: 1.5098763995229427e-05, Test Loss: 1.398399434304514e-05\n",
            "Epoch 977/1000, Training Loss: 1.5073990228296661e-05, Test Loss: 1.396231407780037e-05\n",
            "Epoch 978/1000, Training Loss: 1.5049304013623325e-05, Test Loss: 1.3940714191671222e-05\n",
            "Epoch 979/1000, Training Loss: 1.5024705108610773e-05, Test Loss: 1.3919194398944882e-05\n",
            "Epoch 980/1000, Training Loss: 1.5000193268541782e-05, Test Loss: 1.3897754412703007e-05\n",
            "Epoch 981/1000, Training Loss: 1.4975768246620563e-05, Test Loss: 1.3876393944855323e-05\n",
            "Epoch 982/1000, Training Loss: 1.4951429794012086e-05, Test Loss: 1.3855112706172403e-05\n",
            "Epoch 983/1000, Training Loss: 1.4927177659881899e-05, Test Loss: 1.3833910406318944e-05\n",
            "Epoch 984/1000, Training Loss: 1.4903011591436654e-05, Test Loss: 1.3812786753889385e-05\n",
            "Epoch 985/1000, Training Loss: 1.4878931333962905e-05, Test Loss: 1.3791741456437835e-05\n",
            "Epoch 986/1000, Training Loss: 1.4854936630868083e-05, Test Loss: 1.3770774220514299e-05\n",
            "Epoch 987/1000, Training Loss: 1.4831027223719517e-05, Test Loss: 1.3749884751696834e-05\n",
            "Epoch 988/1000, Training Loss: 1.480720285228501e-05, Test Loss: 1.3729072754624606e-05\n",
            "Epoch 989/1000, Training Loss: 1.4783463254572109e-05, Test Loss: 1.3708337933031726e-05\n",
            "Epoch 990/1000, Training Loss: 1.4759808166868564e-05, Test Loss: 1.3687679989779975e-05\n",
            "Epoch 991/1000, Training Loss: 1.47362373237815e-05, Test Loss: 1.3667098626892296e-05\n",
            "Epoch 992/1000, Training Loss: 1.4712750458277873e-05, Test Loss: 1.3646593545584924e-05\n",
            "Epoch 993/1000, Training Loss: 1.468934730172337e-05, Test Loss: 1.3626164446300693e-05\n",
            "Epoch 994/1000, Training Loss: 1.466602758392293e-05, Test Loss: 1.3605811028742457e-05\n",
            "Epoch 995/1000, Training Loss: 1.464279103315925e-05, Test Loss: 1.3585532991904548e-05\n",
            "Epoch 996/1000, Training Loss: 1.4619637376232866e-05, Test Loss: 1.3565330034105815e-05\n",
            "Epoch 997/1000, Training Loss: 1.4596566338501241e-05, Test Loss: 1.354520185302173e-05\n",
            "Epoch 998/1000, Training Loss: 1.457357764391788e-05, Test Loss: 1.3525148145717133e-05\n",
            "Epoch 999/1000, Training Loss: 1.4550671015071639e-05, Test Loss: 1.3505168608677672e-05\n",
            "Epoch 1000/1000, Training Loss: 1.452784617322493e-05, Test Loss: 1.3485262937841058e-05\n",
            "Epoch 1/1000, Training Loss: 0.003165917704742038, Test Loss: 0.0031112969894320493\n",
            "Epoch 2/1000, Training Loss: 0.0031027036258436825, Test Loss: 0.0030207082073599135\n",
            "Epoch 3/1000, Training Loss: 0.0030926098285639077, Test Loss: 0.003002791982373164\n",
            "Epoch 4/1000, Training Loss: 0.003088965142107014, Test Loss: 0.0029964301700257582\n",
            "Epoch 5/1000, Training Loss: 0.0030868762266984583, Test Loss: 0.002993377423606766\n",
            "Epoch 6/1000, Training Loss: 0.0030853721417526347, Test Loss: 0.002991555858287626\n",
            "Epoch 7/1000, Training Loss: 0.0030841500696548695, Test Loss: 0.0029902541051328984\n",
            "Epoch 8/1000, Training Loss: 0.0030830808348037155, Test Loss: 0.0029891887425256853\n",
            "Epoch 9/1000, Training Loss: 0.003082099857784382, Test Loss: 0.002988238187130666\n",
            "Epoch 10/1000, Training Loss: 0.003081172498283129, Test Loss: 0.002987347943830446\n",
            "Epoch 11/1000, Training Loss: 0.003080279472911818, Test Loss: 0.0029864926302226365\n",
            "Epoch 12/1000, Training Loss: 0.0030794097396071346, Test Loss: 0.00298565987729528\n",
            "Epoch 13/1000, Training Loss: 0.0030785567895638384, Test Loss: 0.0029848432948014446\n",
            "Epoch 14/1000, Training Loss: 0.003077716652540216, Test Loss: 0.0029840393259706888\n",
            "Epoch 15/1000, Training Loss: 0.003076886802727032, Test Loss: 0.0029832458075517267\n",
            "Epoch 16/1000, Training Loss: 0.0030760655496248103, Test Loss: 0.0029824612924245057\n",
            "Epoch 17/1000, Training Loss: 0.003075251694535533, Test Loss: 0.0029816847207756055\n",
            "Epoch 18/1000, Training Loss: 0.0030744443344517857, Test Loss: 0.002980915254704552\n",
            "Epoch 19/1000, Training Loss: 0.003073642748661275, Test Loss: 0.002980152191693387\n",
            "Epoch 20/1000, Training Loss: 0.0030728463322369885, Test Loss: 0.002979394917341727\n",
            "Epoch 21/1000, Training Loss: 0.003072054556361838, Test Loss: 0.002978642878309034\n",
            "Epoch 22/1000, Training Loss: 0.003071266944168352, Test Loss: 0.002977895566019933\n",
            "Epoch 23/1000, Training Loss: 0.003070483055652925, Test Loss: 0.0029771525063116474\n",
            "Epoch 24/1000, Training Loss: 0.003069702477971933, Test Loss: 0.002976413252489062\n",
            "Epoch 25/1000, Training Loss: 0.0030689248189851607, Test Loss: 0.002975677380415604\n",
            "Epoch 26/1000, Training Loss: 0.003068149702800874, Test Loss: 0.002974944484875352\n",
            "Epoch 27/1000, Training Loss: 0.003067376766587078, Test Loss: 0.0029742141767668696\n",
            "Epoch 28/1000, Training Loss: 0.0030666056582083277, Test Loss: 0.002973486080867624\n",
            "Epoch 29/1000, Training Loss: 0.0030658360344191426, Test Loss: 0.00297275983400789\n",
            "Epoch 30/1000, Training Loss: 0.003065067559446012, Test Loss: 0.0029720350835506747\n",
            "Epoch 31/1000, Training Loss: 0.0030642999038501552, Test Loss: 0.002971311486108218\n",
            "Epoch 32/1000, Training Loss: 0.0030635327435995573, Test Loss: 0.002970588706446428\n",
            "Epoch 33/1000, Training Loss: 0.0030627657593012957, Test Loss: 0.0029698664165417436\n",
            "Epoch 34/1000, Training Loss: 0.003061998635559397, Test Loss: 0.00296914429476352\n",
            "Epoch 35/1000, Training Loss: 0.003061231060432746, Test Loss: 0.0029684220251609984\n",
            "Epoch 36/1000, Training Loss: 0.0030604627249738153, Test Loss: 0.00296769929683814\n",
            "Epoch 37/1000, Training Loss: 0.003059693322833329, Test Loss: 0.00296697580340272\n",
            "Epoch 38/1000, Training Loss: 0.0030589225499191145, Test Loss: 0.0029662512424785243\n",
            "Epoch 39/1000, Training Loss: 0.003058150104099724, Test Loss: 0.002965525315271338\n",
            "Epoch 40/1000, Training Loss: 0.0030573756849451615, Test Loss: 0.0029647977261809733\n",
            "Epoch 41/1000, Training Loss: 0.0030565989934984838, Test Loss: 0.002964068182452764\n",
            "Epoch 42/1000, Training Loss: 0.0030558197320730836, Test Loss: 0.002963336393863038\n",
            "Epoch 43/1000, Training Loss: 0.0030550376040713967, Test Loss: 0.0029626020724338557\n",
            "Epoch 44/1000, Training Loss: 0.0030542523138214874, Test Loss: 0.002961864932173049\n",
            "Epoch 45/1000, Training Loss: 0.003053463566428528, Test Loss: 0.002961124688836159\n",
            "Epoch 46/1000, Training Loss: 0.0030526710676387413, Test Loss: 0.0029603810597074\n",
            "Epoch 47/1000, Training Loss: 0.0030518745237137035, Test Loss: 0.0029596337633971417\n",
            "Epoch 48/1000, Training Loss: 0.00305107364131332, Test Loss: 0.002958882519653828\n",
            "Epoch 49/1000, Training Loss: 0.0030502681273860055, Test Loss: 0.0029581270491884815\n",
            "Epoch 50/1000, Training Loss: 0.0030494576890648913, Test Loss: 0.0029573670735102476\n",
            "Epoch 51/1000, Training Loss: 0.0030486420335690254, Test Loss: 0.0029566023147716246\n",
            "Epoch 52/1000, Training Loss: 0.003047820868108764, Test Loss: 0.0029558324956222306\n",
            "Epoch 53/1000, Training Loss: 0.003046993899794617, Test Loss: 0.0029550573390700963\n",
            "Epoch 54/1000, Training Loss: 0.003046160835549003, Test Loss: 0.0029542765683496335\n",
            "Epoch 55/1000, Training Loss: 0.0030453213820204163, Test Loss: 0.002953489906795525\n",
            "Epoch 56/1000, Training Loss: 0.003044475245499608, Test Loss: 0.002952697077721894\n",
            "Epoch 57/1000, Training Loss: 0.0030436221318374695, Test Loss: 0.0029518978043061965\n",
            "Epoch 58/1000, Training Loss: 0.003042761746364342, Test Loss: 0.002951091809477379\n",
            "Epoch 59/1000, Training Loss: 0.0030418937938105463, Test Loss: 0.002950278815807847\n",
            "Epoch 60/1000, Training Loss: 0.0030410179782279544, Test Loss: 0.0029494585454089162\n",
            "Epoch 61/1000, Training Loss: 0.003040134002912486, Test Loss: 0.002948630719829455\n",
            "Epoch 62/1000, Training Loss: 0.003039241570327399, Test Loss: 0.0029477950599574075\n",
            "Epoch 63/1000, Training Loss: 0.0030383403820273304, Test Loss: 0.0029469512859240264\n",
            "Epoch 64/1000, Training Loss: 0.003037430138583015, Test Loss: 0.0029460991170106006\n",
            "Epoch 65/1000, Training Loss: 0.003036510539506655, Test Loss: 0.002945238271557518\n",
            "Epoch 66/1000, Training Loss: 0.0030355812831779298, Test Loss: 0.0029443684668755514\n",
            "Epoch 67/1000, Training Loss: 0.0030346420667706365, Test Loss: 0.0029434894191592282\n",
            "Epoch 68/1000, Training Loss: 0.0030336925861799797, Test Loss: 0.002942600843402243\n",
            "Epoch 69/1000, Training Loss: 0.0030327325359505424, Test Loss: 0.002941702453314803\n",
            "Epoch 70/1000, Training Loss: 0.0030317616092049565, Test Loss: 0.002940793961242878\n",
            "Epoch 71/1000, Training Loss: 0.0030307794975733245, Test Loss: 0.0029398750780893317\n",
            "Epoch 72/1000, Training Loss: 0.0030297858911234534, Test Loss: 0.0029389455132368906\n",
            "Epoch 73/1000, Training Loss: 0.003028780478291949, Test Loss: 0.0029380049744729686\n",
            "Epoch 74/1000, Training Loss: 0.0030277629458162337, Test Loss: 0.002937053167916332\n",
            "Epoch 75/1000, Training Loss: 0.0030267329786675765, Test Loss: 0.0029360897979456456\n",
            "Epoch 76/1000, Training Loss: 0.003025690259985206, Test Loss: 0.002935114567129911\n",
            "Epoch 77/1000, Training Loss: 0.0030246344710115787, Test Loss: 0.0029341271761608332\n",
            "Epoch 78/1000, Training Loss: 0.0030235652910289164, Test Loss: 0.0029331273237871696\n",
            "Epoch 79/1000, Training Loss: 0.003022482397297087, Test Loss: 0.0029321147067511177\n",
            "Epoch 80/1000, Training Loss: 0.003021385464992946, Test Loss: 0.0029310890197267927\n",
            "Epoch 81/1000, Training Loss: 0.0030202741671512146, Test Loss: 0.002930049955260858\n",
            "Epoch 82/1000, Training Loss: 0.0030191481746070416, Test Loss: 0.002928997203715415\n",
            "Epoch 83/1000, Training Loss: 0.0030180071559403336, Test Loss: 0.002927930453213196\n",
            "Epoch 84/1000, Training Loss: 0.003016850777421974, Test Loss: 0.0029268493895851623\n",
            "Epoch 85/1000, Training Loss: 0.00301567870296207, Test Loss: 0.0029257536963206167\n",
            "Epoch 86/1000, Training Loss: 0.0030144905940603273, Test Loss: 0.0029246430545198984\n",
            "Epoch 87/1000, Training Loss: 0.003013286109758704, Test Loss: 0.002923517142849783\n",
            "Epoch 88/1000, Training Loss: 0.0030120649065964655, Test Loss: 0.002922375637501695\n",
            "Epoch 89/1000, Training Loss: 0.0030108266385677775, Test Loss: 0.0029212182121528345\n",
            "Epoch 90/1000, Training Loss: 0.0030095709570819827, Test Loss: 0.0029200445379303492\n",
            "Epoch 91/1000, Training Loss: 0.003008297510926707, Test Loss: 0.0029188542833786498\n",
            "Epoch 92/1000, Training Loss: 0.0030070059462339264, Test Loss: 0.002917647114430024\n",
            "Epoch 93/1000, Training Loss: 0.003005695906449176, Test Loss: 0.002916422694378647\n",
            "Epoch 94/1000, Training Loss: 0.003004367032304025, Test Loss: 0.002915180683858144\n",
            "Epoch 95/1000, Training Loss: 0.003003018961792, Test Loss: 0.002913920740822816\n",
            "Epoch 96/1000, Training Loss: 0.003001651330148107, Test Loss: 0.002912642520532691\n",
            "Epoch 97/1000, Training Loss: 0.0030002637698321156, Test Loss: 0.0029113456755425282\n",
            "Epoch 98/1000, Training Loss: 0.002998855910515782, Test Loss: 0.0029100298556949135\n",
            "Epoch 99/1000, Training Loss: 0.0029974273790741818, Test Loss: 0.002908694708117605\n",
            "Epoch 100/1000, Training Loss: 0.002995977799581315, Test Loss: 0.002907339877225277\n",
            "Epoch 101/1000, Training Loss: 0.002994506793310184, Test Loss: 0.002905965004725801\n",
            "Epoch 102/1000, Training Loss: 0.002993013978737501, Test Loss: 0.002904569729631222\n",
            "Epoch 103/1000, Training Loss: 0.002991498971553232, Test Loss: 0.0029031536882736077\n",
            "Epoch 104/1000, Training Loss: 0.002989961384675145, Test Loss: 0.0029017165143258926\n",
            "Epoch 105/1000, Training Loss: 0.002988400828268572, Test Loss: 0.002900257838827894\n",
            "Epoch 106/1000, Training Loss: 0.0029868169097715595, Test Loss: 0.0028987772902176728\n",
            "Epoch 107/1000, Training Loss: 0.0029852092339256175, Test Loss: 0.002897274494368375\n",
            "Epoch 108/1000, Training Loss: 0.002983577402812257, Test Loss: 0.0028957490746307412\n",
            "Epoch 109/1000, Training Loss: 0.0029819210158955256, Test Loss: 0.0028942006518814425\n",
            "Epoch 110/1000, Training Loss: 0.0029802396700707302, Test Loss: 0.002892628844577391\n",
            "Epoch 111/1000, Training Loss: 0.002978532959719576, Test Loss: 0.0028910332688162226\n",
            "Epoch 112/1000, Training Loss: 0.002976800476771901, Test Loss: 0.002889413538403091\n",
            "Epoch 113/1000, Training Loss: 0.0029750418107742415, Test Loss: 0.002887769264923962\n",
            "Epoch 114/1000, Training Loss: 0.002973256548965426, Test Loss: 0.002886100057825553\n",
            "Epoch 115/1000, Training Loss: 0.0029714442763594164, Test Loss: 0.0028844055245021015\n",
            "Epoch 116/1000, Training Loss: 0.0029696045758356025, Test Loss: 0.0028826852703891202\n",
            "Epoch 117/1000, Training Loss: 0.0029677370282367786, Test Loss: 0.002880938899064305\n",
            "Epoch 118/1000, Training Loss: 0.0029658412124750014, Test Loss: 0.0028791660123557556\n",
            "Epoch 119/1000, Training Loss: 0.002963916705645565, Test Loss: 0.0028773662104576657\n",
            "Epoch 120/1000, Training Loss: 0.0029619630831492806, Test Loss: 0.002875539092053661\n",
            "Epoch 121/1000, Training Loss: 0.0029599799188233028, Test Loss: 0.0028736842544478964\n",
            "Epoch 122/1000, Training Loss: 0.002957966785080697, Test Loss: 0.002871801293704112\n",
            "Epoch 123/1000, Training Loss: 0.0029559232530589606, Test Loss: 0.002869889804792758\n",
            "Epoch 124/1000, Training Loss: 0.0029538488927777135, Test Loss: 0.0028679493817463336\n",
            "Epoch 125/1000, Training Loss: 0.002951743273305746, Test Loss: 0.002865979617823097\n",
            "Epoch 126/1000, Training Loss: 0.002949605962937636, Test Loss: 0.002863980105679228\n",
            "Epoch 127/1000, Training Loss: 0.0029474365293801195, Test Loss: 0.002861950437549607\n",
            "Epoch 128/1000, Training Loss: 0.0029452345399484025, Test Loss: 0.0028598902054372834\n",
            "Epoch 129/1000, Training Loss: 0.002942999561772586, Test Loss: 0.002857799001311757\n",
            "Epoch 130/1000, Training Loss: 0.0029407311620143774, Test Loss: 0.0028556764173161327\n",
            "Epoch 131/1000, Training Loss: 0.002938428908094239, Test Loss: 0.002853522045983232\n",
            "Epoch 132/1000, Training Loss: 0.0029360923679291216, Test Loss: 0.0028513354804607376\n",
            "Epoch 133/1000, Training Loss: 0.0029337211101809002, Test Loss: 0.0028491163147453674\n",
            "Epoch 134/1000, Training Loss: 0.002931314704515634, Test Loss: 0.002846864143926149\n",
            "Epoch 135/1000, Training Loss: 0.002928872721873746, Test Loss: 0.002844578564436759\n",
            "Epoch 136/1000, Training Loss: 0.0029263947347511754, Test Loss: 0.002842259174316938\n",
            "Epoch 137/1000, Training Loss: 0.002923880317491582, Test Loss: 0.0028399055734829198\n",
            "Epoch 138/1000, Training Loss: 0.002921329046589592, Test Loss: 0.002837517364006813\n",
            "Epoch 139/1000, Training Loss: 0.002918740501005114, Test Loss: 0.002835094150404842\n",
            "Epoch 140/1000, Training Loss: 0.0029161142624886636, Test Loss: 0.002832635539934321\n",
            "Epoch 141/1000, Training Loss: 0.0029134499159176384, Test Loss: 0.002830141142899189\n",
            "Epoch 142/1000, Training Loss: 0.002910747049643443, Test Loss: 0.002827610572963941\n",
            "Epoch 143/1000, Training Loss: 0.0029080052558492974, Test Loss: 0.0028250434474756904\n",
            "Epoch 144/1000, Training Loss: 0.002905224130918567, Test Loss: 0.0028224393877941166\n",
            "Epoch 145/1000, Training Loss: 0.002902403275813345, Test Loss: 0.002819798019628962\n",
            "Epoch 146/1000, Training Loss: 0.002899542296463022, Test Loss: 0.00281711897338474\n",
            "Epoch 147/1000, Training Loss: 0.0028966408041624847, Test Loss: 0.002814401884512219\n",
            "Epoch 148/1000, Training Loss: 0.002893698415979553, Test Loss: 0.0028116463938662444\n",
            "Epoch 149/1000, Training Loss: 0.0028907147551711777, Test Loss: 0.0028088521480693655\n",
            "Epoch 150/1000, Training Loss: 0.0028876894516078723, Test Loss: 0.0028060187998806925\n",
            "Epoch 151/1000, Training Loss: 0.0028846221422057796, Test Loss: 0.0028031460085693664\n",
            "Epoch 152/1000, Training Loss: 0.0028815124713656827, Test Loss: 0.0028002334402919254\n",
            "Epoch 153/1000, Training Loss: 0.002878360091418214, Test Loss: 0.002797280768472804\n",
            "Epoch 154/1000, Training Loss: 0.0028751646630744065, Test Loss: 0.002794287674187139\n",
            "Epoch 155/1000, Training Loss: 0.0028719258558806667, Test Loss: 0.002791253846544968\n",
            "Epoch 156/1000, Training Loss: 0.002868643348677134, Test Loss: 0.0027881789830758417\n",
            "Epoch 157/1000, Training Loss: 0.0028653168300583187, Test Loss: 0.002785062790112783\n",
            "Epoch 158/1000, Training Loss: 0.0028619459988347795, Test Loss: 0.0027819049831744624\n",
            "Epoch 159/1000, Training Loss: 0.002858530564494532, Test Loss: 0.0027787052873443647\n",
            "Epoch 160/1000, Training Loss: 0.0028550702476627323, Test Loss: 0.0027754634376456304\n",
            "Epoch 161/1000, Training Loss: 0.0028515647805581047, Test Loss: 0.002772179179410193\n",
            "Epoch 162/1000, Training Loss: 0.0028480139074444507, Test Loss: 0.0027688522686407334\n",
            "Epoch 163/1000, Training Loss: 0.0028444173850754335, Test Loss: 0.0027654824723638565\n",
            "Epoch 164/1000, Training Loss: 0.002840774983130773, Test Loss: 0.0027620695689728704\n",
            "Epoch 165/1000, Training Loss: 0.002837086484641799, Test Loss: 0.002758613348558398\n",
            "Epoch 166/1000, Training Loss: 0.002833351686404225, Test Loss: 0.002755113613224982\n",
            "Epoch 167/1000, Training Loss: 0.0028295703993758657, Test Loss: 0.0027515701773917624\n",
            "Epoch 168/1000, Training Loss: 0.0028257424490569015, Test Loss: 0.0027479828680752223\n",
            "Epoch 169/1000, Training Loss: 0.002821867675850161, Test Loss: 0.002744351525151878\n",
            "Epoch 170/1000, Training Loss: 0.002817945935398776, Test Loss: 0.0027406760015987507\n",
            "Epoch 171/1000, Training Loss: 0.0028139770988984203, Test Loss: 0.0027369561637093326\n",
            "Epoch 172/1000, Training Loss: 0.0028099610533812713, Test Loss: 0.002733191891282728\n",
            "Epoch 173/1000, Training Loss: 0.00280589770196864, Test Loss: 0.002729383077783516\n",
            "Epoch 174/1000, Training Loss: 0.00280178696408918, Test Loss: 0.0027255296304698785\n",
            "Epoch 175/1000, Training Loss: 0.002797628775659424, Test Loss: 0.0027216314704874133\n",
            "Epoch 176/1000, Training Loss: 0.002793423089223317, Test Loss: 0.0027176885329260447\n",
            "Epoch 177/1000, Training Loss: 0.0027891698740473204, Test Loss: 0.0027137007668373526\n",
            "Epoch 178/1000, Training Loss: 0.002784869116167557, Test Loss: 0.002709668135209606\n",
            "Epoch 179/1000, Training Loss: 0.002780520818385422, Test Loss: 0.0027055906148978006\n",
            "Epoch 180/1000, Training Loss: 0.0027761250002079755, Test Loss: 0.0027014681965058782\n",
            "Epoch 181/1000, Training Loss: 0.00277168169772942, Test Loss: 0.002697300884218386\n",
            "Epoch 182/1000, Training Loss: 0.0027671909634498747, Test Loss: 0.0026930886955787593\n",
            "Epoch 183/1000, Training Loss: 0.002762652866027681, Test Loss: 0.0026888316612114676\n",
            "Epoch 184/1000, Training Loss: 0.002758067489961409, Test Loss: 0.002684529824485227\n",
            "Epoch 185/1000, Training Loss: 0.0027534349351977603, Test Loss: 0.002680183241114561\n",
            "Epoch 186/1000, Training Loss: 0.002748755316661583, Test Loss: 0.002675791978697003\n",
            "Epoch 187/1000, Training Loss: 0.0027440287637042204, Test Loss: 0.002671356116183304\n",
            "Epoch 188/1000, Training Loss: 0.0027392554194664975, Test Loss: 0.0026668757432780545\n",
            "Epoch 189/1000, Training Loss: 0.0027344354401526937, Test Loss: 0.0026623509597682544\n",
            "Epoch 190/1000, Training Loss: 0.002729568994211955, Test Loss: 0.002657781874777411\n",
            "Epoch 191/1000, Training Loss: 0.0027246562614237015, Test Loss: 0.0026531686059428957\n",
            "Epoch 192/1000, Training Loss: 0.00271969743188371, Test Loss: 0.0026485112785143844\n",
            "Epoch 193/1000, Training Loss: 0.002714692704887715, Test Loss: 0.0026438100243713807\n",
            "Epoch 194/1000, Training Loss: 0.0027096422877095332, Test Loss: 0.0026390649809579438\n",
            "Epoch 195/1000, Training Loss: 0.0027045463942708945, Test Loss: 0.002634276290132892\n",
            "Epoch 196/1000, Training Loss: 0.0026994052437004103, Test Loss: 0.002629444096934033\n",
            "Epoch 197/1000, Training Loss: 0.0026942190587793007, Test Loss: 0.002624568548254997\n",
            "Epoch 198/1000, Training Loss: 0.0026889880642717895, Test Loss: 0.002619649791433626\n",
            "Epoch 199/1000, Training Loss: 0.0026837124851383275, Test Loss: 0.002614687972750975\n",
            "Epoch 200/1000, Training Loss: 0.002678392544630106, Test Loss: 0.0026096832358402477\n",
            "Epoch 201/1000, Training Loss: 0.0026730284622636367, Test Loss: 0.0026046357200052443\n",
            "Epoch 202/1000, Training Loss: 0.0026676204516744807, Test Loss: 0.0025995455584480742\n",
            "Epoch 203/1000, Training Loss: 0.002662168718349606, Test Loss: 0.002594412876406269\n",
            "Epoch 204/1000, Training Loss: 0.00265667345723815, Test Loss: 0.002589237789199527\n",
            "Epoch 205/1000, Training Loss: 0.0026511348502408064, Test Loss: 0.0025840204001867434\n",
            "Epoch 206/1000, Training Loss: 0.0026455530635783996, Test Loss: 0.002578760798634158\n",
            "Epoch 207/1000, Training Loss: 0.002639928245040658, Test Loss: 0.0025734590574957527\n",
            "Epoch 208/1000, Training Loss: 0.0026342605211166, Test Loss: 0.0025681152311073253\n",
            "Epoch 209/1000, Training Loss: 0.002628549994008393, Test Loss: 0.0025627293527959605\n",
            "Epoch 210/1000, Training Loss: 0.0026227967385309948, Test Loss: 0.002557301432406856\n",
            "Epoch 211/1000, Training Loss: 0.0026170007989003543, Test Loss: 0.0025518314537498242\n",
            "Epoch 212/1000, Training Loss: 0.0026111621854134383, Test Loss: 0.0025463193719680094\n",
            "Epoch 213/1000, Training Loss: 0.002605280871023838, Test Loss: 0.0025407651108317547\n",
            "Epoch 214/1000, Training Loss: 0.002599356787817223, Test Loss: 0.002535168559960726\n",
            "Epoch 215/1000, Training Loss: 0.002593389823391459, Test Loss: 0.002529529571977823\n",
            "Epoch 216/1000, Training Loss: 0.00258737981714674, Test Loss: 0.002523847959598678\n",
            "Epoch 217/1000, Training Loss: 0.0025813265564916827, Test Loss: 0.0025181234926608224\n",
            "Epoch 218/1000, Training Loss: 0.002575229772971921, Test Loss: 0.002512355895097027\n",
            "Epoch 219/1000, Training Loss: 0.0025690891383283766, Test Loss: 0.0025065448418575605\n",
            "Epoch 220/1000, Training Loss: 0.0025629042604930676, Test Loss: 0.0025006899557865805\n",
            "Epoch 221/1000, Training Loss: 0.0025566746795309945, Test Loss: 0.0024947908044581483\n",
            "Epoch 222/1000, Training Loss: 0.0025503998635374434, Test Loss: 0.0024888468969778872\n",
            "Epoch 223/1000, Training Loss: 0.0025440792045007985, Test Loss: 0.0024828576807566127\n",
            "Epoch 224/1000, Training Loss: 0.0025377120141418606, Test Loss: 0.0024768225382628627\n",
            "Epoch 225/1000, Training Loss: 0.0025312975197415545, Test Loss: 0.002470740783761651\n",
            "Epoch 226/1000, Training Loss: 0.0025248348599699296, Test Loss: 0.00246461166004743\n",
            "Epoch 227/1000, Training Loss: 0.0025183230807303936, Test Loss: 0.0024584343351797965\n",
            "Epoch 228/1000, Training Loss: 0.0025117611310342637, Test Loss: 0.0024522078992311475\n",
            "Epoch 229/1000, Training Loss: 0.0025051478589219463, Test Loss: 0.002445931361056237\n",
            "Epoch 230/1000, Training Loss: 0.0024984820074483633, Test Loss: 0.0024396036450943796\n",
            "Epoch 231/1000, Training Loss: 0.0024917622107516156, Test Loss: 0.002433223588215912\n",
            "Epoch 232/1000, Training Loss: 0.0024849869902253663, Test Loss: 0.002426789936625445\n",
            "Epoch 233/1000, Training Loss: 0.0024781547508169804, Test Loss: 0.0024203013428354824\n",
            "Epoch 234/1000, Training Loss: 0.0024712637774750744, Test Loss: 0.002413756362725067\n",
            "Epoch 235/1000, Training Loss: 0.0024643122317718584, Test Loss: 0.0024071534526992956\n",
            "Epoch 236/1000, Training Loss: 0.002457298148727371, Test Loss: 0.0024004909669667405\n",
            "Epoch 237/1000, Training Loss: 0.0024502194338644956, Test Loss: 0.0023937671549531816\n",
            "Epoch 238/1000, Training Loss: 0.0024430738605254855, Test Loss: 0.002386980158871361\n",
            "Epoch 239/1000, Training Loss: 0.002435859067482425, Test Loss: 0.0023801280114678935\n",
            "Epoch 240/1000, Training Loss: 0.0024285725568758206, Test Loss: 0.002373208633969841\n",
            "Epoch 241/1000, Training Loss: 0.002421211692517145, Test Loss: 0.002366219834254918\n",
            "Epoch 242/1000, Training Loss: 0.002413773698592605, Test Loss: 0.0023591593052706542\n",
            "Epoch 243/1000, Training Loss: 0.0024062556588066854, Test Loss: 0.0023520246237291108\n",
            "Epoch 244/1000, Training Loss: 0.0023986545160050747, Test Loss: 0.0023448132491050103\n",
            "Epoch 245/1000, Training Loss: 0.0023909670723171934, Test Loss: 0.0023375225229661555\n",
            "Epoch 246/1000, Training Loss: 0.0023831899898589326, Test Loss: 0.002330149668665909\n",
            "Epoch 247/1000, Training Loss: 0.002375319792035939, Test Loss: 0.0023226917914280372\n",
            "Epoch 248/1000, Training Loss: 0.0023673528654871083, Test Loss: 0.002315145878854625\n",
            "Epoch 249/1000, Training Loss: 0.002359285462706592, Test Loss: 0.0023075088018876073\n",
            "Epoch 250/1000, Training Loss: 0.002351113705380579, Test Loss: 0.0022997773162540414\n",
            "Epoch 251/1000, Training Loss: 0.002342833588472326, Test Loss: 0.0022919480644242344\n",
            "Epoch 252/1000, Training Loss: 0.0023344409850853246, Test Loss: 0.0022840175781103666\n",
            "Epoch 253/1000, Training Loss: 0.002325931652130081, Test Loss: 0.002275982281331169\n",
            "Epoch 254/1000, Training Loss: 0.002317301236814689, Test Loss: 0.002267838494065571\n",
            "Epoch 255/1000, Training Loss: 0.0023085452839732693, Test Loss: 0.00225958243651487\n",
            "Epoch 256/1000, Training Loss: 0.0022996592442394177, Test Loss: 0.0022512102339891276\n",
            "Epoch 257/1000, Training Loss: 0.002290638483064135, Test Loss: 0.0022427179224288924\n",
            "Epoch 258/1000, Training Loss: 0.0022814782905694196, Test Loss: 0.002234101454568341\n",
            "Epoch 259/1000, Training Loss: 0.002272173892219881, Test Loss: 0.002225356706740289\n",
            "Epoch 260/1000, Training Loss: 0.0022627204602855572, Test Loss: 0.002216479486317519\n",
            "Epoch 261/1000, Training Loss: 0.002253113126059941, Test Loss: 0.002207465539778784\n",
            "Epoch 262/1000, Training Loss: 0.002243346992787931, Test Loss: 0.0021983105613812615\n",
            "Epoch 263/1000, Training Loss: 0.0022334171492497143, Test Loss: 0.0021890102024151366\n",
            "Epoch 264/1000, Training Loss: 0.0022233186839383614, Test Loss: 0.0021795600810098057\n",
            "Epoch 265/1000, Training Loss: 0.002213046699761694, Test Loss: 0.002169955792455683\n",
            "Epoch 266/1000, Training Loss: 0.002202596329192886, Test Loss: 0.00216019292000061\n",
            "Epoch 267/1000, Training Loss: 0.0021919627497896613, Test Loss: 0.002150267046075884\n",
            "Epoch 268/1000, Training Loss: 0.00218114119999896, Test Loss: 0.0021401737639038894\n",
            "Epoch 269/1000, Training Loss: 0.002170126995162802, Test Loss: 0.0021299086894375693\n",
            "Epoch 270/1000, Training Loss: 0.0021589155436419565, Test Loss: 0.0021194674735816473\n",
            "Epoch 271/1000, Training Loss: 0.002147502362976727, Test Loss: 0.002108845814646414\n",
            "Epoch 272/1000, Training Loss: 0.002135883096009045, Test Loss: 0.0020980394709875475\n",
            "Epoch 273/1000, Training Loss: 0.002124053526896617, Test Loss: 0.0020870442737892414\n",
            "Epoch 274/1000, Training Loss: 0.0021120095969581537, Test Loss: 0.002075856139953205\n",
            "Epoch 275/1000, Training Loss: 0.002099747420298354, Test Loss: 0.0020644710850624714\n",
            "Epoch 276/1000, Training Loss: 0.0020872632991719526, Test Loss: 0.0020528852363962233\n",
            "Epoch 277/1000, Training Loss: 0.0020745537390574297, Test Loss: 0.002041094845979899\n",
            "Epoch 278/1000, Training Loss: 0.002061615463422399, Test Loss: 0.0020290963036629093\n",
            "Epoch 279/1000, Training Loss: 0.0020484454281737867, Test Loss: 0.002016886150224688\n",
            "Epoch 280/1000, Training Loss: 0.0020350408357962383, Test Loss: 0.0020044610905174233\n",
            "Epoch 281/1000, Training Loss: 0.002021399149191276, Test Loss: 0.0019918180066608917\n",
            "Epoch 282/1000, Training Loss: 0.0020075181052370606, Test Loss: 0.0019789539713105653\n",
            "Epoch 283/1000, Training Loss: 0.0019933957280939833, Test Loss: 0.001965866261024561\n",
            "Epoch 284/1000, Training Loss: 0.0019790303422843112, Test Loss: 0.001952552369757493\n",
            "Epoch 285/1000, Training Loss: 0.0019644205855745347, Test Loss: 0.001939010022509938\n",
            "Epoch 286/1000, Training Loss: 0.0019495654216869218, Test Loss: 0.0019252371891605754\n",
            "Epoch 287/1000, Training Loss: 0.001934464152861825, Test Loss: 0.001911232098504291\n",
            "Epoch 288/1000, Training Loss: 0.0019191164322849169, Test Loss: 0.0018969932525134063\n",
            "Epoch 289/1000, Training Loss: 0.0019035222763835596, Test Loss: 0.0018825194408309213\n",
            "Epoch 290/1000, Training Loss: 0.001887682076984561, Test Loss: 0.0018678097554942702\n",
            "Epoch 291/1000, Training Loss: 0.0018715966133116197, Test Loss: 0.0018528636058757558\n",
            "Epoch 292/1000, Training Loss: 0.0018552670637855568, Test Loss: 0.0018376807338119105\n",
            "Epoch 293/1000, Training Loss: 0.0018386950175740841, Test Loss: 0.001822261228878621\n",
            "Epoch 294/1000, Training Loss: 0.0018218824858211302, Test Loss: 0.0018066055437524686\n",
            "Epoch 295/1000, Training Loss: 0.001804831912468859, Test Loss: 0.001790714509581521\n",
            "Epoch 296/1000, Training Loss: 0.0017875461845692445, Test Loss: 0.0017745893512713545\n",
            "Epoch 297/1000, Training Loss: 0.0017700286419666128, Test Loss: 0.0017582317025745294\n",
            "Epoch 298/1000, Training Loss: 0.0017522830862186673, Test Loss: 0.0017416436208547536\n",
            "Epoch 299/1000, Training Loss: 0.0017343137886112988, Test Loss: 0.0017248276013805113\n",
            "Epoch 300/1000, Training Loss: 0.001716125497112739, Test Loss: 0.0017077865909880226\n",
            "Epoch 301/1000, Training Loss: 0.0016977234421052256, Test Loss: 0.0016905240009396352\n",
            "Epoch 302/1000, Training Loss: 0.001679113340727999, Test Loss: 0.0016730437187921895\n",
            "Epoch 303/1000, Training Loss: 0.0016603013996641758, Test Loss: 0.0016553501190805001\n",
            "Epoch 304/1000, Training Loss: 0.001641294316205989, Test Loss: 0.0016374480726141503\n",
            "Epoch 305/1000, Training Loss: 0.0016220992774382708, Test Loss: 0.0016193429541819702\n",
            "Epoch 306/1000, Training Loss: 0.0016027239573888359, Test Loss: 0.0016010406484576853\n",
            "Epoch 307/1000, Training Loss: 0.001583176512006554, Test Loss: 0.001582547553902856\n",
            "Epoch 308/1000, Training Loss: 0.0015634655718433314, Test Loss: 0.0015638705844694028\n",
            "Epoch 309/1000, Training Loss: 0.0015436002323346942, Test Loss: 0.001545017168913964\n",
            "Epoch 310/1000, Training Loss: 0.0015235900415951087, Test Loss: 0.0015259952475501649\n",
            "Epoch 311/1000, Training Loss: 0.001503444985667983, Test Loss: 0.0015068132662824558\n",
            "Epoch 312/1000, Training Loss: 0.001483175471196449, Test Loss: 0.001487480167786775\n",
            "Epoch 313/1000, Training Loss: 0.0014627923055089511, Test Loss: 0.0014680053797285183\n",
            "Epoch 314/1000, Training Loss: 0.0014423066741427746, Test Loss: 0.0014483987999369455\n",
            "Epoch 315/1000, Training Loss: 0.0014217301158587956, Test Loss: 0.0014286707784872258\n",
            "Epoch 316/1000, Training Loss: 0.0014010744952310556, Test Loss: 0.0014088320966759744\n",
            "Epoch 317/1000, Training Loss: 0.0013803519729249174, Test Loss: 0.0013888939429132375\n",
            "Epoch 318/1000, Training Loss: 0.0013595749738070694, Test Loss: 0.0013688678855926701\n",
            "Epoch 319/1000, Training Loss: 0.0013387561530586684, Test Loss: 0.0013487658430414308\n",
            "Epoch 320/1000, Training Loss: 0.001317908360489505, Test Loss: 0.0013286000506916148\n",
            "Epoch 321/1000, Training Loss: 0.0012970446032750495, Test Loss: 0.0013083830256544779\n",
            "Epoch 322/1000, Training Loss: 0.0012761780073598811, Test Loss: 0.001288127528917321\n",
            "Epoch 323/1000, Training Loss: 0.0012553217777893492, Test Loss: 0.0012678465254188575\n",
            "Epoch 324/1000, Training Loss: 0.001234489158246464, Test Loss: 0.0012475531422922533\n",
            "Epoch 325/1000, Training Loss: 0.0012136933900824102, Test Loss: 0.001227260625594333\n",
            "Epoch 326/1000, Training Loss: 0.0011929476711367256, Test Loss: 0.0012069822958644038\n",
            "Epoch 327/1000, Training Loss: 0.0011722651146469675, Test Loss: 0.0011867315028759652\n",
            "Epoch 328/1000, Training Loss: 0.0011516587085473205, Test Loss: 0.0011665215799585942\n",
            "Epoch 329/1000, Training Loss: 0.0011311412754516203, Test Loss: 0.0011463657982755994\n",
            "Epoch 330/1000, Training Loss: 0.0011107254336081792, Test Loss: 0.0011262773214447876\n",
            "Epoch 331/1000, Training Loss: 0.0010904235591023582, Test Loss: 0.0011062691608854697\n",
            "Epoch 332/1000, Training Loss: 0.001070247749567779, Test Loss: 0.0010863541322641084\n",
            "Epoch 333/1000, Training Loss: 0.0010502097896491894, Test Loss: 0.0010665448133947556\n",
            "Epoch 334/1000, Training Loss: 0.0010303211184391758, Test Loss: 0.0010468535039283645\n",
            "Epoch 335/1000, Training Loss: 0.0010105927990879322, Test Loss: 0.0010272921871383042\n",
            "Epoch 336/1000, Training Loss: 0.0009910354907602492, Test Loss: 0.0010078724940782743\n",
            "Epoch 337/1000, Training Loss: 0.0009716594230874736, Test Loss: 0.000988605670354253\n",
            "Epoch 338/1000, Training Loss: 0.0009524743732346572, Test Loss: 0.0009695025457149019\n",
            "Epoch 339/1000, Training Loss: 0.0009334896456749914, Test Loss: 0.0009505735066257587\n",
            "Epoch 340/1000, Training Loss: 0.0009147140547354028, Test Loss: 0.0009318284719526174\n",
            "Epoch 341/1000, Training Loss: 0.0008961559099491263, Test Loss: 0.000913276871839401\n",
            "Epoch 342/1000, Training Loss: 0.000877823004223756, Test Loss: 0.0008949276298264116\n",
            "Epoch 343/1000, Training Loss: 0.000859722604806872, Test Loss: 0.0008767891482168547\n",
            "Epoch 344/1000, Training Loss: 0.0008418614470064323, Test Loss: 0.0008588692966636491\n",
            "Epoch 345/1000, Training Loss: 0.0008242457305997108, Test Loss: 0.000841175403915171\n",
            "Epoch 346/1000, Training Loss: 0.00080688111884313, Test Loss: 0.0008237142526282726\n",
            "Epoch 347/1000, Training Loss: 0.0007897727399759551, Test Loss: 0.000806492077130007\n",
            "Epoch 348/1000, Training Loss: 0.0007729251910937443, Test Loss: 0.0007895145639861634\n",
            "Epoch 349/1000, Training Loss: 0.0007563425442526393, Test Loss: 0.0007727868552150068\n",
            "Epoch 350/1000, Training Loss: 0.0007400283546533353, Test Loss: 0.0007563135539688168\n",
            "Epoch 351/1000, Training Loss: 0.0007239856707437115, Test Loss: 0.0007400987324935813\n",
            "Epoch 352/1000, Training Loss: 0.0007082170460716486, Test Loss: 0.000724145942168482\n",
            "Epoch 353/1000, Training Loss: 0.0006927245527146228, Test Loss: 0.0007084582254215903\n",
            "Epoch 354/1000, Training Loss: 0.0006775097961097958, Test Loss: 0.0006930381293158327\n",
            "Epoch 355/1000, Training Loss: 0.0006625739311079602, Test Loss: 0.0006778877206001485\n",
            "Epoch 356/1000, Training Loss: 0.0006479176790759169, Test Loss: 0.0006630086020236116\n",
            "Epoch 357/1000, Training Loss: 0.0006335413458753585, Test Loss: 0.0006484019297158844\n",
            "Epoch 358/1000, Training Loss: 0.0006194448405511871, Test Loss: 0.0006340684314444718\n",
            "Epoch 359/1000, Training Loss: 0.0006056276945685762, Test Loss: 0.000620008425567978\n",
            "Epoch 360/1000, Training Loss: 0.0005920890814458332, Test Loss: 0.000606221840514747\n",
            "Epoch 361/1000, Training Loss: 0.0005788278366386828, Test Loss: 0.0005927082346270543\n",
            "Epoch 362/1000, Training Loss: 0.0005658424775411068, Test Loss: 0.000579466816222742\n",
            "Epoch 363/1000, Training Loss: 0.0005531312234780091, Test Loss: 0.0005664964637382388\n",
            "Epoch 364/1000, Training Loss: 0.0005406920155754717, Test Loss: 0.000553795745829186\n",
            "Epoch 365/1000, Training Loss: 0.0005285225364050549, Test Loss: 0.000541362941317041\n",
            "Epoch 366/1000, Training Loss: 0.0005166202293094703, Test Loss: 0.0005291960588822233\n",
            "Epoch 367/1000, Training Loss: 0.0005049823173276986, Test Loss: 0.0005172928564161293\n",
            "Epoch 368/1000, Training Loss: 0.000493605821647965, Test Loss: 0.000505650859955483\n",
            "Epoch 369/1000, Training Loss: 0.00048248757952737593, Test Loss: 0.0004942673821335862\n",
            "Epoch 370/1000, Training Loss: 0.00047162426162653535, Test Loss: 0.00048313954009293587\n",
            "Epoch 371/1000, Training Loss: 0.00046101238871685104, Test Loss: 0.0004722642728134159\n",
            "Epoch 372/1000, Training Loss: 0.00045064834772676253, Test Loss: 0.00046163835781900457\n",
            "Epoch 373/1000, Training Loss: 0.00044052840710126636, Test Loss: 0.0004512584272341478\n",
            "Epoch 374/1000, Training Loss: 0.0004306487314564235, Test Loss: 0.00044112098316832847\n",
            "Epoch 375/1000, Training Loss: 0.0004210053955173793, Test Loss: 0.000431222412414178\n",
            "Epoch 376/1000, Training Loss: 0.00041159439733439975, Test Loss: 0.0004215590004504001\n",
            "Epoch 377/1000, Training Loss: 0.0004024116707769975, Test Loss: 0.000412126944746299\n",
            "Epoch 378/1000, Training Loss: 0.00039345309731092125, Test Loss: 0.00040292236736936263\n",
            "Epoch 379/1000, Training Loss: 0.00038471451706707143, Test Loss: 0.000393941326901551\n",
            "Epoch 380/1000, Training Loss: 0.00037619173921497136, Test Loss: 0.00038517982967353715\n",
            "Epoch 381/1000, Training Loss: 0.00036788055165658007, Test Loss: 0.0003766338403292596\n",
            "Epoch 382/1000, Training Loss: 0.0003597767300587375, Test Loss: 0.0003682992917356864\n",
            "Epoch 383/1000, Training Loss: 0.00035187604624475053, Test Loss: 0.0003601720942549787\n",
            "Epoch 384/1000, Training Loss: 0.00034417427596719125, Test Loss: 0.0003522481443978709\n",
            "Epoch 385/1000, Training Loss: 0.00033666720608545276, Test Loss: 0.00034452333287866637\n",
            "Epoch 386/1000, Training Loss: 0.00032935064117239356, Test Loss: 0.0003369935520931806\n",
            "Epoch 387/1000, Training Loss: 0.0003222204095752053, Test Loss: 0.0003296547030419328\n",
            "Epoch 388/1000, Training Loss: 0.0003152723689559088, Test Loss: 0.00032250270172130916\n",
            "Epoch 389/1000, Training Loss: 0.00030850241133709535, Test Loss: 0.0003155334850058507\n",
            "Epoch 390/1000, Training Loss: 0.00030190646767843375, Test Loss: 0.0003087430160449305\n",
            "Epoch 391/1000, Training Loss: 0.00029548051200921463, Test Loss: 0.0003021272891970203\n",
            "Epoch 392/1000, Training Loss: 0.0002892205651418595, Test Loss: 0.0002956823345246602\n",
            "Epoch 393/1000, Training Loss: 0.00028312269799073413, Test Loss: 0.0002894042218728579\n",
            "Epoch 394/1000, Training Loss: 0.00027718303452006375, Test Loss: 0.00028328906455331895\n",
            "Epoch 395/1000, Training Loss: 0.0002713977543439793, Test Loss: 0.0002773330226563692\n",
            "Epoch 396/1000, Training Loss: 0.0002657630950010125, Test Loss: 0.000271532306011922\n",
            "Epoch 397/1000, Training Loss: 0.0002602753539245178, Test Loss: 0.0002658831768202136\n",
            "Epoch 398/1000, Training Loss: 0.0002549308901296111, Test Loss: 0.0002603819519723022\n",
            "Epoch 399/1000, Training Loss: 0.00024972612563645466, Test Loss: 0.000255025005079816\n",
            "Epoch 400/1000, Training Loss: 0.0002446575466486701, Test Loss: 0.000249808768232465\n",
            "Epoch 401/1000, Training Loss: 0.00023972170450494093, Test Loss: 0.0002447297335013476\n",
            "Epoch 402/1000, Training Loss: 0.00023491521642080585, Test Loss: 0.00023978445420512302\n",
            "Epoch 403/1000, Training Loss: 0.00023023476603693902, Test Loss: 0.00023496954595555845\n",
            "Epoch 404/1000, Training Loss: 0.00022567710378920073, Test Loss: 0.00023028168749806782\n",
            "Epoch 405/1000, Training Loss: 0.00022123904711498005, Test Loss: 0.00022571762136218307\n",
            "Epoch 406/1000, Training Loss: 0.00021691748050950928, Test Loss: 0.0002212741543361864\n",
            "Epoch 407/1000, Training Loss: 0.0002127093554450207, Test Loss: 0.00021694815777936226\n",
            "Epoch 408/1000, Training Loss: 0.00020861169016486727, Test Loss: 0.00021273656778466332\n",
            "Epoch 409/1000, Training Loss: 0.00020462156936394436, Test Loss: 0.00020863638520386945\n",
            "Epoch 410/1000, Training Loss: 0.00020073614376609655, Test Loss: 0.00020464467554669983\n",
            "Epoch 411/1000, Training Loss: 0.0001969526296084627, Test Loss: 0.0002007585687646372\n",
            "Epoch 412/1000, Training Loss: 0.00019326830804205244, Test Loss: 0.00019697525892960758\n",
            "Epoch 413/1000, Training Loss: 0.00018968052445728987, Test Loss: 0.00019329200381712372\n",
            "Epoch 414/1000, Training Loss: 0.00018618668774260106, Test Loss: 0.00018970612440283787\n",
            "Epoch 415/1000, Training Loss: 0.00018278426948357192, Test Loss: 0.00018621500428091128\n",
            "Epoch 416/1000, Training Loss: 0.00017947080310970993, Test Loss: 0.00018281608901214196\n",
            "Epoch 417/1000, Training Loss: 0.00017624388299529215, Test Loss: 0.0001795068854091882\n",
            "Epoch 418/1000, Training Loss: 0.00017310116352032718, Test Loss: 0.00017628496076580518\n",
            "Epoch 419/1000, Training Loss: 0.00017004035809721322, Test Loss: 0.00017314794203653\n",
            "Epoch 420/1000, Training Loss: 0.00016705923816821852, Test Loss: 0.00017009351497278982\n",
            "Epoch 421/1000, Training Loss: 0.00016415563217857165, Test Loss: 0.0001671194232210386\n",
            "Epoch 422/1000, Training Loss: 0.0001613274245295026, Test Loss: 0.00016422346738806268\n",
            "Epoch 423/1000, Training Loss: 0.0001585725545152817, Test Loss: 0.0001614035040782947\n",
            "Epoch 424/1000, Training Loss: 0.0001558890152479344, Test Loss: 0.0001586574449075503\n",
            "Epoch 425/1000, Training Loss: 0.00015327485257302318, Test Loss: 0.00015598325549730588\n",
            "Epoch 426/1000, Training Loss: 0.0001507281639795801, Test Loss: 0.00015337895445331247\n",
            "Epoch 427/1000, Training Loss: 0.0001482470975070476, Test Loss: 0.0001508426123320514\n",
            "Epoch 428/1000, Training Loss: 0.0001458298506517251, Test Loss: 0.0001483723505981903\n",
            "Epoch 429/1000, Training Loss: 0.00014347466927514753, Test Loss: 0.0001459663405760616\n",
            "Epoch 430/1000, Training Loss: 0.00014117984651643227, Test Loss: 0.0001436228023977838\n",
            "Epoch 431/1000, Training Loss: 0.00013894372171055293, Test Loss: 0.00014134000395055177\n",
            "Epoch 432/1000, Training Loss: 0.00013676467931423425, Test Loss: 0.00013911625982529865\n",
            "Epoch 433/1000, Training Loss: 0.00013464114784102158, Test Loss: 0.00013694993026877963\n",
            "Epoch 434/1000, Training Loss: 0.00013257159880690868, Test Loss: 0.00013483942014095495\n",
            "Epoch 435/1000, Training Loss: 0.00013055454568773142, Test Loss: 0.0001327831778792893\n",
            "Epoch 436/1000, Training Loss: 0.00012858854288943947, Test Loss: 0.00013077969447152722\n",
            "Epoch 437/1000, Training Loss: 0.0001266721847321807, Test Loss: 0.0001288275024382536\n",
            "Epoch 438/1000, Training Loss: 0.00012480410444905385, Test Loss: 0.00012692517482646917\n",
            "Epoch 439/1000, Training Loss: 0.0001229829732002297, Test Loss: 0.0001250713242152331\n",
            "Epoch 440/1000, Training Loss: 0.00012120749910309443, Test Loss: 0.00012326460173433704\n",
            "Epoch 441/1000, Training Loss: 0.00011947642627892236, Test Loss: 0.00012150369609683078\n",
            "Epoch 442/1000, Training Loss: 0.00011778853391651987, Test Loss: 0.00011978733264612108\n",
            "Epoch 443/1000, Training Loss: 0.00011614263535323198, Test Loss: 0.00011811427241829577\n",
            "Epoch 444/1000, Training Loss: 0.00011453757717356181, Test Loss: 0.00011648331122016244\n",
            "Epoch 445/1000, Training Loss: 0.00011297223832566614, Test Loss: 0.00011489327872350723\n",
            "Epoch 446/1000, Training Loss: 0.00011144552925586644, Test Loss: 0.00011334303757590242\n",
            "Epoch 447/1000, Training Loss: 0.00010995639106129388, Test Loss: 0.00011183148252839838\n",
            "Epoch 448/1000, Training Loss: 0.00010850379466073034, Test Loss: 0.00011035753958031808\n",
            "Epoch 449/1000, Training Loss: 0.00010708673998365012, Test Loss: 0.00010892016514134215\n",
            "Epoch 450/1000, Training Loss: 0.00010570425517744255, Test Loss: 0.00010751834521101324\n",
            "Epoch 451/1000, Training Loss: 0.00010435539583275172, Test Loss: 0.00010615109457572499\n",
            "Epoch 452/1000, Training Loss: 0.00010303924422682319, Test Loss: 0.00010481745602323427\n",
            "Epoch 453/1000, Training Loss: 0.00010175490858474373, Test Loss: 0.00010351649957468102\n",
            "Epoch 454/1000, Training Loss: 0.00010050152235841346, Test Loss: 0.00010224732173407291\n",
            "Epoch 455/1000, Training Loss: 9.927824352306416e-05, Test Loss: 0.00010100904475514525\n",
            "Epoch 456/1000, Training Loss: 9.808425389113447e-05, Test Loss: 9.980081592548901e-05\n",
            "Epoch 457/1000, Training Loss: 9.691875844328689e-05, Test Loss: 9.862180686782641e-05\n",
            "Epoch 458/1000, Training Loss: 9.578098467629853e-05, Test Loss: 9.747121285822264e-05\n",
            "Epoch 459/1000, Training Loss: 9.467018196762567e-05, Test Loss: 9.634825216111329e-05\n",
            "Epoch 460/1000, Training Loss: 9.358562095634216e-05, Test Loss: 9.525216538089751e-05\n",
            "Epoch 461/1000, Training Loss: 9.252659294019244e-05, Test Loss: 9.418221482989076e-05\n",
            "Epoch 462/1000, Training Loss: 9.149240928847431e-05, Test Loss: 9.313768391240745e-05\n",
            "Epoch 463/1000, Training Loss: 9.048240087047446e-05, Test Loss: 9.211787652473185e-05\n",
            "Epoch 464/1000, Training Loss: 8.949591749914421e-05, Test Loss: 9.1122116470699e-05\n",
            "Epoch 465/1000, Training Loss: 8.853232738972508e-05, Test Loss: 9.014974689263355e-05\n",
            "Epoch 466/1000, Training Loss: 8.75910166330241e-05, Test Loss: 8.920012971737012e-05\n",
            "Epoch 467/1000, Training Loss: 8.667138868302367e-05, Test Loss: 8.827264511706786e-05\n",
            "Epoch 468/1000, Training Loss: 8.577286385853079e-05, Test Loss: 8.736669098454461e-05\n",
            "Epoch 469/1000, Training Loss: 8.489487885854635e-05, Test Loss: 8.648168242281892e-05\n",
            "Epoch 470/1000, Training Loss: 8.403688629105215e-05, Test Loss: 8.561705124859096e-05\n",
            "Epoch 471/1000, Training Loss: 8.319835421490838e-05, Test Loss: 8.477224550934496e-05\n",
            "Epoch 472/1000, Training Loss: 8.237876569454953e-05, Test Loss: 8.394672901379054e-05\n",
            "Epoch 473/1000, Training Loss: 8.157761836718474e-05, Test Loss: 8.313998087534051e-05\n",
            "Epoch 474/1000, Training Loss: 8.079442402218874e-05, Test Loss: 8.235149506832565e-05\n",
            "Epoch 475/1000, Training Loss: 8.002870819240319e-05, Test Loss: 8.158077999666448e-05\n",
            "Epoch 476/1000, Training Loss: 7.92800097570239e-05, Test Loss: 8.082735807466621e-05\n",
            "Epoch 477/1000, Training Loss: 7.854788055580802e-05, Test Loss: 8.009076531970202e-05\n",
            "Epoch 478/1000, Training Loss: 7.7831885014303e-05, Test Loss: 7.937055095644611e-05\n",
            "Epoch 479/1000, Training Loss: 7.713159977981019e-05, Test Loss: 7.8666277032386e-05\n",
            "Epoch 480/1000, Training Loss: 7.644661336781305e-05, Test Loss: 7.797751804434881e-05\n",
            "Epoch 481/1000, Training Loss: 7.577652581857786e-05, Test Loss: 7.730386057572384e-05\n",
            "Epoch 482/1000, Training Loss: 7.512094836367967e-05, Test Loss: 7.664490294414677e-05\n",
            "Epoch 483/1000, Training Loss: 7.447950310216662e-05, Test Loss: 7.600025485933939e-05\n",
            "Epoch 484/1000, Training Loss: 7.385182268611595e-05, Test Loss: 7.536953709085916e-05\n",
            "Epoch 485/1000, Training Loss: 7.323755001532422e-05, Test Loss: 7.47523811454885e-05\n",
            "Epoch 486/1000, Training Loss: 7.263633794087196e-05, Test Loss: 7.41484289539998e-05\n",
            "Epoch 487/1000, Training Loss: 7.20478489773361e-05, Test Loss: 7.355733256705666e-05\n",
            "Epoch 488/1000, Training Loss: 7.147175502339077e-05, Test Loss: 7.297875385998854e-05\n",
            "Epoch 489/1000, Training Loss: 7.090773709057914e-05, Test Loss: 7.241236424620263e-05\n",
            "Epoch 490/1000, Training Loss: 7.035548504001113e-05, Test Loss: 7.185784439899109e-05\n",
            "Epoch 491/1000, Training Loss: 6.981469732678274e-05, Test Loss: 7.131488398151009e-05\n",
            "Epoch 492/1000, Training Loss: 6.928508075187491e-05, Test Loss: 7.078318138468017e-05\n",
            "Epoch 493/1000, Training Loss: 6.876635022135063e-05, Test Loss: 7.026244347281911e-05\n",
            "Epoch 494/1000, Training Loss: 6.82582285126013e-05, Test Loss: 6.975238533674833e-05\n",
            "Epoch 495/1000, Training Loss: 6.776044604747543e-05, Test Loss: 6.925273005419367e-05\n",
            "Epoch 496/1000, Training Loss: 6.727274067207909e-05, Test Loss: 6.876320845725634e-05\n",
            "Epoch 497/1000, Training Loss: 6.679485744304519e-05, Test Loss: 6.828355890675639e-05\n",
            "Epoch 498/1000, Training Loss: 6.632654842009895e-05, Test Loss: 6.781352707323881e-05\n",
            "Epoch 499/1000, Training Loss: 6.586757246473134e-05, Test Loss: 6.735286572447754e-05\n",
            "Epoch 500/1000, Training Loss: 6.541769504480136e-05, Test Loss: 6.690133451925755e-05\n",
            "Epoch 501/1000, Training Loss: 6.497668804489148e-05, Test Loss: 6.645869980726821e-05\n",
            "Epoch 502/1000, Training Loss: 6.454432958225543e-05, Test Loss: 6.602473443493191e-05\n",
            "Epoch 503/1000, Training Loss: 6.412040382818231e-05, Test Loss: 6.55992175569812e-05\n",
            "Epoch 504/1000, Training Loss: 6.370470083463479e-05, Test Loss: 6.518193445363831e-05\n",
            "Epoch 505/1000, Training Loss: 6.329701636598551e-05, Test Loss: 6.477267635320089e-05\n",
            "Epoch 506/1000, Training Loss: 6.28971517357173e-05, Test Loss: 6.437124025989913e-05\n",
            "Epoch 507/1000, Training Loss: 6.250491364793267e-05, Test Loss: 6.397742878686393e-05\n",
            "Epoch 508/1000, Training Loss: 6.21201140435312e-05, Test Loss: 6.359104999404475e-05\n",
            "Epoch 509/1000, Training Loss: 6.174256995091741e-05, Test Loss: 6.321191723093731e-05\n",
            "Epoch 510/1000, Training Loss: 6.137210334110703e-05, Test Loss: 6.283984898398835e-05\n",
            "Epoch 511/1000, Training Loss: 6.100854098708863e-05, Test Loss: 6.247466872850447e-05\n",
            "Epoch 512/1000, Training Loss: 6.0651714327334486e-05, Test Loss: 6.211620478497956e-05\n",
            "Epoch 513/1000, Training Loss: 6.030145933331633e-05, Test Loss: 6.176429017966181e-05\n",
            "Epoch 514/1000, Training Loss: 5.995761638092068e-05, Test Loss: 6.141876250926614e-05\n",
            "Epoch 515/1000, Training Loss: 5.9620030125640354e-05, Test Loss: 6.107946380969198e-05\n",
            "Epoch 516/1000, Training Loss: 5.928854938143196e-05, Test Loss: 6.074624042863244e-05\n",
            "Epoch 517/1000, Training Loss: 5.8963027003128796e-05, Test Loss: 6.041894290195207e-05\n",
            "Epoch 518/1000, Training Loss: 5.86433197723011e-05, Test Loss: 6.009742583372576e-05\n",
            "Epoch 519/1000, Training Loss: 5.832928828646919e-05, Test Loss: 5.978154777983583e-05\n",
            "Epoch 520/1000, Training Loss: 5.802079685155115e-05, Test Loss: 5.947117113499045e-05\n",
            "Epoch 521/1000, Training Loss: 5.7717713377467975e-05, Test Loss: 5.916616202310272e-05\n",
            "Epoch 522/1000, Training Loss: 5.741990927680424e-05, Test Loss: 5.886639019090345e-05\n",
            "Epoch 523/1000, Training Loss: 5.712725936642484e-05, Test Loss: 5.857172890468422e-05\n",
            "Epoch 524/1000, Training Loss: 5.683964177196776e-05, Test Loss: 5.828205485009461e-05\n",
            "Epoch 525/1000, Training Loss: 5.655693783513001e-05, Test Loss: 5.799724803489828e-05\n",
            "Epoch 526/1000, Training Loss: 5.627903202365182e-05, Test Loss: 5.771719169458835e-05\n",
            "Epoch 527/1000, Training Loss: 5.600581184392801e-05, Test Loss: 5.744177220078009e-05\n",
            "Epoch 528/1000, Training Loss: 5.573716775616411e-05, Test Loss: 5.7170878972310135e-05\n",
            "Epoch 529/1000, Training Loss: 5.547299309199562e-05, Test Loss: 5.6904404388932546e-05\n",
            "Epoch 530/1000, Training Loss: 5.5213183974512506e-05, Test Loss: 5.664224370756743e-05\n",
            "Epoch 531/1000, Training Loss: 5.495763924059551e-05, Test Loss: 5.6384294980998863e-05\n",
            "Epoch 532/1000, Training Loss: 5.470626036551355e-05, Test Loss: 5.613045897895833e-05\n",
            "Epoch 533/1000, Training Loss: 5.445895138970458e-05, Test Loss: 5.588063911152544e-05\n",
            "Epoch 534/1000, Training Loss: 5.421561884767884e-05, Test Loss: 5.563474135476904e-05\n",
            "Epoch 535/1000, Training Loss: 5.397617169897502e-05, Test Loss: 5.5392674178561665e-05\n",
            "Epoch 536/1000, Training Loss: 5.374052126111801e-05, Test Loss: 5.515434847651074e-05\n",
            "Epoch 537/1000, Training Loss: 5.3508581144508394e-05, Test Loss: 5.4919677497929964e-05\n",
            "Epoch 538/1000, Training Loss: 5.328026718919403e-05, Test Loss: 5.468857678180118e-05\n",
            "Epoch 539/1000, Training Loss: 5.305549740346154e-05, Test Loss: 5.446096409265792e-05\n",
            "Epoch 540/1000, Training Loss: 5.283419190419515e-05, Test Loss: 5.423675935833731e-05\n",
            "Epoch 541/1000, Training Loss: 5.261627285895671e-05, Test Loss: 5.4015884609549624e-05\n",
            "Epoch 542/1000, Training Loss: 5.240166442972187e-05, Test Loss: 5.379826392119162e-05\n",
            "Epoch 543/1000, Training Loss: 5.219029271823716e-05, Test Loss: 5.3583823355378666e-05\n",
            "Epoch 544/1000, Training Loss: 5.198208571294334e-05, Test Loss: 5.337249090612451e-05\n",
            "Epoch 545/1000, Training Loss: 5.1776973237414264e-05, Test Loss: 5.316419644561962e-05\n",
            "Epoch 546/1000, Training Loss: 5.157488690027594e-05, Test Loss: 5.295887167207331e-05\n",
            "Epoch 547/1000, Training Loss: 5.137576004655732e-05, Test Loss: 5.275645005906482e-05\n",
            "Epoch 548/1000, Training Loss: 5.1179527710426225e-05, Test Loss: 5.255686680634985e-05\n",
            "Epoch 549/1000, Training Loss: 5.0986126569274283e-05, Test Loss: 5.236005879209743e-05\n",
            "Epoch 550/1000, Training Loss: 5.079549489911133e-05, Test Loss: 5.216596452649796e-05\n",
            "Epoch 551/1000, Training Loss: 5.0607572531230214e-05, Test Loss: 5.197452410671546e-05\n",
            "Epoch 552/1000, Training Loss: 5.0422300810098455e-05, Test Loss: 5.178567917312501e-05\n",
            "Epoch 553/1000, Training Loss: 5.0239622552450175e-05, Test Loss: 5.159937286682299e-05\n",
            "Epoch 554/1000, Training Loss: 5.005948200753561e-05, Test Loss: 5.1415549788343745e-05\n",
            "Epoch 555/1000, Training Loss: 4.988182481849736e-05, Test Loss: 5.123415595757416e-05\n",
            "Epoch 556/1000, Training Loss: 4.9706597984840954e-05, Test Loss: 5.105513877480767e-05\n",
            "Epoch 557/1000, Training Loss: 4.953374982596236e-05, Test Loss: 5.087844698291786e-05\n",
            "Epoch 558/1000, Training Loss: 4.9363229945708885e-05, Test Loss: 5.070403063061374e-05\n",
            "Epoch 559/1000, Training Loss: 4.9194989197937426e-05, Test Loss: 5.053184103674715e-05\n",
            "Epoch 560/1000, Training Loss: 4.902897965304375e-05, Test Loss: 5.03618307556326e-05\n",
            "Epoch 561/1000, Training Loss: 4.8865154565432414e-05, Test Loss: 5.0193953543364005e-05\n",
            "Epoch 562/1000, Training Loss: 4.870346834190066e-05, Test Loss: 5.002816432508678e-05\n",
            "Epoch 563/1000, Training Loss: 4.854387651091108e-05, Test Loss: 4.986441916319988e-05\n",
            "Epoch 564/1000, Training Loss: 4.838633569272426e-05, Test Loss: 4.970267522646719e-05\n",
            "Epoch 565/1000, Training Loss: 4.823080357036841e-05, Test Loss: 4.9542890759997455e-05\n",
            "Epoch 566/1000, Training Loss: 4.807723886142125e-05, Test Loss: 4.938502505608791e-05\n",
            "Epoch 567/1000, Training Loss: 4.792560129057881e-05, Test Loss: 4.922903842587997e-05\n",
            "Epoch 568/1000, Training Loss: 4.7775851562991396e-05, Test Loss: 4.907489217182898e-05\n",
            "Epoch 569/1000, Training Loss: 4.762795133834061e-05, Test Loss: 4.892254856094453e-05\n",
            "Epoch 570/1000, Training Loss: 4.748186320564137e-05, Test Loss: 4.877197079879375e-05\n",
            "Epoch 571/1000, Training Loss: 4.733755065873963e-05, Test Loss: 4.862312300422767e-05\n",
            "Epoch 572/1000, Training Loss: 4.719497807249409e-05, Test Loss: 4.847597018483034e-05\n",
            "Epoch 573/1000, Training Loss: 4.705411067961886e-05, Test Loss: 4.833047821305488e-05\n",
            "Epoch 574/1000, Training Loss: 4.691491454816449e-05, Test Loss: 4.818661380302732e-05\n",
            "Epoch 575/1000, Training Loss: 4.677735655962534e-05, Test Loss: 4.8044344488008556e-05\n",
            "Epoch 576/1000, Training Loss: 4.664140438765088e-05, Test Loss: 4.7903638598485194e-05\n",
            "Epoch 577/1000, Training Loss: 4.6507026477343545e-05, Test Loss: 4.776446524086892e-05\n",
            "Epoch 578/1000, Training Loss: 4.637419202512903e-05, Test Loss: 4.7626794276806186e-05\n",
            "Epoch 579/1000, Training Loss: 4.624287095917938e-05, Test Loss: 4.749059630305317e-05\n",
            "Epoch 580/1000, Training Loss: 4.611303392037811e-05, Test Loss: 4.735584263191977e-05\n",
            "Epoch 581/1000, Training Loss: 4.5984652243801414e-05, Test Loss: 4.722250527225563e-05\n",
            "Epoch 582/1000, Training Loss: 4.585769794071529e-05, Test Loss: 4.709055691097435e-05\n",
            "Epoch 583/1000, Training Loss: 4.57321436810611e-05, Test Loss: 4.695997089507642e-05\n",
            "Epoch 584/1000, Training Loss: 4.5607962776421364e-05, Test Loss: 4.683072121419164e-05\n",
            "Epoch 585/1000, Training Loss: 4.548512916345169e-05, Test Loss: 4.670278248358895e-05\n",
            "Epoch 586/1000, Training Loss: 4.536361738776732e-05, Test Loss: 4.657612992767229e-05\n",
            "Epoch 587/1000, Training Loss: 4.5243402588266144e-05, Test Loss: 4.645073936392439e-05\n",
            "Epoch 588/1000, Training Loss: 4.512446048187832e-05, Test Loss: 4.6326587187298225e-05\n",
            "Epoch 589/1000, Training Loss: 4.500676734873325e-05, Test Loss: 4.6203650355040205e-05\n",
            "Epoch 590/1000, Training Loss: 4.489030001772932e-05, Test Loss: 4.6081906371935765e-05\n",
            "Epoch 591/1000, Training Loss: 4.477503585249421e-05, Test Loss: 4.596133327595628e-05\n",
            "Epoch 592/1000, Training Loss: 4.4660952737725805e-05, Test Loss: 4.5841909624305895e-05\n",
            "Epoch 593/1000, Training Loss: 4.454802906590411e-05, Test Loss: 4.572361447985353e-05\n",
            "Epoch 594/1000, Training Loss: 4.443624372436113e-05, Test Loss: 4.5606427397934044e-05\n",
            "Epoch 595/1000, Training Loss: 4.432557608270193e-05, Test Loss: 4.549032841351903e-05\n",
            "Epoch 596/1000, Training Loss: 4.421600598056265e-05, Test Loss: 4.53752980287293e-05\n",
            "Epoch 597/1000, Training Loss: 4.4107513715699215e-05, Test Loss: 4.526131720069949e-05\n",
            "Epoch 598/1000, Training Loss: 4.400008003239829e-05, Test Loss: 4.5148367329772836e-05\n",
            "Epoch 599/1000, Training Loss: 4.389368611019857e-05, Test Loss: 4.503643024801604e-05\n",
            "Epoch 600/1000, Training Loss: 4.378831355291292e-05, Test Loss: 4.492548820805002e-05\n",
            "Epoch 601/1000, Training Loss: 4.3683944377947894e-05, Test Loss: 4.4815523872186113e-05\n",
            "Epoch 602/1000, Training Loss: 4.358056100590771e-05, Test Loss: 4.470652030185589e-05\n",
            "Epoch 603/1000, Training Loss: 4.347814625047665e-05, Test Loss: 4.459846094733634e-05\n",
            "Epoch 604/1000, Training Loss: 4.337668330857228e-05, Test Loss: 4.449132963774381e-05\n",
            "Epoch 605/1000, Training Loss: 4.3276155750760607e-05, Test Loss: 4.4385110571310445e-05\n",
            "Epoch 606/1000, Training Loss: 4.317654751192533e-05, Test Loss: 4.427978830591435e-05\n",
            "Epoch 607/1000, Training Loss: 4.307784288218929e-05, Test Loss: 4.417534774987757e-05\n",
            "Epoch 608/1000, Training Loss: 4.298002649807491e-05, Test Loss: 4.407177415300835e-05\n",
            "Epoch 609/1000, Training Loss: 4.288308333389709e-05, Test Loss: 4.396905309788063e-05\n",
            "Epoch 610/1000, Training Loss: 4.278699869338914e-05, Test Loss: 4.386717049135902e-05\n",
            "Epoch 611/1000, Training Loss: 4.269175820154679e-05, Test Loss: 4.376611255634605e-05\n",
            "Epoch 612/1000, Training Loss: 4.259734779668913e-05, Test Loss: 4.3665865823754336e-05\n",
            "Epoch 613/1000, Training Loss: 4.250375372272973e-05, Test Loss: 4.356641712469645e-05\n",
            "Epoch 614/1000, Training Loss: 4.241096252165161e-05, Test Loss: 4.346775358288236e-05\n",
            "Epoch 615/1000, Training Loss: 4.231896102617965e-05, Test Loss: 4.336986260722227e-05\n",
            "Epoch 616/1000, Training Loss: 4.222773635264531e-05, Test Loss: 4.3272731884626514e-05\n",
            "Epoch 617/1000, Training Loss: 4.213727589404307e-05, Test Loss: 4.3176349373006885e-05\n",
            "Epoch 618/1000, Training Loss: 4.2047567313262996e-05, Test Loss: 4.308070329445289e-05\n",
            "Epoch 619/1000, Training Loss: 4.1958598536505034e-05, Test Loss: 4.29857821286002e-05\n",
            "Epoch 620/1000, Training Loss: 4.187035774686564e-05, Test Loss: 4.2891574606171936e-05\n",
            "Epoch 621/1000, Training Loss: 4.178283337808911e-05, Test Loss: 4.2798069702694096e-05\n",
            "Epoch 622/1000, Training Loss: 4.1696014108485154e-05, Test Loss: 4.270525663238191e-05\n",
            "Epoch 623/1000, Training Loss: 4.1609888855003734e-05, Test Loss: 4.26131248421785e-05\n",
            "Epoch 624/1000, Training Loss: 4.152444676746556e-05, Test Loss: 4.252166400596809e-05\n",
            "Epoch 625/1000, Training Loss: 4.143967722294044e-05, Test Loss: 4.2430864018927536e-05\n",
            "Epoch 626/1000, Training Loss: 4.135556982027435e-05, Test Loss: 4.2340714992039835e-05\n",
            "Epoch 627/1000, Training Loss: 4.1272114374755313e-05, Test Loss: 4.225120724674553e-05\n",
            "Epoch 628/1000, Training Loss: 4.11893009129221e-05, Test Loss: 4.216233130974152e-05\n",
            "Epoch 629/1000, Training Loss: 4.1107119667502264e-05, Test Loss: 4.207407790791085e-05\n",
            "Epoch 630/1000, Training Loss: 4.10255610724841e-05, Test Loss: 4.1986437963393505e-05\n",
            "Epoch 631/1000, Training Loss: 4.094461575831653e-05, Test Loss: 4.189940258878437e-05\n",
            "Epoch 632/1000, Training Loss: 4.086427454723009e-05, Test Loss: 4.18129630824582e-05\n",
            "Epoch 633/1000, Training Loss: 4.078452844867977e-05, Test Loss: 4.172711092401538e-05\n",
            "Epoch 634/1000, Training Loss: 4.0705368654905035e-05, Test Loss: 4.1641837769853615e-05\n",
            "Epoch 635/1000, Training Loss: 4.062678653660443e-05, Test Loss: 4.1557135448849635e-05\n",
            "Epoch 636/1000, Training Loss: 4.0548773638719925e-05, Test Loss: 4.147299595815713e-05\n",
            "Epoch 637/1000, Training Loss: 4.0471321676330334e-05, Test Loss: 4.138941145911684e-05\n",
            "Epoch 638/1000, Training Loss: 4.0394422530648965e-05, Test Loss: 4.1306374273270085e-05\n",
            "Epoch 639/1000, Training Loss: 4.031806824512322e-05, Test Loss: 4.122387687847857e-05\n",
            "Epoch 640/1000, Training Loss: 4.0242251021635775e-05, Test Loss: 4.114191190514702e-05\n",
            "Epoch 641/1000, Training Loss: 4.016696321680011e-05, Test Loss: 4.106047213254183e-05\n",
            "Epoch 642/1000, Training Loss: 4.009219733835024e-05, Test Loss: 4.097955048520568e-05\n",
            "Epoch 643/1000, Training Loss: 4.0017946041625667e-05, Test Loss: 4.089914002947025e-05\n",
            "Epoch 644/1000, Training Loss: 3.99442021261412e-05, Test Loss: 4.0819233970053465e-05\n",
            "Epoch 645/1000, Training Loss: 3.987095853224541e-05, Test Loss: 4.073982564674761e-05\n",
            "Epoch 646/1000, Training Loss: 3.979820833786478e-05, Test Loss: 4.0660908531195595e-05\n",
            "Epoch 647/1000, Training Loss: 3.9725944755328226e-05, Test Loss: 4.0582476223747176e-05\n",
            "Epoch 648/1000, Training Loss: 3.965416112827227e-05, Test Loss: 4.0504522450400695e-05\n",
            "Epoch 649/1000, Training Loss: 3.9582850928626005e-05, Test Loss: 4.042704105981991e-05\n",
            "Epoch 650/1000, Training Loss: 3.9512007753668696e-05, Test Loss: 4.035002602043046e-05\n",
            "Epoch 651/1000, Training Loss: 3.944162532316317e-05, Test Loss: 4.0273471417587996e-05\n",
            "Epoch 652/1000, Training Loss: 3.9371697476561905e-05, Test Loss: 4.019737145082675e-05\n",
            "Epoch 653/1000, Training Loss: 3.93022181702823e-05, Test Loss: 4.0121720431169125e-05\n",
            "Epoch 654/1000, Training Loss: 3.923318147504773e-05, Test Loss: 4.004651277851109e-05\n",
            "Epoch 655/1000, Training Loss: 3.916458157329971e-05, Test Loss: 3.997174301907309e-05\n",
            "Epoch 656/1000, Training Loss: 3.9096412756670856e-05, Test Loss: 3.989740578291644e-05\n",
            "Epoch 657/1000, Training Loss: 3.90286694235217e-05, Test Loss: 3.982349580152096e-05\n",
            "Epoch 658/1000, Training Loss: 3.8961346076540724e-05, Test Loss: 3.9750007905429075e-05\n",
            "Epoch 659/1000, Training Loss: 3.889443732040058e-05, Test Loss: 3.96769370219467e-05\n",
            "Epoch 660/1000, Training Loss: 3.88279378594744e-05, Test Loss: 3.960427817290219e-05\n",
            "Epoch 661/1000, Training Loss: 3.8761842495609874e-05, Test Loss: 3.9532026472465564e-05\n",
            "Epoch 662/1000, Training Loss: 3.8696146125955794e-05, Test Loss: 3.946017712502342e-05\n",
            "Epoch 663/1000, Training Loss: 3.863084374084293e-05, Test Loss: 3.9388725423100383e-05\n",
            "Epoch 664/1000, Training Loss: 3.8565930421718566e-05, Test Loss: 3.9317666745344384e-05\n",
            "Epoch 665/1000, Training Loss: 3.85014013391304e-05, Test Loss: 3.924699655455561e-05\n",
            "Epoch 666/1000, Training Loss: 3.843725175076048e-05, Test Loss: 3.9176710395765046e-05\n",
            "Epoch 667/1000, Training Loss: 3.837347699950717e-05, Test Loss: 3.910680389436812e-05\n",
            "Epoch 668/1000, Training Loss: 3.831007251161446e-05, Test Loss: 3.9037272754298435e-05\n",
            "Epoch 669/1000, Training Loss: 3.824703379484636e-05, Test Loss: 3.896811275624834e-05\n",
            "Epoch 670/1000, Training Loss: 3.8184356436706673e-05, Test Loss: 3.889931975594105e-05\n",
            "Epoch 671/1000, Training Loss: 3.812203610270201e-05, Test Loss: 3.883088968243863e-05\n",
            "Epoch 672/1000, Training Loss: 3.806006853464567e-05, Test Loss: 3.8762818536493976e-05\n",
            "Epoch 673/1000, Training Loss: 3.799844954900569e-05, Test Loss: 3.869510238894761e-05\n",
            "Epoch 674/1000, Training Loss: 3.793717503528969e-05, Test Loss: 3.8627737379160744e-05\n",
            "Epoch 675/1000, Training Loss: 3.787624095447092e-05, Test Loss: 3.8560719713489255e-05\n",
            "Epoch 676/1000, Training Loss: 3.781564333745109e-05, Test Loss: 3.849404566379706e-05\n",
            "Epoch 677/1000, Training Loss: 3.7755378283562434e-05, Test Loss: 3.842771156600309e-05\n",
            "Epoch 678/1000, Training Loss: 3.7695441959102545e-05, Test Loss: 3.836171381866701e-05\n",
            "Epoch 679/1000, Training Loss: 3.763583059590785e-05, Test Loss: 3.8296048881608777e-05\n",
            "Epoch 680/1000, Training Loss: 3.757654048995854e-05, Test Loss: 3.8230713274564526e-05\n",
            "Epoch 681/1000, Training Loss: 3.7517568000019476e-05, Test Loss: 3.8165703575873904e-05\n",
            "Epoch 682/1000, Training Loss: 3.745890954631108e-05, Test Loss: 3.8101016421199856e-05\n",
            "Epoch 683/1000, Training Loss: 3.740056160921421e-05, Test Loss: 3.803664850228003e-05\n",
            "Epoch 684/1000, Training Loss: 3.734252072800414e-05, Test Loss: 3.7972596565708144e-05\n",
            "Epoch 685/1000, Training Loss: 3.7284783499616034e-05, Test Loss: 3.790885741175202e-05\n",
            "Epoch 686/1000, Training Loss: 3.722734657743885e-05, Test Loss: 3.784542789318954e-05\n",
            "Epoch 687/1000, Training Loss: 3.717020667013878e-05, Test Loss: 3.778230491418005e-05\n",
            "Epoch 688/1000, Training Loss: 3.711336054050973e-05, Test Loss: 3.7719485429165095e-05\n",
            "Epoch 689/1000, Training Loss: 3.705680500435036e-05, Test Loss: 3.765696644178909e-05\n",
            "Epoch 690/1000, Training Loss: 3.700053692936996e-05, Test Loss: 3.7594745003854384e-05\n",
            "Epoch 691/1000, Training Loss: 3.694455323411772e-05, Test Loss: 3.753281821429474e-05\n",
            "Epoch 692/1000, Training Loss: 3.68888508869381e-05, Test Loss: 3.7471183218178436e-05\n",
            "Epoch 693/1000, Training Loss: 3.683342690495087e-05, Test Loss: 3.740983720573501e-05\n",
            "Epoch 694/1000, Training Loss: 3.677827835305453e-05, Test Loss: 3.734877741140296e-05\n",
            "Epoch 695/1000, Training Loss: 3.672340234295441e-05, Test Loss: 3.728800111290271e-05\n",
            "Epoch 696/1000, Training Loss: 3.666879603221056e-05, Test Loss: 3.72275056303311e-05\n",
            "Epoch 697/1000, Training Loss: 3.661445662331161e-05, Test Loss: 3.7167288325279225e-05\n",
            "Epoch 698/1000, Training Loss: 3.656038136276675e-05, Test Loss: 3.710734659997056e-05\n",
            "Epoch 699/1000, Training Loss: 3.650656754022061e-05, Test Loss: 3.7047677896419235e-05\n",
            "Epoch 700/1000, Training Loss: 3.645301248758849e-05, Test Loss: 3.698827969560812e-05\n",
            "Epoch 701/1000, Training Loss: 3.6399713578210825e-05, Test Loss: 3.692914951668917e-05\n",
            "Epoch 702/1000, Training Loss: 3.63466682260282e-05, Test Loss: 3.687028491619795e-05\n",
            "Epoch 703/1000, Training Loss: 3.6293873884773965e-05, Test Loss: 3.681168348729506e-05\n",
            "Epoch 704/1000, Training Loss: 3.62413280471876e-05, Test Loss: 3.675334285901746e-05\n",
            "Epoch 705/1000, Training Loss: 3.618902824424299e-05, Test Loss: 3.66952606955515e-05\n",
            "Epoch 706/1000, Training Loss: 3.613697204439717e-05, Test Loss: 3.6637434695522615e-05\n",
            "Epoch 707/1000, Training Loss: 3.608515705285498e-05, Test Loss: 3.6579862591301925e-05\n",
            "Epoch 708/1000, Training Loss: 3.603358091085112e-05, Test Loss: 3.652254214832968e-05\n",
            "Epoch 709/1000, Training Loss: 3.5982241294946116e-05, Test Loss: 3.646547116445267e-05\n",
            "Epoch 710/1000, Training Loss: 3.5931135916342924e-05, Test Loss: 3.640864746928015e-05\n",
            "Epoch 711/1000, Training Loss: 3.588026252021373e-05, Test Loss: 3.635206892355318e-05\n",
            "Epoch 712/1000, Training Loss: 3.582961888504599e-05, Test Loss: 3.629573341852741e-05\n",
            "Epoch 713/1000, Training Loss: 3.577920282200196e-05, Test Loss: 3.623963887537644e-05\n",
            "Epoch 714/1000, Training Loss: 3.5729012174290645e-05, Test Loss: 3.618378324459891e-05\n",
            "Epoch 715/1000, Training Loss: 3.567904481655744e-05, Test Loss: 3.6128164505449e-05\n",
            "Epoch 716/1000, Training Loss: 3.5629298654284585e-05, Test Loss: 3.607278066537424e-05\n",
            "Epoch 717/1000, Training Loss: 3.5579771623206336e-05, Test Loss: 3.6017629759469525e-05\n",
            "Epoch 718/1000, Training Loss: 3.553046168873657e-05, Test Loss: 3.5962709849944516e-05\n",
            "Epoch 719/1000, Training Loss: 3.54813668454096e-05, Test Loss: 3.590801902559669e-05\n",
            "Epoch 720/1000, Training Loss: 3.5432485116333533e-05, Test Loss: 3.585355540130735e-05\n",
            "Epoch 721/1000, Training Loss: 3.538381455265503e-05, Test Loss: 3.579931711753837e-05\n",
            "Epoch 722/1000, Training Loss: 3.533535323303538e-05, Test Loss: 3.574530233984712e-05\n",
            "Epoch 723/1000, Training Loss: 3.5287099263141065e-05, Test Loss: 3.569150925841289e-05\n",
            "Epoch 724/1000, Training Loss: 3.523905077514206e-05, Test Loss: 3.5637936087568216e-05\n",
            "Epoch 725/1000, Training Loss: 3.519120592722312e-05, Test Loss: 3.558458106534973e-05\n",
            "Epoch 726/1000, Training Loss: 3.514356290310478e-05, Test Loss: 3.553144245304893e-05\n",
            "Epoch 727/1000, Training Loss: 3.5096119911575106e-05, Test Loss: 3.547851853478175e-05\n",
            "Epoch 728/1000, Training Loss: 3.5048875186032803e-05, Test Loss: 3.542580761706481e-05\n",
            "Epoch 729/1000, Training Loss: 3.5001826984038025e-05, Test Loss: 3.537330802839849e-05\n",
            "Epoch 730/1000, Training Loss: 3.4954973586875064e-05, Test Loss: 3.5321018118865535e-05\n",
            "Epoch 731/1000, Training Loss: 3.49083132991233e-05, Test Loss: 3.526893625973282e-05\n",
            "Epoch 732/1000, Training Loss: 3.486184444823736e-05, Test Loss: 3.5217060843064174e-05\n",
            "Epoch 733/1000, Training Loss: 3.4815565384136716e-05, Test Loss: 3.516539028134296e-05\n",
            "Epoch 734/1000, Training Loss: 3.476947447880511e-05, Test Loss: 3.511392300710185e-05\n",
            "Epoch 735/1000, Training Loss: 3.472357012589592e-05, Test Loss: 3.506265747256382e-05\n",
            "Epoch 736/1000, Training Loss: 3.4677850740348536e-05, Test Loss: 3.501159214928291e-05\n",
            "Epoch 737/1000, Training Loss: 3.463231475801131e-05, Test Loss: 3.4960725527803076e-05\n",
            "Epoch 738/1000, Training Loss: 3.458696063527344e-05, Test Loss: 3.491005611731891e-05\n",
            "Epoch 739/1000, Training Loss: 3.454178684870465e-05, Test Loss: 3.485958244534571e-05\n",
            "Epoch 740/1000, Training Loss: 3.4496791894701244e-05, Test Loss: 3.480930305739333e-05\n",
            "Epoch 741/1000, Training Loss: 3.445197428914134e-05, Test Loss: 3.4759216516653216e-05\n",
            "Epoch 742/1000, Training Loss: 3.440733256704643e-05, Test Loss: 3.470932140368643e-05\n",
            "Epoch 743/1000, Training Loss: 3.436286528224954e-05, Test Loss: 3.465961631612354e-05\n",
            "Epoch 744/1000, Training Loss: 3.431857100707252e-05, Test Loss: 3.46100998683671e-05\n",
            "Epoch 745/1000, Training Loss: 3.427444833200676e-05, Test Loss: 3.4560770691301936e-05\n",
            "Epoch 746/1000, Training Loss: 3.423049586540325e-05, Test Loss: 3.451162743201169e-05\n",
            "Epoch 747/1000, Training Loss: 3.4186712233168426e-05, Test Loss: 3.44626687535047e-05\n",
            "Epoch 748/1000, Training Loss: 3.414309607846534e-05, Test Loss: 3.441389333443758e-05\n",
            "Epoch 749/1000, Training Loss: 3.4099646061422134e-05, Test Loss: 3.436529986885213e-05\n",
            "Epoch 750/1000, Training Loss: 3.405636085884609e-05, Test Loss: 3.4316887065918395e-05\n",
            "Epoch 751/1000, Training Loss: 3.401323916394334e-05, Test Loss: 3.426865364967636e-05\n",
            "Epoch 752/1000, Training Loss: 3.3970279686044115e-05, Test Loss: 3.422059835878866e-05\n",
            "Epoch 753/1000, Training Loss: 3.392748115033566e-05, Test Loss: 3.417271994629865e-05\n",
            "Epoch 754/1000, Training Loss: 3.388484229759775e-05, Test Loss: 3.4125017179388896e-05\n",
            "Epoch 755/1000, Training Loss: 3.384236188394386e-05, Test Loss: 3.4077488839150487e-05\n",
            "Epoch 756/1000, Training Loss: 3.380003868057077e-05, Test Loss: 3.4030133720353595e-05\n",
            "Epoch 757/1000, Training Loss: 3.375787147350954e-05, Test Loss: 3.398295063122528e-05\n",
            "Epoch 758/1000, Training Loss: 3.371585906338251e-05, Test Loss: 3.3935938393227055e-05\n",
            "Epoch 759/1000, Training Loss: 3.367400026516716e-05, Test Loss: 3.3889095840844586e-05\n",
            "Epoch 760/1000, Training Loss: 3.363229390796201e-05, Test Loss: 3.3842421821374866e-05\n",
            "Epoch 761/1000, Training Loss: 3.3590738834758383e-05, Test Loss: 3.379591519472276e-05\n",
            "Epoch 762/1000, Training Loss: 3.354933390221696e-05, Test Loss: 3.374957483319695e-05\n",
            "Epoch 763/1000, Training Loss: 3.350807798044879e-05, Test Loss: 3.3703399621316906e-05\n",
            "Epoch 764/1000, Training Loss: 3.346696995279972e-05, Test Loss: 3.365738845561451e-05\n",
            "Epoch 765/1000, Training Loss: 3.342600871563939e-05, Test Loss: 3.361154024444985e-05\n",
            "Epoch 766/1000, Training Loss: 3.338519317815603e-05, Test Loss: 3.356585390782178e-05\n",
            "Epoch 767/1000, Training Loss: 3.334452226215268e-05, Test Loss: 3.352032837719028e-05\n",
            "Epoch 768/1000, Training Loss: 3.330399490184895e-05, Test Loss: 3.347496259529471e-05\n",
            "Epoch 769/1000, Training Loss: 3.32636100436869e-05, Test Loss: 3.3429755515981944e-05\n",
            "Epoch 770/1000, Training Loss: 3.322336664613933e-05, Test Loss: 3.3384706104036424e-05\n",
            "Epoch 771/1000, Training Loss: 3.318326367952329e-05, Test Loss: 3.333981333500964e-05\n",
            "Epoch 772/1000, Training Loss: 3.31433001258164e-05, Test Loss: 3.329507619505862e-05\n",
            "Epoch 773/1000, Training Loss: 3.310347497847601e-05, Test Loss: 3.325049368078372e-05\n",
            "Epoch 774/1000, Training Loss: 3.306378724226372e-05, Test Loss: 3.32060647990725e-05\n",
            "Epoch 775/1000, Training Loss: 3.3024235933071454e-05, Test Loss: 3.3161788566944175e-05\n",
            "Epoch 776/1000, Training Loss: 3.298482007775203e-05, Test Loss: 3.3117664011399144e-05\n",
            "Epoch 777/1000, Training Loss: 3.2945538713952655e-05, Test Loss: 3.307369016926976e-05\n",
            "Epoch 778/1000, Training Loss: 3.2906390889950934e-05, Test Loss: 3.3029866087074676e-05\n",
            "Epoch 779/1000, Training Loss: 3.286737566449453e-05, Test Loss: 3.298619082087703e-05\n",
            "Epoch 780/1000, Training Loss: 3.282849210664384e-05, Test Loss: 3.294266343614348e-05\n",
            "Epoch 781/1000, Training Loss: 3.2789739295618485e-05, Test Loss: 3.289928300760944e-05\n",
            "Epoch 782/1000, Training Loss: 3.275111632064447e-05, Test Loss: 3.285604861914145e-05\n",
            "Epoch 783/1000, Training Loss: 3.2712622280806305e-05, Test Loss: 3.281295936360509e-05\n",
            "Epoch 784/1000, Training Loss: 3.267425628490101e-05, Test Loss: 3.277001434273971e-05\n",
            "Epoch 785/1000, Training Loss: 3.2636017451294876e-05, Test Loss: 3.2727212667027916e-05\n",
            "Epoch 786/1000, Training Loss: 3.2597904907782244e-05, Test Loss: 3.2684553455571324e-05\n",
            "Epoch 787/1000, Training Loss: 3.255991779144887e-05, Test Loss: 3.264203583596895e-05\n",
            "Epoch 788/1000, Training Loss: 3.252205524853571e-05, Test Loss: 3.259965894419837e-05\n",
            "Epoch 789/1000, Training Loss: 3.2484316434305756e-05, Test Loss: 3.2557421924495295e-05\n",
            "Epoch 790/1000, Training Loss: 3.244670051291466e-05, Test Loss: 3.2515323929242495e-05\n",
            "Epoch 791/1000, Training Loss: 3.240920665728095e-05, Test Loss: 3.2473364118852344e-05\n",
            "Epoch 792/1000, Training Loss: 3.237183404896172e-05, Test Loss: 3.243154166165773e-05\n",
            "Epoch 793/1000, Training Loss: 3.233458187802833e-05, Test Loss: 3.238985573380248e-05\n",
            "Epoch 794/1000, Training Loss: 3.229744934294537e-05, Test Loss: 3.23483055191346e-05\n",
            "Epoch 795/1000, Training Loss: 3.226043565045148e-05, Test Loss: 3.230689020910117e-05\n",
            "Epoch 796/1000, Training Loss: 3.222354001544262e-05, Test Loss: 3.226560900264587e-05\n",
            "Epoch 797/1000, Training Loss: 3.218676166085642e-05, Test Loss: 3.222446110610333e-05\n",
            "Epoch 798/1000, Training Loss: 3.215009981756057e-05, Test Loss: 3.2183445733107894e-05\n",
            "Epoch 799/1000, Training Loss: 3.211355372424075e-05, Test Loss: 3.214256210448861e-05\n",
            "Epoch 800/1000, Training Loss: 3.207712262729262e-05, Test Loss: 3.210180944817605e-05\n",
            "Epoch 801/1000, Training Loss: 3.2040805780714306e-05, Test Loss: 3.206118699910873e-05\n",
            "Epoch 802/1000, Training Loss: 3.20046024460016e-05, Test Loss: 3.202069399913965e-05\n",
            "Epoch 803/1000, Training Loss: 3.196851189204479e-05, Test Loss: 3.19803296969461e-05\n",
            "Epoch 804/1000, Training Loss: 3.193253339502741e-05, Test Loss: 3.194009334794024e-05\n",
            "Epoch 805/1000, Training Loss: 3.1896666238326045e-05, Test Loss: 3.1899984214181495e-05\n",
            "Epoch 806/1000, Training Loss: 3.1860909712412256e-05, Test Loss: 3.186000156428874e-05\n",
            "Epoch 807/1000, Training Loss: 3.182526311475731e-05, Test Loss: 3.1820144673359674e-05\n",
            "Epoch 808/1000, Training Loss: 3.1789725749736685e-05, Test Loss: 3.178041282288358e-05\n",
            "Epoch 809/1000, Training Loss: 3.175429692853734e-05, Test Loss: 3.174080530066211e-05\n",
            "Epoch 810/1000, Training Loss: 3.17189759690666e-05, Test Loss: 3.1701321400729293e-05\n",
            "Epoch 811/1000, Training Loss: 3.168376219586206e-05, Test Loss: 3.166196042327013e-05\n",
            "Epoch 812/1000, Training Loss: 3.164865494000361e-05, Test Loss: 3.162272167454616e-05\n",
            "Epoch 813/1000, Training Loss: 3.161365353902615e-05, Test Loss: 3.1583604466817155e-05\n",
            "Epoch 814/1000, Training Loss: 3.157875733683432e-05, Test Loss: 3.154460811826618e-05\n",
            "Epoch 815/1000, Training Loss: 3.154396568361936e-05, Test Loss: 3.150573195292827e-05\n",
            "Epoch 816/1000, Training Loss: 3.1509277935775624e-05, Test Loss: 3.146697530061447e-05\n",
            "Epoch 817/1000, Training Loss: 3.1474693455820114e-05, Test Loss: 3.142833749684412e-05\n",
            "Epoch 818/1000, Training Loss: 3.144021161231219e-05, Test Loss: 3.138981788277284e-05\n",
            "Epoch 819/1000, Training Loss: 3.14058317797758e-05, Test Loss: 3.13514158051255e-05\n",
            "Epoch 820/1000, Training Loss: 3.137155333862228e-05, Test Loss: 3.131313061612774e-05\n",
            "Epoch 821/1000, Training Loss: 3.133737567507339e-05, Test Loss: 3.127496167343717e-05\n",
            "Epoch 822/1000, Training Loss: 3.1303298181087875e-05, Test Loss: 3.1236908340080605e-05\n",
            "Epoch 823/1000, Training Loss: 3.126932025428817e-05, Test Loss: 3.1198969984389564e-05\n",
            "Epoch 824/1000, Training Loss: 3.123544129788705e-05, Test Loss: 3.116114597993683e-05\n",
            "Epoch 825/1000, Training Loss: 3.120166072061783e-05, Test Loss: 3.112343570547274e-05\n",
            "Epoch 826/1000, Training Loss: 3.116797793666318e-05, Test Loss: 3.108583854486555e-05\n",
            "Epoch 827/1000, Training Loss: 3.113439236558825e-05, Test Loss: 3.104835388704071e-05\n",
            "Epoch 828/1000, Training Loss: 3.11009034322708e-05, Test Loss: 3.101098112592168e-05\n",
            "Epoch 829/1000, Training Loss: 3.106751056683637e-05, Test Loss: 3.097371966036975e-05\n",
            "Epoch 830/1000, Training Loss: 3.103421320459212e-05, Test Loss: 3.093656889412967e-05\n",
            "Epoch 831/1000, Training Loss: 3.100101078596243e-05, Test Loss: 3.08995282357722e-05\n",
            "Epoch 832/1000, Training Loss: 3.096790275642519e-05, Test Loss: 3.0862597098636075e-05\n",
            "Epoch 833/1000, Training Loss: 3.093488856645008e-05, Test Loss: 3.082577490077646e-05\n",
            "Epoch 834/1000, Training Loss: 3.090196767143661e-05, Test Loss: 3.0789061064909435e-05\n",
            "Epoch 835/1000, Training Loss: 3.086913953165308e-05, Test Loss: 3.075245501835758e-05\n",
            "Epoch 836/1000, Training Loss: 3.083640361217922e-05, Test Loss: 3.0715956193002855e-05\n",
            "Epoch 837/1000, Training Loss: 3.080375938284469e-05, Test Loss: 3.0679564025228075e-05\n",
            "Epoch 838/1000, Training Loss: 3.0771206318173545e-05, Test Loss: 3.064327795587295e-05\n",
            "Epoch 839/1000, Training Loss: 3.073874389732658e-05, Test Loss: 3.060709743017917e-05\n",
            "Epoch 840/1000, Training Loss: 3.07063716040455e-05, Test Loss: 3.0571021897743326e-05\n",
            "Epoch 841/1000, Training Loss: 3.0674088926597746e-05, Test Loss: 3.0535050812469146e-05\n",
            "Epoch 842/1000, Training Loss: 3.0641895357722866e-05, Test Loss: 3.0499183632519395e-05\n",
            "Epoch 843/1000, Training Loss: 3.060979039457845e-05, Test Loss: 3.0463419820268714e-05\n",
            "Epoch 844/1000, Training Loss: 3.0577773538688326e-05, Test Loss: 3.0427758842258536e-05\n",
            "Epoch 845/1000, Training Loss: 3.054584429588972e-05, Test Loss: 3.0392200169147933e-05\n",
            "Epoch 846/1000, Training Loss: 3.0514002176283303e-05, Test Loss: 3.0356743275674233e-05\n",
            "Epoch 847/1000, Training Loss: 3.0482246694182213e-05, Test Loss: 3.0321387640604067e-05\n",
            "Epoch 848/1000, Training Loss: 3.045057736806328e-05, Test Loss: 3.0286132746693535e-05\n",
            "Epoch 849/1000, Training Loss: 3.041899372051828e-05, Test Loss: 3.0250978080642692e-05\n",
            "Epoch 850/1000, Training Loss: 3.0387495278204614e-05, Test Loss: 3.0215923133055065e-05\n",
            "Epoch 851/1000, Training Loss: 3.0356081571800542e-05, Test Loss: 3.0180967398395117e-05\n",
            "Epoch 852/1000, Training Loss: 3.0324752135955937e-05, Test Loss: 3.0146110374944945e-05\n",
            "Epoch 853/1000, Training Loss: 3.0293506509248236e-05, Test Loss: 3.0111351564769437e-05\n",
            "Epoch 854/1000, Training Loss: 3.0262344234136583e-05, Test Loss: 3.0076690473671784e-05\n",
            "Epoch 855/1000, Training Loss: 3.023126485691732e-05, Test Loss: 3.0042126611156046e-05\n",
            "Epoch 856/1000, Training Loss: 3.020026792767951e-05, Test Loss: 3.0007659490387375e-05\n",
            "Epoch 857/1000, Training Loss: 3.0169353000263547e-05, Test Loss: 2.997328862815537e-05\n",
            "Epoch 858/1000, Training Loss: 3.0138519632216357e-05, Test Loss: 2.9939013544834412e-05\n",
            "Epoch 859/1000, Training Loss: 3.0107767384750587e-05, Test Loss: 2.9904833764348463e-05\n",
            "Epoch 860/1000, Training Loss: 3.007709582270309e-05, Test Loss: 2.987074881413339e-05\n",
            "Epoch 861/1000, Training Loss: 3.00465045144939e-05, Test Loss: 2.9836758225100206e-05\n",
            "Epoch 862/1000, Training Loss: 3.001599303208604e-05, Test Loss: 2.9802861531602617e-05\n",
            "Epoch 863/1000, Training Loss: 2.9985560950945923e-05, Test Loss: 2.976905827139826e-05\n",
            "Epoch 864/1000, Training Loss: 2.9955207850004928e-05, Test Loss: 2.9735347985615552e-05\n",
            "Epoch 865/1000, Training Loss: 2.992493331161936e-05, Test Loss: 2.9701730218719922e-05\n",
            "Epoch 866/1000, Training Loss: 2.9894736921534583e-05, Test Loss: 2.966820451848059e-05\n",
            "Epoch 867/1000, Training Loss: 2.986461826884522e-05, Test Loss: 2.963477043593493e-05\n",
            "Epoch 868/1000, Training Loss: 2.9834576945960356e-05, Test Loss: 2.960142752535882e-05\n",
            "Epoch 869/1000, Training Loss: 2.9804612548566074e-05, Test Loss: 2.956817534423242e-05\n",
            "Epoch 870/1000, Training Loss: 2.977472467558962e-05, Test Loss: 2.953501345320933e-05\n",
            "Epoch 871/1000, Training Loss: 2.9744912929164023e-05, Test Loss: 2.950194141608356e-05\n",
            "Epoch 872/1000, Training Loss: 2.9715176914593922e-05, Test Loss: 2.9468958799760323e-05\n",
            "Epoch 873/1000, Training Loss: 2.9685516240319913e-05, Test Loss: 2.9436065174223373e-05\n",
            "Epoch 874/1000, Training Loss: 2.965593051788621e-05, Test Loss: 2.9403260112507413e-05\n",
            "Epoch 875/1000, Training Loss: 2.9626419361905394e-05, Test Loss: 2.93705431906669e-05\n",
            "Epoch 876/1000, Training Loss: 2.959698239002719e-05, Test Loss: 2.9337913987745518e-05\n",
            "Epoch 877/1000, Training Loss: 2.9567619222904736e-05, Test Loss: 2.9305372085748868e-05\n",
            "Epoch 878/1000, Training Loss: 2.9538329484162322e-05, Test Loss: 2.9272917069615854e-05\n",
            "Epoch 879/1000, Training Loss: 2.950911280036511e-05, Test Loss: 2.924054852718877e-05\n",
            "Epoch 880/1000, Training Loss: 2.947996880098624e-05, Test Loss: 2.9208266049185235e-05\n",
            "Epoch 881/1000, Training Loss: 2.9450897118377264e-05, Test Loss: 2.9176069229174256e-05\n",
            "Epoch 882/1000, Training Loss: 2.9421897387737008e-05, Test Loss: 2.914395766354435e-05\n",
            "Epoch 883/1000, Training Loss: 2.9392969247081872e-05, Test Loss: 2.9111930951480062e-05\n",
            "Epoch 884/1000, Training Loss: 2.9364112337216232e-05, Test Loss: 2.9079988694931793e-05\n",
            "Epoch 885/1000, Training Loss: 2.9335326301703576e-05, Test Loss: 2.9048130498594966e-05\n",
            "Epoch 886/1000, Training Loss: 2.93066107868373e-05, Test Loss: 2.9016355969880594e-05\n",
            "Epoch 887/1000, Training Loss: 2.9277965441612124e-05, Test Loss: 2.89846647188874e-05\n",
            "Epoch 888/1000, Training Loss: 2.9249389917695876e-05, Test Loss: 2.8953056358382756e-05\n",
            "Epoch 889/1000, Training Loss: 2.922088386940322e-05, Test Loss: 2.8921530503772523e-05\n",
            "Epoch 890/1000, Training Loss: 2.9192446953666007e-05, Test Loss: 2.8890086773078295e-05\n",
            "Epoch 891/1000, Training Loss: 2.916407883000926e-05, Test Loss: 2.8858724786915466e-05\n",
            "Epoch 892/1000, Training Loss: 2.9135779160521704e-05, Test Loss: 2.8827444168465205e-05\n",
            "Epoch 893/1000, Training Loss: 2.910754760983136e-05, Test Loss: 2.879624454345218e-05\n",
            "Epoch 894/1000, Training Loss: 2.907938384507875e-05, Test Loss: 2.8765125540122114e-05\n",
            "Epoch 895/1000, Training Loss: 2.9051287535891954e-05, Test Loss: 2.87340867892182e-05\n",
            "Epoch 896/1000, Training Loss: 2.9023258354360613e-05, Test Loss: 2.870312792395723e-05\n",
            "Epoch 897/1000, Training Loss: 2.8995295975012012e-05, Test Loss: 2.8672248580010094e-05\n",
            "Epoch 898/1000, Training Loss: 2.896740007478549e-05, Test Loss: 2.864144839547374e-05\n",
            "Epoch 899/1000, Training Loss: 2.893957033300844e-05, Test Loss: 2.8610727010854918e-05\n",
            "Epoch 900/1000, Training Loss: 2.8911806431372748e-05, Test Loss: 2.8580084069046467e-05\n",
            "Epoch 901/1000, Training Loss: 2.888410805391063e-05, Test Loss: 2.8549519215304188e-05\n",
            "Epoch 902/1000, Training Loss: 2.885647488697201e-05, Test Loss: 2.8519032097227883e-05\n",
            "Epoch 903/1000, Training Loss: 2.8828906619200116e-05, Test Loss: 2.8488622364738696e-05\n",
            "Epoch 904/1000, Training Loss: 2.880140294150995e-05, Test Loss: 2.8458289670059753e-05\n",
            "Epoch 905/1000, Training Loss: 2.877396354706494e-05, Test Loss: 2.8428033667693917e-05\n",
            "Epoch 906/1000, Training Loss: 2.874658813125571e-05, Test Loss: 2.8397854014405173e-05\n",
            "Epoch 907/1000, Training Loss: 2.8719276391676863e-05, Test Loss: 2.8367750369197856e-05\n",
            "Epoch 908/1000, Training Loss: 2.8692028028106456e-05, Test Loss: 2.833772239329604e-05\n",
            "Epoch 909/1000, Training Loss: 2.866484274248407e-05, Test Loss: 2.830776975012724e-05\n",
            "Epoch 910/1000, Training Loss: 2.8637720238889793e-05, Test Loss: 2.8277892105298014e-05\n",
            "Epoch 911/1000, Training Loss: 2.861066022352327e-05, Test Loss: 2.8248089126578054e-05\n",
            "Epoch 912/1000, Training Loss: 2.8583662404682874e-05, Test Loss: 2.8218360483881162e-05\n",
            "Epoch 913/1000, Training Loss: 2.8556726492745945e-05, Test Loss: 2.8188705849246214e-05\n",
            "Epoch 914/1000, Training Loss: 2.852985220014836e-05, Test Loss: 2.8159124896818818e-05\n",
            "Epoch 915/1000, Training Loss: 2.8503039241364516e-05, Test Loss: 2.8129617302831127e-05\n",
            "Epoch 916/1000, Training Loss: 2.8476287332888104e-05, Test Loss: 2.8100182745587572e-05\n",
            "Epoch 917/1000, Training Loss: 2.8449596193212252e-05, Test Loss: 2.8070820905443482e-05\n",
            "Epoch 918/1000, Training Loss: 2.842296554281114e-05, Test Loss: 2.8041531464788802e-05\n",
            "Epoch 919/1000, Training Loss: 2.8396395104119937e-05, Test Loss: 2.8012314108031776e-05\n",
            "Epoch 920/1000, Training Loss: 2.836988460151713e-05, Test Loss: 2.798316852157896e-05\n",
            "Epoch 921/1000, Training Loss: 2.834343376130549e-05, Test Loss: 2.7954094393820082e-05\n",
            "Epoch 922/1000, Training Loss: 2.8317042311694682e-05, Test Loss: 2.792509141511266e-05\n",
            "Epoch 923/1000, Training Loss: 2.82907099827819e-05, Test Loss: 2.7896159277760586e-05\n",
            "Epoch 924/1000, Training Loss: 2.8264436506534978e-05, Test Loss: 2.786729767600103e-05\n",
            "Epoch 925/1000, Training Loss: 2.8238221616774746e-05, Test Loss: 2.783850630598902e-05\n",
            "Epoch 926/1000, Training Loss: 2.8212065049157447e-05, Test Loss: 2.780978486577864e-05\n",
            "Epoch 927/1000, Training Loss: 2.8185966541157662e-05, Test Loss: 2.7781133055308427e-05\n",
            "Epoch 928/1000, Training Loss: 2.8159925832051282e-05, Test Loss: 2.7752550576384978e-05\n",
            "Epoch 929/1000, Training Loss: 2.813394266289879e-05, Test Loss: 2.7724037132668567e-05\n",
            "Epoch 930/1000, Training Loss: 2.810801677652848e-05, Test Loss: 2.7695592429654475e-05\n",
            "Epoch 931/1000, Training Loss: 2.808214791752071e-05, Test Loss: 2.766721617466284e-05\n",
            "Epoch 932/1000, Training Loss: 2.805633583219081e-05, Test Loss: 2.7638908076819013e-05\n",
            "Epoch 933/1000, Training Loss: 2.803058026857384e-05, Test Loss: 2.761066784703943e-05\n",
            "Epoch 934/1000, Training Loss: 2.8004880976408264e-05, Test Loss: 2.7582495198020205e-05\n",
            "Epoch 935/1000, Training Loss: 2.7979237707120688e-05, Test Loss: 2.7554389844217066e-05\n",
            "Epoch 936/1000, Training Loss: 2.7953650213810038e-05, Test Loss: 2.7526351501834828e-05\n",
            "Epoch 937/1000, Training Loss: 2.7928118251232883e-05, Test Loss: 2.7498379888812444e-05\n",
            "Epoch 938/1000, Training Loss: 2.7902641575787405e-05, Test Loss: 2.7470474724805672e-05\n",
            "Epoch 939/1000, Training Loss: 2.7877219945499565e-05, Test Loss: 2.74426357311788e-05\n",
            "Epoch 940/1000, Training Loss: 2.7851853120007755e-05, Test Loss: 2.7414862630985712e-05\n",
            "Epoch 941/1000, Training Loss: 2.7826540860548193e-05, Test Loss: 2.738715514895706e-05\n",
            "Epoch 942/1000, Training Loss: 2.7801282929940425e-05, Test Loss: 2.735951301148893e-05\n",
            "Epoch 943/1000, Training Loss: 2.7776079092573626e-05, Test Loss: 2.7331935946627644e-05\n",
            "Epoch 944/1000, Training Loss: 2.775092911439174e-05, Test Loss: 2.7304423684056858e-05\n",
            "Epoch 945/1000, Training Loss: 2.772583276288024e-05, Test Loss: 2.7276975955082518e-05\n",
            "Epoch 946/1000, Training Loss: 2.7700789807052163e-05, Test Loss: 2.7249592492624908e-05\n",
            "Epoch 947/1000, Training Loss: 2.767580001743447e-05, Test Loss: 2.7222273031198007e-05\n",
            "Epoch 948/1000, Training Loss: 2.7650863166053747e-05, Test Loss: 2.7195017306903702e-05\n",
            "Epoch 949/1000, Training Loss: 2.762597902642488e-05, Test Loss: 2.716782505741601e-05\n",
            "Epoch 950/1000, Training Loss: 2.760114737353582e-05, Test Loss: 2.7140696021969048e-05\n",
            "Epoch 951/1000, Training Loss: 2.7576367983835533e-05, Test Loss: 2.7113629941343357e-05\n",
            "Epoch 952/1000, Training Loss: 2.755164063522146e-05, Test Loss: 2.708662655785653e-05\n",
            "Epoch 953/1000, Training Loss: 2.752696510702593e-05, Test Loss: 2.705968561534715e-05\n",
            "Epoch 954/1000, Training Loss: 2.7502341180004027e-05, Test Loss: 2.7032806859166016e-05\n",
            "Epoch 955/1000, Training Loss: 2.747776863632143e-05, Test Loss: 2.7005990036164557e-05\n",
            "Epoch 956/1000, Training Loss: 2.7453247259541522e-05, Test Loss: 2.6979234894679368e-05\n",
            "Epoch 957/1000, Training Loss: 2.7428776834613555e-05, Test Loss: 2.6952541184523727e-05\n",
            "Epoch 958/1000, Training Loss: 2.7404357147860857e-05, Test Loss: 2.692590865697447e-05\n",
            "Epoch 959/1000, Training Loss: 2.737998798696801e-05, Test Loss: 2.689933706476232e-05\n",
            "Epoch 960/1000, Training Loss: 2.7355669140970272e-05, Test Loss: 2.6872826162058178e-05\n",
            "Epoch 961/1000, Training Loss: 2.7331400400240642e-05, Test Loss: 2.6846375704463027e-05\n",
            "Epoch 962/1000, Training Loss: 2.7307181556479938e-05, Test Loss: 2.6819985448998477e-05\n",
            "Epoch 963/1000, Training Loss: 2.7283012402703616e-05, Test Loss: 2.679365515409183e-05\n",
            "Epoch 964/1000, Training Loss: 2.725889273323204e-05, Test Loss: 2.6767384579569667e-05\n",
            "Epoch 965/1000, Training Loss: 2.7234822343678024e-05, Test Loss: 2.6741173486643962e-05\n",
            "Epoch 966/1000, Training Loss: 2.7210801030936784e-05, Test Loss: 2.6715021637902212e-05\n",
            "Epoch 967/1000, Training Loss: 2.7186828593175105e-05, Test Loss: 2.6688928797297542e-05\n",
            "Epoch 968/1000, Training Loss: 2.716290482981935e-05, Test Loss: 2.666289473013678e-05\n",
            "Epoch 969/1000, Training Loss: 2.7139029541545888e-05, Test Loss: 2.663691920307233e-05\n",
            "Epoch 970/1000, Training Loss: 2.711520253027024e-05, Test Loss: 2.6611001984089413e-05\n",
            "Epoch 971/1000, Training Loss: 2.7091423599136713e-05, Test Loss: 2.6585142842497894e-05\n",
            "Epoch 972/1000, Training Loss: 2.7067692552507506e-05, Test Loss: 2.6559341548921582e-05\n",
            "Epoch 973/1000, Training Loss: 2.7044009195953214e-05, Test Loss: 2.6533597875286983e-05\n",
            "Epoch 974/1000, Training Loss: 2.7020373336242077e-05, Test Loss: 2.6507911594815064e-05\n",
            "Epoch 975/1000, Training Loss: 2.6996784781330775e-05, Test Loss: 2.6482282482010234e-05\n",
            "Epoch 976/1000, Training Loss: 2.697324334035397e-05, Test Loss: 2.645671031265344e-05\n",
            "Epoch 977/1000, Training Loss: 2.6949748823613904e-05, Test Loss: 2.6431194863787475e-05\n",
            "Epoch 978/1000, Training Loss: 2.692630104257196e-05, Test Loss: 2.6405735913713135e-05\n",
            "Epoch 979/1000, Training Loss: 2.690289980983796e-05, Test Loss: 2.638033324197491e-05\n",
            "Epoch 980/1000, Training Loss: 2.687954493916184e-05, Test Loss: 2.635498662935605e-05\n",
            "Epoch 981/1000, Training Loss: 2.6856236245422915e-05, Test Loss: 2.632969585786468e-05\n",
            "Epoch 982/1000, Training Loss: 2.6832973544621343e-05, Test Loss: 2.6304460710728452e-05\n",
            "Epoch 983/1000, Training Loss: 2.680975665386903e-05, Test Loss: 2.6279280972383892e-05\n",
            "Epoch 984/1000, Training Loss: 2.678658539138029e-05, Test Loss: 2.6254156428468638e-05\n",
            "Epoch 985/1000, Training Loss: 2.6763459576462628e-05, Test Loss: 2.6229086865809714e-05\n",
            "Epoch 986/1000, Training Loss: 2.6740379029507665e-05, Test Loss: 2.6204072072416928e-05\n",
            "Epoch 987/1000, Training Loss: 2.6717343571983155e-05, Test Loss: 2.617911183747554e-05\n",
            "Epoch 988/1000, Training Loss: 2.6694353026423848e-05, Test Loss: 2.615420595133384e-05\n",
            "Epoch 989/1000, Training Loss: 2.6671407216422555e-05, Test Loss: 2.6129354205496307e-05\n",
            "Epoch 990/1000, Training Loss: 2.6648505966621654e-05, Test Loss: 2.6104556392617146e-05\n",
            "Epoch 991/1000, Training Loss: 2.6625649102704753e-05, Test Loss: 2.6079812306489497e-05\n",
            "Epoch 992/1000, Training Loss: 2.6602836451388158e-05, Test Loss: 2.6055121742036184e-05\n",
            "Epoch 993/1000, Training Loss: 2.6580067840413032e-05, Test Loss: 2.603048449530474e-05\n",
            "Epoch 994/1000, Training Loss: 2.6557343098536544e-05, Test Loss: 2.600590036345591e-05\n",
            "Epoch 995/1000, Training Loss: 2.6534662055524224e-05, Test Loss: 2.5981369144758825e-05\n",
            "Epoch 996/1000, Training Loss: 2.6512024542141678e-05, Test Loss: 2.595689063858072e-05\n",
            "Epoch 997/1000, Training Loss: 2.6489430390146523e-05, Test Loss: 2.5932464645378215e-05\n",
            "Epoch 998/1000, Training Loss: 2.6466879432280684e-05, Test Loss: 2.5908090966691614e-05\n",
            "Epoch 999/1000, Training Loss: 2.644437150226253e-05, Test Loss: 2.5883769405136333e-05\n",
            "Epoch 1000/1000, Training Loss: 2.642190643477909e-05, Test Loss: 2.5859499764394356e-05\n",
            "Optimal Number of Hidden Nodes (H*): 19\n",
            "Corresponding Optimal Epoch: 1000\n",
            "Minimum Test Loss: 1.3485262937841058e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# MLP class with one hidden layer\n",
        "class MLP:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        # Initialize the number of nodes in each layer\n",
        "        self.input_nodes = input_nodes + 1  # Including bias\n",
        "        self.hidden_nodes = hidden_nodes + 1  # Including bias\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights_input_hidden = np.random.rand(self.input_nodes, self.hidden_nodes - 1) - 0.5\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_nodes, self.output_nodes) - 0.5\n",
        "\n",
        "        # Include bias in the inputs\n",
        "        self.bias_input = np.random.rand(self.input_nodes) - 0.5\n",
        "        self.bias_hidden = np.random.rand(self.hidden_nodes) - 0.5\n",
        "\n",
        "        # Initialize lists to store training and test losses\n",
        "        self.training_loss_history = []\n",
        "        self.test_loss_history = []\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # Add bias to inputs\n",
        "        inputs_with_bias = np.concatenate((inputs, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the hidden layer\n",
        "        self.hidden_input = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        # Add bias to hidden layer outputs\n",
        "        hidden_output_with_bias = np.concatenate((self.hidden_output, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the output layer\n",
        "        self.final_input = np.dot(hidden_output_with_bias, self.weights_hidden_output)\n",
        "        self.final_output = sigmoid(self.final_input)\n",
        "\n",
        "        return self.final_output\n",
        "\n",
        "    def backward_pass(self, inputs, expected_output, output, learning_rate):\n",
        "        # Compute error\n",
        "        error = expected_output - output\n",
        "\n",
        "        # Gradient for output weights\n",
        "        d_weights_hidden_output = np.dot(np.concatenate((self.hidden_output, [1]), axis=0).reshape(-1,1),\n",
        "                                         error * sigmoid_derivative(output).reshape(1, -1))\n",
        "\n",
        "        # Error for hidden layer\n",
        "        hidden_error = np.dot(self.weights_hidden_output, error * sigmoid_derivative(output))[:-1]\n",
        "\n",
        "        # Gradient for input weights\n",
        "        d_weights_input_hidden = np.dot(np.concatenate((inputs, [1]), axis=0).reshape(-1,1),\n",
        "                                        hidden_error * sigmoid_derivative(self.hidden_output).reshape(1, -1))\n",
        "\n",
        "        # Update the weights\n",
        "        self.weights_hidden_output += learning_rate * d_weights_hidden_output\n",
        "        self.weights_input_hidden += learning_rate * d_weights_input_hidden\n",
        "\n",
        "    def train_and_evaluate(self, dataset, test_inputs, test_expected_output, max_epochs, learning_rate):\n",
        "        # Split dataset into inputs and expected outputs\n",
        "        inputs = dataset[:, :2]\n",
        "        expected_output = dataset[:, 2:]\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(max_epochs):\n",
        "            for j in range(inputs.shape[0]):\n",
        "                input_sample = inputs[j]\n",
        "                output = self.forward_pass(input_sample)\n",
        "                self.backward_pass(input_sample, expected_output[j], output, learning_rate)\n",
        "\n",
        "            # Calculate training loss\n",
        "            training_loss = np.mean(np.square(expected_output - self.predict(inputs)))\n",
        "            self.training_loss_history.append(training_loss)\n",
        "\n",
        "            # Calculate test loss\n",
        "            test_loss = np.mean(np.square(test_expected_output - self.predict(test_inputs)))\n",
        "            self.test_loss_history.append(test_loss)\n",
        "\n",
        "            # Print out progress\n",
        "            print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {training_loss}, Test Loss: {test_loss}\")\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        outputs = np.array([self.forward_pass(input_sample) for input_sample in inputs])\n",
        "        return outputs\n",
        "\n",
        "# Function to calculate the multivariate normal density\n",
        "def multivariate_gaussian_density(x, mu, cov):\n",
        "    n = mu.shape[0]\n",
        "    diff = x - mu\n",
        "    return (1. / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \\\n",
        "           np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))\n",
        "\n",
        "# Parameters for the Gaussian\n",
        "mu_x = np.array([0, 0])\n",
        "cov_x = np.array([[0.3, -0.5],\n",
        "                  [-0.5, 2]])\n",
        "\n",
        "# Set the number of input samples and epochs\n",
        "N = 1600\n",
        "E = 1000\n",
        "\n",
        "# Set the range of hidden nodes\n",
        "hidden_nodes_range = range(5, 21)  # From 5 to 20 hidden nodes\n",
        "\n",
        "# Initialize lists to store test loss and optimal epochs\n",
        "test_loss_values = []\n",
        "training_loss_values = []\n",
        "\n",
        "for H in hidden_nodes_range:\n",
        "    # Generate N training samples randomly\n",
        "    samples = np.zeros((N, 3))\n",
        "    samples[:, 0] = np.random.uniform(-2, 2, N)  # x1\n",
        "    samples[:, 1] = np.random.uniform(-4, 4, N)  # x2\n",
        "\n",
        "    # Calculate the function value for each sample\n",
        "    for i in range(N):\n",
        "        samples[i, 2] = multivariate_gaussian_density(samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Generate new test data\n",
        "    N_test = 100  # Change this to your dataset size for testing\n",
        "    test_samples = np.zeros((N_test, 3))\n",
        "    test_samples[:, 0] = np.random.uniform(-2, 2, N_test)  # x1 range\n",
        "    test_samples[:, 1] = np.random.uniform(-4, 4, N_test)  # x2 range\n",
        "\n",
        "    for i in range(N_test):\n",
        "        test_samples[i, 2] = multivariate_gaussian_density(test_samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Initialize the MLP\n",
        "    mlp = MLP(input_nodes=2, hidden_nodes=H, output_nodes=1)\n",
        "    mlp.train_and_evaluate(samples, test_samples[:, :2], test_samples[:, 2:], max_epochs=E, learning_rate=0.1)\n",
        "\n",
        "    # Record test loss and training loss\n",
        "    test_loss_values.append(mlp.test_loss_history[-1])\n",
        "    training_loss_values.append(mlp.training_loss_history[-1])\n",
        "\n",
        "# Find the index of the minimum test loss value\n",
        "optimal_H_index = np.argmin(test_loss_values)\n",
        "optimal_H = hidden_nodes_range[optimal_H_index]\n",
        "min_test_loss = test_loss_values[optimal_H_index]\n",
        "optimal_epoch_for_optimal_H = E  # Optimal epoch is fixed at E\n",
        "\n",
        "print(f\"Optimal Number of Hidden Nodes (H*): {optimal_H}\")\n",
        "print(f\"Corresponding Optimal Epoch: {optimal_epoch_for_optimal_H}\")\n",
        "print(f\"Minimum Test Loss: {min_test_loss}\")\n",
        "\n",
        "# Plot test loss and training loss against the number of hidden nodes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(hidden_nodes_range, training_loss_values, marker='o', linestyle='-', label='Training Loss')\n",
        "plt.plot(hidden_nodes_range, test_loss_values, marker='o', linestyle='-', label='Test Loss')\n",
        "plt.title('Loss vs Number of Hidden Nodes')\n",
        "plt.xlabel('Number of Hidden Nodes (H)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.axvline(x=optimal_H, color='r', linestyle='--', label=f'Optimal H: {optimal_H}')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}