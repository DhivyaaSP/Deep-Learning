{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1xOebvz1ceh/lcUjgYyfY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhivyaaSP/Deep-Learning/blob/main/Exe_2_2_1_Optimal_Epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M7hFM447PQIF",
        "outputId": "ad5b0b91-41d6-4070-d3da-908b6dd60473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Training Loss: 0.010264724937080203, Test Loss: 0.010645494261191445\n",
            "Epoch 2/1000, Training Loss: 0.005860006093476426, Test Loss: 0.005940389072779067\n",
            "Epoch 3/1000, Training Loss: 0.004609325654041268, Test Loss: 0.00456736454626989\n",
            "Epoch 4/1000, Training Loss: 0.004059551214323992, Test Loss: 0.003949569904949446\n",
            "Epoch 5/1000, Training Loss: 0.0037641188270975245, Test Loss: 0.0036102733821172205\n",
            "Epoch 6/1000, Training Loss: 0.003585671202886375, Test Loss: 0.0034009595067596254\n",
            "Epoch 7/1000, Training Loss: 0.0034692747254009594, Test Loss: 0.00326155758144251\n",
            "Epoch 8/1000, Training Loss: 0.0033891008535588225, Test Loss: 0.0031635248343623067\n",
            "Epoch 9/1000, Training Loss: 0.0033315826969126027, Test Loss: 0.003091716772401834\n",
            "Epoch 10/1000, Training Loss: 0.0032889929581125664, Test Loss: 0.0030374224373148967\n",
            "Epoch 11/1000, Training Loss: 0.0032566493685401605, Test Loss: 0.0029953122230583175\n",
            "Epoch 12/1000, Training Loss: 0.003231572568969852, Test Loss: 0.002961962126618595\n",
            "Epoch 13/1000, Training Loss: 0.0032117901362777863, Test Loss: 0.0029350834834991524\n",
            "Epoch 14/1000, Training Loss: 0.003195952769879775, Test Loss: 0.0029130954812783285\n",
            "Epoch 15/1000, Training Loss: 0.003183111657680355, Test Loss: 0.0028948756443476927\n",
            "Epoch 16/1000, Training Loss: 0.003172583759981908, Test Loss: 0.0028796079218613223\n",
            "Epoch 17/1000, Training Loss: 0.003163867312779117, Test Loss: 0.002866686823834832\n",
            "Epoch 18/1000, Training Loss: 0.00315658716716488, Test Loss: 0.0028556550308737804\n",
            "Epoch 19/1000, Training Loss: 0.0031504584669299306, Test Loss: 0.002846161684519991\n",
            "Epoch 20/1000, Training Loss: 0.0031452619362094935, Test Loss: 0.002837933838018427\n",
            "Epoch 21/1000, Training Loss: 0.0031408267117195906, Test Loss: 0.0028307565029240113\n",
            "Epoch 22/1000, Training Loss: 0.0031370181924688582, Test Loss: 0.0028244584414710613\n",
            "Epoch 23/1000, Training Loss: 0.003133729295999812, Test Loss: 0.002818901879801107\n",
            "Epoch 24/1000, Training Loss: 0.00313087407081146, Test Loss: 0.002813974946932682\n",
            "Epoch 25/1000, Training Loss: 0.0031283829660726575, Test Loss: 0.0028095860407469577\n",
            "Epoch 26/1000, Training Loss: 0.0031261992849566833, Test Loss: 0.0028056595772730226\n",
            "Epoch 27/1000, Training Loss: 0.0031242764951506744, Test Loss: 0.0028021327469020785\n",
            "Epoch 28/1000, Training Loss: 0.0031225761680894523, Test Loss: 0.0027989530129826344\n",
            "Epoch 29/1000, Training Loss: 0.003121066384782271, Test Loss: 0.002796076164218851\n",
            "Epoch 30/1000, Training Loss: 0.003119720491671145, Test Loss: 0.0027934647846980065\n",
            "Epoch 31/1000, Training Loss: 0.0031185161217149237, Test Loss: 0.002791087042032365\n",
            "Epoch 32/1000, Training Loss: 0.0031174344183112646, Test Loss: 0.002788915720080738\n",
            "Epoch 33/1000, Training Loss: 0.003116459415686534, Test Loss: 0.002786927441350273\n",
            "Epoch 34/1000, Training Loss: 0.0031155775409575266, Test Loss: 0.0027851020376963827\n",
            "Epoch 35/1000, Training Loss: 0.003114777211519523, Test Loss: 0.0027834220378470747\n",
            "Epoch 36/1000, Training Loss: 0.0031140485076460345, Test Loss: 0.0027818722476120454\n",
            "Epoch 37/1000, Training Loss: 0.003113382904821811, Test Loss: 0.0027804394041156875\n",
            "Epoch 38/1000, Training Loss: 0.0031127730538103014, Test Loss: 0.002779111889521273\n",
            "Epoch 39/1000, Training Loss: 0.0031122125990894114, Test Loss: 0.002777879492849316\n",
            "Epoch 40/1000, Training Loss: 0.0031116960282964435, Test Loss: 0.0027767332108932976\n",
            "Epoch 41/1000, Training Loss: 0.003111218546864358, Test Loss: 0.00277566508108638\n",
            "Epoch 42/1000, Training Loss: 0.0031107759732229624, Test Loss: 0.0027746680406091372\n",
            "Epoch 43/1000, Training Loss: 0.003110364650865808, Test Loss: 0.002773735807150512\n",
            "Epoch 44/1000, Training Loss: 0.0031099813743093387, Test Loss: 0.0027728627776163455\n",
            "Epoch 45/1000, Training Loss: 0.0031096233265423836, Test Loss: 0.002772043941777295\n",
            "Epoch 46/1000, Training Loss: 0.0031092880260165483, Test Loss: 0.0027712748084024238\n",
            "Epoch 47/1000, Training Loss: 0.003108973281588232, Test Loss: 0.002770551341867964\n",
            "Epoch 48/1000, Training Loss: 0.0031086771541110323, Test Loss: 0.0027698699075867283\n",
            "Epoch 49/1000, Training Loss: 0.0031083979236088753, Test Loss: 0.0027692272248909626\n",
            "Epoch 50/1000, Training Loss: 0.003108134061147086, Test Loss: 0.0027686203262344265\n",
            "Epoch 51/1000, Training Loss: 0.0031078842046702248, Test Loss: 0.002768046521769191\n",
            "Epoch 52/1000, Training Loss: 0.00310764713819891, Test Loss: 0.0027675033685078086\n",
            "Epoch 53/1000, Training Loss: 0.0031074217738787315, Test Loss: 0.002766988643408907\n",
            "Epoch 54/1000, Training Loss: 0.003107207136457184, Test Loss: 0.002766500319829239\n",
            "Epoch 55/1000, Training Loss: 0.0031070023498326795, Test Loss: 0.0027660365468721113\n",
            "Epoch 56/1000, Training Loss: 0.003106806625376092, Test Loss: 0.0027655956312342285\n",
            "Epoch 57/1000, Training Loss: 0.003106619251771987, Test Loss: 0.002765176021213005\n",
            "Epoch 58/1000, Training Loss: 0.0031064395861655626, Test Loss: 0.002764776292586703\n",
            "Epoch 59/1000, Training Loss: 0.003106267046433779, Test Loss: 0.0027643951361217413\n",
            "Epoch 60/1000, Training Loss: 0.0031061011044262675, Test Loss: 0.002764031346496969\n",
            "Epoch 61/1000, Training Loss: 0.003105941280044414, Test Loss: 0.0027636838124645174\n",
            "Epoch 62/1000, Training Loss: 0.003105787136046175, Test Loss: 0.002763351508092113\n",
            "Epoch 63/1000, Training Loss: 0.0031056382734803477, Test Loss: 0.0027630334849530723\n",
            "Epoch 64/1000, Training Loss: 0.003105494327667721, Test Loss: 0.0027627288651484744\n",
            "Epoch 65/1000, Training Loss: 0.003105354964658108, Test Loss: 0.0027624368350614324\n",
            "Epoch 66/1000, Training Loss: 0.0031052198781021246, Test Loss: 0.002762156639756666\n",
            "Epoch 67/1000, Training Loss: 0.003105088786484992, Test Loss: 0.002761887577949942\n",
            "Epoch 68/1000, Training Loss: 0.0031049614306767758, Test Loss: 0.002761628997481611\n",
            "Epoch 69/1000, Training Loss: 0.003104837571759625, Test Loss: 0.002761380291236902\n",
            "Epoch 70/1000, Training Loss: 0.0031047169890978115, Test Loss: 0.0027611408934628185\n",
            "Epoch 71/1000, Training Loss: 0.003104599478620884, Test Loss: 0.002760910276437705\n",
            "Epoch 72/1000, Training Loss: 0.0031044848512941227, Test Loss: 0.0027606879474549473\n",
            "Epoch 73/1000, Training Loss: 0.0031043729317538305, Test Loss: 0.002760473446086948\n",
            "Epoch 74/1000, Training Loss: 0.0031042635570878445, Test Loss: 0.0027602663416995315\n",
            "Epoch 75/1000, Training Loss: 0.003104156575744194, Test Loss: 0.0027600662311905357\n",
            "Epoch 76/1000, Training Loss: 0.003104051846552932, Test Loss: 0.0027598727369293283\n",
            "Epoch 77/1000, Training Loss: 0.003103949237848078, Test Loss: 0.00275968550487675\n",
            "Epoch 78/1000, Training Loss: 0.003103848626678196, Test Loss: 0.0027595042028672623\n",
            "Epoch 79/1000, Training Loss: 0.0031037498980955656, Test Loss: 0.002759328519037205\n",
            "Epoch 80/1000, Training Loss: 0.003103652944515123, Test Loss: 0.0027591581603847786\n",
            "Epoch 81/1000, Training Loss: 0.0031035576651353984, Test Loss: 0.0027589928514490415\n",
            "Epoch 82/1000, Training Loss: 0.003103463965414648, Test Loss: 0.002758832333096509\n",
            "Epoch 83/1000, Training Loss: 0.003103371756596137, Test Loss: 0.002758676361405253\n",
            "Epoch 84/1000, Training Loss: 0.0031032809552773034, Test Loss: 0.002758524706637423\n",
            "Epoch 85/1000, Training Loss: 0.003103191483018094, Test Loss: 0.00275837715229207\n",
            "Epoch 86/1000, Training Loss: 0.003103103265984358, Test Loss: 0.0027582334942310195\n",
            "Epoch 87/1000, Training Loss: 0.0031030162346226287, Test Loss: 0.0027580935398712754\n",
            "Epoch 88/1000, Training Loss: 0.0031029303233630623, Test Loss: 0.002757957107438087\n",
            "Epoch 89/1000, Training Loss: 0.003102845470347661, Test Loss: 0.0027578240252734143\n",
            "Epoch 90/1000, Training Loss: 0.0031027616171812335, Test Loss: 0.0027576941311950494\n",
            "Epoch 91/1000, Training Loss: 0.0031026787087028384, Test Loss: 0.002757567271902096\n",
            "Epoch 92/1000, Training Loss: 0.003102596692775691, Test Loss: 0.0027574433024229587\n",
            "Epoch 93/1000, Training Loss: 0.0031025155200937573, Test Loss: 0.0027573220856023212\n",
            "Epoch 94/1000, Training Loss: 0.0031024351440034287, Test Loss: 0.0027572034916239596\n",
            "Epoch 95/1000, Training Loss: 0.0031023555203388686, Test Loss: 0.0027570873975665167\n",
            "Epoch 96/1000, Training Loss: 0.003102276607269756, Test Loss: 0.0027569736869896134\n",
            "Epoch 97/1000, Training Loss: 0.0031021983651602935, Test Loss: 0.0027568622495479445\n",
            "Epoch 98/1000, Training Loss: 0.0031021207564384656, Test Loss: 0.0027567529806311864\n",
            "Epoch 99/1000, Training Loss: 0.0031020437454746374, Test Loss: 0.002756645781027754\n",
            "Epoch 100/1000, Training Loss: 0.0031019672984686814, Test Loss: 0.002756540556610613\n",
            "Epoch 101/1000, Training Loss: 0.003101891383344897, Test Loss: 0.002756437218043492\n",
            "Epoch 102/1000, Training Loss: 0.003101815969654065, Test Loss: 0.002756335680506024\n",
            "Epoch 103/1000, Training Loss: 0.0031017410284820447, Test Loss: 0.0027562358634364136\n",
            "Epoch 104/1000, Training Loss: 0.003101666532364381, Test Loss: 0.0027561376902903766\n",
            "Epoch 105/1000, Training Loss: 0.0031015924552064405, Test Loss: 0.002756041088315215\n",
            "Epoch 106/1000, Training Loss: 0.003101518772208628, Test Loss: 0.002755945988337935\n",
            "Epoch 107/1000, Training Loss: 0.003101445459796306, Test Loss: 0.0027558523245664567\n",
            "Epoch 108/1000, Training Loss: 0.0031013724955540494, Test Loss: 0.0027557600344029942\n",
            "Epoch 109/1000, Training Loss: 0.003101299858163913, Test Loss: 0.0027556690582687927\n",
            "Epoch 110/1000, Training Loss: 0.0031012275273474116, Test Loss: 0.002755579339439445\n",
            "Epoch 111/1000, Training Loss: 0.0031011554838109557, Test Loss: 0.0027554908238900926\n",
            "Epoch 112/1000, Training Loss: 0.0031010837091944815, Test Loss: 0.0027554034601498384\n",
            "Epoch 113/1000, Training Loss: 0.003101012186023064, Test Loss: 0.002755317199164794\n",
            "Epoch 114/1000, Training Loss: 0.0031009408976613033, Test Loss: 0.002755231994169159\n",
            "Epoch 115/1000, Training Loss: 0.0031008698282702957, Test Loss: 0.0027551478005638673\n",
            "Epoch 116/1000, Training Loss: 0.0031007989627670185, Test Loss: 0.002755064575802266\n",
            "Epoch 117/1000, Training Loss: 0.003100728286785973, Test Loss: 0.00275498227928241\n",
            "Epoch 118/1000, Training Loss: 0.0031006577866429354, Test Loss: 0.002754900872245553\n",
            "Epoch 119/1000, Training Loss: 0.00310058744930068, Test Loss: 0.0027548203176804248\n",
            "Epoch 120/1000, Training Loss: 0.003100517262336566, Test Loss: 0.0027547405802329805\n",
            "Epoch 121/1000, Training Loss: 0.0031004472139118506, Test Loss: 0.00275466162612122\n",
            "Epoch 122/1000, Training Loss: 0.0031003772927426392, Test Loss: 0.0027545834230548395\n",
            "Epoch 123/1000, Training Loss: 0.0031003074880723725, Test Loss: 0.002754505940159359\n",
            "Epoch 124/1000, Training Loss: 0.00310023778964575, Test Loss: 0.0027544291479044917\n",
            "Epoch 125/1000, Training Loss: 0.00310016818768402, Test Loss: 0.0027543530180364866\n",
            "Epoch 126/1000, Training Loss: 0.0031000986728615455, Test Loss: 0.002754277523514211\n",
            "Epoch 127/1000, Training Loss: 0.0031000292362835724, Test Loss: 0.0027542026384487417\n",
            "Epoch 128/1000, Training Loss: 0.0030999598694651494, Test Loss: 0.002754128338046275\n",
            "Epoch 129/1000, Training Loss: 0.0030998905643111075, Test Loss: 0.0027540545985541382\n",
            "Epoch 130/1000, Training Loss: 0.0030998213130970687, Test Loss: 0.0027539813972097403\n",
            "Epoch 131/1000, Training Loss: 0.0030997521084513992, Test Loss: 0.0027539087121922746\n",
            "Epoch 132/1000, Training Loss: 0.0030996829433380846, Test Loss: 0.002753836522577029\n",
            "Epoch 133/1000, Training Loss: 0.003099613811040456, Test Loss: 0.0027537648082921373\n",
            "Epoch 134/1000, Training Loss: 0.0030995447051457236, Test Loss: 0.002753693550077648\n",
            "Epoch 135/1000, Training Loss: 0.003099475619530294, Test Loss: 0.0027536227294467606\n",
            "Epoch 136/1000, Training Loss: 0.003099406548345796, Test Loss: 0.0027535523286491186\n",
            "Epoch 137/1000, Training Loss: 0.00309933748600582, Test Loss: 0.00275348233063604\n",
            "Epoch 138/1000, Training Loss: 0.0030992684271732886, Test Loss: 0.0027534127190275664\n",
            "Epoch 139/1000, Training Loss: 0.0030991993667484653, Test Loss: 0.0027533434780812483\n",
            "Epoch 140/1000, Training Loss: 0.0030991302998575414, Test Loss: 0.0027532745926625384\n",
            "Epoch 141/1000, Training Loss: 0.0030990612218417908, Test Loss: 0.00275320604821674\n",
            "Epoch 142/1000, Training Loss: 0.0030989921282472406, Test Loss: 0.00275313783074239\n",
            "Epoch 143/1000, Training Loss: 0.003098923014814863, Test Loss: 0.0027530699267660163\n",
            "Epoch 144/1000, Training Loss: 0.003098853877471225, Test Loss: 0.0027530023233181948\n",
            "Epoch 145/1000, Training Loss: 0.0030987847123196133, Test Loss: 0.0027529350079108167\n",
            "Epoch 146/1000, Training Loss: 0.0030987155156315684, Test Loss: 0.002752867968515516\n",
            "Epoch 147/1000, Training Loss: 0.0030986462838388466, Test Loss: 0.0027528011935431932\n",
            "Epoch 148/1000, Training Loss: 0.0030985770135257594, Test Loss: 0.0027527346718245675\n",
            "Epoch 149/1000, Training Loss: 0.003098507701421889, Test Loss: 0.0027526683925916955\n",
            "Epoch 150/1000, Training Loss: 0.003098438344395146, Test Loss: 0.0027526023454604275\n",
            "Epoch 151/1000, Training Loss: 0.0030983689394451807, Test Loss: 0.0027525365204137298\n",
            "Epoch 152/1000, Training Loss: 0.003098299483697085, Test Loss: 0.0027524709077858278\n",
            "Epoch 153/1000, Training Loss: 0.003098229974395424, Test Loss: 0.0027524054982471418\n",
            "Epoch 154/1000, Training Loss: 0.0030981604088985313, Test Loss: 0.0027523402827899475\n",
            "Epoch 155/1000, Training Loss: 0.003098090784673097, Test Loss: 0.002752275252714748\n",
            "Epoch 156/1000, Training Loss: 0.003098021099288998, Test Loss: 0.00275221039961731\n",
            "Epoch 157/1000, Training Loss: 0.003097951350414394, Test Loss: 0.002752145715376309\n",
            "Epoch 158/1000, Training Loss: 0.00309788153581104, Test Loss: 0.0027520811921415943\n",
            "Epoch 159/1000, Training Loss: 0.00309781165332984, Test Loss: 0.0027520168223229974\n",
            "Epoch 160/1000, Training Loss: 0.003097741700906599, Test Loss: 0.0027519525985796813\n",
            "Epoch 161/1000, Training Loss: 0.0030976716765579878, Test Loss: 0.002751888513809995\n",
            "Epoch 162/1000, Training Loss: 0.0030976015783776966, Test Loss: 0.002751824561141802\n",
            "Epoch 163/1000, Training Loss: 0.0030975314045327733, Test Loss: 0.002751760733923274\n",
            "Epoch 164/1000, Training Loss: 0.0030974611532601348, Test Loss: 0.0027516970257141006\n",
            "Epoch 165/1000, Training Loss: 0.003097390822863247, Test Loss: 0.002751633430277117\n",
            "Epoch 166/1000, Training Loss: 0.0030973204117089597, Test Loss: 0.002751569941570315\n",
            "Epoch 167/1000, Training Loss: 0.003097249918224497, Test Loss: 0.00275150655373922\n",
            "Epoch 168/1000, Training Loss: 0.0030971793408945884, Test Loss: 0.0027514432611096245\n",
            "Epoch 169/1000, Training Loss: 0.0030971086782587355, Test Loss: 0.0027513800581806364\n",
            "Epoch 170/1000, Training Loss: 0.0030970379289086093, Test Loss: 0.00275131693961806\n",
            "Epoch 171/1000, Training Loss: 0.003096967091485574, Test Loss: 0.002751253900248059\n",
            "Epoch 172/1000, Training Loss: 0.0030968961646783244, Test Loss: 0.0027511909350511126\n",
            "Epoch 173/1000, Training Loss: 0.0030968251472206377, Test Loss: 0.002751128039156231\n",
            "Epoch 174/1000, Training Loss: 0.003096754037889236, Test Loss: 0.002751065207835442\n",
            "Epoch 175/1000, Training Loss: 0.003096682835501739, Test Loss: 0.0027510024364984977\n",
            "Epoch 176/1000, Training Loss: 0.003096611538914729, Test Loss: 0.0027509397206878293\n",
            "Epoch 177/1000, Training Loss: 0.0030965401470218934, Test Loss: 0.002750877056073722\n",
            "Epoch 178/1000, Training Loss: 0.0030964686587522695, Test Loss: 0.002750814438449676\n",
            "Epoch 179/1000, Training Loss: 0.0030963970730685554, Test Loss: 0.002750751863727994\n",
            "Epoch 180/1000, Training Loss: 0.0030963253889655195, Test Loss: 0.0027506893279355387\n",
            "Epoch 181/1000, Training Loss: 0.003096253605468474, Test Loss: 0.0027506268272096757\n",
            "Epoch 182/1000, Training Loss: 0.0030961817216318198, Test Loss: 0.0027505643577943844\n",
            "Epoch 183/1000, Training Loss: 0.003096109736537668, Test Loss: 0.0027505019160365346\n",
            "Epoch 184/1000, Training Loss: 0.003096037649294522, Test Loss: 0.0027504394983823154\n",
            "Epoch 185/1000, Training Loss: 0.0030959654590360197, Test Loss: 0.002750377101373811\n",
            "Epoch 186/1000, Training Loss: 0.003095893164919743, Test Loss: 0.002750314721645718\n",
            "Epoch 187/1000, Training Loss: 0.003095820766126075, Test Loss: 0.0027502523559221935\n",
            "Epoch 188/1000, Training Loss: 0.0030957482618571164, Test Loss: 0.002750190001013836\n",
            "Epoch 189/1000, Training Loss: 0.003095675651335652, Test Loss: 0.002750127653814779\n",
            "Epoch 190/1000, Training Loss: 0.003095602933804168, Test Loss: 0.0027500653112999085\n",
            "Epoch 191/1000, Training Loss: 0.0030955301085239077, Test Loss: 0.0027500029705221857\n",
            "Epoch 192/1000, Training Loss: 0.003095457174773985, Test Loss: 0.002749940628610077\n",
            "Epoch 193/1000, Training Loss: 0.0030953841318505284, Test Loss: 0.0027498782827650847\n",
            "Epoch 194/1000, Training Loss: 0.0030953109790658705, Test Loss: 0.0027498159302593687\n",
            "Epoch 195/1000, Training Loss: 0.003095237715747776, Test Loss: 0.0027497535684334667\n",
            "Epoch 196/1000, Training Loss: 0.0030951643412387053, Test Loss: 0.0027496911946941\n",
            "Epoch 197/1000, Training Loss: 0.0030950908548951104, Test Loss: 0.002749628806512055\n",
            "Epoch 198/1000, Training Loss: 0.0030950172560867697, Test Loss: 0.0027495664014201547\n",
            "Epoch 199/1000, Training Loss: 0.0030949435441961467, Test Loss: 0.0027495039770113017\n",
            "Epoch 200/1000, Training Loss: 0.003094869718617786, Test Loss: 0.002749441530936592\n",
            "Epoch 201/1000, Training Loss: 0.0030947957787577376, Test Loss: 0.0027493790609035024\n",
            "Epoch 202/1000, Training Loss: 0.0030947217240329962, Test Loss: 0.0027493165646741407\n",
            "Epoch 203/1000, Training Loss: 0.0030946475538709893, Test Loss: 0.0027492540400635606\n",
            "Epoch 204/1000, Training Loss: 0.0030945732677090643, Test Loss: 0.002749191484938142\n",
            "Epoch 205/1000, Training Loss: 0.0030944988649940198, Test Loss: 0.0027491288972140214\n",
            "Epoch 206/1000, Training Loss: 0.0030944243451816477, Test Loss: 0.002749066274855586\n",
            "Epoch 207/1000, Training Loss: 0.0030943497077363016, Test Loss: 0.0027490036158740122\n",
            "Epoch 208/1000, Training Loss: 0.0030942749521304814, Test Loss: 0.002748940918325865\n",
            "Epoch 209/1000, Training Loss: 0.0030942000778444445, Test Loss: 0.0027488781803117453\n",
            "Epoch 210/1000, Training Loss: 0.0030941250843658286, Test Loss: 0.002748815399974968\n",
            "Epoch 211/1000, Training Loss: 0.0030940499711892932, Test Loss: 0.002748752575500316\n",
            "Epoch 212/1000, Training Loss: 0.003093974737816182, Test Loss: 0.0027486897051128023\n",
            "Epoch 213/1000, Training Loss: 0.0030938993837541966, Test Loss: 0.0027486267870765056\n",
            "Epoch 214/1000, Training Loss: 0.0030938239085170887, Test Loss: 0.0027485638196934194\n",
            "Epoch 215/1000, Training Loss: 0.003093748311624364, Test Loss: 0.0027485008013023587\n",
            "Epoch 216/1000, Training Loss: 0.003093672592601002, Test Loss: 0.0027484377302778875\n",
            "Epoch 217/1000, Training Loss: 0.003093596750977191, Test Loss: 0.002748374605029302\n",
            "Epoch 218/1000, Training Loss: 0.0030935207862880674, Test Loss: 0.002748311423999618\n",
            "Epoch 219/1000, Training Loss: 0.003093444698073479, Test Loss: 0.002748248185664623\n",
            "Epoch 220/1000, Training Loss: 0.0030933684858777464, Test Loss: 0.002748184888531939\n",
            "Epoch 221/1000, Training Loss: 0.0030932921492494497, Test Loss: 0.0027481215311401168\n",
            "Epoch 222/1000, Training Loss: 0.003093215687741211, Test Loss: 0.002748058112057769\n",
            "Epoch 223/1000, Training Loss: 0.003093139100909498, Test Loss: 0.0027479946298827216\n",
            "Epoch 224/1000, Training Loss: 0.0030930623883144315, Test Loss: 0.0027479310832411986\n",
            "Epoch 225/1000, Training Loss: 0.0030929855495196042, Test Loss: 0.0027478674707870233\n",
            "Epoch 226/1000, Training Loss: 0.0030929085840919013, Test Loss: 0.0027478037912008566\n",
            "Epoch 227/1000, Training Loss: 0.0030928314916013446, Test Loss: 0.002747740043189454\n",
            "Epoch 228/1000, Training Loss: 0.003092754271620925, Test Loss: 0.0027476762254849394\n",
            "Epoch 229/1000, Training Loss: 0.0030926769237264572, Test Loss: 0.0027476123368441064\n",
            "Epoch 230/1000, Training Loss: 0.0030925994474964367, Test Loss: 0.0027475483760477457\n",
            "Epoch 231/1000, Training Loss: 0.0030925218425118983, Test Loss: 0.002747484341899985\n",
            "Epoch 232/1000, Training Loss: 0.0030924441083562894, Test Loss: 0.0027474202332276525\n",
            "Epoch 233/1000, Training Loss: 0.0030923662446153436, Test Loss: 0.0027473560488796567\n",
            "Epoch 234/1000, Training Loss: 0.0030922882508769645, Test Loss: 0.0027472917877263935\n",
            "Epoch 235/1000, Training Loss: 0.0030922101267311097, Test Loss: 0.002747227448659155\n",
            "Epoch 236/1000, Training Loss: 0.0030921318717696857, Test Loss: 0.0027471630305895755\n",
            "Epoch 237/1000, Training Loss: 0.00309205348558644, Test Loss: 0.0027470985324490743\n",
            "Epoch 238/1000, Training Loss: 0.0030919749677768703, Test Loss: 0.0027470339531883293\n",
            "Epoch 239/1000, Training Loss: 0.0030918963179381253, Test Loss: 0.002746969291776759\n",
            "Epoch 240/1000, Training Loss: 0.0030918175356689183, Test Loss: 0.0027469045472020242\n",
            "Epoch 241/1000, Training Loss: 0.0030917386205694397, Test Loss: 0.0027468397184695344\n",
            "Epoch 242/1000, Training Loss: 0.0030916595722412787, Test Loss: 0.0027467748046019814\n",
            "Epoch 243/1000, Training Loss: 0.003091580390287344, Test Loss: 0.0027467098046388768\n",
            "Epoch 244/1000, Training Loss: 0.003091501074311791, Test Loss: 0.0027466447176361063\n",
            "Epoch 245/1000, Training Loss: 0.00309142162391995, Test Loss: 0.002746579542665497\n",
            "Epoch 246/1000, Training Loss: 0.0030913420387182603, Test Loss: 0.002746514278814392\n",
            "Epoch 247/1000, Training Loss: 0.0030912623183142075, Test Loss: 0.002746448925185249\n",
            "Epoch 248/1000, Training Loss: 0.003091182462316259, Test Loss: 0.002746383480895231\n",
            "Epoch 249/1000, Training Loss: 0.0030911024703338076, Test Loss: 0.0027463179450758313\n",
            "Epoch 250/1000, Training Loss: 0.0030910223419771165, Test Loss: 0.0027462523168724847\n",
            "Epoch 251/1000, Training Loss: 0.0030909420768572646, Test Loss: 0.0027461865954442138\n",
            "Epoch 252/1000, Training Loss: 0.003090861674586098, Test Loss: 0.002746120779963262\n",
            "Epoch 253/1000, Training Loss: 0.0030907811347761797, Test Loss: 0.0027460548696147573\n",
            "Epoch 254/1000, Training Loss: 0.0030907004570407456, Test Loss: 0.002745988863596367\n",
            "Epoch 255/1000, Training Loss: 0.003090619640993657, Test Loss: 0.0027459227611179794\n",
            "Epoch 256/1000, Training Loss: 0.0030905386862493623, Test Loss: 0.002745856561401376\n",
            "Epoch 257/1000, Training Loss: 0.0030904575924228573, Test Loss: 0.002745790263679925\n",
            "Epoch 258/1000, Training Loss: 0.003090376359129642, Test Loss: 0.0027457238671982835\n",
            "Epoch 259/1000, Training Loss: 0.0030902949859856897, Test Loss: 0.002745657371212101\n",
            "Epoch 260/1000, Training Loss: 0.00309021347260741, Test Loss: 0.0027455907749877285\n",
            "Epoch 261/1000, Training Loss: 0.0030901318186116137, Test Loss: 0.0027455240778019524\n",
            "Epoch 262/1000, Training Loss: 0.003090050023615485, Test Loss: 0.002745457278941712\n",
            "Epoch 263/1000, Training Loss: 0.0030899680872365476, Test Loss: 0.002745390377703841\n",
            "Epoch 264/1000, Training Loss: 0.0030898860090926394, Test Loss: 0.002745323373394817\n",
            "Epoch 265/1000, Training Loss: 0.003089803788801879, Test Loss: 0.0027452562653304978\n",
            "Epoch 266/1000, Training Loss: 0.003089721425982648, Test Loss: 0.002745189052835893\n",
            "Epoch 267/1000, Training Loss: 0.0030896389202535583, Test Loss: 0.0027451217352449154\n",
            "Epoch 268/1000, Training Loss: 0.0030895562712334314, Test Loss: 0.002745054311900161\n",
            "Epoch 269/1000, Training Loss: 0.0030894734785412736, Test Loss: 0.002744986782152673\n",
            "Epoch 270/1000, Training Loss: 0.0030893905417962576, Test Loss: 0.002744919145361733\n",
            "Epoch 271/1000, Training Loss: 0.0030893074606176984, Test Loss: 0.002744851400894644\n",
            "Epoch 272/1000, Training Loss: 0.0030892242346250345, Test Loss: 0.0027447835481265205\n",
            "Epoch 273/1000, Training Loss: 0.0030891408634378093, Test Loss: 0.0027447155864400922\n",
            "Epoch 274/1000, Training Loss: 0.003089057346675649, Test Loss: 0.0027446475152255014\n",
            "Epoch 275/1000, Training Loss: 0.003088973683958254, Test Loss: 0.0027445793338801124\n",
            "Epoch 276/1000, Training Loss: 0.00308888987490537, Test Loss: 0.00274451104180833\n",
            "Epoch 277/1000, Training Loss: 0.003088805919136785, Test Loss: 0.0027444426384214075\n",
            "Epoch 278/1000, Training Loss: 0.003088721816272303, Test Loss: 0.0027443741231372785\n",
            "Epoch 279/1000, Training Loss: 0.0030886375659317368, Test Loss: 0.0027443054953803793\n",
            "Epoch 280/1000, Training Loss: 0.0030885531677348915, Test Loss: 0.0027442367545814795\n",
            "Epoch 281/1000, Training Loss: 0.003088468621301551, Test Loss: 0.0027441679001775215\n",
            "Epoch 282/1000, Training Loss: 0.0030883839262514645, Test Loss: 0.0027440989316114555\n",
            "Epoch 283/1000, Training Loss: 0.003088299082204337, Test Loss: 0.00274402984833209\n",
            "Epoch 284/1000, Training Loss: 0.003088214088779817, Test Loss: 0.002743960649793934\n",
            "Epoch 285/1000, Training Loss: 0.0030881289455974813, Test Loss: 0.002743891335457049\n",
            "Epoch 286/1000, Training Loss: 0.003088043652276832, Test Loss: 0.0027438219047869077\n",
            "Epoch 287/1000, Training Loss: 0.003087958208437278, Test Loss: 0.0027437523572542534\n",
            "Epoch 288/1000, Training Loss: 0.0030878726136981337, Test Loss: 0.002743682692334956\n",
            "Epoch 289/1000, Training Loss: 0.003087786867678601, Test Loss: 0.002743612909509888\n",
            "Epoch 290/1000, Training Loss: 0.003087700969997769, Test Loss: 0.002743543008264784\n",
            "Epoch 291/1000, Training Loss: 0.0030876149202745977, Test Loss: 0.002743472988090124\n",
            "Epoch 292/1000, Training Loss: 0.0030875287181279165, Test Loss: 0.002743402848480997\n",
            "Epoch 293/1000, Training Loss: 0.0030874423631764114, Test Loss: 0.00274333258893699\n",
            "Epoch 294/1000, Training Loss: 0.0030873558550386206, Test Loss: 0.0027432622089620697\n",
            "Epoch 295/1000, Training Loss: 0.0030872691933329266, Test Loss: 0.0027431917080644568\n",
            "Epoch 296/1000, Training Loss: 0.0030871823776775485, Test Loss: 0.0027431210857565263\n",
            "Epoch 297/1000, Training Loss: 0.0030870954076905373, Test Loss: 0.0027430503415546916\n",
            "Epoch 298/1000, Training Loss: 0.003087008282989766, Test Loss: 0.0027429794749793002\n",
            "Epoch 299/1000, Training Loss: 0.0030869210031929284, Test Loss: 0.0027429084855545268\n",
            "Epoch 300/1000, Training Loss: 0.003086833567917532, Test Loss: 0.0027428373728082757\n",
            "Epoch 301/1000, Training Loss: 0.003086745976780889, Test Loss: 0.0027427661362720763\n",
            "Epoch 302/1000, Training Loss: 0.0030866582294001143, Test Loss: 0.0027426947754809945\n",
            "Epoch 303/1000, Training Loss: 0.0030865703253921207, Test Loss: 0.002742623289973532\n",
            "Epoch 304/1000, Training Loss: 0.003086482264373612, Test Loss: 0.002742551679291534\n",
            "Epoch 305/1000, Training Loss: 0.003086394045961081, Test Loss: 0.0027424799429801063\n",
            "Epoch 306/1000, Training Loss: 0.0030863056697708023, Test Loss: 0.00274240808058752\n",
            "Epoch 307/1000, Training Loss: 0.00308621713541883, Test Loss: 0.002742336091665134\n",
            "Epoch 308/1000, Training Loss: 0.003086128442520993, Test Loss: 0.0027422639757673033\n",
            "Epoch 309/1000, Training Loss: 0.0030860395906928905, Test Loss: 0.002742191732451305\n",
            "Epoch 310/1000, Training Loss: 0.0030859505795498886, Test Loss: 0.0027421193612772554\n",
            "Epoch 311/1000, Training Loss: 0.0030858614087071177, Test Loss: 0.002742046861808034\n",
            "Epoch 312/1000, Training Loss: 0.0030857720777794696, Test Loss: 0.0027419742336092096\n",
            "Epoch 313/1000, Training Loss: 0.003085682586381589, Test Loss: 0.0027419014762489636\n",
            "Epoch 314/1000, Training Loss: 0.0030855929341278778, Test Loss: 0.0027418285892980214\n",
            "Epoch 315/1000, Training Loss: 0.0030855031206324856, Test Loss: 0.0027417555723295794\n",
            "Epoch 316/1000, Training Loss: 0.0030854131455093138, Test Loss: 0.002741682424919238\n",
            "Epoch 317/1000, Training Loss: 0.003085323008372003, Test Loss: 0.0027416091466449396\n",
            "Epoch 318/1000, Training Loss: 0.0030852327088339406, Test Loss: 0.0027415357370868952\n",
            "Epoch 319/1000, Training Loss: 0.0030851422465082512, Test Loss: 0.0027414621958275274\n",
            "Epoch 320/1000, Training Loss: 0.0030850516210077965, Test Loss: 0.0027413885224514076\n",
            "Epoch 321/1000, Training Loss: 0.003084960831945175, Test Loss: 0.002741314716545191\n",
            "Epoch 322/1000, Training Loss: 0.003084869878932714, Test Loss: 0.0027412407776975637\n",
            "Epoch 323/1000, Training Loss: 0.003084778761582474, Test Loss: 0.0027411667054991844\n",
            "Epoch 324/1000, Training Loss: 0.0030846874795062416, Test Loss: 0.002741092499542625\n",
            "Epoch 325/1000, Training Loss: 0.003084596032315532, Test Loss: 0.002741018159422317\n",
            "Epoch 326/1000, Training Loss: 0.0030845044196215815, Test Loss: 0.0027409436847345005\n",
            "Epoch 327/1000, Training Loss: 0.00308441264103535, Test Loss: 0.00274086907507717\n",
            "Epoch 328/1000, Training Loss: 0.0030843206961675197, Test Loss: 0.0027407943300500216\n",
            "Epoch 329/1000, Training Loss: 0.0030842285846284867, Test Loss: 0.0027407194492544145\n",
            "Epoch 330/1000, Training Loss: 0.003084136306028369, Test Loss: 0.0027406444322933023\n",
            "Epoch 331/1000, Training Loss: 0.0030840438599769983, Test Loss: 0.0027405692787712053\n",
            "Epoch 332/1000, Training Loss: 0.003083951246083919, Test Loss: 0.002740493988294154\n",
            "Epoch 333/1000, Training Loss: 0.00308385846395839, Test Loss: 0.002740418560469648\n",
            "Epoch 334/1000, Training Loss: 0.0030837655132093795, Test Loss: 0.0027403429949066096\n",
            "Epoch 335/1000, Training Loss: 0.0030836723934455656, Test Loss: 0.002740267291215343\n",
            "Epoch 336/1000, Training Loss: 0.003083579104275337, Test Loss: 0.0027401914490074912\n",
            "Epoch 337/1000, Training Loss: 0.003083485645306786, Test Loss: 0.002740115467895995\n",
            "Epoch 338/1000, Training Loss: 0.0030833920161477137, Test Loss: 0.0027400393474950567\n",
            "Epoch 339/1000, Training Loss: 0.0030832982164056243, Test Loss: 0.002739963087420095\n",
            "Epoch 340/1000, Training Loss: 0.0030832042456877275, Test Loss: 0.002739886687287712\n",
            "Epoch 341/1000, Training Loss: 0.0030831101036009312, Test Loss: 0.0027398101467156584\n",
            "Epoch 342/1000, Training Loss: 0.00308301578975185, Test Loss: 0.0027397334653227845\n",
            "Epoch 343/1000, Training Loss: 0.003082921303746796, Test Loss: 0.0027396566427290233\n",
            "Epoch 344/1000, Training Loss: 0.003082826645191782, Test Loss: 0.0027395796785553415\n",
            "Epoch 345/1000, Training Loss: 0.0030827318136925196, Test Loss: 0.002739502572423717\n",
            "Epoch 346/1000, Training Loss: 0.0030826368088544163, Test Loss: 0.0027394253239570922\n",
            "Epoch 347/1000, Training Loss: 0.003082541630282579, Test Loss: 0.002739347932779357\n",
            "Epoch 348/1000, Training Loss: 0.00308244627758181, Test Loss: 0.0027392703985153065\n",
            "Epoch 349/1000, Training Loss: 0.003082350750356605, Test Loss: 0.002739192720790619\n",
            "Epoch 350/1000, Training Loss: 0.0030822550482111576, Test Loss: 0.00273911489923182\n",
            "Epoch 351/1000, Training Loss: 0.0030821591707493523, Test Loss: 0.002739036933466254\n",
            "Epoch 352/1000, Training Loss: 0.0030820631175747693, Test Loss: 0.002738958823122059\n",
            "Epoch 353/1000, Training Loss: 0.003081966888290678, Test Loss: 0.002738880567828138\n",
            "Epoch 354/1000, Training Loss: 0.0030818704825000443, Test Loss: 0.002738802167214128\n",
            "Epoch 355/1000, Training Loss: 0.003081773899805521, Test Loss: 0.002738723620910381\n",
            "Epoch 356/1000, Training Loss: 0.0030816771398094545, Test Loss: 0.002738644928547932\n",
            "Epoch 357/1000, Training Loss: 0.0030815802021138803, Test Loss: 0.0027385660897584745\n",
            "Epoch 358/1000, Training Loss: 0.0030814830863205235, Test Loss: 0.0027384871041743414\n",
            "Epoch 359/1000, Training Loss: 0.0030813857920307976, Test Loss: 0.002738407971428474\n",
            "Epoch 360/1000, Training Loss: 0.003081288318845807, Test Loss: 0.0027383286911544054\n",
            "Epoch 361/1000, Training Loss: 0.003081190666366342, Test Loss: 0.0027382492629862286\n",
            "Epoch 362/1000, Training Loss: 0.003081092834192881, Test Loss: 0.002738169686558583\n",
            "Epoch 363/1000, Training Loss: 0.003080994821925591, Test Loss: 0.002738089961506633\n",
            "Epoch 364/1000, Training Loss: 0.0030808966291643253, Test Loss: 0.002738010087466037\n",
            "Epoch 365/1000, Training Loss: 0.003080798255508622, Test Loss: 0.0027379300640729353\n",
            "Epoch 366/1000, Training Loss: 0.003080699700557709, Test Loss: 0.0027378498909639303\n",
            "Epoch 367/1000, Training Loss: 0.0030806009639104963, Test Loss: 0.00273776956777606\n",
            "Epoch 368/1000, Training Loss: 0.003080502045165582, Test Loss: 0.0027376890941467865\n",
            "Epoch 369/1000, Training Loss: 0.0030804029439212466, Test Loss: 0.0027376084697139707\n",
            "Epoch 370/1000, Training Loss: 0.0030803036597754586, Test Loss: 0.0027375276941158573\n",
            "Epoch 371/1000, Training Loss: 0.00308020419232587, Test Loss: 0.0027374467669910563\n",
            "Epoch 372/1000, Training Loss: 0.0030801045411698157, Test Loss: 0.0027373656879785295\n",
            "Epoch 373/1000, Training Loss: 0.003080004705904318, Test Loss: 0.0027372844567175618\n",
            "Epoch 374/1000, Training Loss: 0.003079904686126079, Test Loss: 0.0027372030728477614\n",
            "Epoch 375/1000, Training Loss: 0.0030798044814314873, Test Loss: 0.002737121536009028\n",
            "Epoch 376/1000, Training Loss: 0.0030797040914166142, Test Loss: 0.0027370398458415423\n",
            "Epoch 377/1000, Training Loss: 0.0030796035156772134, Test Loss: 0.002736958001985758\n",
            "Epoch 378/1000, Training Loss: 0.003079502753808724, Test Loss: 0.0027368760040823764\n",
            "Epoch 379/1000, Training Loss: 0.003079401805406265, Test Loss: 0.0027367938517723327\n",
            "Epoch 380/1000, Training Loss: 0.003079300670064642, Test Loss: 0.0027367115446967926\n",
            "Epoch 381/1000, Training Loss: 0.003079199347378339, Test Loss: 0.002736629082497121\n",
            "Epoch 382/1000, Training Loss: 0.0030790978369415268, Test Loss: 0.002736546464814882\n",
            "Epoch 383/1000, Training Loss: 0.0030789961383480544, Test Loss: 0.0027364636912918204\n",
            "Epoch 384/1000, Training Loss: 0.0030788942511914574, Test Loss: 0.0027363807615698493\n",
            "Epoch 385/1000, Training Loss: 0.0030787921750649507, Test Loss: 0.0027362976752910357\n",
            "Epoch 386/1000, Training Loss: 0.003078689909561433, Test Loss: 0.00273621443209759\n",
            "Epoch 387/1000, Training Loss: 0.0030785874542734837, Test Loss: 0.002736131031631853\n",
            "Epoch 388/1000, Training Loss: 0.0030784848087933676, Test Loss: 0.0027360474735362863\n",
            "Epoch 389/1000, Training Loss: 0.0030783819727130267, Test Loss: 0.0027359637574534534\n",
            "Epoch 390/1000, Training Loss: 0.003078278945624091, Test Loss: 0.00273587988302602\n",
            "Epoch 391/1000, Training Loss: 0.0030781757271178666, Test Loss: 0.002735795849896733\n",
            "Epoch 392/1000, Training Loss: 0.0030780723167853465, Test Loss: 0.002735711657708413\n",
            "Epoch 393/1000, Training Loss: 0.003077968714217203, Test Loss: 0.0027356273061039465\n",
            "Epoch 394/1000, Training Loss: 0.003077864919003792, Test Loss: 0.002735542794726272\n",
            "Epoch 395/1000, Training Loss: 0.0030777609307351505, Test Loss: 0.0027354581232183683\n",
            "Epoch 396/1000, Training Loss: 0.0030776567490010003, Test Loss: 0.002735373291223253\n",
            "Epoch 397/1000, Training Loss: 0.003077552373390742, Test Loss: 0.0027352882983839645\n",
            "Epoch 398/1000, Training Loss: 0.0030774478034934598, Test Loss: 0.002735203144343555\n",
            "Epoch 399/1000, Training Loss: 0.0030773430388979215, Test Loss: 0.002735117828745084\n",
            "Epoch 400/1000, Training Loss: 0.0030772380791925773, Test Loss: 0.0027350323512316055\n",
            "Epoch 401/1000, Training Loss: 0.0030771329239655597, Test Loss: 0.0027349467114461644\n",
            "Epoch 402/1000, Training Loss: 0.0030770275728046843, Test Loss: 0.002734860909031782\n",
            "Epoch 403/1000, Training Loss: 0.0030769220252974483, Test Loss: 0.0027347749436314535\n",
            "Epoch 404/1000, Training Loss: 0.003076816281031032, Test Loss: 0.002734688814888134\n",
            "Epoch 405/1000, Training Loss: 0.003076710339592301, Test Loss: 0.0027346025224447364\n",
            "Epoch 406/1000, Training Loss: 0.0030766042005678034, Test Loss: 0.002734516065944121\n",
            "Epoch 407/1000, Training Loss: 0.00307649786354377, Test Loss: 0.002734429445029087\n",
            "Epoch 408/1000, Training Loss: 0.0030763913281061156, Test Loss: 0.002734342659342367\n",
            "Epoch 409/1000, Training Loss: 0.003076284593840439, Test Loss: 0.00273425570852662\n",
            "Epoch 410/1000, Training Loss: 0.003076177660332023, Test Loss: 0.0027341685922244235\n",
            "Epoch 411/1000, Training Loss: 0.003076070527165835, Test Loss: 0.0027340813100782658\n",
            "Epoch 412/1000, Training Loss: 0.0030759631939265264, Test Loss: 0.00273399386173054\n",
            "Epoch 413/1000, Training Loss: 0.0030758556601984343, Test Loss: 0.0027339062468235403\n",
            "Epoch 414/1000, Training Loss: 0.0030757479255655784, Test Loss: 0.0027338184649994533\n",
            "Epoch 415/1000, Training Loss: 0.003075639989611667, Test Loss: 0.00273373051590035\n",
            "Epoch 416/1000, Training Loss: 0.0030755318519200907, Test Loss: 0.0027336423991681834\n",
            "Epoch 417/1000, Training Loss: 0.0030754235120739274, Test Loss: 0.002733554114444781\n",
            "Epoch 418/1000, Training Loss: 0.00307531496965594, Test Loss: 0.0027334656613718393\n",
            "Epoch 419/1000, Training Loss: 0.0030752062242485802, Test Loss: 0.0027333770395909147\n",
            "Epoch 420/1000, Training Loss: 0.0030750972754339818, Test Loss: 0.0027332882487434264\n",
            "Epoch 421/1000, Training Loss: 0.0030749881227939684, Test Loss: 0.0027331992884706436\n",
            "Epoch 422/1000, Training Loss: 0.003074878765910051, Test Loss: 0.0027331101584136836\n",
            "Epoch 423/1000, Training Loss: 0.0030747692043634266, Test Loss: 0.0027330208582135055\n",
            "Epoch 424/1000, Training Loss: 0.003074659437734981, Test Loss: 0.0027329313875109045\n",
            "Epoch 425/1000, Training Loss: 0.0030745494656052867, Test Loss: 0.002732841745946511\n",
            "Epoch 426/1000, Training Loss: 0.0030744392875546073, Test Loss: 0.002732751933160784\n",
            "Epoch 427/1000, Training Loss: 0.003074328903162891, Test Loss: 0.0027326619487940008\n",
            "Epoch 428/1000, Training Loss: 0.0030742183120097805, Test Loss: 0.0027325717924862624\n",
            "Epoch 429/1000, Training Loss: 0.003074107513674602, Test Loss: 0.002732481463877483\n",
            "Epoch 430/1000, Training Loss: 0.0030739965077363775, Test Loss: 0.002732390962607388\n",
            "Epoch 431/1000, Training Loss: 0.0030738852937738145, Test Loss: 0.002732300288315507\n",
            "Epoch 432/1000, Training Loss: 0.003073773871365314, Test Loss: 0.002732209440641172\n",
            "Epoch 433/1000, Training Loss: 0.0030736622400889674, Test Loss: 0.0027321184192235174\n",
            "Epoch 434/1000, Training Loss: 0.003073550399522557, Test Loss: 0.002732027223701467\n",
            "Epoch 435/1000, Training Loss: 0.003073438349243558, Test Loss: 0.002731935853713738\n",
            "Epoch 436/1000, Training Loss: 0.003073326088829137, Test Loss: 0.002731844308898833\n",
            "Epoch 437/1000, Training Loss: 0.0030732136178561542, Test Loss: 0.0027317525888950417\n",
            "Epoch 438/1000, Training Loss: 0.0030731009359011617, Test Loss: 0.002731660693340431\n",
            "Epoch 439/1000, Training Loss: 0.003072988042540408, Test Loss: 0.002731568621872846\n",
            "Epoch 440/1000, Training Loss: 0.0030728749373498325, Test Loss: 0.0027314763741299037\n",
            "Epoch 441/1000, Training Loss: 0.0030727616199050715, Test Loss: 0.002731383949748993\n",
            "Epoch 442/1000, Training Loss: 0.003072648089781457, Test Loss: 0.002731291348367271\n",
            "Epoch 443/1000, Training Loss: 0.0030725343465540147, Test Loss: 0.0027311985696216595\n",
            "Epoch 444/1000, Training Loss: 0.0030724203897974658, Test Loss: 0.0027311056131488387\n",
            "Epoch 445/1000, Training Loss: 0.0030723062190862304, Test Loss: 0.002731012478585248\n",
            "Epoch 446/1000, Training Loss: 0.0030721918339944266, Test Loss: 0.002730919165567085\n",
            "Epoch 447/1000, Training Loss: 0.0030720772340958667, Test Loss: 0.0027308256737302995\n",
            "Epoch 448/1000, Training Loss: 0.003071962418964065, Test Loss: 0.002730732002710591\n",
            "Epoch 449/1000, Training Loss: 0.0030718473881722313, Test Loss: 0.002730638152143406\n",
            "Epoch 450/1000, Training Loss: 0.0030717321412932776, Test Loss: 0.002730544121663938\n",
            "Epoch 451/1000, Training Loss: 0.0030716166778998136, Test Loss: 0.0027304499109071247\n",
            "Epoch 452/1000, Training Loss: 0.0030715009975641524, Test Loss: 0.0027303555195076416\n",
            "Epoch 453/1000, Training Loss: 0.0030713850998583046, Test Loss: 0.002730260947099904\n",
            "Epoch 454/1000, Training Loss: 0.0030712689843539853, Test Loss: 0.0027301661933180647\n",
            "Epoch 455/1000, Training Loss: 0.0030711526506226106, Test Loss: 0.002730071257796006\n",
            "Epoch 456/1000, Training Loss: 0.0030710360982353015, Test Loss: 0.002729976140167347\n",
            "Epoch 457/1000, Training Loss: 0.00307091932676288, Test Loss: 0.002729880840065436\n",
            "Epoch 458/1000, Training Loss: 0.003070802335775873, Test Loss: 0.0027297853571233454\n",
            "Epoch 459/1000, Training Loss: 0.0030706851248445133, Test Loss: 0.0027296896909738775\n",
            "Epoch 460/1000, Training Loss: 0.003070567693538739, Test Loss: 0.002729593841249558\n",
            "Epoch 461/1000, Training Loss: 0.0030704500414281953, Test Loss: 0.0027294978075826344\n",
            "Epoch 462/1000, Training Loss: 0.003070332168082231, Test Loss: 0.0027294015896050716\n",
            "Epoch 463/1000, Training Loss: 0.0030702140730699055, Test Loss: 0.0027293051869485586\n",
            "Epoch 464/1000, Training Loss: 0.003070095755959986, Test Loss: 0.0027292085992445016\n",
            "Epoch 465/1000, Training Loss: 0.0030699772163209488, Test Loss: 0.0027291118261240153\n",
            "Epoch 466/1000, Training Loss: 0.0030698584537209774, Test Loss: 0.002729014867217938\n",
            "Epoch 467/1000, Training Loss: 0.00306973946772797, Test Loss: 0.0027289177221568135\n",
            "Epoch 468/1000, Training Loss: 0.003069620257909532, Test Loss: 0.0027288203905708984\n",
            "Epoch 469/1000, Training Loss: 0.003069500823832983, Test Loss: 0.0027287228720901616\n",
            "Epoch 470/1000, Training Loss: 0.0030693811650653545, Test Loss: 0.0027286251663442796\n",
            "Epoch 471/1000, Training Loss: 0.003069261281173392, Test Loss: 0.0027285272729626327\n",
            "Epoch 472/1000, Training Loss: 0.003069141171723554, Test Loss: 0.0027284291915743108\n",
            "Epoch 473/1000, Training Loss: 0.003069020836282015, Test Loss: 0.002728330921808107\n",
            "Epoch 474/1000, Training Loss: 0.003068900274414665, Test Loss: 0.002728232463292515\n",
            "Epoch 475/1000, Training Loss: 0.0030687794856871097, Test Loss: 0.002728133815655737\n",
            "Epoch 476/1000, Training Loss: 0.003068658469664674, Test Loss: 0.002728034978525673\n",
            "Epoch 477/1000, Training Loss: 0.0030685372259124, Test Loss: 0.0027279359515299207\n",
            "Epoch 478/1000, Training Loss: 0.0030684157539950484, Test Loss: 0.00272783673429578\n",
            "Epoch 479/1000, Training Loss: 0.0030682940534770995, Test Loss: 0.00272773732645025\n",
            "Epoch 480/1000, Training Loss: 0.0030681721239227567, Test Loss: 0.0027276377276200243\n",
            "Epoch 481/1000, Training Loss: 0.003068049964895942, Test Loss: 0.002727537937431495\n",
            "Epoch 482/1000, Training Loss: 0.003067927575960302, Test Loss: 0.0027274379555107466\n",
            "Epoch 483/1000, Training Loss: 0.003067804956679204, Test Loss: 0.0027273377814835613\n",
            "Epoch 484/1000, Training Loss: 0.0030676821066157444, Test Loss: 0.0027272374149754143\n",
            "Epoch 485/1000, Training Loss: 0.0030675590253327395, Test Loss: 0.0027271368556114756\n",
            "Epoch 486/1000, Training Loss: 0.003067435712392734, Test Loss: 0.0027270361030166035\n",
            "Epoch 487/1000, Training Loss: 0.003067312167358001, Test Loss: 0.0027269351568153517\n",
            "Epoch 488/1000, Training Loss: 0.003067188389790538, Test Loss: 0.002726834016631963\n",
            "Epoch 489/1000, Training Loss: 0.003067064379252076, Test Loss: 0.0027267326820903724\n",
            "Epoch 490/1000, Training Loss: 0.0030669401353040702, Test Loss: 0.002726631152814204\n",
            "Epoch 491/1000, Training Loss: 0.003066815657507712, Test Loss: 0.002726529428426771\n",
            "Epoch 492/1000, Training Loss: 0.003066690945423921, Test Loss: 0.0027264275085510754\n",
            "Epoch 493/1000, Training Loss: 0.0030665659986133507, Test Loss: 0.002726325392809808\n",
            "Epoch 494/1000, Training Loss: 0.0030664408166363897, Test Loss: 0.002726223080825346\n",
            "Epoch 495/1000, Training Loss: 0.003066315399053159, Test Loss: 0.0027261205722197575\n",
            "Epoch 496/1000, Training Loss: 0.0030661897454235167, Test Loss: 0.0027260178666147937\n",
            "Epoch 497/1000, Training Loss: 0.003066063855307057, Test Loss: 0.002725914963631896\n",
            "Epoch 498/1000, Training Loss: 0.003065937728263114, Test Loss: 0.002725811862892189\n",
            "Epoch 499/1000, Training Loss: 0.003065811363850757, Test Loss: 0.0027257085640164875\n",
            "Epoch 500/1000, Training Loss: 0.0030656847616287995, Test Loss: 0.002725605066625285\n",
            "Epoch 501/1000, Training Loss: 0.003065557921155793, Test Loss: 0.002725501370338769\n",
            "Epoch 502/1000, Training Loss: 0.0030654308419900324, Test Loss: 0.0027253974747768073\n",
            "Epoch 503/1000, Training Loss: 0.0030653035236895554, Test Loss: 0.002725293379558953\n",
            "Epoch 504/1000, Training Loss: 0.0030651759658121445, Test Loss: 0.0027251890843044465\n",
            "Epoch 505/1000, Training Loss: 0.0030650481679153274, Test Loss: 0.00272508458863221\n",
            "Epoch 506/1000, Training Loss: 0.0030649201295563783, Test Loss: 0.002724979892160853\n",
            "Epoch 507/1000, Training Loss: 0.0030647918502923195, Test Loss: 0.002724874994508667\n",
            "Epoch 508/1000, Training Loss: 0.003064663329679922, Test Loss: 0.002724769895293631\n",
            "Epoch 509/1000, Training Loss: 0.003064534567275705, Test Loss: 0.0027246645941334043\n",
            "Epoch 510/1000, Training Loss: 0.0030644055626359436, Test Loss: 0.0027245590906453346\n",
            "Epoch 511/1000, Training Loss: 0.00306427631531666, Test Loss: 0.0027244533844464546\n",
            "Epoch 512/1000, Training Loss: 0.0030641468248736336, Test Loss: 0.0027243474751534763\n",
            "Epoch 513/1000, Training Loss: 0.003064017090862397, Test Loss: 0.002724241362382799\n",
            "Epoch 514/1000, Training Loss: 0.0030638871128382396, Test Loss: 0.0027241350457505088\n",
            "Epoch 515/1000, Training Loss: 0.0030637568903562075, Test Loss: 0.0027240285248723724\n",
            "Epoch 516/1000, Training Loss: 0.0030636264229711046, Test Loss: 0.0027239217993638445\n",
            "Epoch 517/1000, Training Loss: 0.003063495710237497, Test Loss: 0.002723814868840062\n",
            "Epoch 518/1000, Training Loss: 0.00306336475170971, Test Loss: 0.0027237077329158505\n",
            "Epoch 519/1000, Training Loss: 0.00306323354694183, Test Loss: 0.0027236003912057155\n",
            "Epoch 520/1000, Training Loss: 0.003063102095487711, Test Loss: 0.002723492843323853\n",
            "Epoch 521/1000, Training Loss: 0.0030629703969009673, Test Loss: 0.0027233850888841427\n",
            "Epoch 522/1000, Training Loss: 0.0030628384507349828, Test Loss: 0.0027232771275001494\n",
            "Epoch 523/1000, Training Loss: 0.0030627062565429064, Test Loss: 0.002723168958785125\n",
            "Epoch 524/1000, Training Loss: 0.0030625738138776583, Test Loss: 0.002723060582352007\n",
            "Epoch 525/1000, Training Loss: 0.0030624411222919274, Test Loss: 0.0027229519978134204\n",
            "Epoch 526/1000, Training Loss: 0.003062308181338174, Test Loss: 0.002722843204781678\n",
            "Epoch 527/1000, Training Loss: 0.003062174990568633, Test Loss: 0.0027227342028687774\n",
            "Epoch 528/1000, Training Loss: 0.0030620415495353115, Test Loss: 0.002722624991686408\n",
            "Epoch 529/1000, Training Loss: 0.003061907857789994, Test Loss: 0.0027225155708459434\n",
            "Epoch 530/1000, Training Loss: 0.0030617739148842403, Test Loss: 0.0027224059399584477\n",
            "Epoch 531/1000, Training Loss: 0.0030616397203693923, Test Loss: 0.0027222960986346746\n",
            "Epoch 532/1000, Training Loss: 0.0030615052737965674, Test Loss: 0.002722186046485066\n",
            "Epoch 533/1000, Training Loss: 0.0030613705747166674, Test Loss: 0.0027220757831197535\n",
            "Epoch 534/1000, Training Loss: 0.003061235622680377, Test Loss: 0.0027219653081485616\n",
            "Epoch 535/1000, Training Loss: 0.003061100417238164, Test Loss: 0.0027218546211810013\n",
            "Epoch 536/1000, Training Loss: 0.003060964957940282, Test Loss: 0.0027217437218262797\n",
            "Epoch 537/1000, Training Loss: 0.003060829244336774, Test Loss: 0.002721632609693293\n",
            "Epoch 538/1000, Training Loss: 0.0030606932759774685, Test Loss: 0.002721521284390632\n",
            "Epoch 539/1000, Training Loss: 0.0030605570524119886, Test Loss: 0.0027214097455265755\n",
            "Epoch 540/1000, Training Loss: 0.003060420573189746, Test Loss: 0.0027212979927091047\n",
            "Epoch 541/1000, Training Loss: 0.0030602838378599464, Test Loss: 0.0027211860255458866\n",
            "Epoch 542/1000, Training Loss: 0.003060146845971592, Test Loss: 0.0027210738436442877\n",
            "Epoch 543/1000, Training Loss: 0.0030600095970734808, Test Loss: 0.002720961446611372\n",
            "Epoch 544/1000, Training Loss: 0.003059872090714208, Test Loss: 0.0027208488340538916\n",
            "Epoch 545/1000, Training Loss: 0.00305973432644217, Test Loss: 0.0027207360055783053\n",
            "Epoch 546/1000, Training Loss: 0.003059596303805564, Test Loss: 0.0027206229607907627\n",
            "Epoch 547/1000, Training Loss: 0.003059458022352391, Test Loss: 0.002720509699297116\n",
            "Epoch 548/1000, Training Loss: 0.0030593194816304547, Test Loss: 0.0027203962207029125\n",
            "Epoch 549/1000, Training Loss: 0.0030591806811873686, Test Loss: 0.0027202825246134044\n",
            "Epoch 550/1000, Training Loss: 0.0030590416205705496, Test Loss: 0.0027201686106335388\n",
            "Epoch 551/1000, Training Loss: 0.003058902299327228, Test Loss: 0.002720054478367968\n",
            "Epoch 552/1000, Training Loss: 0.003058762717004444, Test Loss: 0.0027199401274210465\n",
            "Epoch 553/1000, Training Loss: 0.0030586228731490504, Test Loss: 0.0027198255573968304\n",
            "Epoch 554/1000, Training Loss: 0.003058482767307715, Test Loss: 0.0027197107678990796\n",
            "Epoch 555/1000, Training Loss: 0.0030583423990269256, Test Loss: 0.0027195957585312612\n",
            "Epoch 556/1000, Training Loss: 0.003058201767852981, Test Loss: 0.002719480528896544\n",
            "Epoch 557/1000, Training Loss: 0.0030580608733320066, Test Loss: 0.0027193650785978067\n",
            "Epoch 558/1000, Training Loss: 0.003057919715009946, Test Loss: 0.002719249407237632\n",
            "Epoch 559/1000, Training Loss: 0.003057778292432568, Test Loss: 0.0027191335144183173\n",
            "Epoch 560/1000, Training Loss: 0.0030576366051454653, Test Loss: 0.002719017399741863\n",
            "Epoch 561/1000, Training Loss: 0.0030574946526940607, Test Loss: 0.0027189010628099817\n",
            "Epoch 562/1000, Training Loss: 0.0030573524346236016, Test Loss: 0.0027187845032240953\n",
            "Epoch 563/1000, Training Loss: 0.00305720995047917, Test Loss: 0.002718667720585344\n",
            "Epoch 564/1000, Training Loss: 0.003057067199805679, Test Loss: 0.0027185507144945747\n",
            "Epoch 565/1000, Training Loss: 0.0030569241821478766, Test Loss: 0.002718433484552351\n",
            "Epoch 566/1000, Training Loss: 0.003056780897050347, Test Loss: 0.0027183160303589538\n",
            "Epoch 567/1000, Training Loss: 0.003056637344057512, Test Loss: 0.002718198351514376\n",
            "Epoch 568/1000, Training Loss: 0.003056493522713636, Test Loss: 0.0027180804476183316\n",
            "Epoch 569/1000, Training Loss: 0.003056349432562824, Test Loss: 0.002717962318270252\n",
            "Epoch 570/1000, Training Loss: 0.0030562050731490226, Test Loss: 0.0027178439630692864\n",
            "Epoch 571/1000, Training Loss: 0.003056060444016031, Test Loss: 0.0027177253816143094\n",
            "Epoch 572/1000, Training Loss: 0.0030559155447074894, Test Loss: 0.002717606573503912\n",
            "Epoch 573/1000, Training Loss: 0.003055770374766893, Test Loss: 0.0027174875383364112\n",
            "Epoch 574/1000, Training Loss: 0.0030556249337375854, Test Loss: 0.0027173682757098473\n",
            "Epoch 575/1000, Training Loss: 0.0030554792211627668, Test Loss: 0.0027172487852219875\n",
            "Epoch 576/1000, Training Loss: 0.0030553332365854924, Test Loss: 0.0027171290664703227\n",
            "Epoch 577/1000, Training Loss: 0.003055186979548676, Test Loss: 0.002717009119052076\n",
            "Epoch 578/1000, Training Loss: 0.0030550404495950922, Test Loss: 0.0027168889425641975\n",
            "Epoch 579/1000, Training Loss: 0.0030548936462673737, Test Loss: 0.002716768536603364\n",
            "Epoch 580/1000, Training Loss: 0.0030547465691080243, Test Loss: 0.002716647900765988\n",
            "Epoch 581/1000, Training Loss: 0.0030545992176594073, Test Loss: 0.002716527034648214\n",
            "Epoch 582/1000, Training Loss: 0.003054451591463759, Test Loss: 0.0027164059378459206\n",
            "Epoch 583/1000, Training Loss: 0.003054303690063186, Test Loss: 0.0027162846099547224\n",
            "Epoch 584/1000, Training Loss: 0.0030541555129996647, Test Loss: 0.0027161630505699703\n",
            "Epoch 585/1000, Training Loss: 0.0030540070598150497, Test Loss: 0.0027160412592867524\n",
            "Epoch 586/1000, Training Loss: 0.0030538583300510707, Test Loss: 0.0027159192356998978\n",
            "Epoch 587/1000, Training Loss: 0.003053709323249338, Test Loss: 0.002715796979403976\n",
            "Epoch 588/1000, Training Loss: 0.0030535600389513425, Test Loss: 0.002715674489993299\n",
            "Epoch 589/1000, Training Loss: 0.0030534104766984584, Test Loss: 0.0027155517670619223\n",
            "Epoch 590/1000, Training Loss: 0.003053260636031947, Test Loss: 0.002715428810203646\n",
            "Epoch 591/1000, Training Loss: 0.0030531105164929577, Test Loss: 0.002715305619012019\n",
            "Epoch 592/1000, Training Loss: 0.0030529601176225297, Test Loss: 0.0027151821930803356\n",
            "Epoch 593/1000, Training Loss: 0.0030528094389615946, Test Loss: 0.0027150585320016418\n",
            "Epoch 594/1000, Training Loss: 0.0030526584800509805, Test Loss: 0.0027149346353687325\n",
            "Epoch 595/1000, Training Loss: 0.003052507240431411, Test Loss: 0.0027148105027741576\n",
            "Epoch 596/1000, Training Loss: 0.0030523557196435115, Test Loss: 0.002714686133810219\n",
            "Epoch 597/1000, Training Loss: 0.003052203917227807, Test Loss: 0.0027145615280689765\n",
            "Epoch 598/1000, Training Loss: 0.003052051832724729, Test Loss: 0.0027144366851422465\n",
            "Epoch 599/1000, Training Loss: 0.0030518994656746146, Test Loss: 0.0027143116046216002\n",
            "Epoch 600/1000, Training Loss: 0.0030517468156177107, Test Loss: 0.0027141862860983763\n",
            "Epoch 601/1000, Training Loss: 0.0030515938820941733, Test Loss: 0.0027140607291636703\n",
            "Epoch 602/1000, Training Loss: 0.003051440664644076, Test Loss: 0.002713934933408343\n",
            "Epoch 603/1000, Training Loss: 0.0030512871628074066, Test Loss: 0.0027138088984230203\n",
            "Epoch 604/1000, Training Loss: 0.0030511333761240734, Test Loss: 0.0027136826237980968\n",
            "Epoch 605/1000, Training Loss: 0.0030509793041339026, Test Loss: 0.002713556109123732\n",
            "Epoch 606/1000, Training Loss: 0.0030508249463766478, Test Loss: 0.002713429353989859\n",
            "Epoch 607/1000, Training Loss: 0.003050670302391988, Test Loss: 0.002713302357986181\n",
            "Epoch 608/1000, Training Loss: 0.00305051537171953, Test Loss: 0.0027131751207021774\n",
            "Epoch 609/1000, Training Loss: 0.003050360153898812, Test Loss: 0.0027130476417271017\n",
            "Epoch 610/1000, Training Loss: 0.0030502046484693064, Test Loss: 0.0027129199206499836\n",
            "Epoch 611/1000, Training Loss: 0.003050048854970423, Test Loss: 0.0027127919570596334\n",
            "Epoch 612/1000, Training Loss: 0.003049892772941509, Test Loss: 0.0027126637505446405\n",
            "Epoch 613/1000, Training Loss: 0.003049736401921856, Test Loss: 0.002712535300693381\n",
            "Epoch 614/1000, Training Loss: 0.0030495797414506953, Test Loss: 0.0027124066070940123\n",
            "Epoch 615/1000, Training Loss: 0.003049422791067209, Test Loss: 0.002712277669334477\n",
            "Epoch 616/1000, Training Loss: 0.0030492655503105274, Test Loss: 0.0027121484870025097\n",
            "Epoch 617/1000, Training Loss: 0.003049108018719733, Test Loss: 0.0027120190596856327\n",
            "Epoch 618/1000, Training Loss: 0.003048950195833863, Test Loss: 0.0027118893869711603\n",
            "Epoch 619/1000, Training Loss: 0.0030487920811919115, Test Loss: 0.0027117594684462013\n",
            "Epoch 620/1000, Training Loss: 0.003048633674332835, Test Loss: 0.002711629303697663\n",
            "Epoch 621/1000, Training Loss: 0.003048474974795552, Test Loss: 0.0027114988923122446\n",
            "Epoch 622/1000, Training Loss: 0.0030483159821189454, Test Loss: 0.0027113682338764503\n",
            "Epoch 623/1000, Training Loss: 0.0030481566958418683, Test Loss: 0.002711237327976584\n",
            "Epoch 624/1000, Training Loss: 0.0030479971155031433, Test Loss: 0.0027111061741987535\n",
            "Epoch 625/1000, Training Loss: 0.0030478372406415686, Test Loss: 0.002710974772128874\n",
            "Epoch 626/1000, Training Loss: 0.003047677070795921, Test Loss: 0.002710843121352665\n",
            "Epoch 627/1000, Training Loss: 0.0030475166055049536, Test Loss: 0.0027107112214556613\n",
            "Epoch 628/1000, Training Loss: 0.0030473558443074047, Test Loss: 0.002710579072023202\n",
            "Epoch 629/1000, Training Loss: 0.0030471947867419936, Test Loss: 0.0027104466726404503\n",
            "Epoch 630/1000, Training Loss: 0.003047033432347436, Test Loss: 0.0027103140228923765\n",
            "Epoch 631/1000, Training Loss: 0.00304687178066243, Test Loss: 0.002710181122363776\n",
            "Epoch 632/1000, Training Loss: 0.003046709831225676, Test Loss: 0.002710047970639259\n",
            "Epoch 633/1000, Training Loss: 0.0030465475835758636, Test Loss: 0.0027099145673032633\n",
            "Epoch 634/1000, Training Loss: 0.0030463850372516887, Test Loss: 0.0027097809119400475\n",
            "Epoch 635/1000, Training Loss: 0.0030462221917918475, Test Loss: 0.0027096470041337007\n",
            "Epoch 636/1000, Training Loss: 0.0030460590467350436, Test Loss: 0.0027095128434681385\n",
            "Epoch 637/1000, Training Loss: 0.0030458956016199868, Test Loss: 0.0027093784295271085\n",
            "Epoch 638/1000, Training Loss: 0.003045731855985402, Test Loss: 0.0027092437618941954\n",
            "Epoch 639/1000, Training Loss: 0.003045567809370028, Test Loss: 0.0027091088401528145\n",
            "Epoch 640/1000, Training Loss: 0.0030454034613126217, Test Loss: 0.0027089736638862237\n",
            "Epoch 641/1000, Training Loss: 0.0030452388113519613, Test Loss: 0.0027088382326775195\n",
            "Epoch 642/1000, Training Loss: 0.003045073859026849, Test Loss: 0.002708702546109643\n",
            "Epoch 643/1000, Training Loss: 0.003044908603876116, Test Loss: 0.0027085666037653766\n",
            "Epoch 644/1000, Training Loss: 0.0030447430454386215, Test Loss: 0.0027084304052273555\n",
            "Epoch 645/1000, Training Loss: 0.0030445771832532604, Test Loss: 0.002708293950078062\n",
            "Epoch 646/1000, Training Loss: 0.003044411016858964, Test Loss: 0.0027081572378998314\n",
            "Epoch 647/1000, Training Loss: 0.003044244545794704, Test Loss: 0.002708020268274854\n",
            "Epoch 648/1000, Training Loss: 0.003044077769599494, Test Loss: 0.0027078830407851772\n",
            "Epoch 649/1000, Training Loss: 0.003043910687812395, Test Loss: 0.0027077455550127082\n",
            "Epoch 650/1000, Training Loss: 0.003043743299972518, Test Loss: 0.0027076078105392172\n",
            "Epoch 651/1000, Training Loss: 0.0030435756056190265, Test Loss: 0.0027074698069463384\n",
            "Epoch 652/1000, Training Loss: 0.0030434076042911416, Test Loss: 0.002707331543815572\n",
            "Epoch 653/1000, Training Loss: 0.0030432392955281416, Test Loss: 0.0027071930207282903\n",
            "Epoch 654/1000, Training Loss: 0.00304307067886937, Test Loss: 0.002707054237265738\n",
            "Epoch 655/1000, Training Loss: 0.003042901753854234, Test Loss: 0.0027069151930090313\n",
            "Epoch 656/1000, Training Loss: 0.0030427325200222135, Test Loss: 0.002706775887539168\n",
            "Epoch 657/1000, Training Loss: 0.003042562976912857, Test Loss: 0.0027066363204370225\n",
            "Epoch 658/1000, Training Loss: 0.003042393124065795, Test Loss: 0.0027064964912833544\n",
            "Epoch 659/1000, Training Loss: 0.003042222961020731, Test Loss: 0.002706356399658809\n",
            "Epoch 660/1000, Training Loss: 0.003042052487317459, Test Loss: 0.0027062160451439173\n",
            "Epoch 661/1000, Training Loss: 0.003041881702495852, Test Loss: 0.002706075427319103\n",
            "Epoch 662/1000, Training Loss: 0.0030417106060958796, Test Loss: 0.0027059345457646823\n",
            "Epoch 663/1000, Training Loss: 0.0030415391976576006, Test Loss: 0.002705793400060867\n",
            "Epoch 664/1000, Training Loss: 0.003041367476721173, Test Loss: 0.0027056519897877686\n",
            "Epoch 665/1000, Training Loss: 0.0030411954428268538, Test Loss: 0.0027055103145254005\n",
            "Epoch 666/1000, Training Loss: 0.003041023095515005, Test Loss: 0.0027053683738536803\n",
            "Epoch 667/1000, Training Loss: 0.0030408504343260967, Test Loss: 0.0027052261673524325\n",
            "Epoch 668/1000, Training Loss: 0.003040677458800709, Test Loss: 0.002705083694601391\n",
            "Epoch 669/1000, Training Loss: 0.0030405041684795366, Test Loss: 0.002704940955180206\n",
            "Epoch 670/1000, Training Loss: 0.0030403305629033927, Test Loss: 0.0027047979486684377\n",
            "Epoch 671/1000, Training Loss: 0.0030401566416132153, Test Loss: 0.002704654674645571\n",
            "Epoch 672/1000, Training Loss: 0.0030399824041500633, Test Loss: 0.002704511132691008\n",
            "Epoch 673/1000, Training Loss: 0.0030398078500551277, Test Loss: 0.002704367322384077\n",
            "Epoch 674/1000, Training Loss: 0.003039632978869733, Test Loss: 0.002704223243304034\n",
            "Epoch 675/1000, Training Loss: 0.0030394577901353397, Test Loss: 0.002704078895030063\n",
            "Epoch 676/1000, Training Loss: 0.0030392822833935473, Test Loss: 0.002703934277141286\n",
            "Epoch 677/1000, Training Loss: 0.0030391064581861004, Test Loss: 0.0027037893892167568\n",
            "Epoch 678/1000, Training Loss: 0.0030389303140548937, Test Loss: 0.0027036442308354712\n",
            "Epoch 679/1000, Training Loss: 0.0030387538505419697, Test Loss: 0.0027034988015763647\n",
            "Epoch 680/1000, Training Loss: 0.0030385770671895298, Test Loss: 0.002703353101018323\n",
            "Epoch 681/1000, Training Loss: 0.0030383999635399317, Test Loss: 0.0027032071287401756\n",
            "Epoch 682/1000, Training Loss: 0.0030382225391356993, Test Loss: 0.0027030608843207045\n",
            "Epoch 683/1000, Training Loss: 0.003038044793519521, Test Loss: 0.00270291436733865\n",
            "Epoch 684/1000, Training Loss: 0.0030378667262342574, Test Loss: 0.0027027675773727052\n",
            "Epoch 685/1000, Training Loss: 0.003037688336822944, Test Loss: 0.0027026205140015303\n",
            "Epoch 686/1000, Training Loss: 0.003037509624828795, Test Loss: 0.002702473176803743\n",
            "Epoch 687/1000, Training Loss: 0.003037330589795205, Test Loss: 0.002702325565357934\n",
            "Epoch 688/1000, Training Loss: 0.003037151231265758, Test Loss: 0.0027021776792426617\n",
            "Epoch 689/1000, Training Loss: 0.0030369715487842284, Test Loss: 0.002702029518036459\n",
            "Epoch 690/1000, Training Loss: 0.003036791541894583, Test Loss: 0.002701881081317838\n",
            "Epoch 691/1000, Training Loss: 0.003036611210140989, Test Loss: 0.002701732368665289\n",
            "Epoch 692/1000, Training Loss: 0.003036430553067816, Test Loss: 0.002701583379657284\n",
            "Epoch 693/1000, Training Loss: 0.003036249570219639, Test Loss: 0.002701434113872289\n",
            "Epoch 694/1000, Training Loss: 0.003036068261141247, Test Loss: 0.002701284570888755\n",
            "Epoch 695/1000, Training Loss: 0.0030358866253776375, Test Loss: 0.0027011347502851283\n",
            "Epoch 696/1000, Training Loss: 0.003035704662474034, Test Loss: 0.0027009846516398515\n",
            "Epoch 697/1000, Training Loss: 0.0030355223719758795, Test Loss: 0.0027008342745313695\n",
            "Epoch 698/1000, Training Loss: 0.003035339753428843, Test Loss: 0.0027006836185381315\n",
            "Epoch 699/1000, Training Loss: 0.0030351568063788277, Test Loss: 0.0027005326832385936\n",
            "Epoch 700/1000, Training Loss: 0.003034973530371971, Test Loss: 0.002700381468211221\n",
            "Epoch 701/1000, Training Loss: 0.0030347899249546493, Test Loss: 0.0027002299730344973\n",
            "Epoch 702/1000, Training Loss: 0.0030346059896734843, Test Loss: 0.002700078197286921\n",
            "Epoch 703/1000, Training Loss: 0.0030344217240753464, Test Loss: 0.0026999261405470133\n",
            "Epoch 704/1000, Training Loss: 0.003034237127707356, Test Loss: 0.0026997738023933223\n",
            "Epoch 705/1000, Training Loss: 0.0030340522001168924, Test Loss: 0.0026996211824044216\n",
            "Epoch 706/1000, Training Loss: 0.003033866940851596, Test Loss: 0.002699468280158919\n",
            "Epoch 707/1000, Training Loss: 0.0030336813494593706, Test Loss: 0.0026993150952354574\n",
            "Epoch 708/1000, Training Loss: 0.003033495425488393, Test Loss: 0.002699161627212724\n",
            "Epoch 709/1000, Training Loss: 0.0030333091684871106, Test Loss: 0.0026990078756694426\n",
            "Epoch 710/1000, Training Loss: 0.003033122578004252, Test Loss: 0.0026988538401843854\n",
            "Epoch 711/1000, Training Loss: 0.003032935653588828, Test Loss: 0.0026986995203363805\n",
            "Epoch 712/1000, Training Loss: 0.0030327483947901355, Test Loss: 0.002698544915704304\n",
            "Epoch 713/1000, Training Loss: 0.0030325608011577644, Test Loss: 0.002698390025867094\n",
            "Epoch 714/1000, Training Loss: 0.003032372872241601, Test Loss: 0.002698234850403751\n",
            "Epoch 715/1000, Training Loss: 0.0030321846075918313, Test Loss: 0.002698079388893338\n",
            "Epoch 716/1000, Training Loss: 0.003031996006758948, Test Loss: 0.0026979236409149913\n",
            "Epoch 717/1000, Training Loss: 0.003031807069293751, Test Loss: 0.0026977676060479187\n",
            "Epoch 718/1000, Training Loss: 0.0030316177947473577, Test Loss: 0.002697611283871405\n",
            "Epoch 719/1000, Training Loss: 0.0030314281826712025, Test Loss: 0.0026974546739648186\n",
            "Epoch 720/1000, Training Loss: 0.0030312382326170436, Test Loss: 0.0026972977759076116\n",
            "Epoch 721/1000, Training Loss: 0.0030310479441369668, Test Loss: 0.0026971405892793226\n",
            "Epoch 722/1000, Training Loss: 0.003030857316783391, Test Loss: 0.0026969831136595883\n",
            "Epoch 723/1000, Training Loss: 0.0030306663501090724, Test Loss: 0.0026968253486281384\n",
            "Epoch 724/1000, Training Loss: 0.003030475043667109, Test Loss: 0.002696667293764808\n",
            "Epoch 725/1000, Training Loss: 0.0030302833970109466, Test Loss: 0.0026965089486495324\n",
            "Epoch 726/1000, Training Loss: 0.003030091409694381, Test Loss: 0.002696350312862359\n",
            "Epoch 727/1000, Training Loss: 0.0030298990812715632, Test Loss: 0.0026961913859834476\n",
            "Epoch 728/1000, Training Loss: 0.0030297064112970074, Test Loss: 0.002696032167593074\n",
            "Epoch 729/1000, Training Loss: 0.0030295133993255923, Test Loss: 0.0026958726572716397\n",
            "Epoch 730/1000, Training Loss: 0.0030293200449125657, Test Loss: 0.0026957128545996665\n",
            "Epoch 731/1000, Training Loss: 0.003029126347613552, Test Loss: 0.002695552759157808\n",
            "Epoch 732/1000, Training Loss: 0.0030289323069845574, Test Loss: 0.0026953923705268544\n",
            "Epoch 733/1000, Training Loss: 0.0030287379225819675, Test Loss: 0.0026952316882877287\n",
            "Epoch 734/1000, Training Loss: 0.003028543193962563, Test Loss: 0.002695070712021501\n",
            "Epoch 735/1000, Training Loss: 0.0030283481206835163, Test Loss: 0.0026949094413093854\n",
            "Epoch 736/1000, Training Loss: 0.0030281527023023995, Test Loss: 0.0026947478757327493\n",
            "Epoch 737/1000, Training Loss: 0.003027956938377191, Test Loss: 0.0026945860148731115\n",
            "Epoch 738/1000, Training Loss: 0.0030277608284662747, Test Loss: 0.002694423858312151\n",
            "Epoch 739/1000, Training Loss: 0.003027564372128453, Test Loss: 0.0026942614056317165\n",
            "Epoch 740/1000, Training Loss: 0.003027367568922945, Test Loss: 0.0026940986564138163\n",
            "Epoch 741/1000, Training Loss: 0.003027170418409394, Test Loss: 0.0026939356102406364\n",
            "Epoch 742/1000, Training Loss: 0.0030269729201478747, Test Loss: 0.002693772266694539\n",
            "Epoch 743/1000, Training Loss: 0.0030267750736988923, Test Loss: 0.0026936086253580675\n",
            "Epoch 744/1000, Training Loss: 0.003026576878623396, Test Loss: 0.002693444685813951\n",
            "Epoch 745/1000, Training Loss: 0.0030263783344827754, Test Loss: 0.002693280447645108\n",
            "Epoch 746/1000, Training Loss: 0.003026179440838872, Test Loss: 0.0026931159104346554\n",
            "Epoch 747/1000, Training Loss: 0.0030259801972539802, Test Loss: 0.002692951073765904\n",
            "Epoch 748/1000, Training Loss: 0.0030257806032908557, Test Loss: 0.002692785937222375\n",
            "Epoch 749/1000, Training Loss: 0.0030255806585127187, Test Loss: 0.002692620500387791\n",
            "Epoch 750/1000, Training Loss: 0.0030253803624832594, Test Loss: 0.002692454762846091\n",
            "Epoch 751/1000, Training Loss: 0.0030251797147666415, Test Loss: 0.0026922887241814355\n",
            "Epoch 752/1000, Training Loss: 0.0030249787149275124, Test Loss: 0.002692122383978203\n",
            "Epoch 753/1000, Training Loss: 0.0030247773625310037, Test Loss: 0.0026919557418209968\n",
            "Epoch 754/1000, Training Loss: 0.003024575657142737, Test Loss: 0.002691788797294659\n",
            "Epoch 755/1000, Training Loss: 0.003024373598328832, Test Loss: 0.002691621549984261\n",
            "Epoch 756/1000, Training Loss: 0.0030241711856559073, Test Loss: 0.0026914539994751215\n",
            "Epoch 757/1000, Training Loss: 0.003023968418691091, Test Loss: 0.002691286145352798\n",
            "Epoch 758/1000, Training Loss: 0.0030237652970020222, Test Loss: 0.0026911179872031073\n",
            "Epoch 759/1000, Training Loss: 0.003023561820156857, Test Loss: 0.0026909495246121118\n",
            "Epoch 760/1000, Training Loss: 0.0030233579877242756, Test Loss: 0.0026907807571661443\n",
            "Epoch 761/1000, Training Loss: 0.0030231537992734853, Test Loss: 0.0026906116844517946\n",
            "Epoch 762/1000, Training Loss: 0.0030229492543742266, Test Loss: 0.0026904423060559266\n",
            "Epoch 763/1000, Training Loss: 0.003022744352596781, Test Loss: 0.0026902726215656814\n",
            "Epoch 764/1000, Training Loss: 0.003022539093511974, Test Loss: 0.002690102630568474\n",
            "Epoch 765/1000, Training Loss: 0.0030223334766911786, Test Loss: 0.0026899323326520057\n",
            "Epoch 766/1000, Training Loss: 0.0030221275017063265, Test Loss: 0.002689761727404273\n",
            "Epoch 767/1000, Training Loss: 0.0030219211681299075, Test Loss: 0.0026895908144135595\n",
            "Epoch 768/1000, Training Loss: 0.00302171447553498, Test Loss: 0.002689419593268454\n",
            "Epoch 769/1000, Training Loss: 0.003021507423495174, Test Loss: 0.0026892480635578473\n",
            "Epoch 770/1000, Training Loss: 0.0030213000115846956, Test Loss: 0.002689076224870942\n",
            "Epoch 771/1000, Training Loss: 0.0030210922393783357, Test Loss: 0.0026889040767972532\n",
            "Epoch 772/1000, Training Loss: 0.0030208841064514724, Test Loss: 0.002688731618926618\n",
            "Epoch 773/1000, Training Loss: 0.0030206756123800794, Test Loss: 0.002688558850849198\n",
            "Epoch 774/1000, Training Loss: 0.0030204667567407303, Test Loss: 0.002688385772155485\n",
            "Epoch 775/1000, Training Loss: 0.003020257539110602, Test Loss: 0.0026882123824363063\n",
            "Epoch 776/1000, Training Loss: 0.0030200479590674857, Test Loss: 0.002688038681282829\n",
            "Epoch 777/1000, Training Loss: 0.0030198380161897864, Test Loss: 0.002687864668286569\n",
            "Epoch 778/1000, Training Loss: 0.003019627710056535, Test Loss: 0.002687690343039389\n",
            "Epoch 779/1000, Training Loss: 0.003019417040247388, Test Loss: 0.002687515705133512\n",
            "Epoch 780/1000, Training Loss: 0.003019206006342637, Test Loss: 0.0026873407541615187\n",
            "Epoch 781/1000, Training Loss: 0.003018994607923213, Test Loss: 0.0026871654897163617\n",
            "Epoch 782/1000, Training Loss: 0.0030187828445706953, Test Loss: 0.0026869899113913616\n",
            "Epoch 783/1000, Training Loss: 0.003018570715867308, Test Loss: 0.002686814018780219\n",
            "Epoch 784/1000, Training Loss: 0.0030183582213959394, Test Loss: 0.0026866378114770166\n",
            "Epoch 785/1000, Training Loss: 0.0030181453607401373, Test Loss: 0.0026864612890762276\n",
            "Epoch 786/1000, Training Loss: 0.003017932133484118, Test Loss: 0.002686284451172715\n",
            "Epoch 787/1000, Training Loss: 0.0030177185392127725, Test Loss: 0.002686107297361743\n",
            "Epoch 788/1000, Training Loss: 0.003017504577511675, Test Loss: 0.002685929827238983\n",
            "Epoch 789/1000, Training Loss: 0.003017290247967083, Test Loss: 0.0026857520404005137\n",
            "Epoch 790/1000, Training Loss: 0.0030170755501659472, Test Loss: 0.002685573936442833\n",
            "Epoch 791/1000, Training Loss: 0.003016860483695918, Test Loss: 0.002685395514962855\n",
            "Epoch 792/1000, Training Loss: 0.0030166450481453476, Test Loss: 0.0026852167755579237\n",
            "Epoch 793/1000, Training Loss: 0.0030164292431033002, Test Loss: 0.0026850377178258165\n",
            "Epoch 794/1000, Training Loss: 0.0030162130681595546, Test Loss: 0.0026848583413647477\n",
            "Epoch 795/1000, Training Loss: 0.0030159965229046137, Test Loss: 0.002684678645773375\n",
            "Epoch 796/1000, Training Loss: 0.003015779606929707, Test Loss: 0.0026844986306508085\n",
            "Epoch 797/1000, Training Loss: 0.0030155623198267977, Test Loss: 0.002684318295596606\n",
            "Epoch 798/1000, Training Loss: 0.0030153446611885914, Test Loss: 0.0026841376402107955\n",
            "Epoch 799/1000, Training Loss: 0.003015126630608538, Test Loss: 0.0026839566640938658\n",
            "Epoch 800/1000, Training Loss: 0.0030149082276808396, Test Loss: 0.002683775366846776\n",
            "Epoch 801/1000, Training Loss: 0.0030146894520004584, Test Loss: 0.002683593748070971\n",
            "Epoch 802/1000, Training Loss: 0.003014470303163119, Test Loss: 0.0026834118073683732\n",
            "Epoch 803/1000, Training Loss: 0.003014250780765319, Test Loss: 0.0026832295443413954\n",
            "Epoch 804/1000, Training Loss: 0.0030140308844043306, Test Loss: 0.002683046958592946\n",
            "Epoch 805/1000, Training Loss: 0.0030138106136782107, Test Loss: 0.00268286404972644\n",
            "Epoch 806/1000, Training Loss: 0.0030135899681858038, Test Loss: 0.0026826808173457894\n",
            "Epoch 807/1000, Training Loss: 0.00301336894752675, Test Loss: 0.002682497261055429\n",
            "Epoch 808/1000, Training Loss: 0.0030131475513014917, Test Loss: 0.002682313380460306\n",
            "Epoch 809/1000, Training Loss: 0.003012925779111279, Test Loss: 0.0026821291751658984\n",
            "Epoch 810/1000, Training Loss: 0.003012703630558175, Test Loss: 0.002681944644778207\n",
            "Epoch 811/1000, Training Loss: 0.0030124811052450635, Test Loss: 0.0026817597889037792\n",
            "Epoch 812/1000, Training Loss: 0.0030122582027756538, Test Loss: 0.0026815746071497\n",
            "Epoch 813/1000, Training Loss: 0.0030120349227544907, Test Loss: 0.0026813890991236006\n",
            "Epoch 814/1000, Training Loss: 0.003011811264786954, Test Loss: 0.0026812032644336742\n",
            "Epoch 815/1000, Training Loss: 0.0030115872284792733, Test Loss: 0.0026810171026886696\n",
            "Epoch 816/1000, Training Loss: 0.003011362813438525, Test Loss: 0.002680830613497907\n",
            "Epoch 817/1000, Training Loss: 0.003011138019272647, Test Loss: 0.002680643796471276\n",
            "Epoch 818/1000, Training Loss: 0.00301091284559044, Test Loss: 0.002680456651219248\n",
            "Epoch 819/1000, Training Loss: 0.0030106872920015763, Test Loss: 0.002680269177352879\n",
            "Epoch 820/1000, Training Loss: 0.003010461358116604, Test Loss: 0.0026800813744838176\n",
            "Epoch 821/1000, Training Loss: 0.003010235043546956, Test Loss: 0.002679893242224311\n",
            "Epoch 822/1000, Training Loss: 0.0030100083479049546, Test Loss: 0.002679704780187209\n",
            "Epoch 823/1000, Training Loss: 0.003009781270803817, Test Loss: 0.0026795159879859742\n",
            "Epoch 824/1000, Training Loss: 0.0030095538118576647, Test Loss: 0.002679326865234685\n",
            "Epoch 825/1000, Training Loss: 0.003009325970681529, Test Loss: 0.002679137411548041\n",
            "Epoch 826/1000, Training Loss: 0.0030090977468913544, Test Loss: 0.002678947626541373\n",
            "Epoch 827/1000, Training Loss: 0.003008869140104009, Test Loss: 0.0026787575098306485\n",
            "Epoch 828/1000, Training Loss: 0.003008640149937289, Test Loss: 0.002678567061032477\n",
            "Epoch 829/1000, Training Loss: 0.003008410776009926, Test Loss: 0.0026783762797641143\n",
            "Epoch 830/1000, Training Loss: 0.0030081810179415926, Test Loss: 0.002678185165643473\n",
            "Epoch 831/1000, Training Loss: 0.00300795087535291, Test Loss: 0.0026779937182891266\n",
            "Epoch 832/1000, Training Loss: 0.0030077203478654536, Test Loss: 0.0026778019373203156\n",
            "Epoch 833/1000, Training Loss: 0.00300748943510176, Test Loss: 0.0026776098223569565\n",
            "Epoch 834/1000, Training Loss: 0.0030072581366853337, Test Loss: 0.0026774173730196448\n",
            "Epoch 835/1000, Training Loss: 0.003007026452240654, Test Loss: 0.0026772245889296643\n",
            "Epoch 836/1000, Training Loss: 0.0030067943813931793, Test Loss: 0.002677031469708993\n",
            "Epoch 837/1000, Training Loss: 0.003006561923769356, Test Loss: 0.002676838014980307\n",
            "Epoch 838/1000, Training Loss: 0.003006329078996627, Test Loss: 0.0026766442243669913\n",
            "Epoch 839/1000, Training Loss: 0.0030060958467034316, Test Loss: 0.002676450097493144\n",
            "Epoch 840/1000, Training Loss: 0.0030058622265192207, Test Loss: 0.002676255633983584\n",
            "Epoch 841/1000, Training Loss: 0.0030056282180744563, Test Loss: 0.002676060833463854\n",
            "Epoch 842/1000, Training Loss: 0.003005393821000621, Test Loss: 0.0026758656955602343\n",
            "Epoch 843/1000, Training Loss: 0.003005159034930226, Test Loss: 0.0026756702198997406\n",
            "Epoch 844/1000, Training Loss: 0.003004923859496815, Test Loss: 0.0026754744061101405\n",
            "Epoch 845/1000, Training Loss: 0.0030046882943349728, Test Loss: 0.0026752782538199487\n",
            "Epoch 846/1000, Training Loss: 0.0030044523390803323, Test Loss: 0.0026750817626584465\n",
            "Epoch 847/1000, Training Loss: 0.003004215993369579, Test Loss: 0.002674884932255678\n",
            "Epoch 848/1000, Training Loss: 0.0030039792568404594, Test Loss: 0.002674687762242461\n",
            "Epoch 849/1000, Training Loss: 0.003003742129131788, Test Loss: 0.002674490252250396\n",
            "Epoch 850/1000, Training Loss: 0.0030035046098834516, Test Loss: 0.0026742924019118692\n",
            "Epoch 851/1000, Training Loss: 0.0030032666987364217, Test Loss: 0.0026740942108600613\n",
            "Epoch 852/1000, Training Loss: 0.003003028395332752, Test Loss: 0.002673895678728952\n",
            "Epoch 853/1000, Training Loss: 0.0030027896993155933, Test Loss: 0.002673696805153332\n",
            "Epoch 854/1000, Training Loss: 0.0030025506103291987, Test Loss: 0.0026734975897688053\n",
            "Epoch 855/1000, Training Loss: 0.0030023111280189268, Test Loss: 0.002673298032211796\n",
            "Epoch 856/1000, Training Loss: 0.003002071252031252, Test Loss: 0.0026730981321195584\n",
            "Epoch 857/1000, Training Loss: 0.0030018309820137694, Test Loss: 0.002672897889130181\n",
            "Epoch 858/1000, Training Loss: 0.0030015903176152025, Test Loss: 0.0026726973028825957\n",
            "Epoch 859/1000, Training Loss: 0.0030013492584854105, Test Loss: 0.0026724963730165824\n",
            "Epoch 860/1000, Training Loss: 0.003001107804275393, Test Loss: 0.0026722950991727797\n",
            "Epoch 861/1000, Training Loss: 0.0030008659546372995, Test Loss: 0.0026720934809926844\n",
            "Epoch 862/1000, Training Loss: 0.0030006237092244335, Test Loss: 0.00267189151811867\n",
            "Epoch 863/1000, Training Loss: 0.0030003810676912613, Test Loss: 0.0026716892101939834\n",
            "Epoch 864/1000, Training Loss: 0.003000138029693419, Test Loss: 0.0026714865568627545\n",
            "Epoch 865/1000, Training Loss: 0.002999894594887717, Test Loss: 0.002671283557770009\n",
            "Epoch 866/1000, Training Loss: 0.00299965076293215, Test Loss: 0.0026710802125616694\n",
            "Epoch 867/1000, Training Loss: 0.002999406533485902, Test Loss: 0.0026708765208845607\n",
            "Epoch 868/1000, Training Loss: 0.0029991619062093512, Test Loss: 0.0026706724823864266\n",
            "Epoch 869/1000, Training Loss: 0.0029989168807640814, Test Loss: 0.0026704680967159244\n",
            "Epoch 870/1000, Training Loss: 0.002998671456812888, Test Loss: 0.0026702633635226443\n",
            "Epoch 871/1000, Training Loss: 0.002998425634019778, Test Loss: 0.0026700582824571066\n",
            "Epoch 872/1000, Training Loss: 0.0029981794120499884, Test Loss: 0.0026698528531707744\n",
            "Epoch 873/1000, Training Loss: 0.002997932790569981, Test Loss: 0.0026696470753160588\n",
            "Epoch 874/1000, Training Loss: 0.002997685769247461, Test Loss: 0.002669440948546328\n",
            "Epoch 875/1000, Training Loss: 0.0029974383477513744, Test Loss: 0.002669234472515912\n",
            "Epoch 876/1000, Training Loss: 0.0029971905257519193, Test Loss: 0.0026690276468801133\n",
            "Epoch 877/1000, Training Loss: 0.002996942302920552, Test Loss: 0.00266882047129521\n",
            "Epoch 878/1000, Training Loss: 0.002996693678929996, Test Loss: 0.0026686129454184643\n",
            "Epoch 879/1000, Training Loss: 0.002996444653454242, Test Loss: 0.0026684050689081322\n",
            "Epoch 880/1000, Training Loss: 0.0029961952261685666, Test Loss: 0.002668196841423469\n",
            "Epoch 881/1000, Training Loss: 0.002995945396749526, Test Loss: 0.002667988262624738\n",
            "Epoch 882/1000, Training Loss: 0.0029956951648749727, Test Loss: 0.002667779332173214\n",
            "Epoch 883/1000, Training Loss: 0.0029954445302240555, Test Loss: 0.002667570049731194\n",
            "Epoch 884/1000, Training Loss: 0.002995193492477235, Test Loss: 0.0026673604149620054\n",
            "Epoch 885/1000, Training Loss: 0.0029949420513162807, Test Loss: 0.0026671504275300106\n",
            "Epoch 886/1000, Training Loss: 0.0029946902064242843, Test Loss: 0.0026669400871006167\n",
            "Epoch 887/1000, Training Loss: 0.002994437957485664, Test Loss: 0.0026667293933402792\n",
            "Epoch 888/1000, Training Loss: 0.002994185304186174, Test Loss: 0.002666518345916518\n",
            "Epoch 889/1000, Training Loss: 0.002993932246212907, Test Loss: 0.0026663069444979115\n",
            "Epoch 890/1000, Training Loss: 0.002993678783254306, Test Loss: 0.0026660951887541163\n",
            "Epoch 891/1000, Training Loss: 0.002993424915000167, Test Loss: 0.0026658830783558686\n",
            "Epoch 892/1000, Training Loss: 0.0029931706411416496, Test Loss: 0.002665670612974994\n",
            "Epoch 893/1000, Training Loss: 0.0029929159613712813, Test Loss: 0.0026654577922844095\n",
            "Epoch 894/1000, Training Loss: 0.0029926608753829648, Test Loss: 0.0026652446159581428\n",
            "Epoch 895/1000, Training Loss: 0.0029924053828719855, Test Loss: 0.002665031083671326\n",
            "Epoch 896/1000, Training Loss: 0.0029921494835350193, Test Loss: 0.002664817195100214\n",
            "Epoch 897/1000, Training Loss: 0.002991893177070137, Test Loss: 0.002664602949922184\n",
            "Epoch 898/1000, Training Loss: 0.0029916364631768135, Test Loss: 0.002664388347815748\n",
            "Epoch 899/1000, Training Loss: 0.002991379341555933, Test Loss: 0.002664173388460561\n",
            "Epoch 900/1000, Training Loss: 0.0029911218119097983, Test Loss: 0.0026639580715374232\n",
            "Epoch 901/1000, Training Loss: 0.002990863873942135, Test Loss: 0.0026637423967282945\n",
            "Epoch 902/1000, Training Loss: 0.0029906055273580985, Test Loss: 0.002663526363716293\n",
            "Epoch 903/1000, Training Loss: 0.002990346771864284, Test Loss: 0.0026633099721857173\n",
            "Epoch 904/1000, Training Loss: 0.002990087607168729, Test Loss: 0.0026630932218220356\n",
            "Epoch 905/1000, Training Loss: 0.0029898280329809256, Test Loss: 0.002662876112311907\n",
            "Epoch 906/1000, Training Loss: 0.0029895680490118204, Test Loss: 0.0026626586433431843\n",
            "Epoch 907/1000, Training Loss: 0.002989307654973827, Test Loss: 0.0026624408146049246\n",
            "Epoch 908/1000, Training Loss: 0.002989046850580832, Test Loss: 0.0026622226257873916\n",
            "Epoch 909/1000, Training Loss: 0.0029887856355481977, Test Loss: 0.0026620040765820657\n",
            "Epoch 910/1000, Training Loss: 0.002988524009592777, Test Loss: 0.0026617851666816544\n",
            "Epoch 911/1000, Training Loss: 0.0029882619724329097, Test Loss: 0.0026615658957800954\n",
            "Epoch 912/1000, Training Loss: 0.0029879995237884396, Test Loss: 0.0026613462635725693\n",
            "Epoch 913/1000, Training Loss: 0.0029877366633807145, Test Loss: 0.0026611262697555037\n",
            "Epoch 914/1000, Training Loss: 0.0029874733909325936, Test Loss: 0.00266090591402658\n",
            "Epoch 915/1000, Training Loss: 0.00298720970616846, Test Loss: 0.002660685196084745\n",
            "Epoch 916/1000, Training Loss: 0.00298694560881422, Test Loss: 0.0026604641156302163\n",
            "Epoch 917/1000, Training Loss: 0.0029866810985973154, Test Loss: 0.002660242672364491\n",
            "Epoch 918/1000, Training Loss: 0.0029864161752467255, Test Loss: 0.0026600208659903497\n",
            "Epoch 919/1000, Training Loss: 0.0029861508384929803, Test Loss: 0.002659798696211869\n",
            "Epoch 920/1000, Training Loss: 0.0029858850880681597, Test Loss: 0.0026595761627344304\n",
            "Epoch 921/1000, Training Loss: 0.002985618923705907, Test Loss: 0.002659353265264722\n",
            "Epoch 922/1000, Training Loss: 0.002985352345141432, Test Loss: 0.002659130003510748\n",
            "Epoch 923/1000, Training Loss: 0.0029850853521115163, Test Loss: 0.0026589063771818417\n",
            "Epoch 924/1000, Training Loss: 0.0029848179443545246, Test Loss: 0.0026586823859886676\n",
            "Epoch 925/1000, Training Loss: 0.0029845501216104087, Test Loss: 0.002658458029643231\n",
            "Epoch 926/1000, Training Loss: 0.002984281883620714, Test Loss: 0.002658233307858886\n",
            "Epoch 927/1000, Training Loss: 0.0029840132301285865, Test Loss: 0.0026580082203503446\n",
            "Epoch 928/1000, Training Loss: 0.0029837441608787795, Test Loss: 0.0026577827668336795\n",
            "Epoch 929/1000, Training Loss: 0.002983474675617661, Test Loss: 0.0026575569470263377\n",
            "Epoch 930/1000, Training Loss: 0.0029832047740932194, Test Loss: 0.002657330760647147\n",
            "Epoch 931/1000, Training Loss: 0.0029829344560550718, Test Loss: 0.002657104207416322\n",
            "Epoch 932/1000, Training Loss: 0.0029826637212544667, Test Loss: 0.002656877287055471\n",
            "Epoch 933/1000, Training Loss: 0.002982392569444294, Test Loss: 0.0026566499992876098\n",
            "Epoch 934/1000, Training Loss: 0.0029821210003790942, Test Loss: 0.002656422343837159\n",
            "Epoch 935/1000, Training Loss: 0.002981849013815057, Test Loss: 0.002656194320429965\n",
            "Epoch 936/1000, Training Loss: 0.002981576609510035, Test Loss: 0.002655965928793294\n",
            "Epoch 937/1000, Training Loss: 0.002981303787223547, Test Loss: 0.002655737168655851\n",
            "Epoch 938/1000, Training Loss: 0.002981030546716785, Test Loss: 0.002655508039747785\n",
            "Epoch 939/1000, Training Loss: 0.002980756887752622, Test Loss: 0.0026552785418006903\n",
            "Epoch 940/1000, Training Loss: 0.002980482810095617, Test Loss: 0.0026550486745476228\n",
            "Epoch 941/1000, Training Loss: 0.0029802083135120213, Test Loss: 0.0026548184377231023\n",
            "Epoch 942/1000, Training Loss: 0.002979933397769786, Test Loss: 0.0026545878310631236\n",
            "Epoch 943/1000, Training Loss: 0.0029796580626385693, Test Loss: 0.0026543568543051614\n",
            "Epoch 944/1000, Training Loss: 0.0029793823078897398, Test Loss: 0.002654125507188181\n",
            "Epoch 945/1000, Training Loss: 0.0029791061332963852, Test Loss: 0.002653893789452646\n",
            "Epoch 946/1000, Training Loss: 0.002978829538633321, Test Loss: 0.002653661700840522\n",
            "Epoch 947/1000, Training Loss: 0.0029785525236770893, Test Loss: 0.0026534292410952892\n",
            "Epoch 948/1000, Training Loss: 0.002978275088205974, Test Loss: 0.0026531964099619477\n",
            "Epoch 949/1000, Training Loss: 0.002977997232000002, Test Loss: 0.002652963207187026\n",
            "Epoch 950/1000, Training Loss: 0.002977718954840952, Test Loss: 0.0026527296325185895\n",
            "Epoch 951/1000, Training Loss: 0.002977440256512356, Test Loss: 0.002652495685706249\n",
            "Epoch 952/1000, Training Loss: 0.002977161136799512, Test Loss: 0.0026522613665011644\n",
            "Epoch 953/1000, Training Loss: 0.0029768815954894865, Test Loss: 0.0026520266746560563\n",
            "Epoch 954/1000, Training Loss: 0.0029766016323711218, Test Loss: 0.002651791609925214\n",
            "Epoch 955/1000, Training Loss: 0.002976321247235042, Test Loss: 0.002651556172064501\n",
            "Epoch 956/1000, Training Loss: 0.002976040439873658, Test Loss: 0.002651320360831362\n",
            "Epoch 957/1000, Training Loss: 0.002975759210081177, Test Loss: 0.0026510841759848384\n",
            "Epoch 958/1000, Training Loss: 0.002975477557653603, Test Loss: 0.002650847617285565\n",
            "Epoch 959/1000, Training Loss: 0.0029751954823887504, Test Loss: 0.0026506106844957854\n",
            "Epoch 960/1000, Training Loss: 0.0029749129840862427, Test Loss: 0.0026503733773793558\n",
            "Epoch 961/1000, Training Loss: 0.0029746300625475234, Test Loss: 0.0026501356957017576\n",
            "Epoch 962/1000, Training Loss: 0.002974346717575862, Test Loss: 0.002649897639230098\n",
            "Epoch 963/1000, Training Loss: 0.0029740629489763536, Test Loss: 0.0026496592077331267\n",
            "Epoch 964/1000, Training Loss: 0.002973778756555936, Test Loss: 0.0026494204009812326\n",
            "Epoch 965/1000, Training Loss: 0.002973494140123385, Test Loss: 0.0026491812187464663\n",
            "Epoch 966/1000, Training Loss: 0.0029732090994893257, Test Loss: 0.00264894166080253\n",
            "Epoch 967/1000, Training Loss: 0.0029729236344662397, Test Loss: 0.002648701726924802\n",
            "Epoch 968/1000, Training Loss: 0.0029726377448684653, Test Loss: 0.0026484614168903312\n",
            "Epoch 969/1000, Training Loss: 0.00297235143051221, Test Loss: 0.0026482207304778568\n",
            "Epoch 970/1000, Training Loss: 0.0029720646912155513, Test Loss: 0.002647979667467803\n",
            "Epoch 971/1000, Training Loss: 0.0029717775267984435, Test Loss: 0.0026477382276422993\n",
            "Epoch 972/1000, Training Loss: 0.002971489937082728, Test Loss: 0.0026474964107851785\n",
            "Epoch 973/1000, Training Loss: 0.0029712019218921305, Test Loss: 0.002647254216681994\n",
            "Epoch 974/1000, Training Loss: 0.0029709134810522743, Test Loss: 0.0026470116451200124\n",
            "Epoch 975/1000, Training Loss: 0.0029706246143906836, Test Loss: 0.002646768695888241\n",
            "Epoch 976/1000, Training Loss: 0.0029703353217367845, Test Loss: 0.002646525368777417\n",
            "Epoch 977/1000, Training Loss: 0.0029700456029219213, Test Loss: 0.002646281663580028\n",
            "Epoch 978/1000, Training Loss: 0.0029697554577793506, Test Loss: 0.0026460375800903125\n",
            "Epoch 979/1000, Training Loss: 0.002969464886144253, Test Loss: 0.002645793118104271\n",
            "Epoch 980/1000, Training Loss: 0.002969173887853738, Test Loss: 0.002645548277419674\n",
            "Epoch 981/1000, Training Loss: 0.0029688824627468486, Test Loss: 0.002645303057836065\n",
            "Epoch 982/1000, Training Loss: 0.002968590610664566, Test Loss: 0.0026450574591547716\n",
            "Epoch 983/1000, Training Loss: 0.002968298331449818, Test Loss: 0.002644811481178915\n",
            "Epoch 984/1000, Training Loss: 0.002968005624947479, Test Loss: 0.002644565123713412\n",
            "Epoch 985/1000, Training Loss: 0.002967712491004383, Test Loss: 0.0026443183865649924\n",
            "Epoch 986/1000, Training Loss: 0.00296741892946932, Test Loss: 0.0026440712695421907\n",
            "Epoch 987/1000, Training Loss: 0.0029671249401930487, Test Loss: 0.0026438237724553705\n",
            "Epoch 988/1000, Training Loss: 0.0029668305230282984, Test Loss: 0.00264357589511672\n",
            "Epoch 989/1000, Training Loss: 0.0029665356778297734, Test Loss: 0.0026433276373402648\n",
            "Epoch 990/1000, Training Loss: 0.0029662404044541613, Test Loss: 0.002643078998941875\n",
            "Epoch 991/1000, Training Loss: 0.0029659447027601334, Test Loss: 0.0026428299797392747\n",
            "Epoch 992/1000, Training Loss: 0.002965648572608355, Test Loss: 0.002642580579552041\n",
            "Epoch 993/1000, Training Loss: 0.0029653520138614854, Test Loss: 0.0026423307982016216\n",
            "Epoch 994/1000, Training Loss: 0.002965055026384188, Test Loss: 0.0026420806355113378\n",
            "Epoch 995/1000, Training Loss: 0.0029647576100431294, Test Loss: 0.0026418300913063886\n",
            "Epoch 996/1000, Training Loss: 0.002964459764706992, Test Loss: 0.0026415791654138677\n",
            "Epoch 997/1000, Training Loss: 0.0029641614902464696, Test Loss: 0.0026413278576627557\n",
            "Epoch 998/1000, Training Loss: 0.0029638627865342802, Test Loss: 0.0026410761678839446\n",
            "Epoch 999/1000, Training Loss: 0.0029635636534451654, Test Loss: 0.002640824095910234\n",
            "Epoch 1000/1000, Training Loss: 0.002963264090855899, Test Loss: 0.0026405716415763406\n",
            "Optimal Epoch: 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# MLP class with one hidden layer\n",
        "class MLP:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        # Initialize the number of nodes in each layer\n",
        "        self.input_nodes = input_nodes + 1  # Including bias\n",
        "        self.hidden_nodes = hidden_nodes + 1  # Including bias\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights_input_hidden = np.random.rand(self.input_nodes, self.hidden_nodes - 1) - 0.5\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_nodes, self.output_nodes) - 0.5\n",
        "\n",
        "        # Include bias in the inputs\n",
        "        self.bias_input = np.random.rand(self.input_nodes) - 0.5\n",
        "        self.bias_hidden = np.random.rand(self.hidden_nodes) - 0.5\n",
        "\n",
        "        # Initialize lists to store training and test losses\n",
        "        self.training_loss_history = []\n",
        "        self.test_loss_history = []\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # Add bias to inputs\n",
        "        inputs_with_bias = np.concatenate((inputs, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the hidden layer\n",
        "        self.hidden_input = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        # Add bias to hidden layer outputs\n",
        "        hidden_output_with_bias = np.concatenate((self.hidden_output, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the output layer\n",
        "        self.final_input = np.dot(hidden_output_with_bias, self.weights_hidden_output)\n",
        "        self.final_output = sigmoid(self.final_input)\n",
        "\n",
        "        return self.final_output\n",
        "\n",
        "    def backward_pass(self, inputs, expected_output, output, learning_rate):\n",
        "        # Compute error\n",
        "        error = expected_output - output\n",
        "\n",
        "        # Gradient for output weights\n",
        "        d_weights_hidden_output = np.dot(np.concatenate((self.hidden_output, [1]), axis=0).reshape(-1,1),\n",
        "                                         error * sigmoid_derivative(output).reshape(1, -1))\n",
        "\n",
        "        # Error for hidden layer\n",
        "        hidden_error = np.dot(self.weights_hidden_output, error * sigmoid_derivative(output))[:-1]\n",
        "\n",
        "        # Gradient for input weights\n",
        "        d_weights_input_hidden = np.dot(np.concatenate((inputs, [1]), axis=0).reshape(-1,1),\n",
        "                                        hidden_error * sigmoid_derivative(self.hidden_output).reshape(1, -1))\n",
        "\n",
        "        # Update the weights\n",
        "        self.weights_hidden_output += learning_rate * d_weights_hidden_output\n",
        "        self.weights_input_hidden += learning_rate * d_weights_input_hidden\n",
        "\n",
        "    def train_and_evaluate(self, dataset, test_inputs, test_expected_output, max_epochs, learning_rate):\n",
        "        # Split dataset into inputs and expected outputs\n",
        "        inputs = dataset[:, :2]\n",
        "        expected_output = dataset[:, 2:]\n",
        "\n",
        "        # Initialize variables to track optimal epoch and minimum test loss\n",
        "        optimal_epoch = None\n",
        "        min_test_loss = float('inf')\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(max_epochs):\n",
        "            for j in range(inputs.shape[0]):\n",
        "                input_sample = inputs[j]\n",
        "                output = self.forward_pass(input_sample)\n",
        "                self.backward_pass(input_sample, expected_output[j], output, learning_rate)\n",
        "\n",
        "            # Calculate training loss\n",
        "            training_loss = np.mean(np.square(expected_output - self.predict(inputs)))\n",
        "            self.training_loss_history.append(training_loss)\n",
        "\n",
        "            # Calculate test loss\n",
        "            test_loss = np.mean(np.square(test_expected_output - self.predict(test_inputs)))\n",
        "            self.test_loss_history.append(test_loss)\n",
        "\n",
        "            # Check if test loss is minimized\n",
        "            if test_loss < min_test_loss:\n",
        "                min_test_loss = test_loss\n",
        "                optimal_epoch = epoch + 1\n",
        "\n",
        "            # Print out progress\n",
        "            print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {training_loss}, Test Loss: {test_loss}\")\n",
        "\n",
        "        return optimal_epoch\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        outputs = np.array([self.forward_pass(input_sample) for input_sample in inputs])\n",
        "        return outputs\n",
        "\n",
        "# Function to calculate the multivariate normal density\n",
        "def multivariate_gaussian_density(x, mu, cov):\n",
        "    n = mu.shape[0]\n",
        "    diff = x - mu\n",
        "    return (1. / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \\\n",
        "           np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))\n",
        "\n",
        "# Parameters for the Gaussian\n",
        "mu_x = np.array([0, 0])\n",
        "cov_x = np.array([[0.3, -0.5],\n",
        "                  [-0.5, 2]])\n",
        "\n",
        "# Generate N training samples randomly\n",
        "N = 200\n",
        "samples = np.zeros((N, 3))\n",
        "samples[:, 0] = np.random.uniform(-2, 2, N)  # x1\n",
        "samples[:, 1] = np.random.uniform(-4, 4, N)  # x2\n",
        "\n",
        "# Calculate the function value for each sample\n",
        "for i in range(N):\n",
        "    samples[i, 2] = multivariate_gaussian_density(samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "# Generate new test data\n",
        "N_test = 100  # Change this to your dataset size for testing\n",
        "test_samples = np.zeros((N_test, 3))\n",
        "test_samples[:, 0] = np.random.uniform(-2, 2, N_test)  # x1 range\n",
        "test_samples[:, 1] = np.random.uniform(-4, 4, N_test)  # x2 range\n",
        "\n",
        "for i in range(N_test):\n",
        "    test_samples[i, 2] = multivariate_gaussian_density(test_samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "# Initialize the MLP\n",
        "mlp = MLP(input_nodes=2, hidden_nodes=10, output_nodes=1)\n",
        "optimal_epoch = mlp.train_and_evaluate(samples, test_samples[:, :2], test_samples[:, 2:], max_epochs=1000, learning_rate=0.1)\n",
        "\n",
        "print(f\"Optimal Epoch: {optimal_epoch}\")\n",
        "\n",
        "# Plot training and test loss history in a single graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(mlp.training_loss_history, label='Training Loss')\n",
        "plt.plot(mlp.test_loss_history, label='Test Loss')\n",
        "plt.axvline(x=optimal_epoch, color='r', linestyle='--', label=f'Optimal Epoch: {optimal_epoch}')\n",
        "plt.title('Training and Test Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}