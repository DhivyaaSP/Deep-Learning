{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9pNaTQB/GNadIFbqtZY6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhivyaaSP/Deep-Learning/blob/main/Exe_2_2_2_Number_of_training_samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pSSNNEF9TeBX",
        "outputId": "83695086-96d3-4dea-9d7a-1e423d3db076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 4/1000, Training Loss: 0.0033096322354651945, Test Loss: 0.0018320566167452652\n",
            "Epoch 5/1000, Training Loss: 0.003305397325518864, Test Loss: 0.001820732754319953\n",
            "Epoch 6/1000, Training Loss: 0.0033021083186923032, Test Loss: 0.0018143848294014775\n",
            "Epoch 7/1000, Training Loss: 0.003299132820159167, Test Loss: 0.0018104608006332596\n",
            "Epoch 8/1000, Training Loss: 0.0032963067851313303, Test Loss: 0.001807823080672491\n",
            "Epoch 9/1000, Training Loss: 0.0032935867174795717, Test Loss: 0.0018059054462609661\n",
            "Epoch 10/1000, Training Loss: 0.0032909590283025703, Test Loss: 0.0018044052210448602\n",
            "Epoch 11/1000, Training Loss: 0.003288416069958052, Test Loss: 0.001803152919260677\n",
            "Epoch 12/1000, Training Loss: 0.0032859504292976007, Test Loss: 0.0018020505792279327\n",
            "Epoch 13/1000, Training Loss: 0.0032835541002494417, Test Loss: 0.0018010401859133427\n",
            "Epoch 14/1000, Training Loss: 0.003281218764555524, Test Loss: 0.0018000866141268093\n",
            "Epoch 15/1000, Training Loss: 0.0032789361742723948, Test Loss: 0.001799168093930495\n",
            "Epoch 16/1000, Training Loss: 0.0032766984208920614, Test Loss: 0.0017982707645675554\n",
            "Epoch 17/1000, Training Loss: 0.003274498084728653, Test Loss: 0.0017973855195408514\n",
            "Epoch 18/1000, Training Loss: 0.003272328299382771, Test Loss: 0.001796506159867521\n",
            "Epoch 19/1000, Training Loss: 0.0032701827639156065, Test Loss: 0.001795628302894447\n",
            "Epoch 20/1000, Training Loss: 0.003268055725541724, Test Loss: 0.001794748730834288\n",
            "Epoch 21/1000, Training Loss: 0.0032659419471500807, Test Loss: 0.0017938649967565673\n",
            "Epoch 22/1000, Training Loss: 0.003263836668133456, Test Loss: 0.001792975182238418\n",
            "Epoch 23/1000, Training Loss: 0.003261735563351699, Test Loss: 0.0017920777450202124\n",
            "Epoch 24/1000, Training Loss: 0.0032596347028632983, Test Loss: 0.0017911714206076362\n",
            "Epoch 25/1000, Training Loss: 0.0032575305137815314, Test Loss: 0.0017902551566448552\n",
            "Epoch 26/1000, Training Loss: 0.0032554197448802845, Test Loss: 0.0017893280675549705\n",
            "Epoch 27/1000, Training Loss: 0.0032532994341647713, Test Loss: 0.0017883894020071492\n",
            "Epoch 28/1000, Training Loss: 0.0032511668793984315, Test Loss: 0.0017874385187349293\n",
            "Epoch 29/1000, Training Loss: 0.0032490196114600537, Test Loss: 0.0017864748679738767\n",
            "Epoch 30/1000, Training Loss: 0.003246855370348386, Test Loss: 0.0017854979768186557\n",
            "Epoch 31/1000, Training Loss: 0.0032446720836288123, Test Loss: 0.0017845074374155667\n",
            "Epoch 32/1000, Training Loss: 0.003242467847112671, Test Loss: 0.0017835028972788944\n",
            "Epoch 33/1000, Training Loss: 0.00324024090756595, Test Loss: 0.0017824840512480407\n",
            "Epoch 34/1000, Training Loss: 0.0032379896472552595, Test Loss: 0.0017814506347458182\n",
            "Epoch 35/1000, Training Loss: 0.0032357125701526113, Test Loss: 0.0017804024180905597\n",
            "Epoch 36/1000, Training Loss: 0.0032334082896347677, Test Loss: 0.0017793392016758726\n",
            "Epoch 37/1000, Training Loss: 0.003231075517527192, Test Loss: 0.0017782608118737744\n",
            "Epoch 38/1000, Training Loss: 0.003228713054356189, Test Loss: 0.0017771670975467058\n",
            "Epoch 39/1000, Training Loss: 0.0032263197806857015, Test Loss: 0.0017760579270756274\n",
            "Epoch 40/1000, Training Loss: 0.0032238946494270904, Test Loss: 0.0017749331858279973\n",
            "Epoch 41/1000, Training Loss: 0.003221436679021231, Test Loss: 0.001773792774002065\n",
            "Epoch 42/1000, Training Loss: 0.0032189449474022917, Test Loss: 0.0017726366047941704\n",
            "Epoch 43/1000, Training Loss: 0.003216418586661726, Test Loss: 0.0017714646028438804\n",
            "Epoch 44/1000, Training Loss: 0.003213856778339355, Test Loss: 0.0017702767029185245\n",
            "Epoch 45/1000, Training Loss: 0.0032112587492759566, Test Loss: 0.0017690728488042454\n",
            "Epoch 46/1000, Training Loss: 0.003208623767968609, Test Loss: 0.0017678529923753667\n",
            "Epoch 47/1000, Training Loss: 0.0032059511413762066, Test Loss: 0.0017666170928177843\n",
            "Epoch 48/1000, Training Loss: 0.003203240212128111, Test Loss: 0.0017653651159853935\n",
            "Epoch 49/1000, Training Loss: 0.0032004903560938994, Test Loss: 0.0017640970338714105\n",
            "Epoch 50/1000, Training Loss: 0.003197700980276639, Test Loss: 0.0017628128241788583\n",
            "Epoch 51/1000, Training Loss: 0.0031948715209961587, Test Loss: 0.0017615124699765453\n",
            "Epoch 52/1000, Training Loss: 0.003192001442332363, Test Loss: 0.001760195959428622\n",
            "Epoch 53/1000, Training Loss: 0.0031890902348018467, Test Loss: 0.0017588632855873505\n",
            "Epoch 54/1000, Training Loss: 0.0031861374142439416, Test Loss: 0.001757514446240011\n",
            "Epoch 55/1000, Training Loss: 0.003183142520894869, Test Loss: 0.001756149443802035\n",
            "Epoch 56/1000, Training Loss: 0.0031801051186309427, Test Loss: 0.0017547682852493474\n",
            "Epoch 57/1000, Training Loss: 0.0031770247943637597, Test Loss: 0.0017533709820838773\n",
            "Epoch 58/1000, Training Loss: 0.0031739011575721244, Test Loss: 0.0017519575503267609\n",
            "Epoch 59/1000, Training Loss: 0.003170733839956982, Test Loss: 0.001750528010534528\n",
            "Epoch 60/1000, Training Loss: 0.003167522495207053, Test Loss: 0.0017490823878340172\n",
            "Epoch 61/1000, Training Loss: 0.0031642667988640506, Test Loss: 0.0017476207119722492\n",
            "Epoch 62/1000, Training Loss: 0.003160966448277417, Test Loss: 0.0017461430173779115\n",
            "Epoch 63/1000, Training Loss: 0.0031576211626394707, Test Loss: 0.0017446493432314214\n",
            "Epoch 64/1000, Training Loss: 0.0031542306830926086, Test Loss: 0.0017431397335408389\n",
            "Epoch 65/1000, Training Loss: 0.003150794772900934, Test Loss: 0.0017416142372211618\n",
            "Epoch 66/1000, Training Loss: 0.0031473132176792497, Test Loss: 0.0017400729081747338\n",
            "Epoch 67/1000, Training Loss: 0.0031437858256728774, Test Loss: 0.0017385158053707008\n",
            "Epoch 68/1000, Training Loss: 0.00314021242808215, Test Loss: 0.0017369429929215828\n",
            "Epoch 69/1000, Training Loss: 0.003136592879425825, Test Loss: 0.001735354540155202\n",
            "Epoch 70/1000, Training Loss: 0.0031329270579379094, Test Loss: 0.0017337505216802448\n",
            "Epoch 71/1000, Training Loss: 0.003129214865992659, Test Loss: 0.0017321310174439478\n",
            "Epoch 72/1000, Training Loss: 0.0031254562305526538, Test Loss: 0.0017304961127803725\n",
            "Epoch 73/1000, Training Loss: 0.0031216511036350402, Test Loss: 0.0017288458984478596\n",
            "Epoch 74/1000, Training Loss: 0.003117799462791099, Test Loss: 0.001727180470654315\n",
            "Epoch 75/1000, Training Loss: 0.0031139013115943828, Test Loss: 0.0017254999310690276\n",
            "Epoch 76/1000, Training Loss: 0.0031099566801326937, Test Loss: 0.0017238043868197511\n",
            "Epoch 77/1000, Training Loss: 0.0031059656254992184, Test Loss: 0.0017220939504738913\n",
            "Epoch 78/1000, Training Loss: 0.003101928232278091, Test Loss: 0.0017203687400025901\n",
            "Epoch 79/1000, Training Loss: 0.0030978446130196994, Test Loss: 0.0017186288787266486\n",
            "Epoch 80/1000, Training Loss: 0.003093714908700945, Test Loss: 0.001716874495243182\n",
            "Epoch 81/1000, Training Loss: 0.0030895392891657036, Test Loss: 0.0017151057233320303\n",
            "Epoch 82/1000, Training Loss: 0.003085317953540633, Test Loss: 0.0017133227018409321\n",
            "Epoch 83/1000, Training Loss: 0.003081051130621477, Test Loss: 0.0017115255745485652\n",
            "Epoch 84/1000, Training Loss: 0.0030767390792249254, Test Loss: 0.0017097144900046048\n",
            "Epoch 85/1000, Training Loss: 0.0030723820885010917, Test Loss: 0.0017078896013460227\n",
            "Epoch 86/1000, Training Loss: 0.0030679804782016173, Test Loss: 0.0017060510660889074\n",
            "Epoch 87/1000, Training Loss: 0.0030635345988983985, Test Loss: 0.0017041990458951778\n",
            "Epoch 88/1000, Training Loss: 0.0030590448321479143, Test Loss: 0.0017023337063136494\n",
            "Epoch 89/1000, Training Loss: 0.003054511590596159, Test Loss: 0.00170045521649501\n",
            "Epoch 90/1000, Training Loss: 0.0030499353180191884, Test Loss: 0.0016985637488803355\n",
            "Epoch 91/1000, Training Loss: 0.0030453164892943418, Test Loss: 0.0016966594788629336\n",
            "Epoch 92/1000, Training Loss: 0.0030406556102972816, Test Loss: 0.0016947425844233927\n",
            "Epoch 93/1000, Training Loss: 0.00303595321772006, Test Loss: 0.0016928132457378223\n",
            "Epoch 94/1000, Training Loss: 0.003031209878805563, Test Loss: 0.0016908716447594715\n",
            "Epoch 95/1000, Training Loss: 0.0030264261909938184, Test Loss: 0.0016889179647739771\n",
            "Epoch 96/1000, Training Loss: 0.003021602781475823, Test Loss: 0.0016869523899286929\n",
            "Epoch 97/1000, Training Loss: 0.0030167403066507625, Test Loss: 0.0016849751047366974\n",
            "Epoch 98/1000, Training Loss: 0.003011839451482732, Test Loss: 0.0016829862935562284\n",
            "Epoch 99/1000, Training Loss: 0.00300690092875331, Test Loss: 0.0016809861400464816\n",
            "Epoch 100/1000, Training Loss: 0.0030019254782066903, Test Loss: 0.0016789748266008502\n",
            "Epoch 101/1000, Training Loss: 0.002996913865584336, Test Loss: 0.0016769525337588923\n",
            "Epoch 102/1000, Training Loss: 0.002991866881546551, Test Loss: 0.00167491943959847\n",
            "Epoch 103/1000, Training Loss: 0.0029867853404787065, Test Loss: 0.0016728757191096786\n",
            "Epoch 104/1000, Training Loss: 0.002981670079180315, Test Loss: 0.001670821543552357\n",
            "Epoch 105/1000, Training Loss: 0.002976521955435566, Test Loss: 0.0016687570797991986\n",
            "Epoch 106/1000, Training Loss: 0.002971341846464422, Test Loss: 0.0016666824896665153\n",
            "Epoch 107/1000, Training Loss: 0.0029661306472538863, Test Loss: 0.0016645979292350614\n",
            "Epoch 108/1000, Training Loss: 0.0029608892687695222, Test Loss: 0.0016625035481632963\n",
            "Epoch 109/1000, Training Loss: 0.0029556186360478846, Test Loss: 0.001660399488995724\n",
            "Epoch 110/1000, Training Loss: 0.002950319686171032, Test Loss: 0.0016582858864690661\n",
            "Epoch 111/1000, Training Loss: 0.00294499336612485, Test Loss: 0.0016561628668190845\n",
            "Epoch 112/1000, Training Loss: 0.002939640630543468, Test Loss: 0.0016540305470910555\n",
            "Epoch 113/1000, Training Loss: 0.0029342624393426135, Test Loss: 0.0016518890344569429\n",
            "Epoch 114/1000, Training Loss: 0.0029288597552452698, Test Loss: 0.0016497384255423988\n",
            "Epoch 115/1000, Training Loss: 0.002923433541203566, Test Loss: 0.0016475788057667647\n",
            "Epoch 116/1000, Training Loss: 0.0029179847577213204, Test Loss: 0.0016454102486992972\n",
            "Epoch 117/1000, Training Loss: 0.002912514360082187, Test Loss: 0.0016432328154348454\n",
            "Epoch 118/1000, Training Loss: 0.0029070232954887865, Test Loss: 0.001641046553992174\n",
            "Epoch 119/1000, Training Loss: 0.0029015125001186693, Test Loss: 0.001638851498738099\n",
            "Epoch 120/1000, Training Loss: 0.0028959828961033513, Test Loss: 0.001636647669840576\n",
            "Epoch 121/1000, Training Loss: 0.00289043538843701, Test Loss: 0.0016344350727537143\n",
            "Epoch 122/1000, Training Loss: 0.002884870861821773, Test Loss: 0.001632213697737663\n",
            "Epoch 123/1000, Training Loss: 0.002879290177456736, Test Loss: 0.0016299835194161175\n",
            "Epoch 124/1000, Training Loss: 0.0028736941697781325, Test Loss: 0.001627744496374054\n",
            "Epoch 125/1000, Training Loss: 0.0028680836431581595, Test Loss: 0.001625496570798107\n",
            "Epoch 126/1000, Training Loss: 0.0028624593685701006, Test Loss: 0.0016232396681618035\n",
            "Epoch 127/1000, Training Loss: 0.0028568220802273926, Test Loss: 0.0016209736969576103\n",
            "Epoch 128/1000, Training Loss: 0.0028511724722042765, Test Loss: 0.0016186985484775032\n",
            "Epoch 129/1000, Training Loss: 0.0028455111950455546, Test Loss: 0.0016164140966435062\n",
            "Epoch 130/1000, Training Loss: 0.002839838852372839, Test Loss: 0.0016141201978893236\n",
            "Epoch 131/1000, Training Loss: 0.002834155997494441, Test Loss: 0.0016118166910938792\n",
            "Epoch 132/1000, Training Loss: 0.0028284631300258024, Test Loss: 0.0016095033975672768\n",
            "Epoch 133/1000, Training Loss: 0.00282276069252699, Test Loss: 0.001607180121089271\n",
            "Epoch 134/1000, Training Loss: 0.00281704906716344, Test Loss: 0.0016048466480000803\n",
            "Epoch 135/1000, Training Loss: 0.002811328572395646, Test Loss: 0.0016025027473429076\n",
            "Epoch 136/1000, Training Loss: 0.002805599459703046, Test Loss: 0.0016001481710572207\n",
            "Epoch 137/1000, Training Loss: 0.002799861910346795, Test Loss: 0.0015977826542214013\n",
            "Epoch 138/1000, Training Loss: 0.0027941160321755965, Test Loss: 0.0015954059153429942\n",
            "Epoch 139/1000, Training Loss: 0.002788361856478111, Test Loss: 0.0015930176566944026\n",
            "Epoch 140/1000, Training Loss: 0.002782599334884918, Test Loss: 0.0015906175646914064\n",
            "Epoch 141/1000, Training Loss: 0.00277682833632232, Test Loss: 0.0015882053103115012\n",
            "Epoch 142/1000, Training Loss: 0.0027710486440196626, Test Loss: 0.0015857805495486474\n",
            "Epoch 143/1000, Training Loss: 0.0027652599525712063, Test Loss: 0.001583342923900525\n",
            "Epoch 144/1000, Training Loss: 0.002759461865052927, Test Loss: 0.0015808920608840122\n",
            "Epoch 145/1000, Training Loss: 0.00275365389019403, Test Loss: 0.0015784275745741407\n",
            "Epoch 146/1000, Training Loss: 0.0027478354396023143, Test Loss: 0.0015759490661613157\n",
            "Epoch 147/1000, Training Loss: 0.002742005825041973, Test Loss: 0.0015734561245211544\n",
            "Epoch 148/1000, Training Loss: 0.0027361642557618714, Test Loss: 0.0015709483267908229\n",
            "Epoch 149/1000, Training Loss: 0.002730309835871794, Test Loss: 0.0015684252389452282\n",
            "Epoch 150/1000, Training Loss: 0.002724441561763738, Test Loss: 0.001565886416366001\n",
            "Epoch 151/1000, Training Loss: 0.0027185583195748968, Test Loss: 0.0015633314043956418\n",
            "Epoch 152/1000, Training Loss: 0.0027126588826885904, Test Loss: 0.0015607597388686565\n",
            "Epoch 153/1000, Training Loss: 0.0027067419092691608, Test Loss: 0.0015581709466110636\n",
            "Epoch 154/1000, Training Loss: 0.0027008059398265726, Test Loss: 0.0015555645458989326\n",
            "Epoch 155/1000, Training Loss: 0.0026948493948063244, Test Loss: 0.0015529400468661863\n",
            "Epoch 156/1000, Training Loss: 0.0026888705722001773, Test Loss: 0.0015502969518511737\n",
            "Epoch 157/1000, Training Loss: 0.0026828676451732146, Test Loss: 0.0015476347556709438\n",
            "Epoch 158/1000, Training Loss: 0.002676838659702795, Test Loss: 0.001544952945811474\n",
            "Epoch 159/1000, Training Loss: 0.002670781532225152, Test Loss: 0.0015422510025214638\n",
            "Epoch 160/1000, Training Loss: 0.0026646940472855895, Test Loss: 0.001539528398796544\n",
            "Epoch 161/1000, Training Loss: 0.0026585738551885875, Test Loss: 0.0015367846002401295\n",
            "Epoch 162/1000, Training Loss: 0.002652418469644541, Test Loss: 0.0015340190647863577\n",
            "Epoch 163/1000, Training Loss: 0.0026462252654103433, Test Loss: 0.0015312312422698926\n",
            "Epoch 164/1000, Training Loss: 0.00263999147592166, Test Loss: 0.0015284205738265704\n",
            "Epoch 165/1000, Training Loss: 0.0026337141909153726, Test Loss: 0.0015255864911082808\n",
            "Epoch 166/1000, Training Loss: 0.0026273903540415234, Test Loss: 0.0015227284152946952\n",
            "Epoch 167/1000, Training Loss: 0.002621016760464899, Test Loss: 0.0015198457558838773\n",
            "Epoch 168/1000, Training Loss: 0.0026145900544573765, Test Loss: 0.0015169379092432136\n",
            "Epoch 169/1000, Training Loss: 0.002608106726983197, Test Loss: 0.0015140042569016167\n",
            "Epoch 170/1000, Training Loss: 0.002601563113280454, Test Loss: 0.0015110441635634746\n",
            "Epoch 171/1000, Training Loss: 0.0025949553904432666, Test Loss: 0.0015080569748246104\n",
            "Epoch 172/1000, Training Loss: 0.0025882795750104036, Test Loss: 0.00150504201457027\n",
            "Epoch 173/1000, Training Loss: 0.0025815315205674257, Test Loss: 0.0015019985820351807\n",
            "Epoch 174/1000, Training Loss: 0.002574706915370787, Test Loss: 0.001498925948505865\n",
            "Epoch 175/1000, Training Loss: 0.0025678012800037474, Test Loss: 0.0014958233536458065\n",
            "Epoch 176/1000, Training Loss: 0.0025608099650753623, Test Loss: 0.0014926900014245775\n",
            "Epoch 177/1000, Training Loss: 0.002553728148975206, Test Loss: 0.0014895250556329043\n",
            "Epoch 178/1000, Training Loss: 0.0025465508356978593, Test Loss: 0.0014863276349667742\n",
            "Epoch 179/1000, Training Loss: 0.0025392728527525005, Test Loss: 0.0014830968076648184\n",
            "Epoch 180/1000, Training Loss: 0.0025318888491740756, Test Loss: 0.001479831585685086\n",
            "Epoch 181/1000, Training Loss: 0.0025243932936536467, Test Loss: 0.0014765309184088936\n",
            "Epoch 182/1000, Training Loss: 0.00251678047280625, Test Loss: 0.0014731936858616368\n",
            "Epoch 183/1000, Training Loss: 0.0025090444895952647, Test Loss: 0.0014698186914427539\n",
            "Epoch 184/1000, Training Loss: 0.002501179261932529, Test Loss: 0.001466404654159425\n",
            "Epoch 185/1000, Training Loss: 0.00249317852147336, Test Loss: 0.0014629502003611387\n",
            "Epoch 186/1000, Training Loss: 0.00248503581262511, Test Loss: 0.00145945385497499\n",
            "Epoch 187/1000, Training Loss: 0.002476744491786926, Test Loss: 0.0014559140322441366\n",
            "Epoch 188/1000, Training Loss: 0.0024682977268368324, Test Loss: 0.0014523290259743408\n",
            "Epoch 189/1000, Training Loss: 0.002459688496880187, Test Loss: 0.0014486969992960302\n",
            "Epoch 190/1000, Training Loss: 0.002450909592270981, Test Loss: 0.0014450159739514182\n",
            "Epoch 191/1000, Training Loss: 0.0024419536149142052, Test Loss: 0.0014412838191181694\n",
            "Epoch 192/1000, Training Loss: 0.002432812978854004, Test Loss: 0.0014374982397828177\n",
            "Epoch 193/1000, Training Loss: 0.0024234799111483144, Test Loss: 0.001433656764678583\n",
            "Epoch 194/1000, Training Loss: 0.0024139464530267534, Test Loss: 0.0014297567338037544\n",
            "Epoch 195/1000, Training Loss: 0.00240420446132467, Test Loss: 0.00142579528553838\n",
            "Epoch 196/1000, Training Loss: 0.0023942456101832647, Test Loss: 0.0014217693433793213\n",
            "Epoch 197/1000, Training Loss: 0.0023840613930037804, Test Loss: 0.0014176756023171668\n",
            "Epoch 198/1000, Training Loss: 0.002373643124644137, Test Loss: 0.001413510514883968\n",
            "Epoch 199/1000, Training Loss: 0.00236298194384954, Test Loss: 0.0014092702769093034\n",
            "Epoch 200/1000, Training Loss: 0.0023520688159162726, Test Loss: 0.0014049508130352323\n",
            "Epoch 201/1000, Training Loss: 0.0023408945356008875, Test Loss: 0.0014005477620597582\n",
            "Epoch 202/1000, Training Loss: 0.0023294497303075652, Test Loss: 0.001396056462205845\n",
            "Epoch 203/1000, Training Loss: 0.002317724863615917, Test Loss: 0.0013914719364505645\n",
            "Epoch 204/1000, Training Loss: 0.002305710239252237, Test Loss: 0.0013867888780997209\n",
            "Epoch 205/1000, Training Loss: 0.0022933960056612038, Test Loss: 0.0013820016368593837\n",
            "Epoch 206/1000, Training Loss: 0.0022807721614044084, Test Loss: 0.001377104205739865\n",
            "Epoch 207/1000, Training Loss: 0.002267828561698985, Test Loss: 0.0013720902092321577\n",
            "Epoch 208/1000, Training Loss: 0.0022545549265157137, Test Loss: 0.0013669528933228658\n",
            "Epoch 209/1000, Training Loss: 0.002240940850782481, Test Loss: 0.001361685118061909\n",
            "Epoch 210/1000, Training Loss: 0.0022269758173867372, Test Loss: 0.0013562793535666254\n",
            "Epoch 211/1000, Training Loss: 0.002212649213838724, Test Loss: 0.0013507276805329114\n",
            "Epoch 212/1000, Training Loss: 0.0021979503536442395, Test Loss: 0.001345021796523041\n",
            "Epoch 213/1000, Training Loss: 0.0021828685036375645, Test Loss: 0.0013391530295013168\n",
            "Epoch 214/1000, Training Loss: 0.002167392918736107, Test Loss: 0.0013331123602796197\n",
            "Epoch 215/1000, Training Loss: 0.0021515128857893307, Test Loss: 0.0013268904556980102\n",
            "Epoch 216/1000, Training Loss: 0.002135217778393374, Test Loss: 0.0013204777144783866\n",
            "Epoch 217/1000, Training Loss: 0.0021184971247129865, Test Loss: 0.0013138643277260118\n",
            "Epoch 218/1000, Training Loss: 0.002101340690473194, Test Loss: 0.0013070403559838426\n",
            "Epoch 219/1000, Training Loss: 0.002083738579328263, Test Loss: 0.0012999958245360516\n",
            "Epoch 220/1000, Training Loss: 0.002065681352754299, Test Loss: 0.0012927208382769988\n",
            "Epoch 221/1000, Training Loss: 0.0020471601714085784, Test Loss: 0.001285205716880853\n",
            "Epoch 222/1000, Training Loss: 0.0020281669595148904, Test Loss: 0.001277441150203238\n",
            "Epoch 223/1000, Training Loss: 0.0020086945932304636, Test Loss: 0.0012694183728103076\n",
            "Epoch 224/1000, Training Loss: 0.0019887371130903326, Test Loss: 0.0012611293552726931\n",
            "Epoch 225/1000, Training Loss: 0.001968289959482023, Test Loss: 0.0012525670084163093\n",
            "Epoch 226/1000, Training Loss: 0.001947350228665683, Test Loss: 0.001243725395154492\n",
            "Epoch 227/1000, Training Loss: 0.0019259169451350477, Test Loss: 0.001234599942934394\n",
            "Epoch 228/1000, Training Loss: 0.0019039913441594696, Test Loss: 0.0012251876483468246\n",
            "Epoch 229/1000, Training Loss: 0.001881577156244699, Test Loss: 0.0012154872642320695\n",
            "Epoch 230/1000, Training Loss: 0.001858680883137076, Test Loss: 0.0012054994588413716\n",
            "Epoch 231/1000, Training Loss: 0.0018353120530584465, Test Loss: 0.0011952269364617228\n",
            "Epoch 232/1000, Training Loss: 0.0018114834413282801, Test Loss: 0.0011846745095354199\n",
            "Epoch 233/1000, Training Loss: 0.0017872112416644115, Test Loss: 0.001173849113812465\n",
            "Epoch 234/1000, Training Loss: 0.0017625151735160024, Test Loss: 0.001162759760495052\n",
            "Epoch 235/1000, Training Loss: 0.00173741851199686, Test Loss: 0.0011514174226020532\n",
            "Epoch 236/1000, Training Loss: 0.0017119480294970591, Test Loss: 0.0011398348567162229\n",
            "Epoch 237/1000, Training Loss: 0.0016861338418677355, Test Loss: 0.0011280263655794084\n",
            "Epoch 238/1000, Training Loss: 0.0016600091570414565, Test Loss: 0.0011160075112754554\n",
            "Epoch 239/1000, Training Loss: 0.001633609929724954, Test Loss: 0.0011037947925297146\n",
            "Epoch 240/1000, Training Loss: 0.001606974431867879, Test Loss: 0.0010914053025016161\n",
            "Epoch 241/1000, Training Loss: 0.0015801427543410866, Test Loss: 0.0010788563849688094\n",
            "Epoch 242/1000, Training Loss: 0.001553156259994186, Test Loss: 0.0010661653067590203\n",
            "Epoch 243/1000, Training Loss: 0.0015260570114325734, Test Loss: 0.0010533489626403365\n",
            "Epoch 244/1000, Training Loss: 0.0014988871980748026, Test Loss: 0.0010404236258148391\n",
            "Epoch 245/1000, Training Loss: 0.0014716885862001055, Test Loss: 0.0010274047530552158\n",
            "Epoch 246/1000, Training Loss: 0.001444502012934917, Test Loss: 0.0010143068488941043\n",
            "Epoch 247/1000, Training Loss: 0.0014173669408679018, Test Loss: 0.001001143388680678\n",
            "Epoch 248/1000, Training Loss: 0.0013903210848012176, Test Loss: 0.0009879267962627516\n",
            "Epoch 249/1000, Training Loss: 0.0013634001166752772, Test Loss: 0.0009746684689102557\n",
            "Epoch 250/1000, Training Loss: 0.0013366374495298614, Test Loss: 0.000961378840066365\n",
            "Epoch 251/1000, Training Loss: 0.001310064096940934, Test Loss: 0.0009480674696182564\n",
            "Epoch 252/1000, Training Loss: 0.0012837086009819443, Test Loss: 0.0009347431514937056\n",
            "Epoch 253/1000, Training Loss: 0.0012575970195057313, Test Loss: 0.0009214140292899256\n",
            "Epoch 254/1000, Training Loss: 0.0012317529623832233, Test Loss: 0.0009080877120622955\n",
            "Epoch 255/1000, Training Loss: 0.0012061976661153547, Test Loss: 0.0008947713840852225\n",
            "Epoch 256/1000, Training Loss: 0.0011809500967443897, Test Loss: 0.0008814719041268832\n",
            "Epoch 257/1000, Training Loss: 0.0011560270720021913, Test Loss: 0.0008681958913923832\n",
            "Epoch 258/1000, Training Loss: 0.0011314433949352587, Test Loss: 0.0008549497966849396\n",
            "Epoch 259/1000, Training Loss: 0.001107211992661571, Test Loss: 0.0008417399584616463\n",
            "Epoch 260/1000, Training Loss: 0.001083344055309183, Test Loss: 0.0008285726443118625\n",
            "Epoch 261/1000, Training Loss: 0.0010598491714697816, Test Loss: 0.0008154540789809134\n",
            "Epoch 262/1000, Training Loss: 0.0010367354576190603, Test Loss: 0.0008023904604355677\n",
            "Epoch 263/1000, Training Loss: 0.0010140096798865468, Test Loss: 0.0007893879656617835\n",
            "Epoch 264/1000, Training Loss: 0.0009916773672994288, Test Loss: 0.0007764527479422828\n",
            "Epoch 265/1000, Training Loss: 0.0009697429161910186, Test Loss: 0.0007635909273190584\n",
            "Epoch 266/1000, Training Loss: 0.0009482096858762035, Test Loss: 0.0007508085758369884\n",
            "Epoch 267/1000, Training Loss: 0.0009270800859786512, Test Loss: 0.0007381116990145147\n",
            "Epoch 268/1000, Training Loss: 0.0009063556559725051, Test Loss: 0.0007255062148163548\n",
            "Epoch 269/1000, Training Loss: 0.0008860371375988106, Test Loss: 0.0007129979312252903\n",
            "Epoch 270/1000, Training Loss: 0.0008661245408542572, Test Loss: 0.0007005925233357897\n",
            "Epoch 271/1000, Training Loss: 0.0008466172042443961, Test Loss: 0.0006882955107274861\n",
            "Epoch 272/1000, Training Loss: 0.0008275138499594492, Test Loss: 0.0006761122357259492\n",
            "Epoch 273/1000, Training Loss: 0.0008088126345788732, Test Loss: 0.0006640478430228588\n",
            "Epoch 274/1000, Training Loss: 0.0007905111958495642, Test Loss: 0.0006521072610093325\n",
            "Epoch 275/1000, Training Loss: 0.0007726066960174979, Test Loss: 0.0006402951850730708\n",
            "Epoch 276/1000, Training Loss: 0.0007550958621285162, Test Loss: 0.0006286160630229529\n",
            "Epoch 277/1000, Training Loss: 0.0007379750236530063, Test Loss: 0.0006170740827310446\n",
            "Epoch 278/1000, Training Loss: 0.000721240147733537, Test Loss: 0.0006056731620213103\n",
            "Epoch 279/1000, Training Loss: 0.0007048868723049027, Test Loss: 0.0005944169407850598\n",
            "Epoch 280/1000, Training Loss: 0.000688910537292468, Test Loss: 0.0005833087752634434\n",
            "Epoch 281/1000, Training Loss: 0.0006733062140576458, Test Loss: 0.0005723517344067231\n",
            "Epoch 282/1000, Training Loss: 0.0006580687332279905, Test Loss: 0.0005615485981964821\n",
            "Epoch 283/1000, Training Loss: 0.0006431927110235573, Test Loss: 0.0005509018578000549\n",
            "Epoch 284/1000, Training Loss: 0.0006286725741701416, Test Loss: 0.0005404137174148476\n",
            "Epoch 285/1000, Training Loss: 0.000614502583473173, Test Loss: 0.0005300860976529463\n",
            "Epoch 286/1000, Training Loss: 0.0006006768561129732, Test Loss: 0.000519920640313393\n",
            "Epoch 287/1000, Training Loss: 0.0005871893867120025, Test Loss: 0.0005099187143889247\n",
            "Epoch 288/1000, Training Loss: 0.0005740340672172446, Test Loss: 0.0005000814231564883\n",
            "Epoch 289/1000, Training Loss: 0.0005612047056355867, Test Loss: 0.0004904096122048628\n",
            "Epoch 290/1000, Training Loss: 0.0005486950436563443, Test Loss: 0.0004809038782587478\n",
            "Epoch 291/1000, Training Loss: 0.0005364987731928735, Test Loss: 0.00047156457866548706\n",
            "Epoch 292/1000, Training Loss: 0.000524609551873839, Test Loss: 0.00046239184141849384\n",
            "Epoch 293/1000, Training Loss: 0.0005130210175143539, Test Loss: 0.0004533855755998917\n",
            "Epoch 294/1000, Training Loss: 0.0005017268015971633, Test Loss: 0.0004445454821336867\n",
            "Epoch 295/1000, Training Loss: 0.0004907205417945934, Test Loss: 0.00043587106474964597\n",
            "Epoch 296/1000, Training Loss: 0.000479995893562611, Test Loss: 0.00042736164106713456\n",
            "Epoch 297/1000, Training Loss: 0.00046954654083916023, Test Loss: 0.0004190163537169409\n",
            "Epoch 298/1000, Training Loss: 0.00045936620587975504, Test Loss: 0.0004108341814276961\n",
            "Epoch 299/1000, Training Loss: 0.00044944865826409023, Test Loss: 0.0004028139500119029\n",
            "Epoch 300/1000, Training Loss: 0.00043978772310805864, Test Loss: 0.00039495434319437015\n",
            "Epoch 301/1000, Training Loss: 0.0004303772885161601, Test Loss: 0.0003872539132334832\n",
            "Epoch 302/1000, Training Loss: 0.00042121131230959757, Test Loss: 0.00037971109129270584\n",
            "Epoch 303/1000, Training Loss: 0.0004122838280656014, Test Loss: 0.0003723241975263353\n",
            "Epoch 304/1000, Training Loss: 0.0004035889505035076, Test Loss: 0.00036509145084960266\n",
            "Epoch 305/1000, Training Loss: 0.0003951208802529908, Test Loss: 0.000358010978368829\n",
            "Epoch 306/1000, Training Loss: 0.0003868739080395016, Test Loss: 0.00035108082445245653\n",
            "Epoch 307/1000, Training Loss: 0.0003788424183214958, Test Loss: 0.0003442989594284581\n",
            "Epoch 308/1000, Training Loss: 0.0003710208924133681, Test Loss: 0.0003376632878977829\n",
            "Epoch 309/1000, Training Loss: 0.0003634039111273239, Test Loss: 0.00033117165665730113\n",
            "Epoch 310/1000, Training Loss: 0.0003559861569664201, Test Loss: 0.00032482186222902845\n",
            "Epoch 311/1000, Training Loss: 0.0003487624159001566, Test Loss: 0.00031861165799536946\n",
            "Epoch 312/1000, Training Loss: 0.00034172757875283564, Test Loss: 0.00031253876094272415\n",
            "Epoch 313/1000, Training Loss: 0.0003348766422338264, Test Loss: 0.00030660085801799137\n",
            "Epoch 314/1000, Training Loss: 0.0003282047096376577, Test Loss: 0.0003007956121044936\n",
            "Epoch 315/1000, Training Loss: 0.00032170699124069467, Test Loss: 0.00029512066762545823\n",
            "Epoch 316/1000, Training Loss: 0.0003153788044198251, Test Loss: 0.00028957365578452824\n",
            "Epoch 317/1000, Training Loss: 0.000309215573517361, Test Loss: 0.00028415219945392335\n",
            "Epoch 318/1000, Training Loss: 0.0003032128294751172, Test Loss: 0.0002788539177218024\n",
            "Epoch 319/1000, Training Loss: 0.0002973662092592967, Test Loss: 0.0002736764301110276\n",
            "Epoch 320/1000, Training Loss: 0.00029167145509664377, Test Loss: 0.0002686173604821141\n",
            "Epoch 321/1000, Training Loss: 0.00028612441354103574, Test Loss: 0.00026367434063347584\n",
            "Epoch 322/1000, Training Loss: 0.00028072103438852707, Test Loss: 0.00025884501361236287\n",
            "Epoch 323/1000, Training Loss: 0.00027545736945766564, Test Loss: 0.00025412703674995105\n",
            "Epoch 324/1000, Training Loss: 0.00027032957125079843, Test Loss: 0.00024951808443406267\n",
            "Epoch 325/1000, Training Loss: 0.00026533389151097515, Test Loss: 0.00024501585063295547\n",
            "Epoch 326/1000, Training Loss: 0.00026046667968801, Test Loss: 0.0002406180511833705\n",
            "Epoch 327/1000, Training Loss: 0.0002557243813262869, Test Loss: 0.00023632242585587622\n",
            "Epoch 328/1000, Training Loss: 0.0002511035363859047, Test Loss: 0.00023212674021022464\n",
            "Epoch 329/1000, Training Loss: 0.00024660077750787224, Test Loss: 0.0002280287872530888\n",
            "Epoch 330/1000, Training Loss: 0.00024221282823321671, Test Loss: 0.00022402638891025418\n",
            "Epoch 331/1000, Training Loss: 0.0002379365011849841, Test Loss: 0.00022011739732481723\n",
            "Epoch 332/1000, Training Loss: 0.00023376869622144495, Test Loss: 0.00021629969599265133\n",
            "Epoch 333/1000, Training Loss: 0.00022970639856801059, Test Loss: 0.000212571200745886\n",
            "Epoch 334/1000, Training Loss: 0.00022574667693471952, Test Loss: 0.00020892986059471092\n",
            "Epoch 335/1000, Training Loss: 0.00022188668162554195, Test Loss: 0.00020537365843741322\n",
            "Epoch 336/1000, Training Loss: 0.00021812364264509472, Test Loss: 0.0002019006116480037\n",
            "Epoch 337/1000, Training Loss: 0.00021445486780785812, Test Loss: 0.00019850877255043532\n",
            "Epoch 338/1000, Training Loss: 0.00021087774085445606, Test Loss: 0.00019519622878791507\n",
            "Epoch 339/1000, Training Loss: 0.00020738971957908297, Test Loss: 0.00019196110359537163\n",
            "Epoch 340/1000, Training Loss: 0.00020398833397168803, Test Loss: 0.00018880155598270863\n",
            "Epoch 341/1000, Training Loss: 0.0002006711843781815, Test Loss: 0.00018571578083608078\n",
            "Epoch 342/1000, Training Loss: 0.00019743593968146983, Test Loss: 0.00018270200894396062\n",
            "Epoch 343/1000, Training Loss: 0.00019428033550583832, Test Loss: 0.00017975850695441944\n",
            "Epoch 344/1000, Training Loss: 0.00019120217244685076, Test Loss: 0.00017688357726964224\n",
            "Epoch 345/1000, Training Loss: 0.00018819931432864921, Test Loss: 0.00017407555788331505\n",
            "Epoch 346/1000, Training Loss: 0.00018526968649026725, Test Loss: 0.00017133282216620345\n",
            "Epoch 347/1000, Training Loss: 0.0001824112741022828, Test Loss: 0.00016865377860482395\n",
            "Epoch 348/1000, Training Loss: 0.00017962212051500937, Test Loss: 0.00016603687049792542\n",
            "Epoch 349/1000, Training Loss: 0.00017690032563910202, Test Loss: 0.0001634805756150111\n",
            "Epoch 350/1000, Training Loss: 0.00017424404435934646, Test Loss: 0.00016098340582099938\n",
            "Epoch 351/1000, Training Loss: 0.00017165148498219756, Test Loss: 0.00015854390667073537\n",
            "Epoch 352/1000, Training Loss: 0.0001691209077174947, Test Loss: 0.00015616065697685085\n",
            "Epoch 353/1000, Training Loss: 0.0001666506231946305, Test Loss: 0.0001538322683541985\n",
            "Epoch 354/1000, Training Loss: 0.00016423899101333239, Test Loss: 0.00015155738474387936\n",
            "Epoch 355/1000, Training Loss: 0.00016188441832910833, Test Loss: 0.00014933468191960758\n",
            "Epoch 356/1000, Training Loss: 0.00015958535847329113, Test Loss: 0.0001471628669789976\n",
            "Epoch 357/1000, Training Loss: 0.00015734030960755217, Test Loss: 0.00014504067782214057\n",
            "Epoch 358/1000, Training Loss: 0.0001551478134126622, Test Loss: 0.00014296688261963383\n",
            "Epoch 359/1000, Training Loss: 0.00015300645381118993, Test Loss: 0.00014094027927209876\n",
            "Epoch 360/1000, Training Loss: 0.0001509148557238147, Test Loss: 0.00013895969486298719\n",
            "Epoch 361/1000, Training Loss: 0.00014887168385882337, Test Loss: 0.0001370239851064247\n",
            "Epoch 362/1000, Training Loss: 0.00014687564153433876, Test Loss: 0.00013513203379155692\n",
            "Epoch 363/1000, Training Loss: 0.00014492546953281292, Test Loss: 0.0001332827522249091\n",
            "Epoch 364/1000, Training Loss: 0.00014301994498721715, Test Loss: 0.00013147507867195403\n",
            "Epoch 365/1000, Training Loss: 0.00014115788029841125, Test Loss: 0.00012970797779912957\n",
            "Epoch 366/1000, Training Loss: 0.0001393381220830741, Test Loss: 0.0001279804401173157\n",
            "Epoch 367/1000, Training Loss: 0.00013755955015161867, Test Loss: 0.00012629148142777135\n",
            "Epoch 368/1000, Training Loss: 0.00013582107651546952, Test Loss: 0.00012464014227138098\n",
            "Epoch 369/1000, Training Loss: 0.00013412164442306183, Test Loss: 0.00012302548738198996\n",
            "Epoch 370/1000, Training Loss: 0.0001324602274239348, Test Loss: 0.00012144660514453717\n",
            "Epoch 371/1000, Training Loss: 0.0001308358284602627, Test Loss: 0.00011990260705860616\n",
            "Epoch 372/1000, Training Loss: 0.00012924747898518531, Test Loss: 0.00011839262720795014\n",
            "Epoch 373/1000, Training Loss: 0.00012769423810728107, Test Loss: 0.00011691582173648309\n",
            "Epoch 374/1000, Training Loss: 0.00012617519176051767, Test Loss: 0.00011547136833116696\n",
            "Epoch 375/1000, Training Loss: 0.00012468945189905795, Test Loss: 0.00011405846571217686\n",
            "Epoch 376/1000, Training Loss: 0.00012323615571626278, Test Loss: 0.00011267633313067333\n",
            "Epoch 377/1000, Training Loss: 0.00012181446488723817, Test Loss: 0.00011132420987444937\n",
            "Epoch 378/1000, Training Loss: 0.0001204235648343285, Test Loss: 0.00011000135478171825\n",
            "Epoch 379/1000, Training Loss: 0.0001190626640148882, Test Loss: 0.00010870704576320353\n",
            "Epoch 380/1000, Training Loss: 0.00011773099323074743, Test Loss: 0.00010744057933272823\n",
            "Epoch 381/1000, Training Loss: 0.00011642780495875513, Test Loss: 0.0001062012701464238\n",
            "Epoch 382/1000, Training Loss: 0.000115152372701792, Test Loss: 0.00010498845055064132\n",
            "Epoch 383/1000, Training Loss: 0.00011390399035968378, Test Loss: 0.00010380147013867385\n",
            "Epoch 384/1000, Training Loss: 0.0001126819716194313, Test Loss: 0.00010263969531630309\n",
            "Epoch 385/1000, Training Loss: 0.00011148564936418682, Test Loss: 0.00010150250887621072\n",
            "Epoch 386/1000, Training Loss: 0.00011031437510044052, Test Loss: 0.00010038930958127849\n",
            "Epoch 387/1000, Training Loss: 0.0001091675184028576, Test Loss: 9.929951175671123e-05\n",
            "Epoch 388/1000, Training Loss: 0.00010804446637626157, Test Loss: 9.823254489101972e-05\n",
            "Epoch 389/1000, Training Loss: 0.00010694462313423376, Test Loss: 9.71878532457626e-05\n",
            "Epoch 390/1000, Training Loss: 0.00010586740929382204, Test Loss: 9.616489547401484e-05\n",
            "Epoch 391/1000, Training Loss: 0.00010481226148588267, Test Loss: 9.51631442474825e-05\n",
            "Epoch 392/1000, Training Loss: 0.00010377863188056901, Test Loss: 9.418208589218482e-05\n",
            "Epoch 393/1000, Training Loss: 0.00010276598772750834, Test Loss: 9.322122003261498e-05\n",
            "Epoch 394/1000, Training Loss: 0.00010177381091019844, Test Loss: 9.228005924426225e-05\n",
            "Epoch 395/1000, Training Loss: 0.00010080159751421767, Test Loss: 9.135812871441916e-05\n",
            "Epoch 396/1000, Training Loss: 9.984885740876414e-05, Test Loss: 9.045496591110918e-05\n",
            "Epoch 397/1000, Training Loss: 9.891511384117433e-05, Test Loss: 8.957012026007329e-05\n",
            "Epoch 398/1000, Training Loss: 9.799990304395805e-05, Test Loss: 8.870315282964143e-05\n",
            "Epoch 399/1000, Training Loss: 9.710277385399521e-05, Test Loss: 8.785363602338422e-05\n",
            "Epoch 400/1000, Training Loss: 9.622328734349933e-05, Test Loss: 8.702115328041048e-05\n",
            "Epoch 401/1000, Training Loss: 9.536101646236653e-05, Test Loss: 8.620529878315644e-05\n",
            "Epoch 402/1000, Training Loss: 9.451554569157295e-05, Test Loss: 8.540567717255409e-05\n",
            "Epoch 403/1000, Training Loss: 9.368647070725612e-05, Test Loss: 8.462190327042008e-05\n",
            "Epoch 404/1000, Training Loss: 9.287339805514552e-05, Test Loss: 8.385360180893045e-05\n",
            "Epoch 405/1000, Training Loss: 9.207594483501527e-05, Test Loss: 8.310040716703633e-05\n",
            "Epoch 406/1000, Training Loss: 9.129373839484117e-05, Test Loss: 8.236196311368042e-05\n",
            "Epoch 407/1000, Training Loss: 9.052641603435869e-05, Test Loss: 8.16379225576695e-05\n",
            "Epoch 408/1000, Training Loss: 8.977362471771112e-05, Test Loss: 8.092794730406162e-05\n",
            "Epoch 409/1000, Training Loss: 8.903502079490928e-05, Test Loss: 8.02317078169166e-05\n",
            "Epoch 410/1000, Training Loss: 8.831026973182347e-05, Test Loss: 7.954888298828865e-05\n",
            "Epoch 411/1000, Training Loss: 8.75990458484244e-05, Test Loss: 7.887915991329375e-05\n",
            "Epoch 412/1000, Training Loss: 8.690103206502392e-05, Test Loss: 7.822223367113065e-05\n",
            "Epoch 413/1000, Training Loss: 8.621591965625946e-05, Test Loss: 7.757780711191949e-05\n",
            "Epoch 414/1000, Training Loss: 8.554340801255712e-05, Test Loss: 7.694559064919319e-05\n",
            "Epoch 415/1000, Training Loss: 8.48832044088644e-05, Test Loss: 7.632530205794781e-05\n",
            "Epoch 416/1000, Training Loss: 8.42350237804051e-05, Test Loss: 7.571666627808676e-05\n",
            "Epoch 417/1000, Training Loss: 8.359858850522282e-05, Test Loss: 7.511941522313024e-05\n",
            "Epoch 418/1000, Training Loss: 8.297362819332327e-05, Test Loss: 7.453328759408146e-05\n",
            "Epoch 419/1000, Training Loss: 8.235987948218393e-05, Test Loss: 7.395802869829698e-05\n",
            "Epoch 420/1000, Training Loss: 8.175708583844045e-05, Test Loss: 7.339339027325042e-05\n",
            "Epoch 421/1000, Training Loss: 8.116499736554348e-05, Test Loss: 7.28391303150581e-05\n",
            "Epoch 422/1000, Training Loss: 8.0583370617209e-05, Test Loss: 7.229501291164986e-05\n",
            "Epoch 423/1000, Training Loss: 8.001196841647134e-05, Test Loss: 7.176080808047057e-05\n",
            "Epoch 424/1000, Training Loss: 7.945055968015703e-05, Test Loss: 7.123629161057767e-05\n",
            "Epoch 425/1000, Training Loss: 7.889891924862229e-05, Test Loss: 7.072124490904862e-05\n",
            "Epoch 426/1000, Training Loss: 7.835682772056979e-05, Test Loss: 7.021545485155387e-05\n",
            "Epoch 427/1000, Training Loss: 7.782407129280344e-05, Test Loss: 6.971871363701762e-05\n",
            "Epoch 428/1000, Training Loss: 7.730044160474539e-05, Test Loss: 6.923081864622989e-05\n",
            "Epoch 429/1000, Training Loss: 7.67857355875825e-05, Test Loss: 6.875157230432949e-05\n",
            "Epoch 430/1000, Training Loss: 7.627975531788343e-05, Test Loss: 6.828078194703203e-05\n",
            "Epoch 431/1000, Training Loss: 7.578230787555892e-05, Test Loss: 6.781825969052586e-05\n",
            "Epoch 432/1000, Training Loss: 7.529320520602067e-05, Test Loss: 6.736382230492585e-05\n",
            "Epoch 433/1000, Training Loss: 7.481226398640099e-05, Test Loss: 6.69172910911757e-05\n",
            "Epoch 434/1000, Training Loss: 7.433930549572047e-05, Test Loss: 6.647849176133784e-05\n",
            "Epoch 435/1000, Training Loss: 7.387415548887548e-05, Test Loss: 6.604725432214852e-05\n",
            "Epoch 436/1000, Training Loss: 7.341664407431839e-05, Test Loss: 6.562341296176852e-05\n",
            "Epoch 437/1000, Training Loss: 7.296660559532418e-05, Test Loss: 6.520680593963068e-05\n",
            "Epoch 438/1000, Training Loss: 7.252387851472783e-05, Test Loss: 6.479727547930684e-05\n",
            "Epoch 439/1000, Training Loss: 7.20883053030282e-05, Test Loss: 6.439466766430918e-05\n",
            "Epoch 440/1000, Training Loss: 7.165973232975089e-05, Test Loss: 6.399883233674443e-05\n",
            "Epoch 441/1000, Training Loss: 7.12380097579673e-05, Test Loss: 6.360962299874205e-05\n",
            "Epoch 442/1000, Training Loss: 7.082299144187636e-05, Test Loss: 6.322689671658308e-05\n",
            "Epoch 443/1000, Training Loss: 7.041453482735685e-05, Test Loss: 6.285051402745406e-05\n",
            "Epoch 444/1000, Training Loss: 7.001250085538697e-05, Test Loss: 6.24803388487469e-05\n",
            "Epoch 445/1000, Training Loss: 6.96167538682543e-05, Test Loss: 6.211623838984183e-05\n",
            "Epoch 446/1000, Training Loss: 6.922716151847491e-05, Test Loss: 6.175808306630987e-05\n",
            "Epoch 447/1000, Training Loss: 6.884359468031698e-05, Test Loss: 6.14057464164496e-05\n",
            "Epoch 448/1000, Training Loss: 6.846592736387053e-05, Test Loss: 6.105910502010577e-05\n",
            "Epoch 449/1000, Training Loss: 6.809403663157721e-05, Test Loss: 6.0718038419709326e-05\n",
            "Epoch 450/1000, Training Loss: 6.772780251714453e-05, Test Loss: 6.038242904346567e-05\n",
            "Epoch 451/1000, Training Loss: 6.736710794677234e-05, Test Loss: 6.0052162130639596e-05\n",
            "Epoch 452/1000, Training Loss: 6.701183866262464e-05, Test Loss: 5.97271256588797e-05\n",
            "Epoch 453/1000, Training Loss: 6.666188314847323e-05, Test Loss: 5.9407210273512634e-05\n",
            "Epoch 454/1000, Training Loss: 6.631713255745211e-05, Test Loss: 5.90923092187716e-05\n",
            "Epoch 455/1000, Training Loss: 6.597748064185571e-05, Test Loss: 5.878231827088553e-05\n",
            "Epoch 456/1000, Training Loss: 6.564282368492042e-05, Test Loss: 5.8477135672992287e-05\n",
            "Epoch 457/1000, Training Loss: 6.53130604345294e-05, Test Loss: 5.8176662071814066e-05\n",
            "Epoch 458/1000, Training Loss: 6.498809203878692e-05, Test Loss: 5.788080045605398e-05\n",
            "Epoch 459/1000, Training Loss: 6.46678219833963e-05, Test Loss: 5.7589456096457626e-05\n",
            "Epoch 460/1000, Training Loss: 6.435215603079746e-05, Test Loss: 5.730253648749965e-05\n",
            "Epoch 461/1000, Training Loss: 6.404100216100708e-05, Test Loss: 5.701995129064499e-05\n",
            "Epoch 462/1000, Training Loss: 6.373427051411e-05, Test Loss: 5.6741612279145907e-05\n",
            "Epoch 463/1000, Training Loss: 6.343187333435545e-05, Test Loss: 5.646743328432702e-05\n",
            "Epoch 464/1000, Training Loss: 6.313372491580651e-05, Test Loss: 5.619733014331867e-05\n",
            "Epoch 465/1000, Training Loss: 6.28397415494987e-05, Test Loss: 5.5931220648195844e-05\n",
            "Epoch 466/1000, Training Loss: 6.25498414720689e-05, Test Loss: 5.5669024496492154e-05\n",
            "Epoch 467/1000, Training Loss: 6.226394481579805e-05, Test Loss: 5.541066324303361e-05\n",
            "Epoch 468/1000, Training Loss: 6.198197356003455e-05, Test Loss: 5.5156060253073186e-05\n",
            "Epoch 469/1000, Training Loss: 6.170385148396256e-05, Test Loss: 5.4905140656679375e-05\n",
            "Epoch 470/1000, Training Loss: 6.142950412066235e-05, Test Loss: 5.465783130434443e-05\n",
            "Epoch 471/1000, Training Loss: 6.115885871243539e-05, Test Loss: 5.441406072378187e-05\n",
            "Epoch 472/1000, Training Loss: 6.0891844167354526e-05, Test Loss: 5.417375907788118e-05\n",
            "Epoch 473/1000, Training Loss: 6.0628391017000804e-05, Test Loss: 5.393685812377454e-05\n",
            "Epoch 474/1000, Training Loss: 6.036843137535351e-05, Test Loss: 5.3703291173006744e-05\n",
            "Epoch 475/1000, Training Loss: 6.0111898898807596e-05, Test Loss: 5.347299305275984e-05\n",
            "Epoch 476/1000, Training Loss: 5.985872874726632e-05, Test Loss: 5.324590006810208e-05\n",
            "Epoch 477/1000, Training Loss: 5.960885754630045e-05, Test Loss: 5.3021949965250844e-05\n",
            "Epoch 478/1000, Training Loss: 5.936222335033216e-05, Test Loss: 5.280108189580204e-05\n",
            "Epoch 479/1000, Training Loss: 5.9118765606808906e-05, Test Loss: 5.258323638191018e-05\n",
            "Epoch 480/1000, Training Loss: 5.887842512135081e-05, Test Loss: 5.23683552823857e-05\n",
            "Epoch 481/1000, Training Loss: 5.864114402383409e-05, Test Loss: 5.215638175969142e-05\n",
            "Epoch 482/1000, Training Loss: 5.84068657353886e-05, Test Loss: 5.194726024780728e-05\n",
            "Epoch 483/1000, Training Loss: 5.817553493628048e-05, Test Loss: 5.1740936420941944e-05\n",
            "Epoch 484/1000, Training Loss: 5.7947097534656945e-05, Test Loss: 5.1537357163067694e-05\n",
            "Epoch 485/1000, Training Loss: 5.772150063612722e-05, Test Loss: 5.133647053825436e-05\n",
            "Epoch 486/1000, Training Loss: 5.749869251415381e-05, Test Loss: 5.1138225761783987e-05\n",
            "Epoch 487/1000, Training Loss: 5.727862258123808e-05, Test Loss: 5.094257317202075e-05\n",
            "Epoch 488/1000, Training Loss: 5.7061241360867796e-05, Test Loss: 5.074946420301379e-05\n",
            "Epoch 489/1000, Training Loss: 5.684650046020971e-05, Test Loss: 5.055885135781933e-05\n",
            "Epoch 490/1000, Training Loss: 5.663435254353217e-05, Test Loss: 5.037068818251823e-05\n",
            "Epoch 491/1000, Training Loss: 5.6424751306325476e-05, Test Loss: 5.0184929240910055e-05\n",
            "Epoch 492/1000, Training Loss: 5.621765145010916e-05, Test Loss: 5.0001530089862e-05\n",
            "Epoch 493/1000, Training Loss: 5.60130086578998e-05, Test Loss: 4.982044725530324e-05\n",
            "Epoch 494/1000, Training Loss: 5.5810779570332715e-05, Test Loss: 4.964163820883973e-05\n",
            "Epoch 495/1000, Training Loss: 5.561092176240294e-05, Test Loss: 4.946506134497235e-05\n",
            "Epoch 496/1000, Training Loss: 5.5413393720818866e-05, Test Loss: 4.9290675958905065e-05\n",
            "Epoch 497/1000, Training Loss: 5.5218154821953664e-05, Test Loss: 4.9118442224929405e-05\n",
            "Epoch 498/1000, Training Loss: 5.50251653103679e-05, Test Loss: 4.894832117536301e-05\n",
            "Epoch 499/1000, Training Loss: 5.4834386277894176e-05, Test Loss: 4.8780274680029675e-05\n",
            "Epoch 500/1000, Training Loss: 5.4645779643265606e-05, Test Loss: 4.8614265426268034e-05\n",
            "Epoch 501/1000, Training Loss: 5.445930813227811e-05, Test Loss: 4.845025689945443e-05\n",
            "Epoch 502/1000, Training Loss: 5.4274935258464284e-05, Test Loss: 4.828821336402306e-05\n",
            "Epoch 503/1000, Training Loss: 5.409262530426968e-05, Test Loss: 4.812809984497325e-05\n",
            "Epoch 504/1000, Training Loss: 5.3912343302715704e-05, Test Loss: 4.796988210984823e-05\n",
            "Epoch 505/1000, Training Loss: 5.373405501953466e-05, Test Loss: 4.781352665117251e-05\n",
            "Epoch 506/1000, Training Loss: 5.35577269357688e-05, Test Loss: 4.765900066933936e-05\n",
            "Epoch 507/1000, Training Loss: 5.338332623081513e-05, Test Loss: 4.750627205593134e-05\n",
            "Epoch 508/1000, Training Loss: 5.321082076590567e-05, Test Loss: 4.735530937746595e-05\n",
            "Epoch 509/1000, Training Loss: 5.304017906801394e-05, Test Loss: 4.7206081859551854e-05\n",
            "Epoch 510/1000, Training Loss: 5.2871370314169075e-05, Test Loss: 4.705855937144547e-05\n",
            "Epoch 511/1000, Training Loss: 5.270436431617592e-05, Test Loss: 4.691271241100233e-05\n",
            "Epoch 512/1000, Training Loss: 5.253913150572335e-05, Test Loss: 4.67685120900022e-05\n",
            "Epoch 513/1000, Training Loss: 5.2375642919870024e-05, Test Loss: 4.66259301198485e-05\n",
            "Epoch 514/1000, Training Loss: 5.2213870186901855e-05, Test Loss: 4.64849387976256e-05\n",
            "Epoch 515/1000, Training Loss: 5.2053785512549264e-05, Test Loss: 4.634551099250652e-05\n",
            "Epoch 516/1000, Training Loss: 5.1895361666549035e-05, Test Loss: 4.620762013250157e-05\n",
            "Epoch 517/1000, Training Loss: 5.173857196955331e-05, Test Loss: 4.60712401915391e-05\n",
            "Epoch 518/1000, Training Loss: 5.158339028036298e-05, Test Loss: 4.593634567686952e-05\n",
            "Epoch 519/1000, Training Loss: 5.142979098348767e-05, Test Loss: 4.580291161678208e-05\n",
            "Epoch 520/1000, Training Loss: 5.1277748977015794e-05, Test Loss: 4.567091354863015e-05\n",
            "Epoch 521/1000, Training Loss: 5.112723966079216e-05, Test Loss: 4.554032750715326e-05\n",
            "Epoch 522/1000, Training Loss: 5.097823892488956e-05, Test Loss: 4.541113001308763e-05\n",
            "Epoch 523/1000, Training Loss: 5.083072313837058e-05, Test Loss: 4.528329806206252e-05\n",
            "Epoch 524/1000, Training Loss: 5.068466913833317e-05, Test Loss: 4.515680911377072e-05\n",
            "Epoch 525/1000, Training Loss: 5.0540054219224624e-05, Test Loss: 4.5031641081403556e-05\n",
            "Epoch 526/1000, Training Loss: 5.039685612242488e-05, Test Loss: 4.490777232135127e-05\n",
            "Epoch 527/1000, Training Loss: 5.0255053026089723e-05, Test Loss: 4.4785181623153954e-05\n",
            "Epoch 528/1000, Training Loss: 5.011462353524379e-05, Test Loss: 4.466384819970173e-05\n",
            "Epoch 529/1000, Training Loss: 4.997554667212268e-05, Test Loss: 4.454375167767558e-05\n",
            "Epoch 530/1000, Training Loss: 4.9837801866750826e-05, Test Loss: 4.442487208821981e-05\n",
            "Epoch 531/1000, Training Loss: 4.97013689477545e-05, Test Loss: 4.4307189857848106e-05\n",
            "Epoch 532/1000, Training Loss: 4.9566228133401546e-05, Test Loss: 4.419068579956728e-05\n",
            "Epoch 533/1000, Training Loss: 4.943236002285916e-05, Test Loss: 4.4075341104218595e-05\n",
            "Epoch 534/1000, Training Loss: 4.929974558767202e-05, Test Loss: 4.396113733203466e-05\n",
            "Epoch 535/1000, Training Loss: 4.916836616344333e-05, Test Loss: 4.38480564043936e-05\n",
            "Epoch 536/1000, Training Loss: 4.903820344172569e-05, Test Loss: 4.373608059578199e-05\n",
            "Epoch 537/1000, Training Loss: 4.8909239462104885e-05, Test Loss: 4.362519252594501e-05\n",
            "Epoch 538/1000, Training Loss: 4.8781456604482156e-05, Test Loss: 4.351537515223457e-05\n",
            "Epoch 539/1000, Training Loss: 4.86548375815421e-05, Test Loss: 4.340661176213515e-05\n",
            "Epoch 540/1000, Training Loss: 4.852936543140281e-05, Test Loss: 4.329888596597452e-05\n",
            "Epoch 541/1000, Training Loss: 4.840502351044872e-05, Test Loss: 4.319218168980967e-05\n",
            "Epoch 542/1000, Training Loss: 4.8281795486333306e-05, Test Loss: 4.308648316848129e-05\n",
            "Epoch 543/1000, Training Loss: 4.815966533115253e-05, Test Loss: 4.2981774938837534e-05\n",
            "Epoch 544/1000, Training Loss: 4.8038617314784925e-05, Test Loss: 4.2878041833119615e-05\n",
            "Epoch 545/1000, Training Loss: 4.791863599839197e-05, Test Loss: 4.277526897250638e-05\n",
            "Epoch 546/1000, Training Loss: 4.779970622807276e-05, Test Loss: 4.267344176081257e-05\n",
            "Epoch 547/1000, Training Loss: 4.7681813128674676e-05, Test Loss: 4.257254587833487e-05\n",
            "Epoch 548/1000, Training Loss: 4.7564942097750744e-05, Test Loss: 4.2472567275851486e-05\n",
            "Epoch 549/1000, Training Loss: 4.744907879966192e-05, Test Loss: 4.237349216875594e-05\n",
            "Epoch 550/1000, Training Loss: 4.733420915982277e-05, Test Loss: 4.2275307031336835e-05\n",
            "Epoch 551/1000, Training Loss: 4.722031935908144e-05, Test Loss: 4.2177998591189576e-05\n",
            "Epoch 552/1000, Training Loss: 4.7107395828235194e-05, Test Loss: 4.208155382376118e-05\n",
            "Epoch 553/1000, Training Loss: 4.6995425242678106e-05, Test Loss: 4.1985959947024985e-05\n",
            "Epoch 554/1000, Training Loss: 4.6884394517174944e-05, Test Loss: 4.189120441628012e-05\n",
            "Epoch 555/1000, Training Loss: 4.6774290800756146e-05, Test Loss: 4.179727491907253e-05\n",
            "Epoch 556/1000, Training Loss: 4.666510147173945e-05, Test Loss: 4.170415937023736e-05\n",
            "Epoch 557/1000, Training Loss: 4.6556814132863894e-05, Test Loss: 4.161184590705586e-05\n",
            "Epoch 558/1000, Training Loss: 4.6449416606544105e-05, Test Loss: 4.152032288452898e-05\n",
            "Epoch 559/1000, Training Loss: 4.6342896930230274e-05, Test Loss: 4.142957887075478e-05\n",
            "Epoch 560/1000, Training Loss: 4.623724335188217e-05, Test Loss: 4.133960264242188e-05\n",
            "Epoch 561/1000, Training Loss: 4.61324443255482e-05, Test Loss: 4.1250383180402914e-05\n",
            "Epoch 562/1000, Training Loss: 4.602848850704693e-05, Test Loss: 4.116190966545163e-05\n",
            "Epoch 563/1000, Training Loss: 4.592536474975085e-05, Test Loss: 4.107417147399947e-05\n",
            "Epoch 564/1000, Training Loss: 4.5823062100469445e-05, Test Loss: 4.09871581740536e-05\n",
            "Epoch 565/1000, Training Loss: 4.572156979542556e-05, Test Loss: 4.090085952118187e-05\n",
            "Epoch 566/1000, Training Loss: 4.562087725632806e-05, Test Loss: 4.0815265454599174e-05\n",
            "Epoch 567/1000, Training Loss: 4.5520974086534406e-05, Test Loss: 4.0730366093338314e-05\n",
            "Epoch 568/1000, Training Loss: 4.542185006730385e-05, Test Loss: 4.064615173251131e-05\n",
            "Epoch 569/1000, Training Loss: 4.532349515413512e-05, Test Loss: 4.0562612839657706e-05\n",
            "Epoch 570/1000, Training Loss: 4.522589947319005e-05, Test Loss: 4.047974005117375e-05\n",
            "Epoch 571/1000, Training Loss: 4.512905331780112e-05, Test Loss: 4.039752416882614e-05\n",
            "Epoch 572/1000, Training Loss: 4.50329471450556e-05, Test Loss: 4.0315956156343496e-05\n",
            "Epoch 573/1000, Training Loss: 4.4937571572461893e-05, Test Loss: 4.0235027136086947e-05\n",
            "Epoch 574/1000, Training Loss: 4.48429173746917e-05, Test Loss: 4.0154728385798524e-05\n",
            "Epoch 575/1000, Training Loss: 4.474897548039564e-05, Test Loss: 4.0075051335417374e-05\n",
            "Epoch 576/1000, Training Loss: 4.4655736969091044e-05, Test Loss: 3.9995987563974546e-05\n",
            "Epoch 577/1000, Training Loss: 4.456319306812471e-05, Test Loss: 3.9917528796556284e-05\n",
            "Epoch 578/1000, Training Loss: 4.4471335149699045e-05, Test Loss: 3.983966690133534e-05\n",
            "Epoch 579/1000, Training Loss: 4.4380154727971435e-05, Test Loss: 3.976239388666865e-05\n",
            "Epoch 580/1000, Training Loss: 4.428964345621529e-05, Test Loss: 3.968570189826179e-05\n",
            "Epoch 581/1000, Training Loss: 4.41997931240481e-05, Test Loss: 3.96095832163983e-05\n",
            "Epoch 582/1000, Training Loss: 4.411059565472073e-05, Test Loss: 3.953403025322974e-05\n",
            "Epoch 583/1000, Training Loss: 4.402204310246844e-05, Test Loss: 3.9459035550125814e-05\n",
            "Epoch 584/1000, Training Loss: 4.393412764992183e-05, Test Loss: 3.938459177508632e-05\n",
            "Epoch 585/1000, Training Loss: 4.384684160557381e-05, Test Loss: 3.93106917202077e-05\n",
            "Epoch 586/1000, Training Loss: 4.3760177401307445e-05, Test Loss: 3.923732829920922e-05\n",
            "Epoch 587/1000, Training Loss: 4.3674127589974944e-05, Test Loss: 3.916449454501237e-05\n",
            "Epoch 588/1000, Training Loss: 4.358868484303429e-05, Test Loss: 3.909218360737605e-05\n",
            "Epoch 589/1000, Training Loss: 4.350384194823607e-05, Test Loss: 3.902038875058111e-05\n",
            "Epoch 590/1000, Training Loss: 4.3419591807363584e-05, Test Loss: 3.8949103351169815e-05\n",
            "Epoch 591/1000, Training Loss: 4.3335927434021166e-05, Test Loss: 3.887832089573187e-05\n",
            "Epoch 592/1000, Training Loss: 4.3252841951473545e-05, Test Loss: 3.880803497874128e-05\n",
            "Epoch 593/1000, Training Loss: 4.31703285905316e-05, Test Loss: 3.873823930044127e-05\n",
            "Epoch 594/1000, Training Loss: 4.30883806874868e-05, Test Loss: 3.866892766477461e-05\n",
            "Epoch 595/1000, Training Loss: 4.300699168208774e-05, Test Loss: 3.860009397735974e-05\n",
            "Epoch 596/1000, Training Loss: 4.292615511556408e-05, Test Loss: 3.853173224351039e-05\n",
            "Epoch 597/1000, Training Loss: 4.284586462869422e-05, Test Loss: 3.84638365663013e-05\n",
            "Epoch 598/1000, Training Loss: 4.276611395991219e-05, Test Loss: 3.8396401144673403e-05\n",
            "Epoch 599/1000, Training Loss: 4.2686896943459546e-05, Test Loss: 3.832942027158026e-05\n",
            "Epoch 600/1000, Training Loss: 4.260820750757456e-05, Test Loss: 3.826288833217731e-05\n",
            "Epoch 601/1000, Training Loss: 4.2530039672722025e-05, Test Loss: 3.819679980204644e-05\n",
            "Epoch 602/1000, Training Loss: 4.2452387549863694e-05, Test Loss: 3.813114924546446e-05\n",
            "Epoch 603/1000, Training Loss: 4.237524533876227e-05, Test Loss: 3.806593131370422e-05\n",
            "Epoch 604/1000, Training Loss: 4.2298607326323144e-05, Test Loss: 3.8001140743372534e-05\n",
            "Epoch 605/1000, Training Loss: 4.22224678849756e-05, Test Loss: 3.7936772354789775e-05\n",
            "Epoch 606/1000, Training Loss: 4.214682147108542e-05, Test Loss: 3.787282105039794e-05\n",
            "Epoch 607/1000, Training Loss: 4.2071662623399676e-05, Test Loss: 3.780928181320209e-05\n",
            "Epoch 608/1000, Training Loss: 4.1996985961530115e-05, Test Loss: 3.7746149705253274e-05\n",
            "Epoch 609/1000, Training Loss: 4.192278618446626e-05, Test Loss: 3.768341986615632e-05\n",
            "Epoch 610/1000, Training Loss: 4.184905806911932e-05, Test Loss: 3.7621087511610715e-05\n",
            "Epoch 611/1000, Training Loss: 4.1775796468898205e-05, Test Loss: 3.755914793198491e-05\n",
            "Epoch 612/1000, Training Loss: 4.170299631231661e-05, Test Loss: 3.749759649092017e-05\n",
            "Epoch 613/1000, Training Loss: 4.16306526016293e-05, Test Loss: 3.743642862396418e-05\n",
            "Epoch 614/1000, Training Loss: 4.1558760411494545e-05, Test Loss: 3.7375639837230284e-05\n",
            "Epoch 615/1000, Training Loss: 4.148731488767068e-05, Test Loss: 3.731522570609074e-05\n",
            "Epoch 616/1000, Training Loss: 4.141631124573275e-05, Test Loss: 3.72551818738923e-05\n",
            "Epoch 617/1000, Training Loss: 4.134574476982325e-05, Test Loss: 3.7195504050703125e-05\n",
            "Epoch 618/1000, Training Loss: 4.127561081142326e-05, Test Loss: 3.713618801208248e-05\n",
            "Epoch 619/1000, Training Loss: 4.1205904788154124e-05, Test Loss: 3.7077229597879816e-05\n",
            "Epoch 620/1000, Training Loss: 4.113662218259942e-05, Test Loss: 3.701862471105361e-05\n",
            "Epoch 621/1000, Training Loss: 4.106775854115561e-05, Test Loss: 3.6960369316520425e-05\n",
            "Epoch 622/1000, Training Loss: 4.099930947290449e-05, Test Loss: 3.690245944002615e-05\n",
            "Epoch 623/1000, Training Loss: 4.093127064851061e-05, Test Loss: 3.68448911670384e-05\n",
            "Epoch 624/1000, Training Loss: 4.0863637799137606e-05, Test Loss: 3.678766064166606e-05\n",
            "Epoch 625/1000, Training Loss: 4.0796406715391035e-05, Test Loss: 3.673076406559519e-05\n",
            "Epoch 626/1000, Training Loss: 4.072957324628316e-05, Test Loss: 3.6674197697055315e-05\n",
            "Epoch 627/1000, Training Loss: 4.066313329821519e-05, Test Loss: 3.66179578497984e-05\n",
            "Epoch 628/1000, Training Loss: 4.0597082833983824e-05, Test Loss: 3.65620408921064e-05\n",
            "Epoch 629/1000, Training Loss: 4.0531417871807275e-05, Test Loss: 3.6506443245812815e-05\n",
            "Epoch 630/1000, Training Loss: 4.046613448436857e-05, Test Loss: 3.64511613853474e-05\n",
            "Epoch 631/1000, Training Loss: 4.040122879788369e-05, Test Loss: 3.6396191836801444e-05\n",
            "Epoch 632/1000, Training Loss: 4.033669699118303e-05, Test Loss: 3.634153117700976e-05\n",
            "Epoch 633/1000, Training Loss: 4.027253529481529e-05, Test Loss: 3.628717603265248e-05\n",
            "Epoch 634/1000, Training Loss: 4.02087399901681e-05, Test Loss: 3.62331230793761e-05\n",
            "Epoch 635/1000, Training Loss: 4.0145307408604956e-05, Test Loss: 3.6179369040929096e-05\n",
            "Epoch 636/1000, Training Loss: 4.0082233930622804e-05, Test Loss: 3.612591068831887e-05\n",
            "Epoch 637/1000, Training Loss: 4.001951598502373e-05, Test Loss: 3.607274483898456e-05\n",
            "Epoch 638/1000, Training Loss: 3.99571500481042e-05, Test Loss: 3.601986835598534e-05\n",
            "Epoch 639/1000, Training Loss: 3.9895132642860274e-05, Test Loss: 3.596727814720471e-05\n",
            "Epoch 640/1000, Training Loss: 3.983346033820948e-05, Test Loss: 3.591497116457351e-05\n",
            "Epoch 641/1000, Training Loss: 3.977212974822657e-05, Test Loss: 3.5862944403306016e-05\n",
            "Epoch 642/1000, Training Loss: 3.9711137531395594e-05, Test Loss: 3.5811194901152136e-05\n",
            "Epoch 643/1000, Training Loss: 3.9650480389876785e-05, Test Loss: 3.575971973766376e-05\n",
            "Epoch 644/1000, Training Loss: 3.9590155068787e-05, Test Loss: 3.5708516033478004e-05\n",
            "Epoch 645/1000, Training Loss: 3.9530158355495366e-05, Test Loss: 3.5657580949611945e-05\n",
            "Epoch 646/1000, Training Loss: 3.94704870789319e-05, Test Loss: 3.5606911686772446e-05\n",
            "Epoch 647/1000, Training Loss: 3.941113810891021e-05, Test Loss: 3.555650548468061e-05\n",
            "Epoch 648/1000, Training Loss: 3.935210835546266e-05, Test Loss: 3.550635962140744e-05\n",
            "Epoch 649/1000, Training Loss: 3.929339476818938e-05, Test Loss: 3.545647141272413e-05\n",
            "Epoch 650/1000, Training Loss: 3.923499433561904e-05, Test Loss: 3.540683821146438e-05\n",
            "Epoch 651/1000, Training Loss: 3.917690408458286e-05, Test Loss: 3.535745740690092e-05\n",
            "Epoch 652/1000, Training Loss: 3.9119121079600525e-05, Test Loss: 3.530832642413113e-05\n",
            "Epoch 653/1000, Training Loss: 3.906164242227711e-05, Test Loss: 3.5259442723477943e-05\n",
            "Epoch 654/1000, Training Loss: 3.900446525071213e-05, Test Loss: 3.5210803799899216e-05\n",
            "Epoch 655/1000, Training Loss: 3.894758673892036e-05, Test Loss: 3.516240718241161e-05\n",
            "Epoch 656/1000, Training Loss: 3.889100409626269e-05, Test Loss: 3.511425043352337e-05\n",
            "Epoch 657/1000, Training Loss: 3.8834714566888544e-05, Test Loss: 3.506633114867871e-05\n",
            "Epoch 658/1000, Training Loss: 3.877871542918908e-05, Test Loss: 3.50186469557141e-05\n",
            "Epoch 659/1000, Training Loss: 3.8723003995260044e-05, Test Loss: 3.497119551432347e-05\n",
            "Epoch 660/1000, Training Loss: 3.86675776103754e-05, Test Loss: 3.492397451553432e-05\n",
            "Epoch 661/1000, Training Loss: 3.8612433652469717e-05, Test Loss: 3.487698168119258e-05\n",
            "Epoch 662/1000, Training Loss: 3.855756953163259e-05, Test Loss: 3.4830214763462016e-05\n",
            "Epoch 663/1000, Training Loss: 3.850298268961004e-05, Test Loss: 3.478367154432477e-05\n",
            "Epoch 664/1000, Training Loss: 3.8448670599316415e-05, Test Loss: 3.473734983510114e-05\n",
            "Epoch 665/1000, Training Loss: 3.8394630764355066e-05, Test Loss: 3.469124747596935e-05\n",
            "Epoch 666/1000, Training Loss: 3.834086071854911e-05, Test Loss: 3.464536233550295e-05\n",
            "Epoch 667/1000, Training Loss: 3.8287358025478325e-05, Test Loss: 3.45996923102097e-05\n",
            "Epoch 668/1000, Training Loss: 3.8234120278027305e-05, Test Loss: 3.455423532408291e-05\n",
            "Epoch 669/1000, Training Loss: 3.818114509794081e-05, Test Loss: 3.450898932816078e-05\n",
            "Epoch 670/1000, Training Loss: 3.8128430135385926e-05, Test Loss: 3.4463952300093666e-05\n",
            "Epoch 671/1000, Training Loss: 3.807597306852542e-05, Test Loss: 3.4419122243720365e-05\n",
            "Epoch 672/1000, Training Loss: 3.802377160309522e-05, Test Loss: 3.4374497188649646e-05\n",
            "Epoch 673/1000, Training Loss: 3.797182347199328e-05, Test Loss: 3.433007518985266e-05\n",
            "Epoch 674/1000, Training Loss: 3.792012643487224e-05, Test Loss: 3.428585432726138e-05\n",
            "Epoch 675/1000, Training Loss: 3.786867827774182e-05, Test Loss: 3.424183270537445e-05\n",
            "Epoch 676/1000, Training Loss: 3.781747681257938e-05, Test Loss: 3.419800845287161e-05\n",
            "Epoch 677/1000, Training Loss: 3.776651987694335e-05, Test Loss: 3.415437972223336e-05\n",
            "Epoch 678/1000, Training Loss: 3.7715805333598007e-05, Test Loss: 3.41109446893681e-05\n",
            "Epoch 679/1000, Training Loss: 3.7665331070142885e-05, Test Loss: 3.4067701553247954e-05\n",
            "Epoch 680/1000, Training Loss: 3.761509499864773e-05, Test Loss: 3.402464853554882e-05\n",
            "Epoch 681/1000, Training Loss: 3.7565095055297614e-05, Test Loss: 3.3981783880299096e-05\n",
            "Epoch 682/1000, Training Loss: 3.7515329200040264e-05, Test Loss: 3.39391058535321e-05\n",
            "Epoch 683/1000, Training Loss: 3.7465795416241825e-05, Test Loss: 3.38966127429481e-05\n",
            "Epoch 684/1000, Training Loss: 3.741649171034802e-05, Test Loss: 3.3854302857579515e-05\n",
            "Epoch 685/1000, Training Loss: 3.736741611155242e-05, Test Loss: 3.3812174527465126e-05\n",
            "Epoch 686/1000, Training Loss: 3.7318566671468814e-05, Test Loss: 3.3770226103327145e-05\n",
            "Epoch 687/1000, Training Loss: 3.726994146381029e-05, Test Loss: 3.372845595625588e-05\n",
            "Epoch 688/1000, Training Loss: 3.722153858407417e-05, Test Loss: 3.368686247740094e-05\n",
            "Epoch 689/1000, Training Loss: 3.717335614923122e-05, Test Loss: 3.364544407766457e-05\n",
            "Epoch 690/1000, Training Loss: 3.712539229742128e-05, Test Loss: 3.3604199187404315e-05\n",
            "Epoch 691/1000, Training Loss: 3.707764518765466e-05, Test Loss: 3.3563126256139126e-05\n",
            "Epoch 692/1000, Training Loss: 3.703011299951692e-05, Test Loss: 3.3522223752260455e-05\n",
            "Epoch 693/1000, Training Loss: 3.698279393288016e-05, Test Loss: 3.3481490162748986e-05\n",
            "Epoch 694/1000, Training Loss: 3.693568620761879e-05, Test Loss: 3.344092399289626e-05\n",
            "Epoch 695/1000, Training Loss: 3.68887880633307e-05, Test Loss: 3.340052376603261e-05\n",
            "Epoch 696/1000, Training Loss: 3.6842097759061714e-05, Test Loss: 3.33602880232565e-05\n",
            "Epoch 697/1000, Training Loss: 3.679561357303565e-05, Test Loss: 3.3320215323171585e-05\n",
            "Epoch 698/1000, Training Loss: 3.674933380239103e-05, Test Loss: 3.328030424162878e-05\n",
            "Epoch 699/1000, Training Loss: 3.6703256762916705e-05, Test Loss: 3.32405533714689e-05\n",
            "Epoch 700/1000, Training Loss: 3.6657380788798474e-05, Test Loss: 3.320096132227555e-05\n",
            "Epoch 701/1000, Training Loss: 3.661170423236566e-05, Test Loss: 3.316152672012651e-05\n",
            "Epoch 702/1000, Training Loss: 3.656622546384282e-05, Test Loss: 3.3122248207353744e-05\n",
            "Epoch 703/1000, Training Loss: 3.652094287110716e-05, Test Loss: 3.308312444230591e-05\n",
            "Epoch 704/1000, Training Loss: 3.6475854859446766e-05, Test Loss: 3.304415409911386e-05\n",
            "Epoch 705/1000, Training Loss: 3.643095985132687e-05, Test Loss: 3.3005335867463094e-05\n",
            "Epoch 706/1000, Training Loss: 3.638625628615637e-05, Test Loss: 3.2966668452366676e-05\n",
            "Epoch 707/1000, Training Loss: 3.6341742620061714e-05, Test Loss: 3.292815057394544e-05\n",
            "Epoch 708/1000, Training Loss: 3.6297417325660535e-05, Test Loss: 3.288978096720953e-05\n",
            "Epoch 709/1000, Training Loss: 3.6253278891841316e-05, Test Loss: 3.285155838184303e-05\n",
            "Epoch 710/1000, Training Loss: 3.620932582354765e-05, Test Loss: 3.281348158199677e-05\n",
            "Epoch 711/1000, Training Loss: 3.6165556641564016e-05, Test Loss: 3.27755493460796e-05\n",
            "Epoch 712/1000, Training Loss: 3.6121969882305174e-05, Test Loss: 3.273776046655416e-05\n",
            "Epoch 713/1000, Training Loss: 3.6078564097610287e-05, Test Loss: 3.2700113749739346e-05\n",
            "Epoch 714/1000, Training Loss: 3.603533785453923e-05, Test Loss: 3.266260801561257e-05\n",
            "Epoch 715/1000, Training Loss: 3.599228973517275e-05, Test Loss: 3.262524209761724e-05\n",
            "Epoch 716/1000, Training Loss: 3.594941833641578e-05, Test Loss: 3.25880148424723e-05\n",
            "Epoch 717/1000, Training Loss: 3.590672226980273e-05, Test Loss: 3.255092510998463e-05\n",
            "Epoch 718/1000, Training Loss: 3.586420016130887e-05, Test Loss: 3.251397177286885e-05\n",
            "Epoch 719/1000, Training Loss: 3.582185065116121e-05, Test Loss: 3.247715371656249e-05\n",
            "Epoch 720/1000, Training Loss: 3.577967239365401e-05, Test Loss: 3.24404698390513e-05\n",
            "Epoch 721/1000, Training Loss: 3.57376640569676e-05, Test Loss: 3.2403919050693724e-05\n",
            "Epoch 722/1000, Training Loss: 3.569582432298979e-05, Test Loss: 3.236750027404941e-05\n",
            "Epoch 723/1000, Training Loss: 3.5654151887139694e-05, Test Loss: 3.2331212443710504e-05\n",
            "Epoch 724/1000, Training Loss: 3.561264545819369e-05, Test Loss: 3.229505450613441e-05\n",
            "Epoch 725/1000, Training Loss: 3.557130375811569e-05, Test Loss: 3.225902541948178e-05\n",
            "Epoch 726/1000, Training Loss: 3.5530125521889636e-05, Test Loss: 3.222312415345527e-05\n",
            "Epoch 727/1000, Training Loss: 3.548910949735363e-05, Test Loss: 3.2187349689141855e-05\n",
            "Epoch 728/1000, Training Loss: 3.544825444503637e-05, Test Loss: 3.2151701018855714e-05\n",
            "Epoch 729/1000, Training Loss: 3.5407559137999554e-05, Test Loss: 3.2116177145988554e-05\n",
            "Epoch 730/1000, Training Loss: 3.5367022361677236e-05, Test Loss: 3.208077708485468e-05\n",
            "Epoch 731/1000, Training Loss: 3.532664291372248e-05, Test Loss: 3.204549986054717e-05\n",
            "Epoch 732/1000, Training Loss: 3.5286419603853685e-05, Test Loss: 3.2010344508789647e-05\n",
            "Epoch 733/1000, Training Loss: 3.524635125370359e-05, Test Loss: 3.197531007579287e-05\n",
            "Epoch 734/1000, Training Loss: 3.520643669667183e-05, Test Loss: 3.194039561811558e-05\n",
            "Epoch 735/1000, Training Loss: 3.5166674777778496e-05, Test Loss: 3.19056002025244e-05\n",
            "Epoch 736/1000, Training Loss: 3.5127064353520625e-05, Test Loss: 3.1870922905857454e-05\n",
            "Epoch 737/1000, Training Loss: 3.508760429172972e-05, Test Loss: 3.1836362814889384e-05\n",
            "Epoch 738/1000, Training Loss: 3.504829347143272e-05, Test Loss: 3.180191902620005e-05\n",
            "Epoch 739/1000, Training Loss: 3.500913078271455e-05, Test Loss: 3.176759064604391e-05\n",
            "Epoch 740/1000, Training Loss: 3.497011512658261e-05, Test Loss: 3.173337679022184e-05\n",
            "Epoch 741/1000, Training Loss: 3.4931245414833314e-05, Test Loss: 3.169927658395537e-05\n",
            "Epoch 742/1000, Training Loss: 3.48925205699213e-05, Test Loss: 3.1665289161762916e-05\n",
            "Epoch 743/1000, Training Loss: 3.485393952482914e-05, Test Loss: 3.163141366733664e-05\n",
            "Epoch 744/1000, Training Loss: 3.481550122294002e-05, Test Loss: 3.1597649253422566e-05\n",
            "Epoch 745/1000, Training Loss: 3.477720461791296e-05, Test Loss: 3.156399508170242e-05\n",
            "Epoch 746/1000, Training Loss: 3.473904867355725e-05, Test Loss: 3.153045032267714e-05\n",
            "Epoch 747/1000, Training Loss: 3.4701032363711976e-05, Test Loss: 3.149701415555235e-05\n",
            "Epoch 748/1000, Training Loss: 3.4663154672125005e-05, Test Loss: 3.146368576812403e-05\n",
            "Epoch 749/1000, Training Loss: 3.4625414592334626e-05, Test Loss: 3.143046435666891e-05\n",
            "Epoch 750/1000, Training Loss: 3.45878111275534e-05, Test Loss: 3.139734912583508e-05\n",
            "Epoch 751/1000, Training Loss: 3.455034329055244e-05, Test Loss: 3.136433928853286e-05\n",
            "Epoch 752/1000, Training Loss: 3.451301010354801e-05, Test Loss: 3.133143406582977e-05\n",
            "Epoch 753/1000, Training Loss: 3.447581059809024e-05, Test Loss: 3.129863268684539e-05\n",
            "Epoch 754/1000, Training Loss: 3.4438743814952296e-05, Test Loss: 3.1265934388648526e-05\n",
            "Epoch 755/1000, Training Loss: 3.440180880402243e-05, Test Loss: 3.1233338416156064e-05\n",
            "Epoch 756/1000, Training Loss: 3.4365004624196995e-05, Test Loss: 3.120084402203336e-05\n",
            "Epoch 757/1000, Training Loss: 3.4328330343274044e-05, Test Loss: 3.116845046659534e-05\n",
            "Epoch 758/1000, Training Loss: 3.429178503784978e-05, Test Loss: 3.113615701770975e-05\n",
            "Epoch 759/1000, Training Loss: 3.4255367793216445e-05, Test Loss: 3.110396295070224e-05\n",
            "Epoch 760/1000, Training Loss: 3.4219077703261264e-05, Test Loss: 3.1071867548262485e-05\n",
            "Epoch 761/1000, Training Loss: 3.418291387036563e-05, Test Loss: 3.1039870100350973e-05\n",
            "Epoch 762/1000, Training Loss: 3.414687540530839e-05, Test Loss: 3.100796990410872e-05\n",
            "Epoch 763/1000, Training Loss: 3.411096142716749e-05, Test Loss: 3.09761662637665e-05\n",
            "Epoch 764/1000, Training Loss: 3.40751710632254e-05, Test Loss: 3.0944458490557065e-05\n",
            "Epoch 765/1000, Training Loss: 3.403950344887477e-05, Test Loss: 3.091284590262789e-05\n",
            "Epoch 766/1000, Training Loss: 3.4003957727524985e-05, Test Loss: 3.088132782495578e-05\n",
            "Epoch 767/1000, Training Loss: 3.396853305051123e-05, Test Loss: 3.0849903589261345e-05\n",
            "Epoch 768/1000, Training Loss: 3.393322857700395e-05, Test Loss: 3.0818572533926934e-05\n",
            "Epoch 769/1000, Training Loss: 3.38980434739197e-05, Test Loss: 3.0787334003913416e-05\n",
            "Epoch 770/1000, Training Loss: 3.386297691583214e-05, Test Loss: 3.075618735067869e-05\n",
            "Epoch 771/1000, Training Loss: 3.3828028084887845e-05, Test Loss: 3.072513193210075e-05\n",
            "Epoch 772/1000, Training Loss: 3.3793196170718755e-05, Test Loss: 3.069416711239676e-05\n",
            "Epoch 773/1000, Training Loss: 3.375848037035819e-05, Test Loss: 3.066329226204582e-05\n",
            "Epoch 774/1000, Training Loss: 3.37238798881581e-05, Test Loss: 3.063250675771337e-05\n",
            "Epoch 775/1000, Training Loss: 3.36893939357071e-05, Test Loss: 3.060180998217592e-05\n",
            "Epoch 776/1000, Training Loss: 3.365502173174827e-05, Test Loss: 3.057120132424604e-05\n",
            "Epoch 777/1000, Training Loss: 3.3620762502101986e-05, Test Loss: 3.054068017870258e-05\n",
            "Epoch 778/1000, Training Loss: 3.3586615479584e-05, Test Loss: 3.0510245946213835e-05\n",
            "Epoch 779/1000, Training Loss: 3.355257990393015e-05, Test Loss: 3.0479898033270144e-05\n",
            "Epoch 780/1000, Training Loss: 3.351865502171781e-05, Test Loss: 3.0449635852112543e-05\n",
            "Epoch 781/1000, Training Loss: 3.3484840086291533e-05, Test Loss: 3.0419458820664386e-05\n",
            "Epoch 782/1000, Training Loss: 3.345113435768761e-05, Test Loss: 3.0389366362463705e-05\n",
            "Epoch 783/1000, Training Loss: 3.3417537102561186e-05, Test Loss: 3.0359357906595096e-05\n",
            "Epoch 784/1000, Training Loss: 3.338404759411286e-05, Test Loss: 3.0329432887624916e-05\n",
            "Epoch 785/1000, Training Loss: 3.335066511201727e-05, Test Loss: 3.0299590745535394e-05\n",
            "Epoch 786/1000, Training Loss: 3.331738894235212e-05, Test Loss: 3.0269830925661235e-05\n",
            "Epoch 787/1000, Training Loss: 3.328421837752856e-05, Test Loss: 3.024015287862546e-05\n",
            "Epoch 788/1000, Training Loss: 3.325115271622252e-05, Test Loss: 3.0210556060277807e-05\n",
            "Epoch 789/1000, Training Loss: 3.321819126330535e-05, Test Loss: 3.0181039931632773e-05\n",
            "Epoch 790/1000, Training Loss: 3.318533332977858e-05, Test Loss: 3.015160395880933e-05\n",
            "Epoch 791/1000, Training Loss: 3.3152578232706295e-05, Test Loss: 3.0122247612971404e-05\n",
            "Epoch 792/1000, Training Loss: 3.3119925295149474e-05, Test Loss: 3.0092970370267364e-05\n",
            "Epoch 793/1000, Training Loss: 3.308737384610271e-05, Test Loss: 3.0063771711773724e-05\n",
            "Epoch 794/1000, Training Loss: 3.30549232204299e-05, Test Loss: 3.0034651123436998e-05\n",
            "Epoch 795/1000, Training Loss: 3.3022572758800555e-05, Test Loss: 3.00056080960171e-05\n",
            "Epoch 796/1000, Training Loss: 3.299032180762888e-05, Test Loss: 2.9976642125032195e-05\n",
            "Epoch 797/1000, Training Loss: 3.295816971901182e-05, Test Loss: 2.9947752710703476e-05\n",
            "Epoch 798/1000, Training Loss: 3.292611585066839e-05, Test Loss: 2.9918939357899917e-05\n",
            "Epoch 799/1000, Training Loss: 3.289415956588025e-05, Test Loss: 2.98902015760854e-05\n",
            "Epoch 800/1000, Training Loss: 3.286230023343282e-05, Test Loss: 2.9861538879268103e-05\n",
            "Epoch 801/1000, Training Loss: 3.2830537227556456e-05, Test Loss: 2.9832950785944186e-05\n",
            "Epoch 802/1000, Training Loss: 3.279886992786931e-05, Test Loss: 2.980443681905024e-05\n",
            "Epoch 803/1000, Training Loss: 3.276729771932115e-05, Test Loss: 2.9775996505911136e-05\n",
            "Epoch 804/1000, Training Loss: 3.273581999213597e-05, Test Loss: 2.9747629378189862e-05\n",
            "Epoch 805/1000, Training Loss: 3.270443614175763e-05, Test Loss: 2.9719334971838358e-05\n",
            "Epoch 806/1000, Training Loss: 3.267314556879444e-05, Test Loss: 2.969111282704909e-05\n",
            "Epoch 807/1000, Training Loss: 3.264194767896608e-05, Test Loss: 2.966296248820819e-05\n",
            "Epoch 808/1000, Training Loss: 3.26108418830497e-05, Test Loss: 2.963488350384534e-05\n",
            "Epoch 809/1000, Training Loss: 3.2579827596827183e-05, Test Loss: 2.9606875426590523e-05\n",
            "Epoch 810/1000, Training Loss: 3.254890424103401e-05, Test Loss: 2.957893781312595e-05\n",
            "Epoch 811/1000, Training Loss: 3.251807124130595e-05, Test Loss: 2.9551070224139585e-05\n",
            "Epoch 812/1000, Training Loss: 3.2487328028130824e-05, Test Loss: 2.9523272224283488e-05\n",
            "Epoch 813/1000, Training Loss: 3.245667403679705e-05, Test Loss: 2.9495543382127e-05\n",
            "Epoch 814/1000, Training Loss: 3.242610870734349e-05, Test Loss: 2.9467883270113253e-05\n",
            "Epoch 815/1000, Training Loss: 3.239563148451212e-05, Test Loss: 2.944029146451805e-05\n",
            "Epoch 816/1000, Training Loss: 3.2365241817699056e-05, Test Loss: 2.9412767545405383e-05\n",
            "Epoch 817/1000, Training Loss: 3.23349391609071e-05, Test Loss: 2.9385311096586193e-05\n",
            "Epoch 818/1000, Training Loss: 3.230472297269759e-05, Test Loss: 2.9357921705576558e-05\n",
            "Epoch 819/1000, Training Loss: 3.2274592716145394e-05, Test Loss: 2.9330598963557367e-05\n",
            "Epoch 820/1000, Training Loss: 3.22445478587919e-05, Test Loss: 2.9303342465334378e-05\n",
            "Epoch 821/1000, Training Loss: 3.2214587872600555e-05, Test Loss: 2.9276151809297404e-05\n",
            "Epoch 822/1000, Training Loss: 3.218471223391079e-05, Test Loss: 2.9249026597381636e-05\n",
            "Epoch 823/1000, Training Loss: 3.2154920423394664e-05, Test Loss: 2.9221966435029023e-05\n",
            "Epoch 824/1000, Training Loss: 3.2125211926013015e-05, Test Loss: 2.9194970931149552e-05\n",
            "Epoch 825/1000, Training Loss: 3.209558623097168e-05, Test Loss: 2.9168039698083416e-05\n",
            "Epoch 826/1000, Training Loss: 3.206604283167886e-05, Test Loss: 2.9141172351564584e-05\n",
            "Epoch 827/1000, Training Loss: 3.203658122570327e-05, Test Loss: 2.911436851068274e-05\n",
            "Epoch 828/1000, Training Loss: 3.200720091473253e-05, Test Loss: 2.908762779784816e-05\n",
            "Epoch 829/1000, Training Loss: 3.1977901404531775e-05, Test Loss: 2.9060949838755672e-05\n",
            "Epoch 830/1000, Training Loss: 3.194868220490221e-05, Test Loss: 2.9034334262348252e-05\n",
            "Epoch 831/1000, Training Loss: 3.1919542829641504e-05, Test Loss: 2.900778070078281e-05\n",
            "Epoch 832/1000, Training Loss: 3.189048279650438e-05, Test Loss: 2.8981288789395456e-05\n",
            "Epoch 833/1000, Training Loss: 3.186150162716274e-05, Test Loss: 2.895485816666886e-05\n",
            "Epoch 834/1000, Training Loss: 3.18325988471669e-05, Test Loss: 2.8928488474195776e-05\n",
            "Epoch 835/1000, Training Loss: 3.180377398590715e-05, Test Loss: 2.890217935664825e-05\n",
            "Epoch 836/1000, Training Loss: 3.177502657657629e-05, Test Loss: 2.8875930461743868e-05\n",
            "Epoch 837/1000, Training Loss: 3.1746356156131894e-05, Test Loss: 2.8849741440213443e-05\n",
            "Epoch 838/1000, Training Loss: 3.1717762265258924e-05, Test Loss: 2.8823611945769396e-05\n",
            "Epoch 839/1000, Training Loss: 3.168924444833339e-05, Test Loss: 2.8797541635073712e-05\n",
            "Epoch 840/1000, Training Loss: 3.166080225338594e-05, Test Loss: 2.8771530167706638e-05\n",
            "Epoch 841/1000, Training Loss: 3.163243523206657e-05, Test Loss: 2.8745577206136752e-05\n",
            "Epoch 842/1000, Training Loss: 3.1604142939609175e-05, Test Loss: 2.871968241569013e-05\n",
            "Epoch 843/1000, Training Loss: 3.1575924934796026e-05, Test Loss: 2.869384546452e-05\n",
            "Epoch 844/1000, Training Loss: 3.154778077992373e-05, Test Loss: 2.8668066023577505e-05\n",
            "Epoch 845/1000, Training Loss: 3.1519710040768966e-05, Test Loss: 2.8642343766582052e-05\n",
            "Epoch 846/1000, Training Loss: 3.149171228655447e-05, Test Loss: 2.8616678369992787e-05\n",
            "Epoch 847/1000, Training Loss: 3.146378708991671e-05, Test Loss: 2.8591069512979805e-05\n",
            "Epoch 848/1000, Training Loss: 3.143593402687138e-05, Test Loss: 2.856551687739703e-05\n",
            "Epoch 849/1000, Training Loss: 3.140815267678127e-05, Test Loss: 2.8540020147751673e-05\n",
            "Epoch 850/1000, Training Loss: 3.138044262232547e-05, Test Loss: 2.8514579011179546e-05\n",
            "Epoch 851/1000, Training Loss: 3.135280344946544e-05, Test Loss: 2.8489193157416366e-05\n",
            "Epoch 852/1000, Training Loss: 3.1325234747414455e-05, Test Loss: 2.8463862278771458e-05\n",
            "Epoch 853/1000, Training Loss: 3.129773610860615e-05, Test Loss: 2.843858607010039e-05\n",
            "Epoch 854/1000, Training Loss: 3.127030712866476e-05, Test Loss: 2.8413364228780956e-05\n",
            "Epoch 855/1000, Training Loss: 3.124294740637344e-05, Test Loss: 2.8388196454684707e-05\n",
            "Epoch 856/1000, Training Loss: 3.121565654364446e-05, Test Loss: 2.83630824501527e-05\n",
            "Epoch 857/1000, Training Loss: 3.118843414548975e-05, Test Loss: 2.833802191996944e-05\n",
            "Epoch 858/1000, Training Loss: 3.11612798199911e-05, Test Loss: 2.8313014571339352e-05\n",
            "Epoch 859/1000, Training Loss: 3.1134193178271526e-05, Test Loss: 2.8288060113861728e-05\n",
            "Epoch 860/1000, Training Loss: 3.1107173834466284e-05, Test Loss: 2.826315825950464e-05\n",
            "Epoch 861/1000, Training Loss: 3.108022140569388e-05, Test Loss: 2.8238308722583654e-05\n",
            "Epoch 862/1000, Training Loss: 3.105333551202883e-05, Test Loss: 2.821351121973537e-05\n",
            "Epoch 863/1000, Training Loss: 3.102651577647353e-05, Test Loss: 2.8188765469895957e-05\n",
            "Epoch 864/1000, Training Loss: 3.099976182493007e-05, Test Loss: 2.8164071194277033e-05\n",
            "Epoch 865/1000, Training Loss: 3.0973073286174394e-05, Test Loss: 2.8139428116342815e-05\n",
            "Epoch 866/1000, Training Loss: 3.09464497918278e-05, Test Loss: 2.8114835961787656e-05\n",
            "Epoch 867/1000, Training Loss: 3.091989097633154e-05, Test Loss: 2.8090294458513552e-05\n",
            "Epoch 868/1000, Training Loss: 3.0893396476919706e-05, Test Loss: 2.806580333660758e-05\n",
            "Epoch 869/1000, Training Loss: 3.08669659335941e-05, Test Loss: 2.8041362328321193e-05\n",
            "Epoch 870/1000, Training Loss: 3.0840598989097454e-05, Test Loss: 2.8016971168047054e-05\n",
            "Epoch 871/1000, Training Loss: 3.081429528888891e-05, Test Loss: 2.7992629592300056e-05\n",
            "Epoch 872/1000, Training Loss: 3.0788054481118283e-05, Test Loss: 2.7968337339693374e-05\n",
            "Epoch 873/1000, Training Loss: 3.076187621660139e-05, Test Loss: 2.7944094150919566e-05\n",
            "Epoch 874/1000, Training Loss: 3.0735760148795176e-05, Test Loss: 2.7919899768729248e-05\n",
            "Epoch 875/1000, Training Loss: 3.070970593377331e-05, Test Loss: 2.7895753937910722e-05\n",
            "Epoch 876/1000, Training Loss: 3.06837132302024e-05, Test Loss: 2.7871656405270333e-05\n",
            "Epoch 877/1000, Training Loss: 3.0657781699318246e-05, Test Loss: 2.7847606919612524e-05\n",
            "Epoch 878/1000, Training Loss: 3.063191100490209e-05, Test Loss: 2.782360523171939e-05\n",
            "Epoch 879/1000, Training Loss: 3.060610081325689e-05, Test Loss: 2.7799651094331593e-05\n",
            "Epoch 880/1000, Training Loss: 3.058035079318466e-05, Test Loss: 2.7775744262130113e-05\n",
            "Epoch 881/1000, Training Loss: 3.055466061596365e-05, Test Loss: 2.7751884491716084e-05\n",
            "Epoch 882/1000, Training Loss: 3.052902995532612e-05, Test Loss: 2.7728071541592623e-05\n",
            "Epoch 883/1000, Training Loss: 3.05034584874349e-05, Test Loss: 2.7704305172145754e-05\n",
            "Epoch 884/1000, Training Loss: 3.0477945890862516e-05, Test Loss: 2.7680585145627504e-05\n",
            "Epoch 885/1000, Training Loss: 3.0452491846568484e-05, Test Loss: 2.7656911226135384e-05\n",
            "Epoch 886/1000, Training Loss: 3.042709603787805e-05, Test Loss: 2.763328317959665e-05\n",
            "Epoch 887/1000, Training Loss: 3.0401758150460898e-05, Test Loss: 2.7609700773749494e-05\n",
            "Epoch 888/1000, Training Loss: 3.0376477872309535e-05, Test Loss: 2.7586163778126477e-05\n",
            "Epoch 889/1000, Training Loss: 3.035125489371893e-05, Test Loss: 2.7562671964035807e-05\n",
            "Epoch 890/1000, Training Loss: 3.032608890726558e-05, Test Loss: 2.7539225104544985e-05\n",
            "Epoch 891/1000, Training Loss: 3.0300979607786752e-05, Test Loss: 2.751582297446488e-05\n",
            "Epoch 892/1000, Training Loss: 3.0275926692360273e-05, Test Loss: 2.7492465350331145e-05\n",
            "Epoch 893/1000, Training Loss: 3.025092986028515e-05, Test Loss: 2.7469152010389008e-05\n",
            "Epoch 894/1000, Training Loss: 3.0225988813060277e-05, Test Loss: 2.7445882734576325e-05\n",
            "Epoch 895/1000, Training Loss: 3.0201103254366108e-05, Test Loss: 2.742265730450793e-05\n",
            "Epoch 896/1000, Training Loss: 3.0176272890045027e-05, Test Loss: 2.739947550345898e-05\n",
            "Epoch 897/1000, Training Loss: 3.015149742808142e-05, Test Loss: 2.7376337116350033e-05\n",
            "Epoch 898/1000, Training Loss: 3.01267765785834e-05, Test Loss: 2.7353241929730254e-05\n",
            "Epoch 899/1000, Training Loss: 3.010211005376308e-05, Test Loss: 2.7330189731762752e-05\n",
            "Epoch 900/1000, Training Loss: 3.0077497567919043e-05, Test Loss: 2.730718031220996e-05\n",
            "Epoch 901/1000, Training Loss: 3.0052938837417265e-05, Test Loss: 2.728421346241733e-05\n",
            "Epoch 902/1000, Training Loss: 3.0028433580673075e-05, Test Loss: 2.7261288975298628e-05\n",
            "Epoch 903/1000, Training Loss: 3.000398151813223e-05, Test Loss: 2.723840664532135e-05\n",
            "Epoch 904/1000, Training Loss: 2.9979582372254332e-05, Test Loss: 2.7215566268492556e-05\n",
            "Epoch 905/1000, Training Loss: 2.995523586749467e-05, Test Loss: 2.7192767642344367e-05\n",
            "Epoch 906/1000, Training Loss: 2.9930941730286468e-05, Test Loss: 2.7170010565918406e-05\n",
            "Epoch 907/1000, Training Loss: 2.9906699689023495e-05, Test Loss: 2.7147294839753147e-05\n",
            "Epoch 908/1000, Training Loss: 2.988250947404326e-05, Test Loss: 2.7124620265869608e-05\n",
            "Epoch 909/1000, Training Loss: 2.9858370817610184e-05, Test Loss: 2.7101986647757324e-05\n",
            "Epoch 910/1000, Training Loss: 2.98342834538981e-05, Test Loss: 2.70793937903607e-05\n",
            "Epoch 911/1000, Training Loss: 2.981024711897415e-05, Test Loss: 2.7056841500065355e-05\n",
            "Epoch 912/1000, Training Loss: 2.9786261550782523e-05, Test Loss: 2.7034329584684552e-05\n",
            "Epoch 913/1000, Training Loss: 2.9762326489127567e-05, Test Loss: 2.7011857853447694e-05\n",
            "Epoch 914/1000, Training Loss: 2.9738441675657936e-05, Test Loss: 2.6989426116984893e-05\n",
            "Epoch 915/1000, Training Loss: 2.9714606853851127e-05, Test Loss: 2.696703418731551e-05\n",
            "Epoch 916/1000, Training Loss: 2.9690821768996803e-05, Test Loss: 2.6944681877834434e-05\n",
            "Epoch 917/1000, Training Loss: 2.966708616818188e-05, Test Loss: 2.6922369003301233e-05\n",
            "Epoch 918/1000, Training Loss: 2.964339980027456e-05, Test Loss: 2.6900095379825047e-05\n",
            "Epoch 919/1000, Training Loss: 2.9619762415908905e-05, Test Loss: 2.687786082485348e-05\n",
            "Epoch 920/1000, Training Loss: 2.959617376747093e-05, Test Loss: 2.685566515716189e-05\n",
            "Epoch 921/1000, Training Loss: 2.957263360908184e-05, Test Loss: 2.6833508196838305e-05\n",
            "Epoch 922/1000, Training Loss: 2.9549141696584338e-05, Test Loss: 2.681138976527344e-05\n",
            "Epoch 923/1000, Training Loss: 2.9525697787527798e-05, Test Loss: 2.6789309685149067e-05\n",
            "Epoch 924/1000, Training Loss: 2.950230164115287e-05, Test Loss: 2.6767267780424916e-05\n",
            "Epoch 925/1000, Training Loss: 2.9478953018378306e-05, Test Loss: 2.6745263876327712e-05\n",
            "Epoch 926/1000, Training Loss: 2.9455651681785333e-05, Test Loss: 2.6723297799339887e-05\n",
            "Epoch 927/1000, Training Loss: 2.9432397395605242e-05, Test Loss: 2.670136937718861e-05\n",
            "Epoch 928/1000, Training Loss: 2.9409189925703612e-05, Test Loss: 2.6679478438832897e-05\n",
            "Epoch 929/1000, Training Loss: 2.9386029039567767e-05, Test Loss: 2.665762481445553e-05\n",
            "Epoch 930/1000, Training Loss: 2.936291450629227e-05, Test Loss: 2.6635808335448403e-05\n",
            "Epoch 931/1000, Training Loss: 2.9339846096565702e-05, Test Loss: 2.6614028834403763e-05\n",
            "Epoch 932/1000, Training Loss: 2.93168235826574e-05, Test Loss: 2.659228614510382e-05\n",
            "Epoch 933/1000, Training Loss: 2.9293846738403885e-05, Test Loss: 2.657058010250985e-05\n",
            "Epoch 934/1000, Training Loss: 2.927091533919551e-05, Test Loss: 2.654891054275003e-05\n",
            "Epoch 935/1000, Training Loss: 2.9248029161963926e-05, Test Loss: 2.652727730311084e-05\n",
            "Epoch 936/1000, Training Loss: 2.9225187985169053e-05, Test Loss: 2.6505680222026165e-05\n",
            "Epoch 937/1000, Training Loss: 2.9202391588785665e-05, Test Loss: 2.648411913906722e-05\n",
            "Epoch 938/1000, Training Loss: 2.917963975429161e-05, Test Loss: 2.6462593894932475e-05\n",
            "Epoch 939/1000, Training Loss: 2.9156932264654905e-05, Test Loss: 2.6441104331437017e-05\n",
            "Epoch 940/1000, Training Loss: 2.9134268904321166e-05, Test Loss: 2.6419650291503662e-05\n",
            "Epoch 941/1000, Training Loss: 2.911164945920124e-05, Test Loss: 2.6398231619152183e-05\n",
            "Epoch 942/1000, Training Loss: 2.9089073716659735e-05, Test Loss: 2.6376848159490882e-05\n",
            "Epoch 943/1000, Training Loss: 2.9066541465502283e-05, Test Loss: 2.635549975870605e-05\n",
            "Epoch 944/1000, Training Loss: 2.9044052495964002e-05, Test Loss: 2.6334186264052256e-05\n",
            "Epoch 945/1000, Training Loss: 2.9021606599697304e-05, Test Loss: 2.631290752384489e-05\n",
            "Epoch 946/1000, Training Loss: 2.899920356976029e-05, Test Loss: 2.6291663387447554e-05\n",
            "Epoch 947/1000, Training Loss: 2.897684320060586e-05, Test Loss: 2.6270453705266086e-05\n",
            "Epoch 948/1000, Training Loss: 2.8954525288069024e-05, Test Loss: 2.6249278328737403e-05\n",
            "Epoch 949/1000, Training Loss: 2.8932249629356486e-05, Test Loss: 2.6228137110322675e-05\n",
            "Epoch 950/1000, Training Loss: 2.8910016023035186e-05, Test Loss: 2.6207029903495392e-05\n",
            "Epoch 951/1000, Training Loss: 2.8887824269020795e-05, Test Loss: 2.618595656273517e-05\n",
            "Epoch 952/1000, Training Loss: 2.886567416856695e-05, Test Loss: 2.6164916943517655e-05\n",
            "Epoch 953/1000, Training Loss: 2.8843565524254783e-05, Test Loss: 2.614391090230688e-05\n",
            "Epoch 954/1000, Training Loss: 2.8821498139981115e-05, Test Loss: 2.6122938296545816e-05\n",
            "Epoch 955/1000, Training Loss: 2.8799471820948535e-05, Test Loss: 2.610199898464817e-05\n",
            "Epoch 956/1000, Training Loss: 2.8777486373654533e-05, Test Loss: 2.6081092825990543e-05\n",
            "Epoch 957/1000, Training Loss: 2.8755541605880704e-05, Test Loss: 2.6060219680903352e-05\n",
            "Epoch 958/1000, Training Loss: 2.873363732668303e-05, Test Loss: 2.6039379410662545e-05\n",
            "Epoch 959/1000, Training Loss: 2.8711773346380884e-05, Test Loss: 2.6018571877483598e-05\n",
            "Epoch 960/1000, Training Loss: 2.8689949476546637e-05, Test Loss: 2.599779694451032e-05\n",
            "Epoch 961/1000, Training Loss: 2.86681655299969e-05, Test Loss: 2.5977054475809194e-05\n",
            "Epoch 962/1000, Training Loss: 2.8646421320781052e-05, Test Loss: 2.595634433636083e-05\n",
            "Epoch 963/1000, Training Loss: 2.862471666417177e-05, Test Loss: 2.593566639205169e-05\n",
            "Epoch 964/1000, Training Loss: 2.8603051376655358e-05, Test Loss: 2.591502050966725e-05\n",
            "Epoch 965/1000, Training Loss: 2.858142527592213e-05, Test Loss: 2.589440655688394e-05\n",
            "Epoch 966/1000, Training Loss: 2.855983818085636e-05, Test Loss: 2.587382440226129e-05\n",
            "Epoch 967/1000, Training Loss: 2.8538289911526514e-05, Test Loss: 2.585327391523471e-05\n",
            "Epoch 968/1000, Training Loss: 2.8516780289177122e-05, Test Loss: 2.583275496610887e-05\n",
            "Epoch 969/1000, Training Loss: 2.8495309136218056e-05, Test Loss: 2.581226742604931e-05\n",
            "Epoch 970/1000, Training Loss: 2.8473876276215445e-05, Test Loss: 2.579181116707512e-05\n",
            "Epoch 971/1000, Training Loss: 2.845248153388268e-05, Test Loss: 2.5771386062052106e-05\n",
            "Epoch 972/1000, Training Loss: 2.8431124735072033e-05, Test Loss: 2.5750991984686972e-05\n",
            "Epoch 973/1000, Training Loss: 2.8409805706764e-05, Test Loss: 2.5730628809518147e-05\n",
            "Epoch 974/1000, Training Loss: 2.83885242770597e-05, Test Loss: 2.5710296411909638e-05\n",
            "Epoch 975/1000, Training Loss: 2.8367280275171745e-05, Test Loss: 2.5689994668045138e-05\n",
            "Epoch 976/1000, Training Loss: 2.8346073531415043e-05, Test Loss: 2.5669723454918687e-05\n",
            "Epoch 977/1000, Training Loss: 2.8324903877198666e-05, Test Loss: 2.564948265033156e-05\n",
            "Epoch 978/1000, Training Loss: 2.8303771145016342e-05, Test Loss: 2.562927213288206e-05\n",
            "Epoch 979/1000, Training Loss: 2.8282675168439238e-05, Test Loss: 2.5609091781960638e-05\n",
            "Epoch 980/1000, Training Loss: 2.8261615782106718e-05, Test Loss: 2.5588941477743967e-05\n",
            "Epoch 981/1000, Training Loss: 2.8240592821717718e-05, Test Loss: 2.5568821101186975e-05\n",
            "Epoch 982/1000, Training Loss: 2.821960612402294e-05, Test Loss: 2.5548730534016516e-05\n",
            "Epoch 983/1000, Training Loss: 2.8198655526816514e-05, Test Loss: 2.5528669658726356e-05\n",
            "Epoch 984/1000, Training Loss: 2.8177740868927762e-05, Test Loss: 2.5508638358569578e-05\n",
            "Epoch 985/1000, Training Loss: 2.815686199021338e-05, Test Loss: 2.548863651755303e-05\n",
            "Epoch 986/1000, Training Loss: 2.813601873154868e-05, Test Loss: 2.5468664020430164e-05\n",
            "Epoch 987/1000, Training Loss: 2.8115210934820857e-05, Test Loss: 2.5448720752696605e-05\n",
            "Epoch 988/1000, Training Loss: 2.8094438442920505e-05, Test Loss: 2.542880660058258e-05\n",
            "Epoch 989/1000, Training Loss: 2.8073701099733327e-05, Test Loss: 2.5408921451047813e-05\n",
            "Epoch 990/1000, Training Loss: 2.8052998750133586e-05, Test Loss: 2.5389065191774597e-05\n",
            "Epoch 991/1000, Training Loss: 2.8032331239975558e-05, Test Loss: 2.5369237711163464e-05\n",
            "Epoch 992/1000, Training Loss: 2.801169841608625e-05, Test Loss: 2.5349438898325457e-05\n",
            "Epoch 993/1000, Training Loss: 2.7991100126258054e-05, Test Loss: 2.5329668643078515e-05\n",
            "Epoch 994/1000, Training Loss: 2.7970536219240896e-05, Test Loss: 2.5309926835939673e-05\n",
            "Epoch 995/1000, Training Loss: 2.7950006544735516e-05, Test Loss: 2.5290213368119983e-05\n",
            "Epoch 996/1000, Training Loss: 2.792951095338555e-05, Test Loss: 2.5270528131520406e-05\n",
            "Epoch 997/1000, Training Loss: 2.790904929677095e-05, Test Loss: 2.5250871018724967e-05\n",
            "Epoch 998/1000, Training Loss: 2.788862142740004e-05, Test Loss: 2.5231241922994085e-05\n",
            "Epoch 999/1000, Training Loss: 2.786822719870313e-05, Test Loss: 2.521164073826157e-05\n",
            "Epoch 1000/1000, Training Loss: 2.784786646502539e-05, Test Loss: 2.5192067359128402e-05\n",
            "Epoch 1/1000, Training Loss: 0.0033078562829338136, Test Loss: 0.0023103673802000015\n",
            "Epoch 2/1000, Training Loss: 0.0031484318648333245, Test Loss: 0.0019737777321064266\n",
            "Epoch 3/1000, Training Loss: 0.0031210374487406823, Test Loss: 0.0018866999628687998\n",
            "Epoch 4/1000, Training Loss: 0.003112785949271628, Test Loss: 0.0018520217907001216\n",
            "Epoch 5/1000, Training Loss: 0.0031092852140678713, Test Loss: 0.0018356595398889542\n",
            "Epoch 6/1000, Training Loss: 0.0031073337177535756, Test Loss: 0.0018272643589381445\n",
            "Epoch 7/1000, Training Loss: 0.0031059960237437567, Test Loss: 0.0018227409921301083\n",
            "Epoch 8/1000, Training Loss: 0.0031049427223741365, Test Loss: 0.0018202136286838261\n",
            "Epoch 9/1000, Training Loss: 0.003104039086529117, Test Loss: 0.0018187486900943229\n",
            "Epoch 10/1000, Training Loss: 0.003103222927956783, Test Loss: 0.0018178593776001335\n",
            "Epoch 11/1000, Training Loss: 0.0031024627848605492, Test Loss: 0.0018172852950148364\n",
            "Epoch 12/1000, Training Loss: 0.003101741648825978, Test Loss: 0.0018168852597922882\n",
            "Epoch 13/1000, Training Loss: 0.003101049854568926, Test Loss: 0.001816582208142865\n",
            "Epoch 14/1000, Training Loss: 0.003100381664965544, Test Loss: 0.001816333947960948\n",
            "Epoch 15/1000, Training Loss: 0.0030997335148412403, Test Loss: 0.0018161173528377693\n",
            "Epoch 16/1000, Training Loss: 0.0030991030666413025, Test Loss: 0.0018159197400929026\n",
            "Epoch 17/1000, Training Loss: 0.0030984886885816612, Test Loss: 0.0018157341471668497\n",
            "Epoch 18/1000, Training Loss: 0.003097889161415523, Test Loss: 0.0018155567400007548\n",
            "Epoch 19/1000, Training Loss: 0.0030973035118444137, Test Loss: 0.0018153853915089777\n",
            "Epoch 20/1000, Training Loss: 0.0030967309170603684, Test Loss: 0.0018152189029164753\n",
            "Epoch 21/1000, Training Loss: 0.003096170649580753, Test Loss: 0.0018150565781596545\n",
            "Epoch 22/1000, Training Loss: 0.0030956220450442393, Test Loss: 0.001814897991940735\n",
            "Epoch 23/1000, Training Loss: 0.0030950844831602177, Test Loss: 0.0018147428637945357\n",
            "Epoch 24/1000, Training Loss: 0.003094557376236073, Test Loss: 0.0018145909900456395\n",
            "Epoch 25/1000, Training Loss: 0.0030940401621007423, Test Loss: 0.001814442207280444\n",
            "Epoch 26/1000, Training Loss: 0.0030935322996018624, Test Loss: 0.001814296372910267\n",
            "Epoch 27/1000, Training Loss: 0.003093033265627349, Test Loss: 0.0018141533549584374\n",
            "Epoch 28/1000, Training Loss: 0.0030925425530436952, Test Loss: 0.0018140130267941664\n",
            "Epoch 29/1000, Training Loss: 0.0030920596691960843, Test Loss: 0.0018138752644964483\n",
            "Epoch 30/1000, Training Loss: 0.003091584134760781, Test Loss: 0.0018137399455990667\n",
            "Epoch 31/1000, Training Loss: 0.0030911154828244147, Test Loss: 0.0018136069485474168\n",
            "Epoch 32/1000, Training Loss: 0.003090653258113797, Test Loss: 0.0018134761525112222\n",
            "Epoch 33/1000, Training Loss: 0.003090197016328859, Test Loss: 0.0018133474373660087\n",
            "Epoch 34/1000, Training Loss: 0.0030897463235485556, Test Loss: 0.0018132206837463433\n",
            "Epoch 35/1000, Training Loss: 0.0030893007556901035, Test Loss: 0.0018130957731219213\n",
            "Epoch 36/1000, Training Loss: 0.0030888598980084317, Test Loss: 0.0018129725878726854\n",
            "Epoch 37/1000, Training Loss: 0.0030884233446268833, Test Loss: 0.0018128510113522198\n",
            "Epoch 38/1000, Training Loss: 0.003087990698092888, Test Loss: 0.0018127309279354416\n",
            "Epoch 39/1000, Training Loss: 0.0030875615689541784, Test Loss: 0.0018126122230496917\n",
            "Epoch 40/1000, Training Loss: 0.0030871355753523304, Test Loss: 0.0018124947831899929\n",
            "Epoch 41/1000, Training Loss: 0.0030867123426313267, Test Loss: 0.001812378495919813\n",
            "Epoch 42/1000, Training Loss: 0.0030862915029594555, Test Loss: 0.0018122632498589338\n",
            "Epoch 43/1000, Training Loss: 0.00308587269496334, Test Loss: 0.0018121489346598863\n",
            "Epoch 44/1000, Training Loss: 0.0030854555633732204, Test Loss: 0.0018120354409745596\n",
            "Epoch 45/1000, Training Loss: 0.0030850397586788926, Test Loss: 0.0018119226604121905\n",
            "Epoch 46/1000, Training Loss: 0.003084624936795881, Test Loss: 0.0018118104854900382\n",
            "Epoch 47/1000, Training Loss: 0.003084210758741582, Test Loss: 0.0018116988095777944\n",
            "Epoch 48/1000, Training Loss: 0.003083796890321217, Test Loss: 0.0018115875268367588\n",
            "Epoch 49/1000, Training Loss: 0.003083383001823522, Test Loss: 0.0018114765321545964\n",
            "Epoch 50/1000, Training Loss: 0.003082968767726146, Test Loss: 0.0018113657210765707\n",
            "Epoch 51/1000, Training Loss: 0.0030825538664107795, Test Loss: 0.001811254989733877\n",
            "Epoch 52/1000, Training Loss: 0.0030821379798880726, Test Loss: 0.001811144234769826\n",
            "Epoch 53/1000, Training Loss: 0.00308172079353239, Test Loss: 0.0018110333532643768\n",
            "Epoch 54/1000, Training Loss: 0.003081301995826493, Test Loss: 0.0018109222426576407\n",
            "Epoch 55/1000, Training Loss: 0.0030808812781162142, Test Loss: 0.001810810800672774\n",
            "Epoch 56/1000, Training Loss: 0.003080458334375202, Test Loss: 0.0018106989252387557\n",
            "Epoch 57/1000, Training Loss: 0.0030800328609798047, Test Loss: 0.0018105865144133846\n",
            "Epoch 58/1000, Training Loss: 0.003079604556494139, Test Loss: 0.0018104734663069145\n",
            "Epoch 59/1000, Training Loss: 0.0030791731214654034, Test Loss: 0.0018103596790066496\n",
            "Epoch 60/1000, Training Loss: 0.0030787382582294414, Test Loss: 0.001810245050502724\n",
            "Epoch 61/1000, Training Loss: 0.0030782996707265898, Test Loss: 0.0018101294786154296\n",
            "Epoch 62/1000, Training Loss: 0.003077857064327781, Test Loss: 0.001810012860924257\n",
            "Epoch 63/1000, Training Loss: 0.003077410145670892, Test Loss: 0.0018098950946988728\n",
            "Epoch 64/1000, Training Loss: 0.0030769586225072726, Test Loss: 0.0018097760768322453\n",
            "Epoch 65/1000, Training Loss: 0.003076502203558398, Test Loss: 0.0018096557037760655\n",
            "Epoch 66/1000, Training Loss: 0.0030760405983825396, Test Loss: 0.001809533871478599\n",
            "Epoch 67/1000, Training Loss: 0.0030755735172513384, Test Loss: 0.001809410475325104\n",
            "Epoch 68/1000, Training Loss: 0.0030751006710361445, Test Loss: 0.0018092854100809025\n",
            "Epoch 69/1000, Training Loss: 0.0030746217711039355, Test Loss: 0.0018091585698372294\n",
            "Epoch 70/1000, Training Loss: 0.003074136529222641, Test Loss: 0.0018090298479598627\n",
            "Epoch 71/1000, Training Loss: 0.0030736446574756225, Test Loss: 0.0018088991370406365\n",
            "Epoch 72/1000, Training Loss: 0.0030731458681850852, Test Loss: 0.0018087663288518386\n",
            "Epoch 73/1000, Training Loss: 0.003072639873844115, Test Loss: 0.00180863131430353\n",
            "Epoch 74/1000, Training Loss: 0.003072126387057044, Test Loss: 0.0018084939834037395\n",
            "Epoch 75/1000, Training Loss: 0.0030716051204878105, Test Loss: 0.0018083542252215808\n",
            "Epoch 76/1000, Training Loss: 0.0030710757868159387, Test Loss: 0.0018082119278531785\n",
            "Epoch 77/1000, Training Loss: 0.0030705380986997335, Test Loss: 0.0018080669783904232\n",
            "Epoch 78/1000, Training Loss: 0.0030699917687462912, Test Loss: 0.0018079192628924256\n",
            "Epoch 79/1000, Training Loss: 0.0030694365094878465, Test Loss: 0.001807768666359611\n",
            "Epoch 80/1000, Training Loss: 0.003068872033363986, Test Loss: 0.001807615072710369\n",
            "Epoch 81/1000, Training Loss: 0.0030682980527092334, Test Loss: 0.0018074583647601152\n",
            "Epoch 82/1000, Training Loss: 0.0030677142797454577, Test Loss: 0.0018072984242026144\n",
            "Epoch 83/1000, Training Loss: 0.0030671204265785615, Test Loss: 0.0018071351315934886\n",
            "Epoch 84/1000, Training Loss: 0.0030665162051988844, Test Loss: 0.001806968366335654\n",
            "Epoch 85/1000, Training Loss: 0.003065901327484707, Test Loss: 0.0018067980066665834\n",
            "Epoch 86/1000, Training Loss: 0.0030652755052082807, Test Loss: 0.001806623929647172\n",
            "Epoch 87/1000, Training Loss: 0.003064638450043731, Test Loss: 0.0018064460111520172\n",
            "Epoch 88/1000, Training Loss: 0.0030639898735762305, Test Loss: 0.0018062641258608692\n",
            "Epoch 89/1000, Training Loss: 0.0030633294873117943, Test Loss: 0.001806078147251085\n",
            "Epoch 90/1000, Training Loss: 0.003062657002687064, Test Loss: 0.0018058879475908013\n",
            "Epoch 91/1000, Training Loss: 0.003061972131078452, Test Loss: 0.0018056933979325985\n",
            "Epoch 92/1000, Training Loss: 0.0030612745838100035, Test Loss: 0.0018054943681074468\n",
            "Epoch 93/1000, Training Loss: 0.0030605640721593796, Test Loss: 0.001805290726718627\n",
            "Epoch 94/1000, Training Loss: 0.0030598403073613368, Test Loss: 0.0018050823411354222\n",
            "Epoch 95/1000, Training Loss: 0.0030591030006081395, Test Loss: 0.00180486907748629\n",
            "Epoch 96/1000, Training Loss: 0.003058351863046334, Test Loss: 0.0018046508006513062\n",
            "Epoch 97/1000, Training Loss: 0.0030575866057693654, Test Loss: 0.0018044273742535905\n",
            "Epoch 98/1000, Training Loss: 0.003056806939805534, Test Loss: 0.001804198660649502\n",
            "Epoch 99/1000, Training Loss: 0.0030560125761008414, Test Loss: 0.0018039645209173335\n",
            "Epoch 100/1000, Training Loss: 0.003055203225496308, Test Loss: 0.001803724814844357\n",
            "Epoch 101/1000, Training Loss: 0.0030543785986993943, Test Loss: 0.0018034794009119002\n",
            "Epoch 102/1000, Training Loss: 0.0030535384062492185, Test Loss: 0.0018032281362783565\n",
            "Epoch 103/1000, Training Loss: 0.0030526823584752913, Test Loss: 0.0018029708767598935\n",
            "Epoch 104/1000, Training Loss: 0.0030518101654495896, Test Loss: 0.0018027074768087318\n",
            "Epoch 105/1000, Training Loss: 0.0030509215369317957, Test Loss: 0.0018024377894888008\n",
            "Epoch 106/1000, Training Loss: 0.003050016182307653, Test Loss: 0.0018021616664487272\n",
            "Epoch 107/1000, Training Loss: 0.003049093810520404, Test Loss: 0.0018018789578920361\n",
            "Epoch 108/1000, Training Loss: 0.003048154129995372, Test Loss: 0.0018015895125444643\n",
            "Epoch 109/1000, Training Loss: 0.0030471968485578137, Test Loss: 0.0018012931776184124\n",
            "Epoch 110/1000, Training Loss: 0.003046221673344231, Test Loss: 0.0018009897987745105\n",
            "Epoch 111/1000, Training Loss: 0.0030452283107074004, Test Loss: 0.0018006792200802896\n",
            "Epoch 112/1000, Training Loss: 0.003044216466115475, Test Loss: 0.0018003612839660765\n",
            "Epoch 113/1000, Training Loss: 0.00304318584404553, Test Loss: 0.0018000358311781644\n",
            "Epoch 114/1000, Training Loss: 0.0030421361478720588, Test Loss: 0.0017997027007294263\n",
            "Epoch 115/1000, Training Loss: 0.003041067079750931, Test Loss: 0.0017993617298474747\n",
            "Epoch 116/1000, Training Loss: 0.003039978340499445, Test Loss: 0.0017990127539206655\n",
            "Epoch 117/1000, Training Loss: 0.003038869629473121, Test Loss: 0.0017986556064420824\n",
            "Epoch 118/1000, Training Loss: 0.003037740644439983, Test Loss: 0.0017982901189518372\n",
            "Epoch 119/1000, Training Loss: 0.003036591081453127, Test Loss: 0.0017979161209780027\n",
            "Epoch 120/1000, Training Loss: 0.0030354206347223923, Test Loss: 0.0017975334399764507\n",
            "Epoch 121/1000, Training Loss: 0.003034228996486079, Test Loss: 0.001797141901270119\n",
            "Epoch 122/1000, Training Loss: 0.003033015856883627, Test Loss: 0.0017967413279879173\n",
            "Epoch 123/1000, Training Loss: 0.003031780903830274, Test Loss: 0.0017963315410039792\n",
            "Epoch 124/1000, Training Loss: 0.0030305238228947227, Test Loss: 0.0017959123588775002\n",
            "Epoch 125/1000, Training Loss: 0.0030292442971809035, Test Loss: 0.0017954835977938355\n",
            "Epoch 126/1000, Training Loss: 0.003027942007214936, Test Loss: 0.0017950450715073774\n",
            "Epoch 127/1000, Training Loss: 0.003026616630838447, Test Loss: 0.0017945965912867221\n",
            "Epoch 128/1000, Training Loss: 0.0030252678431094077, Test Loss: 0.0017941379658628286\n",
            "Epoch 129/1000, Training Loss: 0.0030238953162117013, Test Loss: 0.0017936690013807001\n",
            "Epoch 130/1000, Training Loss: 0.0030224987193746126, Test Loss: 0.001793189501355348\n",
            "Epoch 131/1000, Training Loss: 0.0030210777188034985, Test Loss: 0.0017926992666326218\n",
            "Epoch 132/1000, Training Loss: 0.003019631977622868, Test Loss: 0.0017921980953556399\n",
            "Epoch 133/1000, Training Loss: 0.003018161155833123, Test Loss: 0.0017916857829375253\n",
            "Epoch 134/1000, Training Loss: 0.003016664910282212, Test Loss: 0.0017911621220411845\n",
            "Epoch 135/1000, Training Loss: 0.0030151428946534604, Test Loss: 0.001790626902566846\n",
            "Epoch 136/1000, Training Loss: 0.0030135947594708036, Test Loss: 0.0017900799116480853\n",
            "Epoch 137/1000, Training Loss: 0.0030120201521226697, Test Loss: 0.001789520933657118\n",
            "Epoch 138/1000, Training Loss: 0.003010418716905729, Test Loss: 0.001788949750220055\n",
            "Epoch 139/1000, Training Loss: 0.003008790095089698, Test Loss: 0.0017883661402429262\n",
            "Epoch 140/1000, Training Loss: 0.0030071339250043764, Test Loss: 0.0017877698799491023\n",
            "Epoch 141/1000, Training Loss: 0.0030054498421500414, Test Loss: 0.0017871607429289236\n",
            "Epoch 142/1000, Training Loss: 0.003003737479332304, Test Loss: 0.0017865385002021484\n",
            "Epoch 143/1000, Training Loss: 0.0030019964668224672, Test Loss: 0.0017859029202939693\n",
            "Epoch 144/1000, Training Loss: 0.0030002264325443835, Test Loss: 0.0017852537693251588\n",
            "Epoch 145/1000, Training Loss: 0.002998427002288741, Test Loss: 0.0017845908111169487\n",
            "Epoch 146/1000, Training Loss: 0.0029965977999556388, Test Loss: 0.0017839138073113003\n",
            "Epoch 147/1000, Training Loss: 0.002994738447826233, Test Loss: 0.0017832225175068992\n",
            "Epoch 148/1000, Training Loss: 0.002992848566864148, Test Loss: 0.0017825166994114737\n",
            "Epoch 149/1000, Training Loss: 0.002990927777047242, Test Loss: 0.0017817961090107596\n",
            "Epoch 150/1000, Training Loss: 0.002988975697730228, Test Loss: 0.00178106050075441\n",
            "Epoch 151/1000, Training Loss: 0.0029869919480384954, Test Loss: 0.0017803096277591193\n",
            "Epoch 152/1000, Training Loss: 0.002984976147293405, Test Loss: 0.0017795432420291107\n",
            "Epoch 153/1000, Training Loss: 0.0029829279154691257, Test Loss: 0.0017787610946940174\n",
            "Epoch 154/1000, Training Loss: 0.0029808468736809957, Test Loss: 0.0017779629362641822\n",
            "Epoch 155/1000, Training Loss: 0.0029787326447051807, Test Loss: 0.00177714851690317\n",
            "Epoch 156/1000, Training Loss: 0.0029765848535292625, Test Loss: 0.0017763175867173044\n",
            "Epoch 157/1000, Training Loss: 0.0029744031279332096, Test Loss: 0.0017754698960618647\n",
            "Epoch 158/1000, Training Loss: 0.00297218709909997, Test Loss: 0.0017746051958634186\n",
            "Epoch 159/1000, Training Loss: 0.0029699364022547745, Test Loss: 0.0017737232379577683\n",
            "Epoch 160/1000, Training Loss: 0.002967650677331989, Test Loss: 0.0017728237754427473\n",
            "Epoch 161/1000, Training Loss: 0.0029653295696681894, Test Loss: 0.0017719065630450834\n",
            "Epoch 162/1000, Training Loss: 0.0029629727307198903, Test Loss: 0.0017709713575003235\n",
            "Epoch 163/1000, Training Loss: 0.0029605798188041686, Test Loss: 0.0017700179179447576\n",
            "Epoch 164/1000, Training Loss: 0.0029581504998601837, Test Loss: 0.0017690460063182081\n",
            "Epoch 165/1000, Training Loss: 0.002955684448229427, Test Loss: 0.0017680553877763216\n",
            "Epoch 166/1000, Training Loss: 0.00295318134745227, Test Loss: 0.001767045831111017\n",
            "Epoch 167/1000, Training Loss: 0.0029506408910782264, Test Loss: 0.0017660171091775825\n",
            "Epoch 168/1000, Training Loss: 0.0029480627834871293, Test Loss: 0.001764968999326837\n",
            "Epoch 169/1000, Training Loss: 0.002945446740718243, Test Loss: 0.001763901283840773\n",
            "Epoch 170/1000, Training Loss: 0.0029427924913041667, Test Loss: 0.0017628137503698691\n",
            "Epoch 171/1000, Training Loss: 0.0029400997771062438, Test Loss: 0.001761706192370499\n",
            "Epoch 172/1000, Training Loss: 0.002937368354148032, Test Loss: 0.0017605784095405038\n",
            "Epoch 173/1000, Training Loss: 0.0029345979934433043, Test Loss: 0.0017594302082512769\n",
            "Epoch 174/1000, Training Loss: 0.002931788481814959, Test Loss: 0.0017582614019745417\n",
            "Epoch 175/1000, Training Loss: 0.002928939622701122, Test Loss: 0.0017570718117020812\n",
            "Epoch 176/1000, Training Loss: 0.002926051236944737, Test Loss: 0.0017558612663568004\n",
            "Epoch 177/1000, Training Loss: 0.0029231231635628765, Test Loss: 0.0017546296031934025\n",
            "Epoch 178/1000, Training Loss: 0.0029201552604920583, Test Loss: 0.0017533766681872268\n",
            "Epoch 179/1000, Training Loss: 0.002917147405305884, Test Loss: 0.0017521023164097705\n",
            "Epoch 180/1000, Training Loss: 0.0029140994959013884, Test Loss: 0.0017508064123895808\n",
            "Epoch 181/1000, Training Loss: 0.002911011451150596, Test Loss: 0.0017494888304573598\n",
            "Epoch 182/1000, Training Loss: 0.002907883211513898, Test Loss: 0.0017481494550742238\n",
            "Epoch 183/1000, Training Loss: 0.0029047147396120384, Test Loss: 0.001746788181142241\n",
            "Epoch 184/1000, Training Loss: 0.002901506020753641, Test Loss: 0.001745404914296547\n",
            "Epoch 185/1000, Training Loss: 0.0028982570634154565, Test Loss: 0.0017439995711784923\n",
            "Epoch 186/1000, Training Loss: 0.002894967899672666, Test Loss: 0.0017425720796894703\n",
            "Epoch 187/1000, Training Loss: 0.002891638585576864, Test Loss: 0.0017411223792252227\n",
            "Epoch 188/1000, Training Loss: 0.0028882692014795723, Test Loss: 0.0017396504208905866\n",
            "Epoch 189/1000, Training Loss: 0.002884859852299379, Test Loss: 0.0017381561676949268\n",
            "Epoch 190/1000, Training Loss: 0.0028814106677310736, Test Loss: 0.001736639594728455\n",
            "Epoch 191/1000, Training Loss: 0.0028779218023954234, Test Loss: 0.001735100689319999\n",
            "Epoch 192/1000, Training Loss: 0.002874393435928474, Test Loss: 0.0017335394511767215\n",
            "Epoch 193/1000, Training Loss: 0.0028708257730095605, Test Loss: 0.0017319558925066327\n",
            "Epoch 194/1000, Training Loss: 0.0028672190433274114, Test Loss: 0.0017303500381245804\n",
            "Epoch 195/1000, Training Loss: 0.0028635735014840382, Test Loss: 0.0017287219255427047\n",
            "Epoch 196/1000, Training Loss: 0.002859889426836276, Test Loss: 0.0017270716050462823\n",
            "Epoch 197/1000, Training Loss: 0.0028561671232751177, Test Loss: 0.0017253991397559812\n",
            "Epoch 198/1000, Training Loss: 0.0028524069189431295, Test Loss: 0.001723704605677489\n",
            "Epoch 199/1000, Training Loss: 0.0028486091658904604, Test Loss: 0.001721988091739578\n",
            "Epoch 200/1000, Training Loss: 0.002844774239670115, Test Loss: 0.001720249699821518\n",
            "Epoch 201/1000, Training Loss: 0.002840902538873303, Test Loss: 0.001718489544770832\n",
            "Epoch 202/1000, Training Loss: 0.0028369944846058062, Test Loss: 0.001716707754412138\n",
            "Epoch 203/1000, Training Loss: 0.0028330505199064353, Test Loss: 0.0017149044695479651\n",
            "Epoch 204/1000, Training Loss: 0.0028290711091087063, Test Loss: 0.0017130798439520837\n",
            "Epoch 205/1000, Training Loss: 0.0028250567371470078, Test Loss: 0.001711234044355966\n",
            "Epoch 206/1000, Training Loss: 0.0028210079088085076, Test Loss: 0.0017093672504286683\n",
            "Epoch 207/1000, Training Loss: 0.002816925147932185, Test Loss: 0.0017074796547504926\n",
            "Epoch 208/1000, Training Loss: 0.002812808996556364, Test Loss: 0.0017055714627804855\n",
            "Epoch 209/1000, Training Loss: 0.0028086600140161634, Test Loss: 0.0017036428928176685\n",
            "Epoch 210/1000, Training Loss: 0.002804478775992316, Test Loss: 0.001701694175955856\n",
            "Epoch 211/1000, Training Loss: 0.0028002658735128286, Test Loss: 0.0016997255560316478\n",
            "Epoch 212/1000, Training Loss: 0.002796021911908943, Test Loss: 0.001697737289565069\n",
            "Epoch 213/1000, Training Loss: 0.002791747509726933, Test Loss: 0.0016957296456922048\n",
            "Epoch 214/1000, Training Loss: 0.002787443297597231, Test Loss: 0.0016937029060890166\n",
            "Epoch 215/1000, Training Loss: 0.0027831099170624145, Test Loss: 0.001691657364885353\n",
            "Epoch 216/1000, Training Loss: 0.0027787480193656607, Test Loss: 0.0016895933285682005\n",
            "Epoch 217/1000, Training Loss: 0.0027743582642012032, Test Loss: 0.0016875111158729035\n",
            "Epoch 218/1000, Training Loss: 0.00276994131842846, Test Loss: 0.0016854110576612095\n",
            "Epoch 219/1000, Training Loss: 0.002765497854751478, Test Loss: 0.0016832934967847715\n",
            "Epoch 220/1000, Training Loss: 0.002761028550365414, Test Loss: 0.0016811587879328174\n",
            "Epoch 221/1000, Training Loss: 0.0027565340855718232, Test Loss: 0.00167900729746251\n",
            "Epoch 222/1000, Training Loss: 0.0027520151423645637, Test Loss: 0.0016768394032107062\n",
            "Epoch 223/1000, Training Loss: 0.002747472402988212, Test Loss: 0.001674655494285644\n",
            "Epoch 224/1000, Training Loss: 0.0027429065484709334, Test Loss: 0.0016724559708372474\n",
            "Epoch 225/1000, Training Loss: 0.002738318257133827, Test Loss: 0.0016702412438047074\n",
            "Epoch 226/1000, Training Loss: 0.0027337082030788194, Test Loss: 0.0016680117346401185\n",
            "Epoch 227/1000, Training Loss: 0.002729077054657263, Test Loss: 0.0016657678750069595\n",
            "Epoch 228/1000, Training Loss: 0.0027244254729214316, Test Loss: 0.0016635101064524066\n",
            "Epoch 229/1000, Training Loss: 0.00271975411006119, Test Loss: 0.0016612388800524714\n",
            "Epoch 230/1000, Training Loss: 0.0027150636078281413, Test Loss: 0.0016589546560291585\n",
            "Epoch 231/1000, Training Loss: 0.00271035459594959, Test Loss: 0.0016566579033389925\n",
            "Epoch 232/1000, Training Loss: 0.002705627690534704, Test Loss: 0.0016543490992322945\n",
            "Epoch 233/1000, Training Loss: 0.0027008834924752414, Test Loss: 0.0016520287287829284\n",
            "Epoch 234/1000, Training Loss: 0.002696122585843216, Test Loss: 0.001649697284388234\n",
            "Epoch 235/1000, Training Loss: 0.00269134553628785, Test Loss: 0.001647355265239215\n",
            "Epoch 236/1000, Training Loss: 0.002686552889434095, Test Loss: 0.0016450031767609927\n",
            "Epoch 237/1000, Training Loss: 0.002681745169284949, Test Loss: 0.001642641530024003\n",
            "Epoch 238/1000, Training Loss: 0.0026769228766296937, Test Loss: 0.0016402708411263397\n",
            "Epoch 239/1000, Training Loss: 0.002672086487460039, Test Loss: 0.0016378916305480062\n",
            "Epoch 240/1000, Training Loss: 0.00266723645139603, Test Loss: 0.001635504422477882\n",
            "Epoch 241/1000, Training Loss: 0.002662373190123379, Test Loss: 0.0016331097441144788\n",
            "Epoch 242/1000, Training Loss: 0.002657497095843658, Test Loss: 0.0016307081249416668\n",
            "Epoch 243/1000, Training Loss: 0.0026526085297385806, Test Loss: 0.0016283000959807145\n",
            "Epoch 244/1000, Training Loss: 0.002647707820449273, Test Loss: 0.0016258861890201034\n",
            "Epoch 245/1000, Training Loss: 0.002642795262571178, Test Loss: 0.00162346693582474\n",
            "Epoch 246/1000, Training Loss: 0.002637871115164853, Test Loss: 0.001621042867326272\n",
            "Epoch 247/1000, Training Loss: 0.0026329356002825675, Test Loss: 0.0016186145127962352\n",
            "Epoch 248/1000, Training Loss: 0.002627988901510205, Test Loss: 0.0016161823990039567\n",
            "Epoch 249/1000, Training Loss: 0.002623031162523502, Test Loss: 0.0016137470493610162\n",
            "Epoch 250/1000, Training Loss: 0.0026180624856572076, Test Loss: 0.0016113089830541777\n",
            "Epoch 251/1000, Training Loss: 0.00261308293048526, Test Loss: 0.0016088687141686444\n",
            "Epoch 252/1000, Training Loss: 0.002608092512409474, Test Loss: 0.001606426750803403\n",
            "Epoch 253/1000, Training Loss: 0.002603091201253739, Test Loss: 0.001603983594180374\n",
            "Epoch 254/1000, Training Loss: 0.0025980789198600746, Test Loss: 0.001601539737748854\n",
            "Epoch 255/1000, Training Loss: 0.0025930555426822784, Test Loss: 0.0015990956662866424\n",
            "Epoch 256/1000, Training Loss: 0.0025880208943722498, Test Loss: 0.0015966518549989405\n",
            "Epoch 257/1000, Training Loss: 0.002582974748353353, Test Loss: 0.0015942087686158148\n",
            "Epoch 258/1000, Training Loss: 0.0025779168253744927, Test Loss: 0.0015917668604886537\n",
            "Epoch 259/1000, Training Loss: 0.0025728467920378057, Test Loss: 0.0015893265716856672\n",
            "Epoch 260/1000, Training Loss: 0.0025677642592921027, Test Loss: 0.0015868883300858977\n",
            "Epoch 261/1000, Training Loss: 0.002562668780883366, Test Loss: 0.0015844525494706685\n",
            "Epoch 262/1000, Training Loss: 0.002557559851752782, Test Loss: 0.0015820196286107627\n",
            "Epoch 263/1000, Training Loss: 0.0025524369063719115, Test Loss: 0.0015795899503467858\n",
            "Epoch 264/1000, Training Loss: 0.0025472993170036814, Test Loss: 0.0015771638806593366\n",
            "Epoch 265/1000, Training Loss: 0.002542146391876948, Test Loss: 0.0015747417677246908\n",
            "Epoch 266/1000, Training Loss: 0.002536977373261394, Test Loss: 0.001572323940950477\n",
            "Epoch 267/1000, Training Loss: 0.0025317914354285142, Test Loss: 0.0015699107099847085\n",
            "Epoch 268/1000, Training Loss: 0.002526587682483362, Test Loss: 0.001567502363690123\n",
            "Epoch 269/1000, Training Loss: 0.002521365146050633, Test Loss: 0.0015650991690742265\n",
            "Epoch 270/1000, Training Loss: 0.002516122782797519, Test Loss: 0.0015627013701638517\n",
            "Epoch 271/1000, Training Loss: 0.0025108594717745073, Test Loss: 0.0015603091868110445\n",
            "Epoch 272/1000, Training Loss: 0.0025055740115541053, Test Loss: 0.0015579228134151874\n",
            "Epoch 273/1000, Training Loss: 0.002500265117146065, Test Loss: 0.0015555424175438782\n",
            "Epoch 274/1000, Training Loss: 0.002494931416666338, Test Loss: 0.0015531681384328027\n",
            "Epoch 275/1000, Training Loss: 0.0024895714477355156, Test Loss: 0.0015508000853419548\n",
            "Epoch 276/1000, Training Loss: 0.002484183653580922, Test Loss: 0.0015484383357428307\n",
            "Epoch 277/1000, Training Loss: 0.0024787663788150057, Test Loss: 0.001546082933307951\n",
            "Epoch 278/1000, Training Loss: 0.002473317864860832, Test Loss: 0.001543733885670664\n",
            "Epoch 279/1000, Training Loss: 0.002467836244993849, Test Loss: 0.0015413911619196106\n",
            "Epoch 280/1000, Training Loss: 0.0024623195389670903, Test Loss: 0.0015390546897881654\n",
            "Epoch 281/1000, Training Loss: 0.0024567656471851445, Test Loss: 0.0015367243524950602\n",
            "Epoch 282/1000, Training Loss: 0.0024511723443901264, Test Loss: 0.001534399985187921\n",
            "Epoch 283/1000, Training Loss: 0.0024455372728208868, Test Loss: 0.0015320813709367644\n",
            "Epoch 284/1000, Training Loss: 0.0024398579348045646, Test Loss: 0.001529768236219617\n",
            "Epoch 285/1000, Training Loss: 0.002434131684737474, Test Loss: 0.0015274602458371664\n",
            "Epoch 286/1000, Training Loss: 0.002428355720410344, Test Loss: 0.0015251569971883082\n",
            "Epoch 287/1000, Training Loss: 0.002422527073630905, Test Loss: 0.0015228580138329465\n",
            "Epoch 288/1000, Training Loss: 0.002416642600095195, Test Loss: 0.0015205627382631334\n",
            "Epoch 289/1000, Training Loss: 0.002410698968457431, Test Loss: 0.0015182705237983254\n",
            "Epoch 290/1000, Training Loss: 0.0024046926485474384, Test Loss: 0.001515980625515585\n",
            "Epoch 291/1000, Training Loss: 0.0023986198986842623, Test Loss: 0.0015136921901209633\n",
            "Epoch 292/1000, Training Loss: 0.002392476752035294, Test Loss: 0.0015114042446642928\n",
            "Epoch 293/1000, Training Loss: 0.002386259001972007, Test Loss: 0.001509115683996689\n",
            "Epoch 294/1000, Training Loss: 0.0023799621863769575, Test Loss: 0.0015068252568682392\n",
            "Epoch 295/1000, Training Loss: 0.0023735815708623, Test Loss: 0.001504531550563394\n",
            "Epoch 296/1000, Training Loss: 0.0023671121308685055, Test Loss: 0.00150223297397384\n",
            "Epoch 297/1000, Training Loss: 0.0023605485326239047, Test Loss: 0.0014999277390137053\n",
            "Epoch 298/1000, Training Loss: 0.002353885112962181, Test Loss: 0.00149761384029096\n",
            "Epoch 299/1000, Training Loss: 0.002347115858017122, Test Loss: 0.0014952890329623988\n",
            "Epoch 300/1000, Training Loss: 0.002340234380843152, Test Loss: 0.001492950808719178\n",
            "Epoch 301/1000, Training Loss: 0.0023332338980482023, Test Loss: 0.0014905963698765548\n",
            "Epoch 302/1000, Training Loss: 0.0023261072055742343, Test Loss: 0.0014882226015773099\n",
            "Epoch 303/1000, Training Loss: 0.00231884665382237, Test Loss: 0.001485826042164714\n",
            "Epoch 304/1000, Training Loss: 0.002311444122396966, Test Loss: 0.0014834028518406794\n",
            "Epoch 305/1000, Training Loss: 0.00230389099483861, Test Loss: 0.0014809487797995203\n",
            "Epoch 306/1000, Training Loss: 0.0022961781338331896, Test Loss: 0.0014784591301210185\n",
            "Epoch 307/1000, Training Loss: 0.0022882958575258967, Test Loss: 0.0014759287268203273\n",
            "Epoch 308/1000, Training Loss: 0.002280233917738258, Test Loss: 0.0014733518785899037\n",
            "Epoch 309/1000, Training Loss: 0.002271981481085275, Test Loss: 0.0014707223439319821\n",
            "Epoch 310/1000, Training Loss: 0.002263527114220142, Test Loss: 0.0014680332975714927\n",
            "Epoch 311/1000, Training Loss: 0.002254858774694982, Test Loss: 0.001465277299258675\n",
            "Epoch 312/1000, Training Loss: 0.0022459638092147817, Test Loss: 0.00146244626631713\n",
            "Epoch 313/1000, Training Loss: 0.0022368289613709526, Test Loss: 0.001459531451562196\n",
            "Epoch 314/1000, Training Loss: 0.0022274403912581216, Test Loss: 0.0014565234284986154\n",
            "Epoch 315/1000, Training Loss: 0.002217783709683352, Test Loss: 0.001453412085992252\n",
            "Epoch 316/1000, Training Loss: 0.0022078440299423616, Test Loss: 0.0014501866348790777\n",
            "Epoch 317/1000, Training Loss: 0.0021976060403233835, Test Loss: 0.0014468356291983953\n",
            "Epoch 318/1000, Training Loss: 0.0021870541005559584, Test Loss: 0.0014433470048819427\n",
            "Epoch 319/1000, Training Loss: 0.002176172365287086, Test Loss: 0.0014397081387523348\n",
            "Epoch 320/1000, Training Loss: 0.0021649449372708185, Test Loss: 0.0014359059305334356\n",
            "Epoch 321/1000, Training Loss: 0.0021533560522247453, Test Loss: 0.0014319269101973969\n",
            "Epoch 322/1000, Training Loss: 0.002141390296169303, Test Loss: 0.0014277573723159415\n",
            "Epoch 323/1000, Training Loss: 0.0021290328544725794, Test Loss: 0.0014233835381031357\n",
            "Epoch 324/1000, Training Loss: 0.002116269789760278, Test Loss: 0.0014187917445097247\n",
            "Epoch 325/1000, Training Loss: 0.0021030883433595647, Test Loss: 0.0014139686580627627\n",
            "Epoch 326/1000, Training Loss: 0.00208947725214495, Test Loss: 0.001408901509189897\n",
            "Epoch 327/1000, Training Loss: 0.0020754270697529264, Test Loss: 0.0014035783406315882\n",
            "Epoch 328/1000, Training Loss: 0.002060930478430675, Test Loss: 0.0013979882613911747\n",
            "Epoch 329/1000, Training Loss: 0.002045982575659152, Test Loss: 0.0013921216957229875\n",
            "Epoch 330/1000, Training Loss: 0.002030581118552581, Test Loss: 0.0013859706151732904\n",
            "Epoch 331/1000, Training Loss: 0.002014726709264304, Test Loss: 0.0013795287409399594\n",
            "Epoch 332/1000, Training Loss: 0.0019984229064909185, Test Loss: 0.0013727917040469622\n",
            "Epoch 333/1000, Training Loss: 0.001981676251737186, Test Loss: 0.00136575715220351\n",
            "Epoch 334/1000, Training Loss: 0.00196449620410609, Test Loss: 0.0013584247947735508\n",
            "Epoch 335/1000, Training Loss: 0.0019468949835665386, Test Loss: 0.0013507963808969184\n",
            "Epoch 336/1000, Training Loss: 0.0019288873292566686, Test Loss: 0.0013428756101860977\n",
            "Epoch 337/1000, Training Loss: 0.0019104901856110226, Test Loss: 0.0013346679801321836\n",
            "Epoch 338/1000, Training Loss: 0.001891722334178161, Test Loss: 0.0013261805788611378\n",
            "Epoch 339/1000, Training Loss: 0.0018726039923057664, Test Loss: 0.001317421835655033\n",
            "Epoch 340/1000, Training Loss: 0.0018531564010699554, Test Loss: 0.001308401244249668\n",
            "Epoch 341/1000, Training Loss: 0.0018334014238969437, Test Loss: 0.001299129075066244\n",
            "Epoch 342/1000, Training Loss: 0.001813361174556964, Test Loss: 0.0012896160921713434\n",
            "Epoch 343/1000, Training Loss: 0.0017930576891166177, Test Loss: 0.001279873289039654\n",
            "Epoch 344/1000, Training Loss: 0.0017725126516410692, Test Loss: 0.0012699116544419942\n",
            "Epoch 345/1000, Training Loss: 0.0017517471785585084, Test Loss: 0.0012597419764190266\n",
            "Epoch 346/1000, Training Loss: 0.0017307816621494406, Test Loss: 0.0012493746887670366\n",
            "Epoch 347/1000, Training Loss: 0.0017096356699585137, Test Loss: 0.0012388197611411756\n",
            "Epoch 348/1000, Training Loss: 0.0016883278942287857, Test Loss: 0.0012280866310576772\n",
            "Epoch 349/1000, Training Loss: 0.001666876143755334, Test Loss: 0.0012171841739145285\n",
            "Epoch 350/1000, Training Loss: 0.0016452973697582975, Test Loss: 0.0012061207057013964\n",
            "Epoch 351/1000, Training Loss: 0.0016236077173241825, Test Loss: 0.0011949040122950824\n",
            "Epoch 352/1000, Training Loss: 0.0016018225944683712, Test Loss: 0.00118354139903672\n",
            "Epoch 353/1000, Training Loss: 0.001579956751742353, Test Loss: 0.0011720397545316099\n",
            "Epoch 354/1000, Training Loss: 0.0015580243663805872, Test Loss: 0.0011604056231661055\n",
            "Epoch 355/1000, Training Loss: 0.001536039126121848, Test Loss: 0.0011486452815745844\n",
            "Epoch 356/1000, Training Loss: 0.001514014308951471, Test Loss: 0.0011367648151100911\n",
            "Epoch 357/1000, Training Loss: 0.0014919628560311736, Test Loss: 0.001124770191197283\n",
            "Epoch 358/1000, Training Loss: 0.0014698974359761109, Test Loss: 0.0011126673272218149\n",
            "Epoch 359/1000, Training Loss: 0.0014478304993900482, Test Loss: 0.0011004621513038696\n",
            "Epoch 360/1000, Training Loss: 0.0014257743231791346, Test Loss: 0.0010881606548989236\n",
            "Epoch 361/1000, Training Loss: 0.0014037410446426167, Test Loss: 0.001075768936662273\n",
            "Epoch 362/1000, Training Loss: 0.0013817426856995577, Test Loss: 0.0010632932374093507\n",
            "Epoch 363/1000, Training Loss: 0.0013597911678716703, Test Loss: 0.0010507399663107334\n",
            "Epoch 364/1000, Training Loss: 0.001337898318820431, Test Loss: 0.0010381157186905084\n",
            "Epoch 365/1000, Training Loss: 0.0013160758713483488, Test Loss: 0.001025427285961392\n",
            "Epoch 366/1000, Training Loss: 0.001294335455832976, Test Loss: 0.0010126816583416332\n",
            "Epoch 367/1000, Training Loss: 0.0012726885870808016, Test Loss: 0.000999886021067825\n",
            "Epoch 368/1000, Training Loss: 0.0012511466465760937, Test Loss: 0.0009870477448537963\n",
            "Epoch 369/1000, Training Loss: 0.001229720861065181, Test Loss: 0.0009741743713561228\n",
            "Epoch 370/1000, Training Loss: 0.0012084222783663061, Test Loss: 0.0009612735943984265\n",
            "Epoch 371/1000, Training Loss: 0.001187261741233658, Test Loss: 0.0009483532376837654\n",
            "Epoch 372/1000, Training Loss: 0.001166249860035726, Test Loss: 0.000935421229692\n",
            "Epoch 373/1000, Training Loss: 0.0011453969849355777, Test Loss: 0.0009224855764191102\n",
            "Epoch 374/1000, Training Loss: 0.0011247131781861636, Test Loss: 0.0009095543325709751\n",
            "Epoch 375/1000, Training Loss: 0.0011042081870795434, Test Loss: 0.0008966355717767514\n",
            "Epoch 376/1000, Training Loss: 0.0010838914180155423, Test Loss: 0.00088373735633762\n",
            "Epoch 377/1000, Training Loss: 0.00106377191208469, Test Loss: 0.0008708677069769089\n",
            "Epoch 378/1000, Training Loss: 0.0010438583224923212, Test Loss: 0.0008580345730075816\n",
            "Epoch 379/1000, Training Loss: 0.001024158894086689, Test Loss: 0.0008452458032839323\n",
            "Epoch 380/1000, Training Loss: 0.0010046814451937019, Test Loss: 0.0008325091182560797\n",
            "Epoch 381/1000, Training Loss: 0.0009854333519053163, Test Loss: 0.0008198320833992006\n",
            "Epoch 382/1000, Training Loss: 0.0009664215349173696, Test Loss: 0.0008072220842445437\n",
            "Epoch 383/1000, Training Loss: 0.0009476524489664012, Test Loss: 0.0007946863031965862\n",
            "Epoch 384/1000, Training Loss: 0.0009291320748733875, Test Loss: 0.0007822316982802089\n",
            "Epoch 385/1000, Training Loss: 0.0009108659141654336, Test Loss: 0.0007698649839239156\n",
            "Epoch 386/1000, Training Loss: 0.0008928589862145349, Test Loss: 0.0007575926138500203\n",
            "Epoch 387/1000, Training Loss: 0.0008751158278049316, Test Loss: 0.0007454207661104868\n",
            "Epoch 388/1000, Training Loss: 0.0008576404950174683, Test Loss: 0.0007333553302777009\n",
            "Epoch 389/1000, Training Loss: 0.0008404365673005512, Test Loss: 0.000721401896773228\n",
            "Epoch 390/1000, Training Loss: 0.0008235071535823446, Test Loss: 0.000709565748294384\n",
            "Epoch 391/1000, Training Loss: 0.0008068549002675335, Test Loss: 0.0006978518532780512\n",
            "Epoch 392/1000, Training Loss: 0.000790482000954179, Test Loss: 0.0006862648613239703\n",
            "Epoch 393/1000, Training Loss: 0.0007743902077013438, Test Loss: 0.0006748091004852296\n",
            "Epoch 394/1000, Training Loss: 0.0007585808436761461, Test Loss: 0.0006634885763219607\n",
            "Epoch 395/1000, Training Loss: 0.000743054817009272, Test Loss: 0.0006523069726050368\n",
            "Epoch 396/1000, Training Loss: 0.0007278126356905532, Test Loss: 0.0006412676535499273\n",
            "Epoch 397/1000, Training Loss: 0.0007128544233405048, Test Loss: 0.000630373667456245\n",
            "Epoch 398/1000, Training Loss: 0.0006981799356996703, Test Loss: 0.0006196277516261174\n",
            "Epoch 399/1000, Training Loss: 0.0006837885776846539, Test Loss: 0.0006090323384336438\n",
            "Epoch 400/1000, Training Loss: 0.0006696794208679632, Test Loss: 0.0005985895624188497\n",
            "Epoch 401/1000, Training Loss: 0.0006558512212475411, Test Loss: 0.0005883012682816931\n",
            "Epoch 402/1000, Training Loss: 0.0006423024371814758, Test Loss: 0.0005781690196552628\n",
            "Epoch 403/1000, Training Loss: 0.0006290312473728496, Test Loss: 0.0005681941085416738\n",
            "Epoch 404/1000, Training Loss: 0.0006160355687999473, Test Loss: 0.0005583775652996921\n",
            "Epoch 405/1000, Training Loss: 0.0006033130744967471, Test Loss: 0.0005487201690788194\n",
            "Epoch 406/1000, Training Loss: 0.0005908612110985644, Test Loss: 0.0005392224586011256\n",
            "Epoch 407/1000, Training Loss: 0.0005786772160773643, Test Loss: 0.0005298847431988422\n",
            "Epoch 408/1000, Training Loss: 0.0005667581346004847, Test Loss: 0.0005207071140226624\n",
            "Epoch 409/1000, Training Loss: 0.0005551008359554616, Test Loss: 0.0005116894553426354\n",
            "Epoch 410/1000, Training Loss: 0.0005437020294921843, Test Loss: 0.0005028314558707197\n",
            "Epoch 411/1000, Training Loss: 0.0005325582800414922, Test Loss: 0.0004941326200408225\n",
            "Epoch 412/1000, Training Loss: 0.0005216660227768894, Test Loss: 0.00048559227918900207\n",
            "Epoch 413/1000, Training Loss: 0.0005110215774929419, Test Loss: 0.00047720960258286385\n",
            "Epoch 414/1000, Training Loss: 0.0005006211622803265, Test Loss: 0.000468983608255608\n",
            "Epoch 415/1000, Training Loss: 0.0004904609065833692, Test Loss: 0.000460913173605926\n",
            "Epoch 416/1000, Training Loss: 0.0004805368636311883, Test Loss: 0.00045299704573057147\n",
            "Epoch 417/1000, Training Loss: 0.0004708450222383693, Test Loss: 0.0004452338514616587\n",
            "Epoch 418/1000, Training Loss: 0.00046138131797541403, Test Loss: 0.0004376221070855914\n",
            "Epoch 419/1000, Training Loss: 0.0004521416437128953, Test Loss: 0.0004301602277248684\n",
            "Epoch 420/1000, Training Loss: 0.00044312185954672686, Test Loss: 0.0004228465363683242\n",
            "Epoch 421/1000, Training Loss: 0.00043431780211473177, Test Loss: 0.0004156792725388405\n",
            "Epoch 422/1000, Training Loss: 0.00042572529331729695, Test Loss: 0.0004086566005911795\n",
            "Epoch 423/1000, Training Loss: 0.00041734014845691087, Test Loss: 0.0004017766176354064\n",
            "Epoch 424/1000, Training Loss: 0.00040915818381324166, Test Loss: 0.0003950373610842356\n",
            "Epoch 425/1000, Training Loss: 0.00040117522367180057, Test Loss: 0.0003884368158249576\n",
            "Epoch 426/1000, Training Loss: 0.000393387106825409, Test Loss: 0.0003819729210187077\n",
            "Epoch 427/1000, Training Loss: 0.00038578969256857653, Test Loss: 0.00037564357653167705\n",
            "Epoch 428/1000, Training Loss: 0.00037837886620554416, Test Loss: 0.0003694466490045075\n",
            "Epoch 429/1000, Training Loss: 0.0003711505440931634, Test Loss: 0.0003633799775673318\n",
            "Epoch 430/1000, Training Loss: 0.0003641006782400816, Test Loss: 0.00035744137920923737\n",
            "Epoch 431/1000, Training Loss: 0.0003572252604836874, Test Loss: 0.0003516286538116471\n",
            "Epoch 432/1000, Training Loss: 0.0003505203262662893, Test Loss: 0.0003459395888560898\n",
            "Epoch 433/1000, Training Loss: 0.0003439819580317348, Test Loss: 0.0003403719638173201\n",
            "Epoch 434/1000, Training Loss: 0.00033760628826343114, Test Loss: 0.0003349235542531791\n",
            "Epoch 435/1000, Training Loss: 0.0003313895021842359, Test Loss: 0.0003295921356030412\n",
            "Epoch 436/1000, Training Loss: 0.00032532784013829957, Test Loss: 0.00032437548670676415\n",
            "Epoch 437/1000, Training Loss: 0.00031941759967433, Test Loss: 0.0003192713930562911\n",
            "Epoch 438/1000, Training Loss: 0.000313655137349148, Test Loss: 0.00031427764979198114\n",
            "Epoch 439/1000, Training Loss: 0.000308036870269827, Test Loss: 0.0003093920644558307\n",
            "Epoch 440/1000, Training Loss: 0.0003025592773919591, Test Loss: 0.00030461245951346727\n",
            "Epoch 441/1000, Training Loss: 0.0002972189005909037, Test Loss: 0.00029993667465672415\n",
            "Epoch 442/1000, Training Loss: 0.0002920123455222404, Test Loss: 0.0002953625688984242\n",
            "Epoch 443/1000, Training Loss: 0.00028693628228679784, Test Loss: 0.0002908880224705931\n",
            "Epoch 444/1000, Training Loss: 0.0002819874459150232, Test Loss: 0.0002865109385371685\n",
            "Epoch 445/1000, Training Loss: 0.00027716263668468616, Test Loss: 0.00028222924473191176\n",
            "Epoch 446/1000, Training Loss: 0.00027245872028518105, Test Loss: 0.0002780408945318223\n",
            "Epoch 447/1000, Training Loss: 0.00026787262784103876, Test Loss: 0.00027394386847610724\n",
            "Epoch 448/1000, Training Loss: 0.00026340135580649497, Test Loss: 0.000269936175240245\n",
            "Epoch 449/1000, Training Loss: 0.0002590419657424112, Test Loss: 0.0002660158525745054\n",
            "Epoch 450/1000, Training Loss: 0.0002547915839860481, Test Loss: 0.00026218096811567176\n",
            "Epoch 451/1000, Training Loss: 0.0002506474012236547, Test Loss: 0.0002584296200805061\n",
            "Epoch 452/1000, Training Loss: 0.000246606671975241, Test Loss: 0.0002547599378490716\n",
            "Epoch 453/1000, Training Loss: 0.00024266671400017628, Test Loss: 0.00025117008244553146\n",
            "Epoch 454/1000, Training Loss: 0.00023882490763192586, Test Loss: 0.0002476582469239131\n",
            "Epoch 455/1000, Training Loss: 0.00023507869504940752, Test Loss: 0.0002442226566656373\n",
            "Epoch 456/1000, Training Loss: 0.00023142557949224797, Test Loss: 0.00024086156959562214\n",
            "Epoch 457/1000, Training Loss: 0.00022786312442643985, Test Loss: 0.0002375732763230375\n",
            "Epoch 458/1000, Training Loss: 0.00022438895266661997, Test Loss: 0.000234356100212758\n",
            "Epoch 459/1000, Training Loss: 0.00022100074546069708, Test Loss: 0.00023120839739307838\n",
            "Epoch 460/1000, Training Loss: 0.00021769624154209183, Test Loss: 0.00022812855670493746\n",
            "Epoch 461/1000, Training Loss: 0.00021447323615451567, Test Loss: 0.0002251149995976637\n",
            "Epoch 462/1000, Training Loss: 0.00021132958005380234, Test Loss: 0.00022216617997589501\n",
            "Epoch 463/1000, Training Loss: 0.0002082631784909436, Test Loss: 0.00021928058400203478\n",
            "Epoch 464/1000, Training Loss: 0.0002052719901802118, Test Loss: 0.00021645672985842979\n",
            "Epoch 465/1000, Training Loss: 0.00020235402625584737, Test Loss: 0.00021369316747309666\n",
            "Epoch 466/1000, Training Loss: 0.00019950734922055758, Test Loss: 0.00021098847821257602\n",
            "Epoch 467/1000, Training Loss: 0.0001967300718887696, Test Loss: 0.0002083412745453532\n",
            "Epoch 468/1000, Training Loss: 0.0001940203563273461, Test Loss: 0.00020575019967895746\n",
            "Epoch 469/1000, Training Loss: 0.00019137641279620346, Test Loss: 0.00020321392717369749\n",
            "Epoch 470/1000, Training Loss: 0.00018879649869106305, Test Loss: 0.00020073116053578172\n",
            "Epoch 471/1000, Training Loss: 0.0001862789174903561, Test Loss: 0.00019830063279234068\n",
            "Epoch 472/1000, Training Loss: 0.00018382201770807648, Test Loss: 0.00019592110605073574\n",
            "Epoch 473/1000, Training Loss: 0.00018142419185426258, Test Loss: 0.0001935913710443493\n",
            "Epoch 474/1000, Training Loss: 0.00017908387540454244, Test Loss: 0.0001913102466669075\n",
            "Epoch 475/1000, Training Loss: 0.00017679954578006066, Test Loss: 0.0001890765794971706\n",
            "Epoch 476/1000, Training Loss: 0.0001745697213389548, Test Loss: 0.00018688924331576015\n",
            "Epoch 477/1000, Training Loss: 0.00017239296038041728, Test Loss: 0.0001847471386157347\n",
            "Epoch 478/1000, Training Loss: 0.00017026786016223434, Test Loss: 0.00018264919210836348\n",
            "Epoch 479/1000, Training Loss: 0.00016819305593260313, Test Loss: 0.00018059435622545678\n",
            "Epoch 480/1000, Training Loss: 0.0001661672199769114, Test Loss: 0.00017858160861953073\n",
            "Epoch 481/1000, Training Loss: 0.00016418906068005307, Test Loss: 0.00017660995166289287\n",
            "Epoch 482/1000, Training Loss: 0.000162257321604793, Test Loss: 0.0001746784119467271\n",
            "Epoch 483/1000, Training Loss: 0.00016037078058658082, Test Loss: 0.00017278603978111473\n",
            "Epoch 484/1000, Training Loss: 0.00015852824884517714, Test Loss: 0.00017093190869688708\n",
            "Epoch 485/1000, Training Loss: 0.0001567285701133066, Test Loss: 0.00016911511494999406\n",
            "Epoch 486/1000, Training Loss: 0.00015497061978260977, Test Loss: 0.00016733477702923243\n",
            "Epoch 487/1000, Training Loss: 0.00015325330406700438, Test Loss: 0.00016559003516788097\n",
            "Epoch 488/1000, Training Loss: 0.00015157555918354914, Test Loss: 0.00016388005085982716\n",
            "Epoch 489/1000, Training Loss: 0.00014993635055087382, Test Loss: 0.00016220400638073648\n",
            "Epoch 490/1000, Training Loss: 0.00014833467200517502, Test Loss: 0.0001605611043146937\n",
            "Epoch 491/1000, Training Loss: 0.00014676954503373246, Test Loss: 0.00015895056708669882\n",
            "Epoch 492/1000, Training Loss: 0.000145240018025895, Test Loss: 0.00015737163650144937\n",
            "Epoch 493/1000, Training Loss: 0.00014374516554142155, Test Loss: 0.00015582357328863795\n",
            "Epoch 494/1000, Training Loss: 0.00014228408759605235, Test Loss: 0.00015430565665509616\n",
            "Epoch 495/1000, Training Loss: 0.00014085590896415222, Test Loss: 0.00015281718384398246\n",
            "Epoch 496/1000, Training Loss: 0.0001394597784982517, Test Loss: 0.00015135746970123\n",
            "Epoch 497/1000, Training Loss: 0.00013809486846528128, Test Loss: 0.00014992584624941294\n",
            "Epoch 498/1000, Training Loss: 0.00013676037389930926, Test Loss: 0.0001485216622691941\n",
            "Epoch 499/1000, Training Loss: 0.00013545551197050897, Test Loss: 0.00014714428288839648\n",
            "Epoch 500/1000, Training Loss: 0.00013417952137015967, Test Loss: 0.00014579308917886456\n",
            "Epoch 501/1000, Training Loss: 0.00013293166171138618, Test Loss: 0.00014446747776110833\n",
            "Epoch 502/1000, Training Loss: 0.00013171121294540484, Test Loss: 0.00014316686041681754\n",
            "Epoch 503/1000, Training Loss: 0.00013051747479297647, Test Loss: 0.00014189066370922783\n",
            "Epoch 504/1000, Training Loss: 0.00012934976619080415, Test Loss: 0.00014063832861136753\n",
            "Epoch 505/1000, Training Loss: 0.0001282074247525984, Test Loss: 0.0001394093101421841\n",
            "Epoch 506/1000, Training Loss: 0.00012708980624448227, Test Loss: 0.00013820307701045256\n",
            "Epoch 507/1000, Training Loss: 0.00012599628407449404, Test Loss: 0.00013701911126652968\n",
            "Epoch 508/1000, Training Loss: 0.00012492624879584466, Test Loss: 0.00013585690796179772\n",
            "Epoch 509/1000, Training Loss: 0.00012387910762368974, Test Loss: 0.00013471597481583595\n",
            "Epoch 510/1000, Training Loss: 0.0001228542839650606, Test Loss: 0.00013359583189114985\n",
            "Epoch 511/1000, Training Loss: 0.00012185121696169836, Test Loss: 0.0001324960112754458\n",
            "Epoch 512/1000, Training Loss: 0.00012086936104547684, Test Loss: 0.00013141605677131956\n",
            "Epoch 513/1000, Training Loss: 0.00011990818550613718, Test Loss: 0.00013035552359332836\n",
            "Epoch 514/1000, Training Loss: 0.00011896717407102532, Test Loss: 0.0001293139780722506\n",
            "Epoch 515/1000, Training Loss: 0.00011804582449654532, Test Loss: 0.00012829099736650214\n",
            "Epoch 516/1000, Training Loss: 0.00011714364817104968, Test Loss: 0.00012728616918056536\n",
            "Epoch 517/1000, Training Loss: 0.00011626016972888802, Test Loss: 0.00012629909149035424\n",
            "Epoch 518/1000, Training Loss: 0.00011539492667531153, Test Loss: 0.00012532937227532058\n",
            "Epoch 519/1000, Training Loss: 0.00011454746902198328, Test Loss: 0.00012437662925728253\n",
            "Epoch 520/1000, Training Loss: 0.00011371735893280407, Test Loss: 0.00012344048964577425\n",
            "Epoch 521/1000, Training Loss: 0.00011290417037979625, Test Loss: 0.0001225205898898235\n",
            "Epoch 522/1000, Training Loss: 0.00011210748880878054, Test Loss: 0.00012161657543604843\n",
            "Epoch 523/1000, Training Loss: 0.00011132691081458178, Test Loss: 0.00012072810049289966\n",
            "Epoch 524/1000, Training Loss: 0.00011056204382551582, Test Loss: 0.00011985482780098663\n",
            "Epoch 525/1000, Training Loss: 0.00010981250579691051, Test Loss: 0.00011899642840931125\n",
            "Epoch 526/1000, Training Loss: 0.00010907792491340227, Test Loss: 0.00011815258145728553\n",
            "Epoch 527/1000, Training Loss: 0.00010835793929979817, Test Loss: 0.000117322973962452\n",
            "Epoch 528/1000, Training Loss: 0.00010765219674023098, Test Loss: 0.0001165073006137162\n",
            "Epoch 529/1000, Training Loss: 0.00010696035440540888, Test Loss: 0.00011570526357001803\n",
            "Epoch 530/1000, Training Loss: 0.00010628207858773252, Test Loss: 0.00011491657226431185\n",
            "Epoch 531/1000, Training Loss: 0.00010561704444403729, Test Loss: 0.00011414094321269176\n",
            "Epoch 532/1000, Training Loss: 0.00010496493574577676, Test Loss: 0.00011337809982859107\n",
            "Epoch 533/1000, Training Loss: 0.00010432544463641741, Test Loss: 0.00011262777224190868\n",
            "Epoch 534/1000, Training Loss: 0.00010369827139585553, Test Loss: 0.00011188969712294841\n",
            "Epoch 535/1000, Training Loss: 0.00010308312421164584, Test Loss: 0.00011116361751106362\n",
            "Epoch 536/1000, Training Loss: 0.00010247971895685246, Test Loss: 0.00011044928264786707\n",
            "Epoch 537/1000, Training Loss: 0.00010188777897433829, Test Loss: 0.00010974644781492886\n",
            "Epoch 538/1000, Training Loss: 0.00010130703486730018, Test Loss: 0.00010905487417581586\n",
            "Epoch 539/1000, Training Loss: 0.00010073722429587713, Test Loss: 0.00010837432862239574\n",
            "Epoch 540/1000, Training Loss: 0.0001001780917796502, Test Loss: 0.00010770458362525762\n",
            "Epoch 541/1000, Training Loss: 9.962938850587159e-05, Test Loss: 0.00010704541708818626\n",
            "Epoch 542/1000, Training Loss: 9.909087214324917e-05, Test Loss: 0.00010639661220654748\n",
            "Epoch 543/1000, Training Loss: 9.856230666113489e-05, Test Loss: 0.00010575795732953328\n",
            "Epoch 544/1000, Training Loss: 9.804346215394792e-05, Test Loss: 0.00010512924582608655\n",
            "Epoch 545/1000, Training Loss: 9.753411467069388e-05, Test Loss: 0.00010451027595450285\n",
            "Epoch 546/1000, Training Loss: 9.70340460494143e-05, Test Loss: 0.00010390085073554076\n",
            "Epoch 547/1000, Training Loss: 9.654304375644407e-05, Test Loss: 0.00010330077782898074\n",
            "Epoch 548/1000, Training Loss: 9.606090073031282e-05, Test Loss: 0.00010270986941352802\n",
            "Epoch 549/1000, Training Loss: 9.558741523017437e-05, Test Loss: 0.00010212794206999021\n",
            "Epoch 550/1000, Training Loss: 9.512239068861968e-05, Test Loss: 0.00010155481666761446\n",
            "Epoch 551/1000, Training Loss: 9.466563556874114e-05, Test Loss: 0.00010099031825350593\n",
            "Epoch 552/1000, Training Loss: 9.421696322533957e-05, Test Loss: 0.00010043427594506856\n",
            "Epoch 553/1000, Training Loss: 9.377619177013735e-05, Test Loss: 9.988652282536368e-05\n",
            "Epoch 554/1000, Training Loss: 9.334314394088073e-05, Test Loss: 9.934689584129442e-05\n",
            "Epoch 555/1000, Training Loss: 9.291764697422631e-05, Test Loss: 9.881523570458098e-05\n",
            "Epoch 556/1000, Training Loss: 9.249953248228888e-05, Test Loss: 9.829138679540869e-05\n",
            "Epoch 557/1000, Training Loss: 9.208863633274815e-05, Test Loss: 9.777519706869256e-05\n",
            "Epoch 558/1000, Training Loss: 9.16847985324136e-05, Test Loss: 9.726651796289617e-05\n",
            "Epoch 559/1000, Training Loss: 9.128786311413252e-05, Test Loss: 9.676520431132208e-05\n",
            "Epoch 560/1000, Training Loss: 9.089767802695497e-05, Test Loss: 9.62711142558005e-05\n",
            "Epoch 561/1000, Training Loss: 9.051409502945438e-05, Test Loss: 9.578410916272381e-05\n",
            "Epoch 562/1000, Training Loss: 9.013696958611262e-05, Test Loss: 9.530405354136497e-05\n",
            "Epoch 563/1000, Training Loss: 8.976616076666679e-05, Test Loss: 9.483081496438009e-05\n",
            "Epoch 564/1000, Training Loss: 8.940153114835808e-05, Test Loss: 9.436426399049862e-05\n",
            "Epoch 565/1000, Training Loss: 8.904294672096038e-05, Test Loss: 9.39042740892713e-05\n",
            "Epoch 566/1000, Training Loss: 8.869027679453848e-05, Test Loss: 9.345072156787173e-05\n",
            "Epoch 567/1000, Training Loss: 8.834339390983909e-05, Test Loss: 9.3003485499872e-05\n",
            "Epoch 568/1000, Training Loss: 8.800217375123539e-05, Test Loss: 9.25624476559252e-05\n",
            "Epoch 569/1000, Training Loss: 8.766649506215826e-05, Test Loss: 9.212749243633396e-05\n",
            "Epoch 570/1000, Training Loss: 8.733623956293487e-05, Test Loss: 9.16985068054182e-05\n",
            "Epoch 571/1000, Training Loss: 8.701129187096662e-05, Test Loss: 9.12753802276563e-05\n",
            "Epoch 572/1000, Training Loss: 8.669153942317494e-05, Test Loss: 9.085800460554697e-05\n",
            "Epoch 573/1000, Training Loss: 8.637687240064727e-05, Test Loss: 9.044627421912574e-05\n",
            "Epoch 574/1000, Training Loss: 8.606718365542381e-05, Test Loss: 9.004008566710818e-05\n",
            "Epoch 575/1000, Training Loss: 8.576236863935626e-05, Test Loss: 8.963933780961894e-05\n",
            "Epoch 576/1000, Training Loss: 8.546232533497576e-05, Test Loss: 8.924393171241837e-05\n",
            "Epoch 577/1000, Training Loss: 8.516695418832414e-05, Test Loss: 8.885377059267588e-05\n",
            "Epoch 578/1000, Training Loss: 8.487615804367041e-05, Test Loss: 8.846875976612714e-05\n",
            "Epoch 579/1000, Training Loss: 8.458984208007653e-05, Test Loss: 8.80888065956904e-05\n",
            "Epoch 580/1000, Training Loss: 8.430791374973898e-05, Test Loss: 8.771382044140659e-05\n",
            "Epoch 581/1000, Training Loss: 8.403028271807984e-05, Test Loss: 8.734371261174557e-05\n",
            "Epoch 582/1000, Training Loss: 8.375686080550125e-05, Test Loss: 8.697839631617453e-05\n",
            "Epoch 583/1000, Training Loss: 8.348756193079483e-05, Test Loss: 8.66177866190109e-05\n",
            "Epoch 584/1000, Training Loss: 8.322230205611635e-05, Test Loss: 8.626180039447289e-05\n",
            "Epoch 585/1000, Training Loss: 8.296099913351149e-05, Test Loss: 8.591035628291563e-05\n",
            "Epoch 586/1000, Training Loss: 8.270357305293476e-05, Test Loss: 8.556337464824366e-05\n",
            "Epoch 587/1000, Training Loss: 8.244994559171876e-05, Test Loss: 8.522077753643332e-05\n",
            "Epoch 588/1000, Training Loss: 8.220004036544725e-05, Test Loss: 8.488248863513259e-05\n",
            "Epoch 589/1000, Training Loss: 8.195378278019932e-05, Test Loss: 8.454843323433728e-05\n",
            "Epoch 590/1000, Training Loss: 8.171109998612012e-05, Test Loss: 8.42185381880962e-05\n",
            "Epoch 591/1000, Training Loss: 8.147192083228253e-05, Test Loss: 8.389273187721342e-05\n",
            "Epoch 592/1000, Training Loss: 8.123617582279234e-05, Test Loss: 8.357094417291657e-05\n",
            "Epoch 593/1000, Training Loss: 8.100379707411644e-05, Test Loss: 8.32531064014928e-05\n",
            "Epoch 594/1000, Training Loss: 8.077471827358835e-05, Test Loss: 8.293915130983066e-05\n",
            "Epoch 595/1000, Training Loss: 8.054887463905299e-05, Test Loss: 8.262901303185746e-05\n",
            "Epoch 596/1000, Training Loss: 8.032620287963421e-05, Test Loss: 8.2322627055852e-05\n",
            "Epoch 597/1000, Training Loss: 8.010664115757039e-05, Test Loss: 8.201993019260109e-05\n",
            "Epoch 598/1000, Training Loss: 7.989012905111197e-05, Test Loss: 8.172086054438882e-05\n",
            "Epoch 599/1000, Training Loss: 7.967660751841913e-05, Test Loss: 8.142535747475557e-05\n",
            "Epoch 600/1000, Training Loss: 7.946601886246786e-05, Test Loss: 8.113336157908087e-05\n",
            "Epoch 601/1000, Training Loss: 7.925830669690685e-05, Test Loss: 8.084481465588842e-05\n",
            "Epoch 602/1000, Training Loss: 7.90534159128444e-05, Test Loss: 8.055965967889555e-05\n",
            "Epoch 603/1000, Training Loss: 7.885129264654462e-05, Test Loss: 8.027784076978262e-05\n",
            "Epoch 604/1000, Training Loss: 7.86518842480069e-05, Test Loss: 7.999930317166062e-05\n",
            "Epoch 605/1000, Training Loss: 7.845513925039537e-05, Test Loss: 7.972399322321131e-05\n",
            "Epoch 606/1000, Training Loss: 7.826100734029841e-05, Test Loss: 7.945185833347954e-05\n",
            "Epoch 607/1000, Training Loss: 7.806943932879787e-05, Test Loss: 7.918284695732296e-05\n",
            "Epoch 608/1000, Training Loss: 7.788038712331952e-05, Test Loss: 7.891690857146989e-05\n",
            "Epoch 609/1000, Training Loss: 7.769380370024461e-05, Test Loss: 7.865399365119291e-05\n",
            "Epoch 610/1000, Training Loss: 7.75096430782591e-05, Test Loss: 7.839405364756037e-05\n",
            "Epoch 611/1000, Training Loss: 7.732786029242484e-05, Test Loss: 7.813704096527967e-05\n",
            "Epoch 612/1000, Training Loss: 7.714841136894511e-05, Test Loss: 7.788290894107784e-05\n",
            "Epoch 613/1000, Training Loss: 7.697125330060933e-05, Test Loss: 7.763161182263872e-05\n",
            "Epoch 614/1000, Training Loss: 7.679634402289618e-05, Test Loss: 7.7383104748069e-05\n",
            "Epoch 615/1000, Training Loss: 7.662364239071808e-05, Test Loss: 7.713734372586917e-05\n",
            "Epoch 616/1000, Training Loss: 7.64531081557838e-05, Test Loss: 7.68942856154115e-05\n",
            "Epoch 617/1000, Training Loss: 7.628470194456831e-05, Test Loss: 7.665388810789986e-05\n",
            "Epoch 618/1000, Training Loss: 7.611838523687101e-05, Test Loss: 7.641610970781013e-05\n",
            "Epoch 619/1000, Training Loss: 7.595412034494108e-05, Test Loss: 7.618090971478177e-05\n",
            "Epoch 620/1000, Training Loss: 7.579187039315923e-05, Test Loss: 7.59482482059694e-05\n",
            "Epoch 621/1000, Training Loss: 7.563159929825721e-05, Test Loss: 7.571808601881647e-05\n",
            "Epoch 622/1000, Training Loss: 7.54732717500629e-05, Test Loss: 7.549038473426689e-05\n",
            "Epoch 623/1000, Training Loss: 7.531685319275388e-05, Test Loss: 7.5265106660391e-05\n",
            "Epoch 624/1000, Training Loss: 7.516230980660562e-05, Test Loss: 7.50422148163937e-05\n",
            "Epoch 625/1000, Training Loss: 7.500960849021824e-05, Test Loss: 7.482167291703635e-05\n",
            "Epoch 626/1000, Training Loss: 7.485871684321495e-05, Test Loss: 7.460344535743365e-05\n",
            "Epoch 627/1000, Training Loss: 7.470960314939162e-05, Test Loss: 7.438749719821393e-05\n",
            "Epoch 628/1000, Training Loss: 7.45622363603096e-05, Test Loss: 7.417379415105156e-05\n",
            "Epoch 629/1000, Training Loss: 7.441658607931742e-05, Test Loss: 7.396230256454408e-05\n",
            "Epoch 630/1000, Training Loss: 7.427262254599186e-05, Test Loss: 7.375298941043843e-05\n",
            "Epoch 631/1000, Training Loss: 7.413031662098047e-05, Test Loss: 7.354582227018162e-05\n",
            "Epoch 632/1000, Training Loss: 7.398963977124427e-05, Test Loss: 7.334076932180111e-05\n",
            "Epoch 633/1000, Training Loss: 7.385056405567896e-05, Test Loss: 7.313779932710495e-05\n",
            "Epoch 634/1000, Training Loss: 7.371306211111252e-05, Test Loss: 7.293688161917833e-05\n",
            "Epoch 635/1000, Training Loss: 7.35771071386666e-05, Test Loss: 7.27379860902006e-05\n",
            "Epoch 636/1000, Training Loss: 7.344267289046414e-05, Test Loss: 7.254108317952639e-05\n",
            "Epoch 637/1000, Training Loss: 7.330973365669163e-05, Test Loss: 7.234614386207986e-05\n",
            "Epoch 638/1000, Training Loss: 7.317826425297972e-05, Test Loss: 7.215313963699921e-05\n",
            "Epoch 639/1000, Training Loss: 7.304824000812294e-05, Test Loss: 7.196204251657384e-05\n",
            "Epoch 640/1000, Training Loss: 7.291963675210617e-05, Test Loss: 7.177282501542839e-05\n",
            "Epoch 641/1000, Training Loss: 7.27924308044394e-05, Test Loss: 7.158546013997579e-05\n",
            "Epoch 642/1000, Training Loss: 7.266659896279295e-05, Test Loss: 7.139992137810292e-05\n",
            "Epoch 643/1000, Training Loss: 7.254211849192256e-05, Test Loss: 7.121618268911772e-05\n",
            "Epoch 644/1000, Training Loss: 7.241896711287477e-05, Test Loss: 7.103421849391401e-05\n",
            "Epoch 645/1000, Training Loss: 7.229712299247031e-05, Test Loss: 7.085400366539454e-05\n",
            "Epoch 646/1000, Training Loss: 7.21765647330496e-05, Test Loss: 7.067551351908031e-05\n",
            "Epoch 647/1000, Training Loss: 7.20572713624854e-05, Test Loss: 7.04987238039708e-05\n",
            "Epoch 648/1000, Training Loss: 7.193922232444307e-05, Test Loss: 7.032361069361308e-05\n",
            "Epoch 649/1000, Training Loss: 7.182239746888645e-05, Test Loss: 7.01501507773543e-05\n",
            "Epoch 650/1000, Training Loss: 7.170677704282781e-05, Test Loss: 6.997832105183497e-05\n",
            "Epoch 651/1000, Training Loss: 7.159234168130474e-05, Test Loss: 6.98080989126426e-05\n",
            "Epoch 652/1000, Training Loss: 7.14790723985885e-05, Test Loss: 6.963946214618143e-05\n",
            "Epoch 653/1000, Training Loss: 7.136695057961319e-05, Test Loss: 6.947238892172856e-05\n",
            "Epoch 654/1000, Training Loss: 7.125595797161281e-05, Test Loss: 6.93068577836531e-05\n",
            "Epoch 655/1000, Training Loss: 7.114607667597533e-05, Test Loss: 6.914284764384339e-05\n",
            "Epoch 656/1000, Training Loss: 7.103728914029487e-05, Test Loss: 6.89803377742787e-05\n",
            "Epoch 657/1000, Training Loss: 7.092957815062455e-05, Test Loss: 6.881930779979281e-05\n",
            "Epoch 658/1000, Training Loss: 7.082292682391967e-05, Test Loss: 6.865973769099457e-05\n",
            "Epoch 659/1000, Training Loss: 7.0717318600669e-05, Test Loss: 6.8501607757339e-05\n",
            "Epoch 660/1000, Training Loss: 7.061273723770858e-05, Test Loss: 6.834489864037061e-05\n",
            "Epoch 661/1000, Training Loss: 7.050916680121336e-05, Test Loss: 6.818959130711581e-05\n",
            "Epoch 662/1000, Training Loss: 7.040659165986001e-05, Test Loss: 6.803566704361739e-05\n",
            "Epoch 663/1000, Training Loss: 7.03049964781617e-05, Test Loss: 6.788310744862713e-05\n",
            "Epoch 664/1000, Training Loss: 7.02043662099602e-05, Test Loss: 6.773189442742803e-05\n",
            "Epoch 665/1000, Training Loss: 7.010468609208404e-05, Test Loss: 6.758201018579371e-05\n",
            "Epoch 666/1000, Training Loss: 7.000594163815833e-05, Test Loss: 6.743343722410733e-05\n",
            "Epoch 667/1000, Training Loss: 6.990811863256544e-05, Test Loss: 6.728615833157405e-05\n",
            "Epoch 668/1000, Training Loss: 6.981120312455496e-05, Test Loss: 6.714015658059313e-05\n",
            "Epoch 669/1000, Training Loss: 6.971518142249425e-05, Test Loss: 6.699541532124044e-05\n",
            "Epoch 670/1000, Training Loss: 6.962004008825994e-05, Test Loss: 6.685191817587527e-05\n",
            "Epoch 671/1000, Training Loss: 6.952576593176607e-05, Test Loss: 6.6709649033869e-05\n",
            "Epoch 672/1000, Training Loss: 6.943234600562137e-05, Test Loss: 6.656859204644761e-05\n",
            "Epoch 673/1000, Training Loss: 6.933976759992004e-05, Test Loss: 6.642873162165318e-05\n",
            "Epoch 674/1000, Training Loss: 6.924801823715259e-05, Test Loss: 6.629005241940027e-05\n",
            "Epoch 675/1000, Training Loss: 6.915708566724423e-05, Test Loss: 6.61525393466674e-05\n",
            "Epoch 676/1000, Training Loss: 6.90669578627081e-05, Test Loss: 6.601617755276516e-05\n",
            "Epoch 677/1000, Training Loss: 6.897762301391493e-05, Test Loss: 6.588095242472601e-05\n",
            "Epoch 678/1000, Training Loss: 6.888906952447833e-05, Test Loss: 6.574684958278631e-05\n",
            "Epoch 679/1000, Training Loss: 6.880128600674659e-05, Test Loss: 6.56138548759682e-05\n",
            "Epoch 680/1000, Training Loss: 6.871426127740416e-05, Test Loss: 6.548195437775953e-05\n",
            "Epoch 681/1000, Training Loss: 6.86279843531767e-05, Test Loss: 6.535113438188513e-05\n",
            "Epoch 682/1000, Training Loss: 6.854244444663548e-05, Test Loss: 6.522138139816456e-05\n",
            "Epoch 683/1000, Training Loss: 6.845763096210572e-05, Test Loss: 6.509268214846757e-05\n",
            "Epoch 684/1000, Training Loss: 6.837353349166638e-05, Test Loss: 6.496502356274992e-05\n",
            "Epoch 685/1000, Training Loss: 6.829014181124782e-05, Test Loss: 6.483839277518205e-05\n",
            "Epoch 686/1000, Training Loss: 6.82074458768179e-05, Test Loss: 6.471277712033912e-05\n",
            "Epoch 687/1000, Training Loss: 6.812543582066028e-05, Test Loss: 6.458816412950217e-05\n",
            "Epoch 688/1000, Training Loss: 6.80441019477355e-05, Test Loss: 6.44645415270131e-05\n",
            "Epoch 689/1000, Training Loss: 6.796343473213363e-05, Test Loss: 6.434189722672675e-05\n",
            "Epoch 690/1000, Training Loss: 6.788342481360224e-05, Test Loss: 6.422021932851884e-05\n",
            "Epoch 691/1000, Training Loss: 6.780406299415963e-05, Test Loss: 6.409949611487567e-05\n",
            "Epoch 692/1000, Training Loss: 6.772534023478356e-05, Test Loss: 6.397971604756445e-05\n",
            "Epoch 693/1000, Training Loss: 6.76472476521799e-05, Test Loss: 6.386086776435355e-05\n",
            "Epoch 694/1000, Training Loss: 6.756977651562288e-05, Test Loss: 6.374294007582868e-05\n",
            "Epoch 695/1000, Training Loss: 6.749291824386821e-05, Test Loss: 6.36259219622429e-05\n",
            "Epoch 696/1000, Training Loss: 6.74166644021385e-05, Test Loss: 6.350980257046272e-05\n",
            "Epoch 697/1000, Training Loss: 6.734100669917601e-05, Test Loss: 6.339457121095143e-05\n",
            "Epoch 698/1000, Training Loss: 6.726593698436666e-05, Test Loss: 6.328021735484299e-05\n",
            "Epoch 699/1000, Training Loss: 6.719144724492274e-05, Test Loss: 6.316673063104565e-05\n",
            "Epoch 700/1000, Training Loss: 6.711752960313583e-05, Test Loss: 6.305410082342701e-05\n",
            "Epoch 701/1000, Training Loss: 6.704417631368997e-05, Test Loss: 6.294231786805567e-05\n",
            "Epoch 702/1000, Training Loss: 6.697137976103442e-05, Test Loss: 6.283137185048474e-05\n",
            "Epoch 703/1000, Training Loss: 6.68991324568178e-05, Test Loss: 6.27212530031079e-05\n",
            "Epoch 704/1000, Training Loss: 6.682742703737846e-05, Test Loss: 6.261195170255749e-05\n",
            "Epoch 705/1000, Training Loss: 6.6756256261292e-05, Test Loss: 6.250345846716244e-05\n",
            "Epoch 706/1000, Training Loss: 6.668561300697578e-05, Test Loss: 6.239576395445878e-05\n",
            "Epoch 707/1000, Training Loss: 6.661549027034103e-05, Test Loss: 6.228885895873439e-05\n",
            "Epoch 708/1000, Training Loss: 6.654588116250503e-05, Test Loss: 6.218273440865677e-05\n",
            "Epoch 709/1000, Training Loss: 6.647677890754879e-05, Test Loss: 6.207738136490815e-05\n",
            "Epoch 710/1000, Training Loss: 6.640817684032647e-05, Test Loss: 6.197279101790082e-05\n",
            "Epoch 711/1000, Training Loss: 6.634006840432426e-05, Test Loss: 6.18689546855219e-05\n",
            "Epoch 712/1000, Training Loss: 6.627244714956403e-05, Test Loss: 6.17658638109267e-05\n",
            "Epoch 713/1000, Training Loss: 6.620530673055649e-05, Test Loss: 6.166350996037987e-05\n",
            "Epoch 714/1000, Training Loss: 6.613864090429603e-05, Test Loss: 6.156188482112982e-05\n",
            "Epoch 715/1000, Training Loss: 6.6072443528303e-05, Test Loss: 6.146098019933939e-05\n",
            "Epoch 716/1000, Training Loss: 6.600670855870655e-05, Test Loss: 6.136078801804201e-05\n",
            "Epoch 717/1000, Training Loss: 6.594143004836962e-05, Test Loss: 6.12613003151537e-05\n",
            "Epoch 718/1000, Training Loss: 6.587660214505481e-05, Test Loss: 6.116250924151302e-05\n",
            "Epoch 719/1000, Training Loss: 6.581221908963151e-05, Test Loss: 6.106440705895933e-05\n",
            "Epoch 720/1000, Training Loss: 6.574827521432081e-05, Test Loss: 6.0966986138465834e-05\n",
            "Epoch 721/1000, Training Loss: 6.568476494097835e-05, Test Loss: 6.087023895828064e-05\n",
            "Epoch 722/1000, Training Loss: 6.562168277941371e-05, Test Loss: 6.077415810212894e-05\n",
            "Epoch 723/1000, Training Loss: 6.555902332574731e-05, Test Loss: 6.067873625743851e-05\n",
            "Epoch 724/1000, Training Loss: 6.549678126080269e-05, Test Loss: 6.058396621360026e-05\n",
            "Epoch 725/1000, Training Loss: 6.54349513485316e-05, Test Loss: 6.048984086027307e-05\n",
            "Epoch 726/1000, Training Loss: 6.537352843447397e-05, Test Loss: 6.0396353185700286e-05\n",
            "Epoch 727/1000, Training Loss: 6.531250744425056e-05, Test Loss: 6.030349627507473e-05\n",
            "Epoch 728/1000, Training Loss: 6.525188338208778e-05, Test Loss: 6.021126330893931e-05\n",
            "Epoch 729/1000, Training Loss: 6.519165132937252e-05, Test Loss: 6.011964756160171e-05\n",
            "Epoch 730/1000, Training Loss: 6.513180644324044e-05, Test Loss: 6.002864239959011e-05\n",
            "Epoch 731/1000, Training Loss: 6.507234395519103e-05, Test Loss: 5.993824128014027e-05\n",
            "Epoch 732/1000, Training Loss: 6.501325916973424e-05, Test Loss: 5.984843774970785e-05\n",
            "Epoch 733/1000, Training Loss: 6.495454746306184e-05, Test Loss: 5.975922544249866e-05\n",
            "Epoch 734/1000, Training Loss: 6.489620428175319e-05, Test Loss: 5.9670598079055e-05\n",
            "Epoch 735/1000, Training Loss: 6.483822514150115e-05, Test Loss: 5.958254946483229e-05\n",
            "Epoch 736/1000, Training Loss: 6.478060562586842e-05, Test Loss: 5.949507348883578e-05\n",
            "Epoch 737/1000, Training Loss: 6.472334138506797e-05, Test Loss: 5.940816412225573e-05\n",
            "Epoch 738/1000, Training Loss: 6.466642813477144e-05, Test Loss: 5.932181541715265e-05\n",
            "Epoch 739/1000, Training Loss: 6.460986165493846e-05, Test Loss: 5.9236021505146154e-05\n",
            "Epoch 740/1000, Training Loss: 6.45536377886737e-05, Test Loss: 5.915077659614196e-05\n",
            "Epoch 741/1000, Training Loss: 6.449775244110368e-05, Test Loss: 5.906607497707686e-05\n",
            "Epoch 742/1000, Training Loss: 6.44422015782814e-05, Test Loss: 5.89819110106855e-05\n",
            "Epoch 743/1000, Training Loss: 6.438698122610852e-05, Test Loss: 5.889827913429788e-05\n",
            "Epoch 744/1000, Training Loss: 6.43320874692833e-05, Test Loss: 5.881517385865061e-05\n",
            "Epoch 745/1000, Training Loss: 6.427751645026768e-05, Test Loss: 5.873258976672348e-05\n",
            "Epoch 746/1000, Training Loss: 6.42232643682769e-05, Test Loss: 5.865052151260081e-05\n",
            "Epoch 747/1000, Training Loss: 6.41693274782874e-05, Test Loss: 5.856896382034281e-05\n",
            "Epoch 748/1000, Training Loss: 6.41157020900679e-05, Test Loss: 5.848791148289319e-05\n",
            "Epoch 749/1000, Training Loss: 6.406238456722744e-05, Test Loss: 5.840735936099555e-05\n",
            "Epoch 750/1000, Training Loss: 6.4009371326284e-05, Test Loss: 5.832730238212728e-05\n",
            "Epoch 751/1000, Training Loss: 6.395665883575186e-05, Test Loss: 5.824773553947271e-05\n",
            "Epoch 752/1000, Training Loss: 6.390424361524423e-05, Test Loss: 5.8168653890879725e-05\n",
            "Epoch 753/1000, Training Loss: 6.385212223459985e-05, Test Loss: 5.8090052557878684e-05\n",
            "Epoch 754/1000, Training Loss: 6.380029131302028e-05, Test Loss: 5.801192672467273e-05\n",
            "Epoch 755/1000, Training Loss: 6.37487475182288e-05, Test Loss: 5.793427163719061e-05\n",
            "Epoch 756/1000, Training Loss: 6.369748756564511e-05, Test Loss: 5.785708260212453e-05\n",
            "Epoch 757/1000, Training Loss: 6.36465082175738e-05, Test Loss: 5.778035498599619e-05\n",
            "Epoch 758/1000, Training Loss: 6.359580628241333e-05, Test Loss: 5.770408421424524e-05\n",
            "Epoch 759/1000, Training Loss: 6.354537861387727e-05, Test Loss: 5.7628265770326744e-05\n",
            "Epoch 760/1000, Training Loss: 6.349522211023015e-05, Test Loss: 5.755289519482258e-05\n",
            "Epoch 761/1000, Training Loss: 6.344533371354096e-05, Test Loss: 5.7477968084575515e-05\n",
            "Epoch 762/1000, Training Loss: 6.339571040895021e-05, Test Loss: 5.740348009183844e-05\n",
            "Epoch 763/1000, Training Loss: 6.33463492239497e-05, Test Loss: 5.732942692342579e-05\n",
            "Epoch 764/1000, Training Loss: 6.329724722767936e-05, Test Loss: 5.72558043399079e-05\n",
            "Epoch 765/1000, Training Loss: 6.324840153023461e-05, Test Loss: 5.718260815478226e-05\n",
            "Epoch 766/1000, Training Loss: 6.31998092819874e-05, Test Loss: 5.7109834233687655e-05\n",
            "Epoch 767/1000, Training Loss: 6.315146767292399e-05, Test Loss: 5.703747849362658e-05\n",
            "Epoch 768/1000, Training Loss: 6.310337393199052e-05, Test Loss: 5.696553690218906e-05\n",
            "Epoch 769/1000, Training Loss: 6.305552532645357e-05, Test Loss: 5.689400547680473e-05\n",
            "Epoch 770/1000, Training Loss: 6.30079191612734e-05, Test Loss: 5.682288028400076e-05\n",
            "Epoch 771/1000, Training Loss: 6.296055277848799e-05, Test Loss: 5.675215743866867e-05\n",
            "Epoch 772/1000, Training Loss: 6.291342355660934e-05, Test Loss: 5.668183310335681e-05\n",
            "Epoch 773/1000, Training Loss: 6.286652891003065e-05, Test Loss: 5.661190348755981e-05\n",
            "Epoch 774/1000, Training Loss: 6.28198662884453e-05, Test Loss: 5.6542364847035316e-05\n",
            "Epoch 775/1000, Training Loss: 6.277343317627709e-05, Test Loss: 5.6473213483114215e-05\n",
            "Epoch 776/1000, Training Loss: 6.272722709211934e-05, Test Loss: 5.640444574204106e-05\n",
            "Epoch 777/1000, Training Loss: 6.268124558818758e-05, Test Loss: 5.633605801431564e-05\n",
            "Epoch 778/1000, Training Loss: 6.263548624977948e-05, Test Loss: 5.626804673404398e-05\n",
            "Epoch 779/1000, Training Loss: 6.258994669474699e-05, Test Loss: 5.620040837831108e-05\n",
            "Epoch 780/1000, Training Loss: 6.25446245729762e-05, Test Loss: 5.6133139466553824e-05\n",
            "Epoch 781/1000, Training Loss: 6.249951756587904e-05, Test Loss: 5.606623655994563e-05\n",
            "Epoch 782/1000, Training Loss: 6.24546233858938e-05, Test Loss: 5.59996962607966e-05\n",
            "Epoch 783/1000, Training Loss: 6.240993977599306e-05, Test Loss: 5.593351521196295e-05\n",
            "Epoch 784/1000, Training Loss: 6.23654645092036e-05, Test Loss: 5.586769009626278e-05\n",
            "Epoch 785/1000, Training Loss: 6.232119538813253e-05, Test Loss: 5.580221763590124e-05\n",
            "Epoch 786/1000, Training Loss: 6.227713024450385e-05, Test Loss: 5.573709459190913e-05\n",
            "Epoch 787/1000, Training Loss: 6.223326693870213e-05, Test Loss: 5.567231776359032e-05\n",
            "Epoch 788/1000, Training Loss: 6.218960335932582e-05, Test Loss: 5.5607883987972284e-05\n",
            "Epoch 789/1000, Training Loss: 6.214613742274831e-05, Test Loss: 5.554379013928146e-05\n",
            "Epoch 790/1000, Training Loss: 6.210286707268627e-05, Test Loss: 5.5480033128407214e-05\n",
            "Epoch 791/1000, Training Loss: 6.2059790279775e-05, Test Loss: 5.541660990238411e-05\n",
            "Epoch 792/1000, Training Loss: 6.201690504115497e-05, Test Loss: 5.5353517443884205e-05\n",
            "Epoch 793/1000, Training Loss: 6.197420938006224e-05, Test Loss: 5.529075277071979e-05\n",
            "Epoch 794/1000, Training Loss: 6.19317013454273e-05, Test Loss: 5.522831293534763e-05\n",
            "Epoch 795/1000, Training Loss: 6.188937901148072e-05, Test Loss: 5.516619502437892e-05\n",
            "Epoch 796/1000, Training Loss: 6.184724047736847e-05, Test Loss: 5.5104396158112994e-05\n",
            "Epoch 797/1000, Training Loss: 6.180528386676897e-05, Test Loss: 5.504291349006007e-05\n",
            "Epoch 798/1000, Training Loss: 6.176350732752289e-05, Test Loss: 5.498174420648435e-05\n",
            "Epoch 799/1000, Training Loss: 6.172190903126424e-05, Test Loss: 5.492088552594277e-05\n",
            "Epoch 800/1000, Training Loss: 6.168048717306149e-05, Test Loss: 5.4860334698853403e-05\n",
            "Epoch 801/1000, Training Loss: 6.163923997106419e-05, Test Loss: 5.480008900703908e-05\n",
            "Epoch 802/1000, Training Loss: 6.159816566615406e-05, Test Loss: 5.474014576330707e-05\n",
            "Epoch 803/1000, Training Loss: 6.155726252160385e-05, Test Loss: 5.468050231101222e-05\n",
            "Epoch 804/1000, Training Loss: 6.151652882274277e-05, Test Loss: 5.462115602365369e-05\n",
            "Epoch 805/1000, Training Loss: 6.147596287662567e-05, Test Loss: 5.4562104304447245e-05\n",
            "Epoch 806/1000, Training Loss: 6.143556301171094e-05, Test Loss: 5.4503344585934774e-05\n",
            "Epoch 807/1000, Training Loss: 6.139532757753967e-05, Test Loss: 5.4444874329573954e-05\n",
            "Epoch 808/1000, Training Loss: 6.135525494442515e-05, Test Loss: 5.43866910253486e-05\n",
            "Epoch 809/1000, Training Loss: 6.131534350314416e-05, Test Loss: 5.432879219138595e-05\n",
            "Epoch 810/1000, Training Loss: 6.127559166463521e-05, Test Loss: 5.427117537357612e-05\n",
            "Epoch 811/1000, Training Loss: 6.123599785970099e-05, Test Loss: 5.421383814518777e-05\n",
            "Epoch 812/1000, Training Loss: 6.119656053871833e-05, Test Loss: 5.41567781065161e-05\n",
            "Epoch 813/1000, Training Loss: 6.11572781713492e-05, Test Loss: 5.409999288450893e-05\n",
            "Epoch 814/1000, Training Loss: 6.111814924626046e-05, Test Loss: 5.404348013241441e-05\n",
            "Epoch 815/1000, Training Loss: 6.107917227084539e-05, Test Loss: 5.3987237529426504e-05\n",
            "Epoch 816/1000, Training Loss: 6.104034577095259e-05, Test Loss: 5.3931262780345026e-05\n",
            "Epoch 817/1000, Training Loss: 6.100166829061669e-05, Test Loss: 5.387555361522695e-05\n",
            "Epoch 818/1000, Training Loss: 6.0963138391795906e-05, Test Loss: 5.382010778905605e-05\n",
            "Epoch 819/1000, Training Loss: 6.092475465411214e-05, Test Loss: 5.376492308141578e-05\n",
            "Epoch 820/1000, Training Loss: 6.0886515674598196e-05, Test Loss: 5.3709997296155316e-05\n",
            "Epoch 821/1000, Training Loss: 6.0848420067445696e-05, Test Loss: 5.3655328261083206e-05\n",
            "Epoch 822/1000, Training Loss: 6.081046646375961e-05, Test Loss: 5.360091382764047e-05\n",
            "Epoch 823/1000, Training Loss: 6.077265351131631e-05, Test Loss: 5.3546751870592416e-05\n",
            "Epoch 824/1000, Training Loss: 6.0734979874326154e-05, Test Loss: 5.3492840287733175e-05\n",
            "Epoch 825/1000, Training Loss: 6.069744423319753e-05, Test Loss: 5.3439176999570194e-05\n",
            "Epoch 826/1000, Training Loss: 6.06600452843085e-05, Test Loss: 5.338575994903845e-05\n",
            "Epoch 827/1000, Training Loss: 6.062278173978091e-05, Test Loss: 5.333258710120969e-05\n",
            "Epoch 828/1000, Training Loss: 6.058565232725426e-05, Test Loss: 5.327965644299609e-05\n",
            "Epoch 829/1000, Training Loss: 6.054865578967108e-05, Test Loss: 5.322696598287596e-05\n",
            "Epoch 830/1000, Training Loss: 6.0511790885057874e-05, Test Loss: 5.317451375060941e-05\n",
            "Epoch 831/1000, Training Loss: 6.0475056386316575e-05, Test Loss: 5.3122297796973046e-05\n",
            "Epoch 832/1000, Training Loss: 6.0438451081012576e-05, Test Loss: 5.307031619348015e-05\n",
            "Epoch 833/1000, Training Loss: 6.04019737711717e-05, Test Loss: 5.301856703211373e-05\n",
            "Epoch 834/1000, Training Loss: 6.0365623273079105e-05, Test Loss: 5.2967048425077814e-05\n",
            "Epoch 835/1000, Training Loss: 6.032939841707828e-05, Test Loss: 5.291575850451652e-05\n",
            "Epoch 836/1000, Training Loss: 6.02932980473783e-05, Test Loss: 5.286469542229049e-05\n",
            "Epoch 837/1000, Training Loss: 6.025732102186061e-05, Test Loss: 5.28138573496967e-05\n",
            "Epoch 838/1000, Training Loss: 6.022146621188918e-05, Test Loss: 5.2763242477236745e-05\n",
            "Epoch 839/1000, Training Loss: 6.018573250212657e-05, Test Loss: 5.271284901437273e-05\n",
            "Epoch 840/1000, Training Loss: 6.015011879034847e-05, Test Loss: 5.266267518928548e-05\n",
            "Epoch 841/1000, Training Loss: 6.0114623987264915e-05, Test Loss: 5.261271924863563e-05\n",
            "Epoch 842/1000, Training Loss: 6.0079247016342926e-05, Test Loss: 5.2562979457337734e-05\n",
            "Epoch 843/1000, Training Loss: 6.0043986813632376e-05, Test Loss: 5.2513454098329126e-05\n",
            "Epoch 844/1000, Training Loss: 6.00088423275923e-05, Test Loss: 5.246414147233232e-05\n",
            "Epoch 845/1000, Training Loss: 5.997381251892547e-05, Test Loss: 5.241503989765667e-05\n",
            "Epoch 846/1000, Training Loss: 5.9938896360408506e-05, Test Loss: 5.236614770995563e-05\n",
            "Epoch 847/1000, Training Loss: 5.990409283673055e-05, Test Loss: 5.2317463262024e-05\n",
            "Epoch 848/1000, Training Loss: 5.986940094433021e-05, Test Loss: 5.2268984923578556e-05\n",
            "Epoch 849/1000, Training Loss: 5.983481969123908e-05, Test Loss: 5.222071108105264e-05\n",
            "Epoch 850/1000, Training Loss: 5.980034809692379e-05, Test Loss: 5.2172640137388614e-05\n",
            "Epoch 851/1000, Training Loss: 5.9765985192132947e-05, Test Loss: 5.212477051182801e-05\n",
            "Epoch 852/1000, Training Loss: 5.973173001874657e-05, Test Loss: 5.207710063972255e-05\n",
            "Epoch 853/1000, Training Loss: 5.969758162962601e-05, Test Loss: 5.2029628972324804e-05\n",
            "Epoch 854/1000, Training Loss: 5.966353908846653e-05, Test Loss: 5.1982353976594155e-05\n",
            "Epoch 855/1000, Training Loss: 5.9629601469655644e-05, Test Loss: 5.193527413501449e-05\n",
            "Epoch 856/1000, Training Loss: 5.959576785812835e-05, Test Loss: 5.1888387945397054e-05\n",
            "Epoch 857/1000, Training Loss: 5.9562037349228036e-05, Test Loss: 5.184169392068603e-05\n",
            "Epoch 858/1000, Training Loss: 5.952840904856889e-05, Test Loss: 5.17951905887961e-05\n",
            "Epoch 859/1000, Training Loss: 5.94948820718996e-05, Test Loss: 5.174887649240603e-05\n",
            "Epoch 860/1000, Training Loss: 5.946145554496996e-05, Test Loss: 5.170275018879413e-05\n",
            "Epoch 861/1000, Training Loss: 5.942812860339943e-05, Test Loss: 5.165681024966039e-05\n",
            "Epoch 862/1000, Training Loss: 5.9394900392546445e-05, Test Loss: 5.161105526094802e-05\n",
            "Epoch 863/1000, Training Loss: 5.936177006738311e-05, Test Loss: 5.1565483822678954e-05\n",
            "Epoch 864/1000, Training Loss: 5.9328736792366144e-05, Test Loss: 5.152009454877429e-05\n",
            "Epoch 865/1000, Training Loss: 5.929579974131494e-05, Test Loss: 5.1474886066895104e-05\n",
            "Epoch 866/1000, Training Loss: 5.926295809729041e-05, Test Loss: 5.1429857018280105e-05\n",
            "Epoch 867/1000, Training Loss: 5.9230211052472543e-05, Test Loss: 5.138500605757501e-05\n",
            "Epoch 868/1000, Training Loss: 5.9197557808044224e-05, Test Loss: 5.1340331852678366e-05\n",
            "Epoch 869/1000, Training Loss: 5.916499757407295e-05, Test Loss: 5.1295833084586406e-05\n",
            "Epoch 870/1000, Training Loss: 5.913252956939637e-05, Test Loss: 5.12515084472285e-05\n",
            "Epoch 871/1000, Training Loss: 5.9100153021510434e-05, Test Loss: 5.120735664732344e-05\n",
            "Epoch 872/1000, Training Loss: 5.906786716645603e-05, Test Loss: 5.1163376404220975e-05\n",
            "Epoch 873/1000, Training Loss: 5.9035671248710105e-05, Test Loss: 5.1119566449758745e-05\n",
            "Epoch 874/1000, Training Loss: 5.900356452107786e-05, Test Loss: 5.107592552811068e-05\n",
            "Epoch 875/1000, Training Loss: 5.897154624458546e-05, Test Loss: 5.103245239564578e-05\n",
            "Epoch 876/1000, Training Loss: 5.893961568837486e-05, Test Loss: 5.0989145820778395e-05\n",
            "Epoch 877/1000, Training Loss: 5.8907772129601505e-05, Test Loss: 5.094600458383852e-05\n",
            "Epoch 878/1000, Training Loss: 5.887601485333063e-05, Test Loss: 5.090302747691821e-05\n",
            "Epoch 879/1000, Training Loss: 5.884434315243845e-05, Test Loss: 5.0860213303748156e-05\n",
            "Epoch 880/1000, Training Loss: 5.8812756327511984e-05, Test Loss: 5.0817560879552985e-05\n",
            "Epoch 881/1000, Training Loss: 5.8781253686751814e-05, Test Loss: 5.0775069030919646e-05\n",
            "Epoch 882/1000, Training Loss: 5.874983454587683e-05, Test Loss: 5.073273659567076e-05\n",
            "Epoch 883/1000, Training Loss: 5.8718498228026905e-05, Test Loss: 5.069056242272759e-05\n",
            "Epoch 884/1000, Training Loss: 5.868724406367316e-05, Test Loss: 5.0648545371984606e-05\n",
            "Epoch 885/1000, Training Loss: 5.8656071390522856e-05, Test Loss: 5.0606684314185595e-05\n",
            "Epoch 886/1000, Training Loss: 5.8624979553430245e-05, Test Loss: 5.056497813079452e-05\n",
            "Epoch 887/1000, Training Loss: 5.859396790430602e-05, Test Loss: 5.052342571387376e-05\n",
            "Epoch 888/1000, Training Loss: 5.856303580203198e-05, Test Loss: 5.0482025965970356e-05\n",
            "Epoch 889/1000, Training Loss: 5.8532182612369895e-05, Test Loss: 5.0440777799978346e-05\n",
            "Epoch 890/1000, Training Loss: 5.8501407707879225e-05, Test Loss: 5.039968013904224e-05\n",
            "Epoch 891/1000, Training Loss: 5.84707104678312e-05, Test Loss: 5.0358731916419834e-05\n",
            "Epoch 892/1000, Training Loss: 5.844009027812592e-05, Test Loss: 5.031793207538267e-05\n",
            "Epoch 893/1000, Training Loss: 5.840954653121114e-05, Test Loss: 5.0277279569099844e-05\n",
            "Epoch 894/1000, Training Loss: 5.837907862600023e-05, Test Loss: 5.0236773360515734e-05\n",
            "Epoch 895/1000, Training Loss: 5.834868596779325e-05, Test Loss: 5.019641242225083e-05\n",
            "Epoch 896/1000, Training Loss: 5.8318367968196765e-05, Test Loss: 5.0156195736481255e-05\n",
            "Epoch 897/1000, Training Loss: 5.828812404504931e-05, Test Loss: 5.0116122294847676e-05\n",
            "Epoch 898/1000, Training Loss: 5.825795362234295e-05, Test Loss: 5.007619109833638e-05\n",
            "Epoch 899/1000, Training Loss: 5.8227856130147946e-05, Test Loss: 5.003640115717126e-05\n",
            "Epoch 900/1000, Training Loss: 5.819783100453889e-05, Test Loss: 4.999675149072691e-05\n",
            "Epoch 901/1000, Training Loss: 5.816787768752202e-05, Test Loss: 4.9957241127407906e-05\n",
            "Epoch 902/1000, Training Loss: 5.813799562696239e-05, Test Loss: 4.991786910455851e-05\n",
            "Epoch 903/1000, Training Loss: 5.8108184276512086e-05, Test Loss: 4.987863446836006e-05\n",
            "Epoch 904/1000, Training Loss: 5.8078443095541454e-05, Test Loss: 4.983953627373462e-05\n",
            "Epoch 905/1000, Training Loss: 5.804877154906896e-05, Test Loss: 4.980057358424941e-05\n",
            "Epoch 906/1000, Training Loss: 5.801916910769321e-05, Test Loss: 4.976174547201228e-05\n",
            "Epoch 907/1000, Training Loss: 5.7989635247525437e-05, Test Loss: 4.9723051017589674e-05\n",
            "Epoch 908/1000, Training Loss: 5.796016945012257e-05, Test Loss: 4.96844893099008e-05\n",
            "Epoch 909/1000, Training Loss: 5.793077120242249e-05, Test Loss: 4.964605944613564e-05\n",
            "Epoch 910/1000, Training Loss: 5.7901439996680014e-05, Test Loss: 4.960776053166448e-05\n",
            "Epoch 911/1000, Training Loss: 5.787217533040182e-05, Test Loss: 4.9569591679938686e-05\n",
            "Epoch 912/1000, Training Loss: 5.784297670628372e-05, Test Loss: 4.953155201240494e-05\n",
            "Epoch 913/1000, Training Loss: 5.781384363215055e-05, Test Loss: 4.9493640658432584e-05\n",
            "Epoch 914/1000, Training Loss: 5.778477562089184e-05, Test Loss: 4.945585675520316e-05\n",
            "Epoch 915/1000, Training Loss: 5.7755772190404736e-05, Test Loss: 4.941819944764152e-05\n",
            "Epoch 916/1000, Training Loss: 5.7726832863532734e-05, Test Loss: 4.938066788832905e-05\n",
            "Epoch 917/1000, Training Loss: 5.7697957168006937e-05, Test Loss: 4.934326123741732e-05\n",
            "Epoch 918/1000, Training Loss: 5.766914463638965e-05, Test Loss: 4.9305978662551196e-05\n",
            "Epoch 919/1000, Training Loss: 5.7640394806015365e-05, Test Loss: 4.926881933878219e-05\n",
            "Epoch 920/1000, Training Loss: 5.761170721893504e-05, Test Loss: 4.923178244848933e-05\n",
            "Epoch 921/1000, Training Loss: 5.7583081421861204e-05, Test Loss: 4.9194867181303976e-05\n",
            "Epoch 922/1000, Training Loss: 5.755451696611238e-05, Test Loss: 4.915807273402917e-05\n",
            "Epoch 923/1000, Training Loss: 5.752601340755967e-05, Test Loss: 4.912139831056325e-05\n",
            "Epoch 924/1000, Training Loss: 5.749757030657334e-05, Test Loss: 4.908484312182632e-05\n",
            "Epoch 925/1000, Training Loss: 5.746918722796856e-05, Test Loss: 4.904840638567761e-05\n",
            "Epoch 926/1000, Training Loss: 5.7440863740955237e-05, Test Loss: 4.901208732684897e-05\n",
            "Epoch 927/1000, Training Loss: 5.741259941908716e-05, Test Loss: 4.8975885176872526e-05\n",
            "Epoch 928/1000, Training Loss: 5.738439384020946e-05, Test Loss: 4.893979917400146e-05\n",
            "Epoch 929/1000, Training Loss: 5.7356246586409194e-05, Test Loss: 4.89038285631405e-05\n",
            "Epoch 930/1000, Training Loss: 5.732815724396804e-05, Test Loss: 4.886797259578338e-05\n",
            "Epoch 931/1000, Training Loss: 5.7300125403310576e-05, Test Loss: 4.883223052992737e-05\n",
            "Epoch 932/1000, Training Loss: 5.727215065895929e-05, Test Loss: 4.879660163002415e-05\n",
            "Epoch 933/1000, Training Loss: 5.724423260948507e-05, Test Loss: 4.876108516689744e-05\n",
            "Epoch 934/1000, Training Loss: 5.7216370857461455e-05, Test Loss: 4.8725680417679426e-05\n",
            "Epoch 935/1000, Training Loss: 5.7188565009418994e-05, Test Loss: 4.869038666574825e-05\n",
            "Epoch 936/1000, Training Loss: 5.7160814675798076e-05, Test Loss: 4.8655203200656145e-05\n",
            "Epoch 937/1000, Training Loss: 5.7133119470905835e-05, Test Loss: 4.862012931807163e-05\n",
            "Epoch 938/1000, Training Loss: 5.710547901287098e-05, Test Loss: 4.858516431971078e-05\n",
            "Epoch 939/1000, Training Loss: 5.7077892923599395e-05, Test Loss: 4.855030751327371e-05\n",
            "Epoch 940/1000, Training Loss: 5.7050360828732776e-05, Test Loss: 4.85155582123885e-05\n",
            "Epoch 941/1000, Training Loss: 5.702288235760465e-05, Test Loss: 4.848091573654425e-05\n",
            "Epoch 942/1000, Training Loss: 5.699545714319796e-05, Test Loss: 4.844637941103042e-05\n",
            "Epoch 943/1000, Training Loss: 5.6968084822104704e-05, Test Loss: 4.841194856687508e-05\n",
            "Epoch 944/1000, Training Loss: 5.694076503448421e-05, Test Loss: 4.83776225407929e-05\n",
            "Epoch 945/1000, Training Loss: 5.691349742402271e-05, Test Loss: 4.8343400675120166e-05\n",
            "Epoch 946/1000, Training Loss: 5.6886281637893756e-05, Test Loss: 4.830928231775974e-05\n",
            "Epoch 947/1000, Training Loss: 5.685911732671895e-05, Test Loss: 4.827526682211751e-05\n",
            "Epoch 948/1000, Training Loss: 5.683200414452818e-05, Test Loss: 4.8241353547058004e-05\n",
            "Epoch 949/1000, Training Loss: 5.680494174872217e-05, Test Loss: 4.820754185683898e-05\n",
            "Epoch 950/1000, Training Loss: 5.677792980003314e-05, Test Loss: 4.817383112105714e-05\n",
            "Epoch 951/1000, Training Loss: 5.6750967962489077e-05, Test Loss: 4.814022071459131e-05\n",
            "Epoch 952/1000, Training Loss: 5.6724055903375944e-05, Test Loss: 4.810671001756422e-05\n",
            "Epoch 953/1000, Training Loss: 5.669719329320087e-05, Test Loss: 4.8073298415267695e-05\n",
            "Epoch 954/1000, Training Loss: 5.667037980565567e-05, Test Loss: 4.803998529812032e-05\n",
            "Epoch 955/1000, Training Loss: 5.664361511758373e-05, Test Loss: 4.800677006161922e-05\n",
            "Epoch 956/1000, Training Loss: 5.661689890894196e-05, Test Loss: 4.7973652106281195e-05\n",
            "Epoch 957/1000, Training Loss: 5.659023086276743e-05, Test Loss: 4.79406308375968e-05\n",
            "Epoch 958/1000, Training Loss: 5.6563610665143426e-05, Test Loss: 4.790770566597673e-05\n",
            "Epoch 959/1000, Training Loss: 5.6537038005164666e-05, Test Loss: 4.787487600670624e-05\n",
            "Epoch 960/1000, Training Loss: 5.651051257490389e-05, Test Loss: 4.784214127988786e-05\n",
            "Epoch 961/1000, Training Loss: 5.648403406938078e-05, Test Loss: 4.780950091041158e-05\n",
            "Epoch 962/1000, Training Loss: 5.645760218652677e-05, Test Loss: 4.7776954327882376e-05\n",
            "Epoch 963/1000, Training Loss: 5.643121662715465e-05, Test Loss: 4.7744500966589876e-05\n",
            "Epoch 964/1000, Training Loss: 5.640487709492613e-05, Test Loss: 4.771214026545522e-05\n",
            "Epoch 965/1000, Training Loss: 5.637858329632039e-05, Test Loss: 4.7679871667986836e-05\n",
            "Epoch 966/1000, Training Loss: 5.635233494060345e-05, Test Loss: 4.7647694622233085e-05\n",
            "Epoch 967/1000, Training Loss: 5.632613173979755e-05, Test Loss: 4.761560858074307e-05\n",
            "Epoch 968/1000, Training Loss: 5.62999734086507e-05, Test Loss: 4.7583613000513334e-05\n",
            "Epoch 969/1000, Training Loss: 5.627385966460662e-05, Test Loss: 4.755170734294637e-05\n",
            "Epoch 970/1000, Training Loss: 5.624779022777588e-05, Test Loss: 4.751989107380919e-05\n",
            "Epoch 971/1000, Training Loss: 5.622176482090668e-05, Test Loss: 4.748816366319136e-05\n",
            "Epoch 972/1000, Training Loss: 5.6195783169355275e-05, Test Loss: 4.745652458545983e-05\n",
            "Epoch 973/1000, Training Loss: 5.6169845001059545e-05, Test Loss: 4.742497331921926e-05\n",
            "Epoch 974/1000, Training Loss: 5.6143950046507984e-05, Test Loss: 4.739350934726151e-05\n",
            "Epoch 975/1000, Training Loss: 5.611809803871445e-05, Test Loss: 4.736213215653951e-05\n",
            "Epoch 976/1000, Training Loss: 5.6092288713190065e-05, Test Loss: 4.733084123811426e-05\n",
            "Epoch 977/1000, Training Loss: 5.6066521807915624e-05, Test Loss: 4.729963608712004e-05\n",
            "Epoch 978/1000, Training Loss: 5.60407970633155e-05, Test Loss: 4.72685162027252e-05\n",
            "Epoch 979/1000, Training Loss: 5.601511422223123e-05, Test Loss: 4.723748108808855e-05\n",
            "Epoch 980/1000, Training Loss: 5.598947302989538e-05, Test Loss: 4.7206530250327684e-05\n",
            "Epoch 981/1000, Training Loss: 5.5963873233906164e-05, Test Loss: 4.717566320047156e-05\n",
            "Epoch 982/1000, Training Loss: 5.593831458420029e-05, Test Loss: 4.7144879453427795e-05\n",
            "Epoch 983/1000, Training Loss: 5.591279683303044e-05, Test Loss: 4.7114178527945814e-05\n",
            "Epoch 984/1000, Training Loss: 5.5887319734938736e-05, Test Loss: 4.7083559946580805e-05\n",
            "Epoch 985/1000, Training Loss: 5.586188304673204e-05, Test Loss: 4.7053023235648985e-05\n",
            "Epoch 986/1000, Training Loss: 5.583648652745972e-05, Test Loss: 4.702256792520309e-05\n",
            "Epoch 987/1000, Training Loss: 5.581112993838664e-05, Test Loss: 4.699219354898238e-05\n",
            "Epoch 988/1000, Training Loss: 5.5785813042972474e-05, Test Loss: 4.696189964439436e-05\n",
            "Epoch 989/1000, Training Loss: 5.5760535606846325e-05, Test Loss: 4.693168575246228e-05\n",
            "Epoch 990/1000, Training Loss: 5.573529739778556e-05, Test Loss: 4.6901551417806744e-05\n",
            "Epoch 991/1000, Training Loss: 5.571009818569026e-05, Test Loss: 4.6871496188597634e-05\n",
            "Epoch 992/1000, Training Loss: 5.568493774256309e-05, Test Loss: 4.684151961652627e-05\n",
            "Epoch 993/1000, Training Loss: 5.565981584248573e-05, Test Loss: 4.6811621256773954e-05\n",
            "Epoch 994/1000, Training Loss: 5.563473226159843e-05, Test Loss: 4.678180066797564e-05\n",
            "Epoch 995/1000, Training Loss: 5.560968677807574e-05, Test Loss: 4.675205741218408e-05\n",
            "Epoch 996/1000, Training Loss: 5.5584679172107106e-05, Test Loss: 4.67223910548495e-05\n",
            "Epoch 997/1000, Training Loss: 5.555970922587551e-05, Test Loss: 4.6692801164769576e-05\n",
            "Epoch 998/1000, Training Loss: 5.5534776723534754e-05, Test Loss: 4.6663287314073944e-05\n",
            "Epoch 999/1000, Training Loss: 5.5509881451191056e-05, Test Loss: 4.663384907818379e-05\n",
            "Epoch 1000/1000, Training Loss: 5.548502319688067e-05, Test Loss: 4.660448603578213e-05\n",
            "Epoch 1/1000, Training Loss: 0.0033015599496507685, Test Loss: 0.002659068628211812\n",
            "Epoch 2/1000, Training Loss: 0.003156231849668219, Test Loss: 0.002561851479492402\n",
            "Epoch 3/1000, Training Loss: 0.003131640490806932, Test Loss: 0.0025540508328894757\n",
            "Epoch 4/1000, Training Loss: 0.00312369910505603, Test Loss: 0.002554344681613098\n",
            "Epoch 5/1000, Training Loss: 0.003119574735141708, Test Loss: 0.0025549653621982638\n",
            "Epoch 6/1000, Training Loss: 0.003116599555782738, Test Loss: 0.0025550363780602785\n",
            "Epoch 7/1000, Training Loss: 0.003114078689986737, Test Loss: 0.0025546465660484076\n",
            "Epoch 8/1000, Training Loss: 0.003111803467363781, Test Loss: 0.0025539725148083655\n",
            "Epoch 9/1000, Training Loss: 0.0031097008798991774, Test Loss: 0.0025531444205255387\n",
            "Epoch 10/1000, Training Loss: 0.003107738911389265, Test Loss: 0.0025522427202856286\n",
            "Epoch 11/1000, Training Loss: 0.003105899011600986, Test Loss: 0.0025513135672349902\n",
            "Epoch 12/1000, Training Loss: 0.0031041677259577207, Test Loss: 0.002550382175325489\n",
            "Epoch 13/1000, Training Loss: 0.0031025340385024765, Test Loss: 0.002549461621497241\n",
            "Epoch 14/1000, Training Loss: 0.0031009884596989194, Test Loss: 0.0025485581555945817\n",
            "Epoch 15/1000, Training Loss: 0.0030995226568587635, Test Loss: 0.0025476742779562966\n",
            "Epoch 16/1000, Training Loss: 0.003098129259293106, Test Loss: 0.002546810486092805\n",
            "Epoch 17/1000, Training Loss: 0.0030968017252889692, Test Loss: 0.0025459662528714705\n",
            "Epoch 18/1000, Training Loss: 0.003095534236030902, Test Loss: 0.00254514056789047\n",
            "Epoch 19/1000, Training Loss: 0.003094321605288401, Test Loss: 0.002544332232517384\n",
            "Epoch 20/1000, Training Loss: 0.003093159200812766, Test Loss: 0.002543540016605626\n",
            "Epoch 21/1000, Training Loss: 0.0030920428755205655, Test Loss: 0.0025427627378213583\n",
            "Epoch 22/1000, Training Loss: 0.003090968907206216, Test Loss: 0.0025419992979208482\n",
            "Epoch 23/1000, Training Loss: 0.003089933945771879, Test Loss: 0.0025412486953581416\n",
            "Epoch 24/1000, Training Loss: 0.0030889349670919224, Test Loss: 0.0025405100251856606\n",
            "Epoch 25/1000, Training Loss: 0.0030879692327246237, Test Loss: 0.0025397824724597102\n",
            "Epoch 26/1000, Training Loss: 0.0030870342547669907, Test Loss: 0.0025390653026678245\n",
            "Epoch 27/1000, Training Loss: 0.003086127765224234, Test Loss: 0.0025383578511555937\n",
            "Epoch 28/1000, Training Loss: 0.0030852476893345766, Test Loss: 0.002537659512644853\n",
            "Epoch 29/1000, Training Loss: 0.003084392122353018, Test Loss: 0.0025369697314219005\n",
            "Epoch 30/1000, Training Loss: 0.0030835593093546847, Test Loss: 0.0025362879924754585\n",
            "Epoch 31/1000, Training Loss: 0.003082747627669812, Test Loss: 0.002535613813690068\n",
            "Epoch 32/1000, Training Loss: 0.003081955571608601, Test Loss: 0.002534946739100159\n",
            "Epoch 33/1000, Training Loss: 0.003081181739175649, Test Loss: 0.0025342863331535986\n",
            "Epoch 34/1000, Training Loss: 0.0030804248205105484, Test Loss: 0.0025336321759036567\n",
            "Epoch 35/1000, Training Loss: 0.003079683587824236, Test Loss: 0.002532983859034634\n",
            "Epoch 36/1000, Training Loss: 0.0030789568866298363, Test Loss: 0.0025323409826224995\n",
            "Epoch 37/1000, Training Loss: 0.0030782436280926283, Test Loss: 0.0025317031525339457\n",
            "Epoch 38/1000, Training Loss: 0.0030775427823465547, Test Loss: 0.0025310699783727473\n",
            "Epoch 39/1000, Training Loss: 0.003076853372644792, Test Loss: 0.002530441071889622\n",
            "Epoch 40/1000, Training Loss: 0.003076174470229539, Test Loss: 0.0025298160457800033\n",
            "Epoch 41/1000, Training Loss: 0.003075505189821591, Test Loss: 0.0025291945128025446\n",
            "Epoch 42/1000, Training Loss: 0.003074844685643805, Test Loss: 0.0025285760851593442\n",
            "Epoch 43/1000, Training Loss: 0.0030741921479043046, Test Loss: 0.0025279603740866728\n",
            "Epoch 44/1000, Training Loss: 0.003073546799675519, Test Loss: 0.0025273469896121346\n",
            "Epoch 45/1000, Training Loss: 0.0030729078941140625, Test Loss: 0.0025267355404407096\n",
            "Epoch 46/1000, Training Loss: 0.0030722747119741457, Test Loss: 0.002526125633937979\n",
            "Epoch 47/1000, Training Loss: 0.0030716465593739256, Test Loss: 0.002525516876184012\n",
            "Epoch 48/1000, Training Loss: 0.003071022765779933, Test Loss: 0.0025249088720759125\n",
            "Epoch 49/1000, Training Loss: 0.0030704026821797106, Test Loss: 0.0025243012254610154\n",
            "Epoch 50/1000, Training Loss: 0.0030697856794170883, Test Loss: 0.0025236935392861106\n",
            "Epoch 51/1000, Training Loss: 0.003069171146668188, Test Loss: 0.002523085415751011\n",
            "Epoch 52/1000, Training Loss: 0.003068558490039443, Test Loss: 0.002522476456457267\n",
            "Epoch 53/1000, Training Loss: 0.0030679471312716087, Test Loss: 0.00252186626254494\n",
            "Epoch 54/1000, Training Loss: 0.0030673365065360917, Test Loss: 0.0025212544348120953\n",
            "Epoch 55/1000, Training Loss: 0.0030667260653118975, Test Loss: 0.0025206405738131686\n",
            "Epoch 56/1000, Training Loss: 0.003066115269333207, Test Loss: 0.0025200242799335508\n",
            "Epoch 57/1000, Training Loss: 0.0030655035915990556, Test Loss: 0.0025194051534387428\n",
            "Epoch 58/1000, Training Loss: 0.0030648905154378107, Test Loss: 0.0025187827944972273\n",
            "Epoch 59/1000, Training Loss: 0.0030642755336202122, Test Loss: 0.002518156803176842\n",
            "Epoch 60/1000, Training Loss: 0.0030636581475156424, Test Loss: 0.0025175267794149856\n",
            "Epoch 61/1000, Training Loss: 0.0030630378662870592, Test Loss: 0.0025168923229633173\n",
            "Epoch 62/1000, Training Loss: 0.0030624142061206755, Test Loss: 0.002516253033307978\n",
            "Epoch 63/1000, Training Loss: 0.003061786689487028, Test Loss: 0.0025156085095665416\n",
            "Epoch 64/1000, Training Loss: 0.0030611548444305693, Test Loss: 0.002514958350363082\n",
            "Epoch 65/1000, Training Loss: 0.0030605182038852875, Test Loss: 0.0025143021536828437\n",
            "Epoch 66/1000, Training Loss: 0.0030598763050142456, Test Loss: 0.0025136395167080646\n",
            "Epoch 67/1000, Training Loss: 0.00305922868857121, Test Loss: 0.0025129700356365404\n",
            "Epoch 68/1000, Training Loss: 0.003058574898282786, Test Loss: 0.002512293305484531\n",
            "Epoch 69/1000, Training Loss: 0.003057914480249734, Test Loss: 0.0025116089198755714\n",
            "Epoch 70/1000, Training Loss: 0.003057246982366283, Test Loss: 0.0025109164708167547\n",
            "Epoch 71/1000, Training Loss: 0.003056571953756482, Test Loss: 0.002510215548463999\n",
            "Epoch 72/1000, Training Loss: 0.003055888944226733, Test Loss: 0.0025095057408777797\n",
            "Epoch 73/1000, Training Loss: 0.003055197503733822, Test Loss: 0.002508786633770742\n",
            "Epoch 74/1000, Training Loss: 0.003054497181867844, Test Loss: 0.0025080578102485884\n",
            "Epoch 75/1000, Training Loss: 0.003053787527349563, Test Loss: 0.002507318850545578\n",
            "Epoch 76/1000, Training Loss: 0.0030530680875418418, Test Loss: 0.002506569331755917\n",
            "Epoch 77/1000, Training Loss: 0.003052338407974845, Test Loss: 0.0025058088275623225\n",
            "Epoch 78/1000, Training Loss: 0.0030515980318848454, Test Loss: 0.0025050369079629405\n",
            "Epoch 79/1000, Training Loss: 0.0030508464997665297, Test Loss: 0.0025042531389978507\n",
            "Epoch 80/1000, Training Loss: 0.003050083348938777, Test Loss: 0.002503457082476298\n",
            "Epoch 81/1000, Training Loss: 0.00304930811312399, Test Loss: 0.0025026482957058193\n",
            "Epoch 82/1000, Training Loss: 0.0030485203220411085, Test Loss: 0.002501826331224392\n",
            "Epoch 83/1000, Training Loss: 0.0030477195010125326, Test Loss: 0.0025009907365367533\n",
            "Epoch 84/1000, Training Loss: 0.0030469051705852655, Test Loss: 0.0025001410538560377\n",
            "Epoch 85/1000, Training Loss: 0.00304607684616665, Test Loss: 0.0024992768198518578\n",
            "Epoch 86/1000, Training Loss: 0.003045234037675157, Test Loss: 0.0024983975654060235\n",
            "Epoch 87/1000, Training Loss: 0.003044376249206772, Test Loss: 0.002497502815377059\n",
            "Epoch 88/1000, Training Loss: 0.0030435029787176, Test Loss: 0.0024965920883747465\n",
            "Epoch 89/1000, Training Loss: 0.003042613717723372, Test Loss: 0.002495664896545928\n",
            "Epoch 90/1000, Training Loss: 0.0030417079510166424, Test Loss: 0.002494720745372849\n",
            "Epoch 91/1000, Training Loss: 0.0030407851564025214, Test Loss: 0.0024937591334853494\n",
            "Epoch 92/1000, Training Loss: 0.003039844804453868, Test Loss: 0.0024927795524882575\n",
            "Epoch 93/1000, Training Loss: 0.0030388863582869415, Test Loss: 0.0024917814868053944\n",
            "Epoch 94/1000, Training Loss: 0.0030379092733585898, Test Loss: 0.002490764413541604\n",
            "Epoch 95/1000, Training Loss: 0.003036912997286092, Test Loss: 0.002489727802364321\n",
            "Epoch 96/1000, Training Loss: 0.0030358969696908608, Test Loss: 0.00248867111540617\n",
            "Epoch 97/1000, Training Loss: 0.0030348606220672544, Test Loss: 0.002487593807190169\n",
            "Epoch 98/1000, Training Loss: 0.0030338033776777856, Test Loss: 0.0024864953245791357\n",
            "Epoch 99/1000, Training Loss: 0.003032724651476074, Test Loss: 0.0024853751067509164\n",
            "Epoch 100/1000, Training Loss: 0.003031623850058904, Test Loss: 0.0024842325852010745\n",
            "Epoch 101/1000, Training Loss: 0.0030305003716487615, Test Loss: 0.0024830671837747145\n",
            "Epoch 102/1000, Training Loss: 0.0030293536061082593, Test Loss: 0.002481878318729083\n",
            "Epoch 103/1000, Training Loss: 0.003028182934987811, Test Loss: 0.0024806653988286294\n",
            "Epoch 104/1000, Training Loss: 0.0030269877316079275, Test Loss: 0.00247942782547412\n",
            "Epoch 105/1000, Training Loss: 0.0030257673611774403, Test Loss: 0.0024781649928674377\n",
            "Epoch 106/1000, Training Loss: 0.003024521180948917, Test Loss: 0.002476876288213579\n",
            "Epoch 107/1000, Training Loss: 0.003023248540412442, Test Loss: 0.0024755610919613364\n",
            "Epoch 108/1000, Training Loss: 0.0030219487815288425, Test Loss: 0.002474218778084021\n",
            "Epoch 109/1000, Training Loss: 0.0030206212390033195, Test Loss: 0.0024728487144015042\n",
            "Epoch 110/1000, Training Loss: 0.0030192652406002803, Test Loss: 0.002471450262944693\n",
            "Epoch 111/1000, Training Loss: 0.003017880107500035, Test Loss: 0.002470022780363385\n",
            "Epoch 112/1000, Training Loss: 0.0030164651546977913, Test Loss: 0.0024685656183783063\n",
            "Epoch 113/1000, Training Loss: 0.0030150196914451705, Test Loss: 0.0024670781242778646\n",
            "Epoch 114/1000, Training Loss: 0.003013543021734271, Test Loss: 0.002465559641459961\n",
            "Epoch 115/1000, Training Loss: 0.003012034444823961, Test Loss: 0.0024640095100189204\n",
            "Epoch 116/1000, Training Loss: 0.0030104932558078826, Test Loss: 0.0024624270673772987\n",
            "Epoch 117/1000, Training Loss: 0.0030089187462232847, Test Loss: 0.0024608116489620597\n",
            "Epoch 118/1000, Training Loss: 0.0030073102046995113, Test Loss: 0.002459162588924224\n",
            "Epoch 119/1000, Training Loss: 0.003005666917644624, Test Loss: 0.002457479220900804\n",
            "Epoch 120/1000, Training Loss: 0.003003988169968301, Test Loss: 0.0024557608788174216\n",
            "Epoch 121/1000, Training Loss: 0.003002273245838791, Test Loss: 0.0024540068977296522\n",
            "Epoch 122/1000, Training Loss: 0.00300052142947135, Test Loss: 0.002452216614700757\n",
            "Epoch 123/1000, Training Loss: 0.0029987320059452474, Test Loss: 0.002450389369713056\n",
            "Epoch 124/1000, Training Loss: 0.0029969042620460653, Test Loss: 0.0024485245066098423\n",
            "Epoch 125/1000, Training Loss: 0.002995037487129713, Test Loss: 0.002446621374064317\n",
            "Epoch 126/1000, Training Loss: 0.0029931309740042393, Test Loss: 0.002444679326571697\n",
            "Epoch 127/1000, Training Loss: 0.002991184019825273, Test Loss: 0.002442697725460303\n",
            "Epoch 128/1000, Training Loss: 0.002989195927000676, Test Loss: 0.0024406759399170958\n",
            "Epoch 129/1000, Training Loss: 0.002987166004099755, Test Loss: 0.0024386133480228966\n",
            "Epoch 130/1000, Training Loss: 0.0029850935667622568, Test Loss: 0.0024365093377922533\n",
            "Epoch 131/1000, Training Loss: 0.0029829779386022427, Test Loss: 0.002434363308212744\n",
            "Epoch 132/1000, Training Loss: 0.002980818452101884, Test Loss: 0.002432174670278397\n",
            "Epoch 133/1000, Training Loss: 0.0029786144494902355, Test Loss: 0.0024299428480118106\n",
            "Epoch 134/1000, Training Loss: 0.002976365283602114, Test Loss: 0.002427667279469541\n",
            "Epoch 135/1000, Training Loss: 0.0029740703187123283, Test Loss: 0.0024253474177254545\n",
            "Epoch 136/1000, Training Loss: 0.0029717289313407483, Test Loss: 0.002422982731826788\n",
            "Epoch 137/1000, Training Loss: 0.002969340511023918, Test Loss: 0.0024205727077179356\n",
            "Epoch 138/1000, Training Loss: 0.002966904461049294, Test Loss: 0.002418116849127219\n",
            "Epoch 139/1000, Training Loss: 0.0029644201991485602, Test Loss: 0.0024156146784122773\n",
            "Epoch 140/1000, Training Loss: 0.0029618871581469264, Test Loss: 0.0024130657373601008\n",
            "Epoch 141/1000, Training Loss: 0.0029593047865658193, Test Loss: 0.0024104695879382256\n",
            "Epoch 142/1000, Training Loss: 0.002956672549176912, Test Loss: 0.0024078258129941305\n",
            "Epoch 143/1000, Training Loss: 0.0029539899275060205, Test Loss: 0.0024051340169004886\n",
            "Epoch 144/1000, Training Loss: 0.0029512564202860106, Test Loss: 0.0024023938261444784\n",
            "Epoch 145/1000, Training Loss: 0.002948471543858449, Test Loss: 0.002399604889860107\n",
            "Epoch 146/1000, Training Loss: 0.002945634832524377, Test Loss: 0.0023967668803030547\n",
            "Epoch 147/1000, Training Loss: 0.0029427458388452286, Test Loss: 0.002393879493268331\n",
            "Epoch 148/1000, Training Loss: 0.002939804133895445, Test Loss: 0.0023909424484516184\n",
            "Epoch 149/1000, Training Loss: 0.0029368093074690045, Test Loss: 0.002387955489755883\n",
            "Epoch 150/1000, Training Loss: 0.002933760968242546, Test Loss: 0.0023849183855454295\n",
            "Epoch 151/1000, Training Loss: 0.002930658743898315, Test Loss: 0.0023818309288501805\n",
            "Epoch 152/1000, Training Loss: 0.002927502281210552, Test Loss: 0.0023786929375234737\n",
            "Epoch 153/1000, Training Loss: 0.002924291246099366, Test Loss: 0.0023755042543571656\n",
            "Epoch 154/1000, Training Loss: 0.0029210253236563876, Test Loss: 0.002372264747158229\n",
            "Epoch 155/1000, Training Loss: 0.0029177042181467872, Test Loss: 0.002368974308791344\n",
            "Epoch 156/1000, Training Loss: 0.0029143276529923563, Test Loss: 0.002365632857192278\n",
            "Epoch 157/1000, Training Loss: 0.0029108953707404563, Test Loss: 0.0023622403353569755\n",
            "Epoch 158/1000, Training Loss: 0.0029074071330236306, Test Loss: 0.0023587967113113863\n",
            "Epoch 159/1000, Training Loss: 0.0029038627205146047, Test Loss: 0.002355301978067047\n",
            "Epoch 160/1000, Training Loss: 0.0029002619328812517, Test Loss: 0.002351756153567353\n",
            "Epoch 161/1000, Training Loss: 0.0028966045887458785, Test Loss: 0.0023481592806292666\n",
            "Epoch 162/1000, Training Loss: 0.0028928905256528996, Test Loss: 0.002344511426885001\n",
            "Epoch 163/1000, Training Loss: 0.0028891196000486313, Test Loss: 0.002340812684727835\n",
            "Epoch 164/1000, Training Loss: 0.002885291687276532, Test Loss: 0.002337063171265903\n",
            "Epoch 165/1000, Training Loss: 0.002881406681590769, Test Loss: 0.0023332630282872874\n",
            "Epoch 166/1000, Training Loss: 0.002877464496190511, Test Loss: 0.0023294124222392946\n",
            "Epoch 167/1000, Training Loss: 0.0028734650632768343, Test Loss: 0.00232551154422422\n",
            "Epoch 168/1000, Training Loss: 0.002869408334133575, Test Loss: 0.002321560610013357\n",
            "Epoch 169/1000, Training Loss: 0.0028652942792329232, Test Loss: 0.0023175598600804013\n",
            "Epoch 170/1000, Training Loss: 0.002861122888365974, Test Loss: 0.002313509559654749\n",
            "Epoch 171/1000, Training Loss: 0.0028568941707978793, Test Loss: 0.0023094099987946088\n",
            "Epoch 172/1000, Training Loss: 0.002852608155446702, Test Loss: 0.002305261492479198\n",
            "Epoch 173/1000, Training Loss: 0.0028482648910844815, Test Loss: 0.002301064380718637\n",
            "Epoch 174/1000, Training Loss: 0.0028438644465585195, Test Loss: 0.00229681902867962\n",
            "Epoch 175/1000, Training Loss: 0.0028394069110303367, Test Loss: 0.002292525826824255\n",
            "Epoch 176/1000, Training Loss: 0.0028348923942292685, Test Loss: 0.0022881851910589827\n",
            "Epoch 177/1000, Training Loss: 0.0028303210267172133, Test Loss: 0.002283797562889895\n",
            "Epoch 178/1000, Training Loss: 0.002825692960160574, Test Loss: 0.0022793634095802902\n",
            "Epoch 179/1000, Training Loss: 0.002821008367605037, Test Loss: 0.0022748832243058045\n",
            "Epoch 180/1000, Training Loss: 0.0028162674437484745, Test Loss: 0.0022703575263020932\n",
            "Epoch 181/1000, Training Loss: 0.0028114704052068958, Test Loss: 0.0022657868609995777\n",
            "Epoch 182/1000, Training Loss: 0.0028066174907680715, Test Loss: 0.0022611718001394818\n",
            "Epoch 183/1000, Training Loss: 0.0028017089616272045, Test Loss: 0.0022565129418650547\n",
            "Epoch 184/1000, Training Loss: 0.0027967451015987804, Test Loss: 0.002251810910781661\n",
            "Epoch 185/1000, Training Loss: 0.0027917262172985494, Test Loss: 0.002247066357979169\n",
            "Epoch 186/1000, Training Loss: 0.0027866526382894353, Test Loss: 0.002242279961009964\n",
            "Epoch 187/1000, Training Loss: 0.002781524717185046, Test Loss: 0.00223745242381575\n",
            "Epoch 188/1000, Training Loss: 0.002776342829704407, Test Loss: 0.0022325844765962834\n",
            "Epoch 189/1000, Training Loss: 0.002771107374671476, Test Loss: 0.0022276768756131454\n",
            "Epoch 190/1000, Training Loss: 0.002765818773953002, Test Loss: 0.0022227304029216848\n",
            "Epoch 191/1000, Training Loss: 0.002760477472328326, Test Loss: 0.00221774586602433\n",
            "Epoch 192/1000, Training Loss: 0.0027550839372847925, Test Loss: 0.002212724097438613\n",
            "Epoch 193/1000, Training Loss: 0.0027496386587325173, Test Loss: 0.0022076659541733503\n",
            "Epoch 194/1000, Training Loss: 0.0027441421486324403, Test Loss: 0.0022025723171066745\n",
            "Epoch 195/1000, Training Loss: 0.0027385949405316907, Test Loss: 0.002197444090259801\n",
            "Epoch 196/1000, Training Loss: 0.0027329975890005493, Test Loss: 0.0021922821999607062\n",
            "Epoch 197/1000, Training Loss: 0.0027273506689654806, Test Loss: 0.0021870875938921823\n",
            "Epoch 198/1000, Training Loss: 0.0027216547749329746, Test Loss: 0.0021818612400190843\n",
            "Epoch 199/1000, Training Loss: 0.0027159105200992113, Test Loss: 0.0021766041253899106\n",
            "Epoch 200/1000, Training Loss: 0.0027101185353408694, Test Loss: 0.002171317254808292\n",
            "Epoch 201/1000, Training Loss: 0.002704279468082709, Test Loss: 0.0021660016493703265\n",
            "Epoch 202/1000, Training Loss: 0.0026983939810379205, Test Loss: 0.0021606583448641557\n",
            "Epoch 203/1000, Training Loss: 0.0026924627508175735, Test Loss: 0.0021552883900286144\n",
            "Epoch 204/1000, Training Loss: 0.0026864864664058845, Test Loss: 0.0021498928446682134\n",
            "Epoch 205/1000, Training Loss: 0.002680465827498387, Test Loss: 0.0021444727776222124\n",
            "Epoch 206/1000, Training Loss: 0.0026744015427004886, Test Loss: 0.002139029264585965\n",
            "Epoch 207/1000, Training Loss: 0.002668294327584281, Test Loss: 0.002133563385783177\n",
            "Epoch 208/1000, Training Loss: 0.0026621449026018734, Test Loss: 0.0021280762234882023\n",
            "Epoch 209/1000, Training Loss: 0.002655953990853853, Test Loss: 0.002122568859397856\n",
            "Epoch 210/1000, Training Loss: 0.0026497223157119117, Test Loss: 0.002117042371852716\n",
            "Epoch 211/1000, Training Loss: 0.002643450598294979, Test Loss: 0.00211149783290821\n",
            "Epoch 212/1000, Training Loss: 0.002637139554798553, Test Loss: 0.0021059363052561586\n",
            "Epoch 213/1000, Training Loss: 0.002630789893677222, Test Loss: 0.002100358838997751\n",
            "Epoch 214/1000, Training Loss: 0.0026244023126806414, Test Loss: 0.0020947664682691883\n",
            "Epoch 215/1000, Training Loss: 0.002617977495743477, Test Loss: 0.002089160207721477\n",
            "Epoch 216/1000, Training Loss: 0.002611516109729989, Test Loss: 0.002083541048855968\n",
            "Epoch 217/1000, Training Loss: 0.002605018801034097, Test Loss: 0.0020779099562173607\n",
            "Epoch 218/1000, Training Loss: 0.0025984861920358617, Test Loss: 0.002072267863445927\n",
            "Epoch 219/1000, Training Loss: 0.002591918877415302, Test Loss: 0.0020666156691906386\n",
            "Epoch 220/1000, Training Loss: 0.0025853174203244855, Test Loss: 0.0020609542328847764\n",
            "Epoch 221/1000, Training Loss: 0.0025786823484186907, Test Loss: 0.0020552843703854056\n",
            "Epoch 222/1000, Training Loss: 0.0025720141497472533, Test Loss: 0.002049606849477772\n",
            "Epoch 223/1000, Training Loss: 0.002565313268504446, Test Loss: 0.0020439223852453197\n",
            "Epoch 224/1000, Training Loss: 0.0025585801006404165, Test Loss: 0.0020382316353055453\n",
            "Epoch 225/1000, Training Loss: 0.0025518149893317232, Test Loss: 0.0020325351949113217\n",
            "Epoch 226/1000, Training Loss: 0.0025450182203105194, Test Loss: 0.002026833591916662\n",
            "Epoch 227/1000, Training Loss: 0.0025381900170507537, Test Loss: 0.0020211272816050944\n",
            "Epoch 228/1000, Training Loss: 0.0025313305358090673, Test Loss: 0.002015416641377954\n",
            "Epoch 229/1000, Training Loss: 0.002524439860517185, Test Loss: 0.0020097019652989085\n",
            "Epoch 230/1000, Training Loss: 0.0025175179975216577, Test Loss: 0.002003983458489941\n",
            "Epoch 231/1000, Training Loss: 0.002510564870165737, Test Loss: 0.0019982612313727915\n",
            "Epoch 232/1000, Training Loss: 0.00250358031320698, Test Loss: 0.001992535293748592\n",
            "Epoch 233/1000, Training Loss: 0.002496564067062859, Test Loss: 0.0019868055487069747\n",
            "Epoch 234/1000, Training Loss: 0.002489515771875213, Test Loss: 0.0019810717863544317\n",
            "Epoch 235/1000, Training Loss: 0.002482434961382798, Test Loss: 0.001975333677350059\n",
            "Epoch 236/1000, Training Loss: 0.0024753210565894935, Test Loss: 0.001969590766235108\n",
            "Epoch 237/1000, Training Loss: 0.0024681733592138728, Test Loss: 0.0019638424645409035\n",
            "Epoch 238/1000, Training Loss: 0.002460991044903818, Test Loss: 0.0019580880436577356\n",
            "Epoch 239/1000, Training Loss: 0.002453773156197753, Test Loss: 0.0019523266274453347\n",
            "Epoch 240/1000, Training Loss: 0.002446518595211747, Test Loss: 0.0019465571845633817\n",
            "Epoch 241/1000, Training Loss: 0.002439226116029263, Test Loss: 0.0019407785204982484\n",
            "Epoch 242/1000, Training Loss: 0.0024318943167677396, Test Loss: 0.001934989269259938\n",
            "Epoch 243/1000, Training Loss: 0.0024245216312933425, Test Loss: 0.0019291878847207398\n",
            "Epoch 244/1000, Training Loss: 0.0024171063205523395, Test Loss: 0.0019233726315647749\n",
            "Epoch 245/1000, Training Loss: 0.002409646463484348, Test Loss: 0.001917541575815085\n",
            "Epoch 246/1000, Training Loss: 0.002402139947479478, Test Loss: 0.001911692574902516\n",
            "Epoch 247/1000, Training Loss: 0.0023945844583379157, Test Loss: 0.001905823267238193\n",
            "Epoch 248/1000, Training Loss: 0.0023869774696869124, Test Loss: 0.0018999310612490502\n",
            "Epoch 249/1000, Training Loss: 0.0023793162318064896, Test Loss: 0.0018940131238337201\n",
            "Epoch 250/1000, Training Loss: 0.0023715977598113063, Test Loss: 0.0018880663681940326\n",
            "Epoch 251/1000, Training Loss: 0.0023638188211324005, Test Loss: 0.0018820874409958166\n",
            "Epoch 252/1000, Training Loss: 0.002355975922238694, Test Loss: 0.001876072708811464\n",
            "Epoch 253/1000, Training Loss: 0.00234806529453444, Test Loss: 0.0018700182437960953\n",
            "Epoch 254/1000, Training Loss: 0.0023400828793654154, Test Loss: 0.0018639198085495133\n",
            "Epoch 255/1000, Training Loss: 0.002332024312063569, Test Loss: 0.0018577728401173938\n",
            "Epoch 256/1000, Training Loss: 0.0023238849049573612, Test Loss: 0.0018515724330878968\n",
            "Epoch 257/1000, Training Loss: 0.0023156596292734557, Test Loss: 0.0018453133217444762\n",
            "Epoch 258/1000, Training Loss: 0.0023073430958549467, Test Loss: 0.0018389898612424255\n",
            "Epoch 259/1000, Training Loss: 0.0022989295346225506, Test Loss: 0.0018325960077864559\n",
            "Epoch 260/1000, Training Loss: 0.002290412772708478, Test Loss: 0.001826125297799983\n",
            "Epoch 261/1000, Training Loss: 0.0022817862111989525, Test Loss: 0.0018195708260945625\n",
            "Epoch 262/1000, Training Loss: 0.002273042800431236, Test Loss: 0.0018129252230714948\n",
            "Epoch 263/1000, Training Loss: 0.0022641750138057415, Test Loss: 0.001806180631017839\n",
            "Epoch 264/1000, Training Loss: 0.002255174820094695, Test Loss: 0.001799328679597836\n",
            "Epoch 265/1000, Training Loss: 0.0022460336542574635, Test Loss: 0.001792360460689631\n",
            "Epoch 266/1000, Training Loss: 0.0022367423868112023, Test Loss: 0.0017852665027782819\n",
            "Epoch 267/1000, Training Loss: 0.002227291291856386, Test Loss: 0.0017780367451918672\n",
            "Epoch 268/1000, Training Loss: 0.0022176700139229194, Test Loss: 0.001770660512560437\n",
            "Epoch 269/1000, Training Loss: 0.0022078675338878, Test Loss: 0.001763126489990946\n",
            "Epoch 270/1000, Training Loss: 0.002197872134323551, Test Loss: 0.0017554226995878002\n",
            "Epoch 271/1000, Training Loss: 0.002187671364773134, Test Loss: 0.00174753647911176\n",
            "Epoch 272/1000, Training Loss: 0.0021772520076172148, Test Loss: 0.0017394544637626417\n",
            "Epoch 273/1000, Training Loss: 0.002166600045409782, Test Loss: 0.0017311625722959303\n",
            "Epoch 274/1000, Training Loss: 0.002155700630814726, Test Loss: 0.0017226459989417\n",
            "Epoch 275/1000, Training Loss: 0.002144538060586052, Test Loss: 0.0017138892128863816\n",
            "Epoch 276/1000, Training Loss: 0.0021330957554037897, Test Loss: 0.0017048759674009254\n",
            "Epoch 277/1000, Training Loss: 0.0021213562478113223, Test Loss: 0.0016955893210471844\n",
            "Epoch 278/1000, Training Loss: 0.0021093011809995834, Test Loss: 0.0016860116737565476\n",
            "Epoch 279/1000, Training Loss: 0.0020969113217464744, Test Loss: 0.0016761248209336433\n",
            "Epoch 280/1000, Training Loss: 0.002084166591435611, Test Loss: 0.001665910029067394\n",
            "Epoch 281/1000, Training Loss: 0.0020710461197259943, Test Loss: 0.0016553481365964664\n",
            "Epoch 282/1000, Training Loss: 0.0020575283260873285, Test Loss: 0.001644419683929552\n",
            "Epoch 283/1000, Training Loss: 0.002043591034999057, Test Loss: 0.0016331050765054696\n",
            "Epoch 284/1000, Training Loss: 0.002029211631054762, Test Loss: 0.0016213847845259009\n",
            "Epoch 285/1000, Training Loss: 0.002014367260409865, Test Loss: 0.0016092395824310421\n",
            "Epoch 286/1000, Training Loss: 0.0019990350848223724, Test Loss: 0.0015966508302417625\n",
            "Epoch 287/1000, Training Loss: 0.001983192593800666, Test Loss: 0.0015836007974983212\n",
            "Epoch 288/1000, Training Loss: 0.0019668179789092102, Test Loss: 0.0015700730286458758\n",
            "Epoch 289/1000, Training Loss: 0.0019498905719174163, Test Loss: 0.0015560527463519313\n",
            "Epoch 290/1000, Training Loss: 0.0019323913450682586, Test Loss: 0.0015415272864455986\n",
            "Epoch 291/1000, Training Loss: 0.0019143034672317078, Test Loss: 0.0015264865550643776\n",
            "Epoch 292/1000, Training Loss: 0.0018956129041661754, Test Loss: 0.001510923495369338\n",
            "Epoch 293/1000, Training Loss: 0.001876309044798597, Test Loss: 0.0014948345480886743\n",
            "Epoch 294/1000, Training Loss: 0.0018563853288381966, Test Loss: 0.001478220087447256\n",
            "Epoch 295/1000, Training Loss: 0.001835839844884817, Test Loss: 0.0014610848120057266\n",
            "Epoch 296/1000, Training Loss: 0.0018146758633998078, Test Loss: 0.0014434380687957887\n",
            "Epoch 297/1000, Training Loss: 0.0017929022664836028, Test Loss: 0.0014252940890653045\n",
            "Epoch 298/1000, Training Loss: 0.0017705338372757742, Test Loss: 0.0014066721150474478\n",
            "Epoch 299/1000, Training Loss: 0.0017475913765948336, Test Loss: 0.0013875963995259618\n",
            "Epoch 300/1000, Training Loss: 0.0017241016233035264, Test Loss: 0.001368096063682463\n",
            "Epoch 301/1000, Training Loss: 0.001700096967320289, Test Loss: 0.0013482048039129713\n",
            "Epoch 302/1000, Training Loss: 0.0016756149590408094, Test Loss: 0.0013279604451098704\n",
            "Epoch 303/1000, Training Loss: 0.0016506976345012669, Test Loss: 0.0013074043463160094\n",
            "Epoch 304/1000, Training Loss: 0.0016253906899667526, Test Loss: 0.0012865806743711649\n",
            "Epoch 305/1000, Training Loss: 0.0015997425509202226, Test Loss: 0.0012655355714608308\n",
            "Epoch 306/1000, Training Loss: 0.0015738033872565395, Test Loss: 0.0012443162521638058\n",
            "Epoch 307/1000, Training Loss: 0.0015476241281429653, Test Loss: 0.0012229700732031181\n",
            "Epoch 308/1000, Training Loss: 0.0015212555265730356, Test Loss: 0.001201543623184273\n",
            "Epoch 309/1000, Training Loss: 0.001494747315914551, Test Loss: 0.0011800818791271117\n",
            "Epoch 310/1000, Training Loss: 0.001468147490047449, Test Loss: 0.0011586274712848698\n",
            "Epoch 311/1000, Training Loss: 0.001441501726549579, Test Loss: 0.001137220088199872\n",
            "Epoch 312/1000, Training Loss: 0.00141485296032047, Test Loss: 0.0011158960415399196\n",
            "Epoch 313/1000, Training Loss: 0.0013882411042736153, Test Loss: 0.0010946879968051857\n",
            "Epoch 314/1000, Training Loss: 0.0013617029051195913, Test Loss: 0.0010736248633291573\n",
            "Epoch 315/1000, Training Loss: 0.0013352719162198397, Test Loss: 0.0010527318266079094\n",
            "Epoch 316/1000, Training Loss: 0.0013089785660406843, Test Loss: 0.0010320304987845646\n",
            "Epoch 317/1000, Training Loss: 0.0012828502996107493, Test Loss: 0.001011539159355238\n",
            "Epoch 318/1000, Training Loss: 0.0012569117711309623, Test Loss: 0.0009912730575662478\n",
            "Epoch 319/1000, Training Loss: 0.0012311850679769733, Test Loss: 0.0009712447498925099\n",
            "Epoch 320/1000, Training Loss: 0.0012056899492499404, Test Loss: 0.0009514644496135211\n",
            "Epoch 321/1000, Training Loss: 0.0011804440853206157, Test Loss: 0.0009319403700352062\n",
            "Epoch 322/1000, Training Loss: 0.0011554632881180923, Test Loss: 0.0009126790476670093\n",
            "Epoch 323/1000, Training Loss: 0.0011307617249865076, Test Loss: 0.0008936856361558069\n",
            "Epoch 324/1000, Training Loss: 0.0011063521116161264, Test Loss: 0.0008749641656897746\n",
            "Epoch 325/1000, Training Loss: 0.0010822458817748672, Test Loss: 0.0008565177657648017\n",
            "Epoch 326/1000, Training Loss: 0.0010584533333084481, Test Loss: 0.0008383488516245167\n",
            "Epoch 327/1000, Training Loss: 0.0010349837511693819, Test Loss: 0.0008204592763947975\n",
            "Epoch 328/1000, Training Loss: 0.0010118455091286246, Test Loss: 0.0008028504520325049\n",
            "Epoch 329/1000, Training Loss: 0.0009890461523832186, Test Loss: 0.0007855234428131103\n",
            "Epoch 330/1000, Training Loss: 0.0009665924635644623, Test Loss: 0.0007684790353087895\n",
            "Epoch 331/1000, Training Loss: 0.0009444905147374326, Test Loss: 0.0007517177887634411\n",
            "Epoch 332/1000, Training Loss: 0.0009227457079193949, Test Loss: 0.0007352400695426539\n",
            "Epoch 333/1000, Training Loss: 0.0009013628064793201, Test Loss: 0.0007190460729964116\n",
            "Epoch 334/1000, Training Loss: 0.0008803459595513504, Test Loss: 0.0007031358356743652\n",
            "Epoch 335/1000, Training Loss: 0.0008596987213318241, Test Loss: 0.0006875092404173531\n",
            "Epoch 336/1000, Training Loss: 0.0008394240668542526, Test Loss: 0.0006721660164412977\n",
            "Epoch 337/1000, Training Loss: 0.0008195244055661225, Test Loss: 0.0006571057361484616\n",
            "Epoch 338/1000, Training Loss: 0.0008000015937760498, Test Loss: 0.00064232781005607\n",
            "Epoch 339/1000, Training Loss: 0.0007808569468065726, Test Loss: 0.0006278314809282837\n",
            "Epoch 340/1000, Training Loss: 0.0007620912514807695, Test Loss: 0.0006136158179357814\n",
            "Epoch 341/1000, Training Loss: 0.0007437047793910751, Test Loss: 0.0005996797114460875\n",
            "Epoch 342/1000, Training Loss: 0.0007256973012459878, Test Loss: 0.0005860218688643722\n",
            "Epoch 343/1000, Training Loss: 0.0007080681024639081, Test Loss: 0.0005726408117954426\n",
            "Epoch 344/1000, Training Loss: 0.0006908160000805657, Test Loss: 0.000559534874678347\n",
            "Epoch 345/1000, Training Loss: 0.0006739393609555263, Test Loss: 0.0005467022049519274\n",
            "Epoch 346/1000, Training Loss: 0.0006574361212011393, Test Loss: 0.0005341407647384297\n",
            "Epoch 347/1000, Training Loss: 0.0006413038067118012, Test Loss: 0.0005218483339796611\n",
            "Epoch 348/1000, Training Loss: 0.0006255395546397526, Test Loss: 0.0005098225149226865\n",
            "Epoch 349/1000, Training Loss: 0.0006101401356436996, Test Loss: 0.0004980607378269354\n",
            "Epoch 350/1000, Training Loss: 0.0005951019767261544, Test Loss: 0.00048656026774939826\n",
            "Epoch 351/1000, Training Loss: 0.0005804211844726484, Test Loss: 0.00047531821225716764\n",
            "Epoch 352/1000, Training Loss: 0.0005660935685092463, Test Loss: 0.00046433152991520266\n",
            "Epoch 353/1000, Training Loss: 0.0005521146650026359, Test Loss: 0.0004535970394003426\n",
            "Epoch 354/1000, Training Loss: 0.0005384797600382295, Test Loss: 0.00044311142909907354\n",
            "Epoch 355/1000, Training Loss: 0.0005251839127251927, Test Loss: 0.0004328712670553195\n",
            "Epoch 356/1000, Training Loss: 0.0005122219778924065, Test Loss: 0.0004228730111450053\n",
            "Epoch 357/1000, Training Loss: 0.0004995886282549531, Test Loss: 0.00041311301936521256\n",
            "Epoch 358/1000, Training Loss: 0.0004872783759468334, Test Loss: 0.000403587560137548\n",
            "Epoch 359/1000, Training Loss: 0.0004752855933313467, Test Loss: 0.00039429282253695074\n",
            "Epoch 360/1000, Training Loss: 0.00046360453301582137, Test Loss: 0.00038522492636863126\n",
            "Epoch 361/1000, Training Loss: 0.0004522293470118535, Test Loss: 0.0003763799320267476\n",
            "Epoch 362/1000, Training Loss: 0.00044115410499575604, Test Loss: 0.0003677538500788176\n",
            "Epoch 363/1000, Training Loss: 0.0004303728116363148, Test Loss: 0.0003593426505294035\n",
            "Epoch 364/1000, Training Loss: 0.0004198794229683946, Test Loss: 0.0003511422717255714\n",
            "Epoch 365/1000, Training Loss: 0.00040966786180105744, Test Loss: 0.00034314862887461584\n",
            "Epoch 366/1000, Training Loss: 0.00039973203215792457, Test Loss: 0.0003353576221518078\n",
            "Epoch 367/1000, Training Loss: 0.0003900658327555329, Test Loss: 0.00032776514438248973\n",
            "Epoch 368/1000, Training Loss: 0.00038066316953218766, Test Loss: 0.00032036708828844573\n",
            "Epoch 369/1000, Training Loss: 0.0003715179672458778, Test Loss: 0.0003131593532936717\n",
            "Epoch 370/1000, Training Loss: 0.000362624180164604, Test Loss: 0.00030613785188887883\n",
            "Epoch 371/1000, Training Loss: 0.00035397580187666834, Test Loss: 0.0002992985155579152\n",
            "Epoch 372/1000, Training Loss: 0.00034556687425169074, Test Loss: 0.00029263730027238416\n",
            "Epoch 373/1000, Training Loss: 0.0003373914955858067, Test Loss: 0.00028615019156350387\n",
            "Epoch 374/1000, Training Loss: 0.0003294438279663034, Test Loss: 0.0002798332091823266\n",
            "Epoch 375/1000, Training Loss: 0.00032171810389247353, Test Loss: 0.000273682411361314\n",
            "Epoch 376/1000, Training Loss: 0.0003142086321902053, Test Loss: 0.0002676938986915799\n",
            "Epoch 377/1000, Training Loss: 0.00030690980325838284, Test Loss: 0.0002618638176312686\n",
            "Epoch 378/1000, Training Loss: 0.0002998160936851385, Test Loss: 0.0002561883636612497\n",
            "Epoch 379/1000, Training Loss: 0.0002929220702717549, Test Loss: 0.0002506637841048536\n",
            "Epoch 380/1000, Training Loss: 0.00028622239350153635, Test Loss: 0.0002452863806287315\n",
            "Epoch 381/1000, Training Loss: 0.00027971182049006725, Test Loss: 0.00024005251144194596\n",
            "Epoch 382/1000, Training Loss: 0.000273385207452465, Test Loss: 0.00023495859321047104\n",
            "Epoch 383/1000, Training Loss: 0.0002672375117220045, Test Loss: 0.00023000110270400285\n",
            "Epoch 384/1000, Training Loss: 0.00026126379335334116, Test Loss: 0.0002251765781917751\n",
            "Epoch 385/1000, Training Loss: 0.00025545921634218633, Test Loss: 0.00022048162060363702\n",
            "Epoch 386/1000, Training Loss: 0.00024981904949197055, Test Loss: 0.00021591289447225398\n",
            "Epoch 387/1000, Training Loss: 0.0002443386669565054, Test Loss: 0.00021146712867170097\n",
            "Epoch 388/1000, Training Loss: 0.0002390135484863127, Test Loss: 0.0002071411169672491\n",
            "Epoch 389/1000, Training Loss: 0.00023383927940474495, Test Loss: 0.0002029317183904847\n",
            "Epoch 390/1000, Training Loss: 0.0002288115503386022, Test Loss: 0.00019883585745331033\n",
            "Epoch 391/1000, Training Loss: 0.00022392615672653295, Test Loss: 0.000194850524213778\n",
            "Epoch 392/1000, Training Loss: 0.00021917899812703628, Test Loss: 0.00019097277420599226\n",
            "Epoch 393/1000, Training Loss: 0.00021456607734657605, Test Loss: 0.00018719972824578137\n",
            "Epoch 394/1000, Training Loss: 0.00021008349940693078, Test Loss: 0.00018352857212311488\n",
            "Epoch 395/1000, Training Loss: 0.00020572747036963756, Test Loss: 0.00017995655619169236\n",
            "Epoch 396/1000, Training Loss: 0.00020149429603413634, Test Loss: 0.00017648099486546263\n",
            "Epoch 397/1000, Training Loss: 0.00019738038052504811, Test Loss: 0.00017309926603128597\n",
            "Epoch 398/1000, Training Loss: 0.0001933822247828682, Test Loss: 0.00016980881038635223\n",
            "Epoch 399/1000, Training Loss: 0.00018949642497127867, Test Loss: 0.00016660713070840546\n",
            "Epoch 400/1000, Training Loss: 0.00018571967081325893, Test Loss: 0.00016349179106630706\n",
            "Epoch 401/1000, Training Loss: 0.00018204874386722026, Test Loss: 0.00016046041597795701\n",
            "Epoch 402/1000, Training Loss: 0.00017848051575344628, Test Loss: 0.00015751068952209342\n",
            "Epoch 403/1000, Training Loss: 0.0001750119463402781, Test Loss: 0.00015464035441002742\n",
            "Epoch 404/1000, Training Loss: 0.00017164008189866095, Test Loss: 0.00015184721102292742\n",
            "Epoch 405/1000, Training Loss: 0.00016836205323293, Test Loss: 0.0001491291164198557\n",
            "Epoch 406/1000, Training Loss: 0.0001651750737949555, Test Loss: 0.00014648398332133152\n",
            "Epoch 407/1000, Training Loss: 0.00016207643778816868, Test Loss: 0.00014390977907286298\n",
            "Epoch 408/1000, Training Loss: 0.0001590635182673046, Test Loss: 0.0001414045245924925\n",
            "Epoch 409/1000, Training Loss: 0.00015613376523919293, Test Loss: 0.00013896629330611428\n",
            "Epoch 410/1000, Training Loss: 0.00015328470376930304, Test Loss: 0.00013659321007394225\n",
            "Epoch 411/1000, Training Loss: 0.00015051393209835321, Test Loss: 0.00013428345011129387\n",
            "Epoch 412/1000, Training Loss: 0.00014781911977274396, Test Loss: 0.00013203523790650352\n",
            "Epoch 413/1000, Training Loss: 0.0001451980057922258, Test Loss: 0.00012984684613858268\n",
            "Epoch 414/1000, Training Loss: 0.0001426483967777575, Test Loss: 0.00012771659459696018\n",
            "Epoch 415/1000, Training Loss: 0.00014016816516221355, Test Loss: 0.00012564284910544758\n",
            "Epoch 416/1000, Training Loss: 0.00013775524740619214, Test Loss: 0.00012362402045232562\n",
            "Epoch 417/1000, Training Loss: 0.00013540764224095206, Test Loss: 0.00012165856332830186\n",
            "Epoch 418/1000, Training Loss: 0.0001331234089401607, Test Loss: 0.00011974497527386146\n",
            "Epoch 419/1000, Training Loss: 0.00013090066562192375, Test Loss: 0.00011788179563740793\n",
            "Epoch 420/1000, Training Loss: 0.00012873758758230478, Test Loss: 0.00011606760454540154\n",
            "Epoch 421/1000, Training Loss: 0.0001266324056613602, Test Loss: 0.00011430102188558943\n",
            "Epoch 422/1000, Training Loss: 0.00012458340464248426, Test Loss: 0.00011258070630426102\n",
            "Epoch 423/1000, Training Loss: 0.00012258892168573007, Test Loss: 0.00011090535421837263\n",
            "Epoch 424/1000, Training Loss: 0.000120647344795553, Test Loss: 0.00010927369884322795\n",
            "Epoch 425/1000, Training Loss: 0.00011875711132335814, Test Loss: 0.00010768450923636423\n",
            "Epoch 426/1000, Training Loss: 0.00011691670650501954, Test Loss: 0.00010613658935811885\n",
            "Epoch 427/1000, Training Loss: 0.00011512466203348468, Test Loss: 0.00010462877714933119\n",
            "Epoch 428/1000, Training Loss: 0.00011337955466644847, Test Loss: 0.00010315994362652003\n",
            "Epoch 429/1000, Training Loss: 0.00011168000486898286, Test Loss: 0.0001017289919948106\n",
            "Epoch 430/1000, Training Loss: 0.00011002467549094496, Test Loss: 0.00010033485677882667\n",
            "Epoch 431/1000, Training Loss: 0.0001084122704788889, Test Loss: 9.897650297169032e-05\n",
            "Epoch 432/1000, Training Loss: 0.00010684153362216031, Test Loss: 9.76529252022273e-05\n",
            "Epoch 433/1000, Training Loss: 0.00010531124733278604, Test Loss: 9.636314692041976e-05\n",
            "Epoch 434/1000, Training Loss: 0.00010382023145872134, Test Loss: 9.510621960110428e-05\n",
            "Epoch 435/1000, Training Loss: 0.00010236734212997675, Test Loss: 9.388122196587372e-05\n",
            "Epoch 436/1000, Training Loss: 0.0001009514706371142, Test Loss: 9.268725922310915e-05\n",
            "Epoch 437/1000, Training Loss: 9.957154234155268e-05, Test Loss: 9.152346232601865e-05\n",
            "Epoch 438/1000, Training Loss: 9.822651561711502e-05, Test Loss: 9.038898724855015e-05\n",
            "Epoch 439/1000, Training Loss: 9.69153808222167e-05, Test Loss: 8.9283014279008e-05\n",
            "Epoch 440/1000, Training Loss: 9.563715930207639e-05, Test Loss: 8.820474733117875e-05\n",
            "Epoch 441/1000, Training Loss: 9.439090242033111e-05, Test Loss: 8.715341327276663e-05\n",
            "Epoch 442/1000, Training Loss: 9.317569061938473e-05, Test Loss: 8.612826127088489e-05\n",
            "Epoch 443/1000, Training Loss: 9.199063250887796e-05, Test Loss: 8.512856215438721e-05\n",
            "Epoch 444/1000, Training Loss: 9.083486398158856e-05, Test Loss: 8.41536077927556e-05\n",
            "Epoch 445/1000, Training Loss: 8.970754735612985e-05, Test Loss: 8.320271049129108e-05\n",
            "Epoch 446/1000, Training Loss: 8.860787054574988e-05, Test Loss: 8.227520240229716e-05\n",
            "Epoch 447/1000, Training Loss: 8.753504625262912e-05, Test Loss: 8.137043495201737e-05\n",
            "Epoch 448/1000, Training Loss: 8.648831118697128e-05, Test Loss: 8.04877782829839e-05\n",
            "Epoch 449/1000, Training Loss: 8.546692531024349e-05, Test Loss: 7.962662071148678e-05\n",
            "Epoch 450/1000, Training Loss: 8.447017110194599e-05, Test Loss: 7.87863681998809e-05\n",
            "Epoch 451/1000, Training Loss: 8.349735284924174e-05, Test Loss: 7.796644384339242e-05\n",
            "Epoch 452/1000, Training Loss: 8.254779595881982e-05, Test Loss: 7.716628737112352e-05\n",
            "Epoch 453/1000, Training Loss: 8.162084629039851e-05, Test Loss: 7.638535466096273e-05\n",
            "Epoch 454/1000, Training Loss: 8.071586951120637e-05, Test Loss: 7.562311726805336e-05\n",
            "Epoch 455/1000, Training Loss: 7.983225047087613e-05, Test Loss: 7.487906196653275e-05\n",
            "Epoch 456/1000, Training Loss: 7.896939259614556e-05, Test Loss: 7.415269030422797e-05\n",
            "Epoch 457/1000, Training Loss: 7.812671730478449e-05, Test Loss: 7.344351816999284e-05\n",
            "Epoch 458/1000, Training Loss: 7.730366343817589e-05, Test Loss: 7.275107537338007e-05\n",
            "Epoch 459/1000, Training Loss: 7.649968671199998e-05, Test Loss: 7.207490523635044e-05\n",
            "Epoch 460/1000, Training Loss: 7.571425918447512e-05, Test Loss: 7.141456419671381e-05\n",
            "Epoch 461/1000, Training Loss: 7.494686874161253e-05, Test Loss: 7.076962142300023e-05\n",
            "Epoch 462/1000, Training Loss: 7.419701859897918e-05, Test Loss: 7.013965844047757e-05\n",
            "Epoch 463/1000, Training Loss: 7.346422681945306e-05, Test Loss: 6.952426876801927e-05\n",
            "Epoch 464/1000, Training Loss: 7.274802584647186e-05, Test Loss: 6.892305756553684e-05\n",
            "Epoch 465/1000, Training Loss: 7.204796205230374e-05, Test Loss: 6.833564129170193e-05\n",
            "Epoch 466/1000, Training Loss: 7.136359530086215e-05, Test Loss: 6.7761647371681e-05\n",
            "Epoch 467/1000, Training Loss: 7.069449852460414e-05, Test Loss: 6.72007138746053e-05\n",
            "Epoch 468/1000, Training Loss: 7.004025731507491e-05, Test Loss: 6.665248920051923e-05\n",
            "Epoch 469/1000, Training Loss: 6.940046952666388e-05, Test Loss: 6.611663177654616e-05\n",
            "Epoch 470/1000, Training Loss: 6.877474489314715e-05, Test Loss: 6.559280976201123e-05\n",
            "Epoch 471/1000, Training Loss: 6.816270465660989e-05, Test Loss: 6.508070076227612e-05\n",
            "Epoch 472/1000, Training Loss: 6.756398120835262e-05, Test Loss: 6.457999155104127e-05\n",
            "Epoch 473/1000, Training Loss: 6.697821774139907e-05, Test Loss: 6.409037780087686e-05\n",
            "Epoch 474/1000, Training Loss: 6.640506791421621e-05, Test Loss: 6.361156382174485e-05\n",
            "Epoch 475/1000, Training Loss: 6.58441955253059e-05, Test Loss: 6.314326230729102e-05\n",
            "Epoch 476/1000, Training Loss: 6.529527419829592e-05, Test Loss: 6.268519408868008e-05\n",
            "Epoch 477/1000, Training Loss: 6.475798707720525e-05, Test Loss: 6.223708789576072e-05\n",
            "Epoch 478/1000, Training Loss: 6.423202653153331e-05, Test Loss: 6.179868012533812e-05\n",
            "Epoch 479/1000, Training Loss: 6.371709387087609e-05, Test Loss: 6.136971461636713e-05\n",
            "Epoch 480/1000, Training Loss: 6.321289906873517e-05, Test Loss: 6.0949942431840475e-05\n",
            "Epoch 481/1000, Training Loss: 6.271916049523432e-05, Test Loss: 6.053912164719888e-05\n",
            "Epoch 482/1000, Training Loss: 6.223560465844028e-05, Test Loss: 6.0137017145049395e-05\n",
            "Epoch 483/1000, Training Loss: 6.176196595402412e-05, Test Loss: 5.97434004160342e-05\n",
            "Epoch 484/1000, Training Loss: 6.129798642296106e-05, Test Loss: 5.935804936563654e-05\n",
            "Epoch 485/1000, Training Loss: 6.084341551702682e-05, Test Loss: 5.8980748126770665e-05\n",
            "Epoch 486/1000, Training Loss: 6.039800987182954e-05, Test Loss: 5.8611286877978524e-05\n",
            "Epoch 487/1000, Training Loss: 5.996153308712017e-05, Test Loss: 5.824946166706071e-05\n",
            "Epoch 488/1000, Training Loss: 5.953375551415442e-05, Test Loss: 5.789507423998791e-05\n",
            "Epoch 489/1000, Training Loss: 5.9114454049862125e-05, Test Loss: 5.754793187492697e-05\n",
            "Epoch 490/1000, Training Loss: 5.87034119376056e-05, Test Loss: 5.7207847221234814e-05\n",
            "Epoch 491/1000, Training Loss: 5.830041857431144e-05, Test Loss: 5.687463814326858e-05\n",
            "Epoch 492/1000, Training Loss: 5.790526932376162e-05, Test Loss: 5.6548127568868177e-05\n",
            "Epoch 493/1000, Training Loss: 5.7517765335838587e-05, Test Loss: 5.622814334236643e-05\n",
            "Epoch 494/1000, Training Loss: 5.713771337153711e-05, Test Loss: 5.5914518081999434e-05\n",
            "Epoch 495/1000, Training Loss: 5.67649256335418e-05, Test Loss: 5.5607089041574554e-05\n",
            "Epoch 496/1000, Training Loss: 5.6399219602194576e-05, Test Loss: 5.53056979762733e-05\n",
            "Epoch 497/1000, Training Loss: 5.604041787667125e-05, Test Loss: 5.501019101246417e-05\n",
            "Epoch 498/1000, Training Loss: 5.56883480211951e-05, Test Loss: 5.472041852139946e-05\n",
            "Epoch 499/1000, Training Loss: 5.534284241612183e-05, Test Loss: 5.443623499668455e-05\n",
            "Epoch 500/1000, Training Loss: 5.5003738113737626e-05, Test Loss: 5.4157498935399475e-05\n",
            "Epoch 501/1000, Training Loss: 5.467087669861225e-05, Test Loss: 5.3884072722766924e-05\n",
            "Epoch 502/1000, Training Loss: 5.4344104152357454e-05, Test Loss: 5.3615822520255066e-05\n",
            "Epoch 503/1000, Training Loss: 5.4023270722649445e-05, Test Loss: 5.335261815701566e-05\n",
            "Epoch 504/1000, Training Loss: 5.3708230796370217e-05, Test Loss: 5.3094333024547416e-05\n",
            "Epoch 505/1000, Training Loss: 5.339884277673659e-05, Test Loss: 5.2840843974500294e-05\n",
            "Epoch 506/1000, Training Loss: 5.3094968964285595e-05, Test Loss: 5.2592031219510566e-05\n",
            "Epoch 507/1000, Training Loss: 5.27964754415843e-05, Test Loss: 5.234777823698375e-05\n",
            "Epoch 508/1000, Training Loss: 5.2503231961552703e-05, Test Loss: 5.210797167573388e-05\n",
            "Epoch 509/1000, Training Loss: 5.2215111839272686e-05, Test Loss: 5.187250126539279e-05\n",
            "Epoch 510/1000, Training Loss: 5.193199184716841e-05, Test Loss: 5.1641259728497975e-05\n",
            "Epoch 511/1000, Training Loss: 5.165375211345632e-05, Test Loss: 5.141414269519163e-05\n",
            "Epoch 512/1000, Training Loss: 5.138027602375134e-05, Test Loss: 5.11910486204377e-05\n",
            "Epoch 513/1000, Training Loss: 5.111145012572866e-05, Test Loss: 5.0971878703689737e-05\n",
            "Epoch 514/1000, Training Loss: 5.084716403674022e-05, Test Loss: 5.0756536810928465e-05\n",
            "Epoch 515/1000, Training Loss: 5.058731035428999e-05, Test Loss: 5.054492939900055e-05\n",
            "Epoch 516/1000, Training Loss: 5.033178456927951e-05, Test Loss: 5.033696544219218e-05\n",
            "Epoch 517/1000, Training Loss: 5.00804849819266e-05, Test Loss: 5.0132556360959026e-05\n",
            "Epoch 518/1000, Training Loss: 4.983331262027611e-05, Test Loss: 4.9931615952758925e-05\n",
            "Epoch 519/1000, Training Loss: 4.959017116121655e-05, Test Loss: 4.973406032491478e-05\n",
            "Epoch 520/1000, Training Loss: 4.935096685392229e-05, Test Loss: 4.953980782945267e-05\n",
            "Epoch 521/1000, Training Loss: 4.9115608445645456e-05, Test Loss: 4.934877899984959e-05\n",
            "Epoch 522/1000, Training Loss: 4.88840071097702e-05, Test Loss: 4.916089648963357e-05\n",
            "Epoch 523/1000, Training Loss: 4.865607637607865e-05, Test Loss: 4.8976085012789465e-05\n",
            "Epoch 524/1000, Training Loss: 4.843173206313346e-05, Test Loss: 4.879427128590087e-05\n",
            "Epoch 525/1000, Training Loss: 4.821089221272341e-05, Test Loss: 4.861538397198323e-05\n",
            "Epoch 526/1000, Training Loss: 4.79934770263049e-05, Test Loss: 4.843935362596329e-05\n",
            "Epoch 527/1000, Training Loss: 4.7779408803370534e-05, Test Loss: 4.8266112641743746e-05\n",
            "Epoch 528/1000, Training Loss: 4.756861188168225e-05, Test Loss: 4.809559520080842e-05\n",
            "Epoch 529/1000, Training Loss: 4.736101257931943e-05, Test Loss: 4.7927737222327764e-05\n",
            "Epoch 530/1000, Training Loss: 4.715653913847225e-05, Test Loss: 4.776247631471471e-05\n",
            "Epoch 531/1000, Training Loss: 4.695512167093108e-05, Test Loss: 4.7599751728580726e-05\n",
            "Epoch 532/1000, Training Loss: 4.675669210521767e-05, Test Loss: 4.743950431106648e-05\n",
            "Epoch 533/1000, Training Loss: 4.656118413530612e-05, Test Loss: 4.72816764614893e-05\n",
            "Epoch 534/1000, Training Loss: 4.636853317088067e-05, Test Loss: 4.712621208827804e-05\n",
            "Epoch 535/1000, Training Loss: 4.617867628908146e-05, Test Loss: 4.697305656714835e-05\n",
            "Epoch 536/1000, Training Loss: 4.599155218769704e-05, Test Loss: 4.68221567004928e-05\n",
            "Epoch 537/1000, Training Loss: 4.58071011397525e-05, Test Loss: 4.6673460677935156e-05\n",
            "Epoch 538/1000, Training Loss: 4.562526494945027e-05, Test Loss: 4.652691803802876e-05\n",
            "Epoch 539/1000, Training Loss: 4.544598690942063e-05, Test Loss: 4.638247963105267e-05\n",
            "Epoch 540/1000, Training Loss: 4.526921175924747e-05, Test Loss: 4.6240097582879654e-05\n",
            "Epoch 541/1000, Training Loss: 4.509488564521449e-05, Test Loss: 4.609972525987877e-05\n",
            "Epoch 542/1000, Training Loss: 4.4922956081251124e-05, Test Loss: 4.596131723483133e-05\n",
            "Epoch 543/1000, Training Loss: 4.475337191102867e-05, Test Loss: 4.5824829253816866e-05\n",
            "Epoch 544/1000, Training Loss: 4.4586083271172696e-05, Test Loss: 4.5690218204050075e-05\n",
            "Epoch 545/1000, Training Loss: 4.442104155556584e-05, Test Loss: 4.5557442082638533e-05\n",
            "Epoch 546/1000, Training Loss: 4.425819938068998e-05, Test Loss: 4.542645996622918e-05\n",
            "Epoch 547/1000, Training Loss: 4.409751055199445e-05, Test Loss: 4.529723198152432e-05\n",
            "Epoch 548/1000, Training Loss: 4.393893003124432e-05, Test Loss: 4.516971927663538e-05\n",
            "Epoch 549/1000, Training Loss: 4.378241390482625e-05, Test Loss: 4.504388399325336e-05\n",
            "Epoch 550/1000, Training Loss: 4.36279193529787e-05, Test Loss: 4.4919689239607846e-05\n",
            "Epoch 551/1000, Training Loss: 4.3475404619919594e-05, Test Loss: 4.4797099064199536e-05\n",
            "Epoch 552/1000, Training Loss: 4.3324828984842516e-05, Test Loss: 4.46760784302699e-05\n",
            "Epoch 553/1000, Training Loss: 4.317615273375683e-05, Test Loss: 4.4556593191002156e-05\n",
            "Epoch 554/1000, Training Loss: 4.3029337132142736e-05, Test Loss: 4.443861006541746e-05\n",
            "Epoch 555/1000, Training Loss: 4.288434439840023e-05, Test Loss: 4.432209661495586e-05\n",
            "Epoch 556/1000, Training Loss: 4.2741137678063546e-05, Test Loss: 4.420702122071963e-05\n",
            "Epoch 557/1000, Training Loss: 4.259968101876177e-05, Test Loss: 4.409335306135438e-05\n",
            "Epoch 558/1000, Training Loss: 4.245993934589883e-05, Test Loss: 4.398106209155676e-05\n",
            "Epoch 559/1000, Training Loss: 4.232187843903323e-05, Test Loss: 4.387011902118504e-05\n",
            "Epoch 560/1000, Training Loss: 4.218546490893633e-05, Test Loss: 4.3760495294955766e-05\n",
            "Epoch 561/1000, Training Loss: 4.205066617531129e-05, Test Loss: 4.3652163072716166e-05\n",
            "Epoch 562/1000, Training Loss: 4.191745044514281e-05, Test Loss: 4.354509521026089e-05\n",
            "Epoch 563/1000, Training Loss: 4.178578669167173e-05, Test Loss: 4.343926524069467e-05\n",
            "Epoch 564/1000, Training Loss: 4.1655644633967015e-05, Test Loss: 4.333464735630962e-05\n",
            "Epoch 565/1000, Training Loss: 4.152699471707681e-05, Test Loss: 4.323121639097258e-05\n",
            "Epoch 566/1000, Training Loss: 4.1399808092749394e-05, Test Loss: 4.3128947803003594e-05\n",
            "Epoch 567/1000, Training Loss: 4.1274056600695876e-05, Test Loss: 4.3027817658531506e-05\n",
            "Epoch 568/1000, Training Loss: 4.114971275038686e-05, Test Loss: 4.2927802615312595e-05\n",
            "Epoch 569/1000, Training Loss: 4.1026749703363764e-05, Test Loss: 4.2828879907001635e-05\n",
            "Epoch 570/1000, Training Loss: 4.090514125604726e-05, Test Loss: 4.273102732785563e-05\n",
            "Epoch 571/1000, Training Loss: 4.078486182303328e-05, Test Loss: 4.263422321786571e-05\n",
            "Epoch 572/1000, Training Loss: 4.0665886420859035e-05, Test Loss: 4.253844644830297e-05\n",
            "Epoch 573/1000, Training Loss: 4.054819065222265e-05, Test Loss: 4.244367640765787e-05\n",
            "Epoch 574/1000, Training Loss: 4.043175069064661e-05, Test Loss: 4.2349892987974734e-05\n",
            "Epoch 575/1000, Training Loss: 4.031654326557447e-05, Test Loss: 4.225707657156204e-05\n",
            "Epoch 576/1000, Training Loss: 4.020254564787713e-05, Test Loss: 4.2165208018070075e-05\n",
            "Epoch 577/1000, Training Loss: 4.008973563577059e-05, Test Loss: 4.2074268651920985e-05\n",
            "Epoch 578/1000, Training Loss: 3.997809154112161e-05, Test Loss: 4.1984240250092054e-05\n",
            "Epoch 579/1000, Training Loss: 3.986759217613721e-05, Test Loss: 4.1895105030228375e-05\n",
            "Epoch 580/1000, Training Loss: 3.975821684042259e-05, Test Loss: 4.180684563908698e-05\n",
            "Epoch 581/1000, Training Loss: 3.9649945308397436e-05, Test Loss: 4.1719445141293975e-05\n",
            "Epoch 582/1000, Training Loss: 3.954275781706267e-05, Test Loss: 4.1632887008415594e-05\n",
            "Epoch 583/1000, Training Loss: 3.9436635054104e-05, Test Loss: 4.15471551083211e-05\n",
            "Epoch 584/1000, Training Loss: 3.9331558146324074e-05, Test Loss: 4.146223369484403e-05\n",
            "Epoch 585/1000, Training Loss: 3.922750864839307e-05, Test Loss: 4.137810739772231e-05\n",
            "Epoch 586/1000, Training Loss: 3.912446853190928e-05, Test Loss: 4.129476121281079e-05\n",
            "Epoch 587/1000, Training Loss: 3.902242017476069e-05, Test Loss: 4.1212180492567694e-05\n",
            "Epoch 588/1000, Training Loss: 3.8921346350776376e-05, Test Loss: 4.113035093679223e-05\n",
            "Epoch 589/1000, Training Loss: 3.88212302196655e-05, Test Loss: 4.104925858362047e-05\n",
            "Epoch 590/1000, Training Loss: 3.872205531722413e-05, Test Loss: 4.096888980076219e-05\n",
            "Epoch 591/1000, Training Loss: 3.862380554581689e-05, Test Loss: 4.0889231276977036e-05\n",
            "Epoch 592/1000, Training Loss: 3.852646516511362e-05, Test Loss: 4.081027001378306e-05\n",
            "Epoch 593/1000, Training Loss: 3.843001878307902e-05, Test Loss: 4.073199331738664e-05\n",
            "Epoch 594/1000, Training Loss: 3.833445134720801e-05, Test Loss: 4.065438879083555e-05\n",
            "Epoch 595/1000, Training Loss: 3.823974813599578e-05, Test Loss: 4.057744432637794e-05\n",
            "Epoch 596/1000, Training Loss: 3.814589475064357e-05, Test Loss: 4.050114809803381e-05\n",
            "Epoch 597/1000, Training Loss: 3.805287710698417e-05, Test Loss: 4.0425488554364966e-05\n",
            "Epoch 598/1000, Training Loss: 3.796068142762886e-05, Test Loss: 4.035045441143773e-05\n",
            "Epoch 599/1000, Training Loss: 3.786929423432187e-05, Test Loss: 4.027603464597649e-05\n",
            "Epoch 600/1000, Training Loss: 3.7778702340504545e-05, Test Loss: 4.02022184887032e-05\n",
            "Epoch 601/1000, Training Loss: 3.7688892844077794e-05, Test Loss: 4.012899541785263e-05\n",
            "Epoch 602/1000, Training Loss: 3.759985312035865e-05, Test Loss: 4.005635515286324e-05\n",
            "Epoch 603/1000, Training Loss: 3.751157081522406e-05, Test Loss: 3.998428764823839e-05\n",
            "Epoch 604/1000, Training Loss: 3.7424033838439374e-05, Test Loss: 3.9912783087570564e-05\n",
            "Epoch 605/1000, Training Loss: 3.7337230357165545e-05, Test Loss: 3.984183187772315e-05\n",
            "Epoch 606/1000, Training Loss: 3.725114878963764e-05, Test Loss: 3.9771424643174034e-05\n",
            "Epoch 607/1000, Training Loss: 3.716577779901304e-05, Test Loss: 3.970155222050317e-05\n",
            "Epoch 608/1000, Training Loss: 3.708110628738232e-05, Test Loss: 3.963220565303368e-05\n",
            "Epoch 609/1000, Training Loss: 3.6997123389937636e-05, Test Loss: 3.9563376185608896e-05\n",
            "Epoch 610/1000, Training Loss: 3.691381846929881e-05, Test Loss: 3.949505525951507e-05\n",
            "Epoch 611/1000, Training Loss: 3.683118110998656e-05, Test Loss: 3.9427234507533794e-05\n",
            "Epoch 612/1000, Training Loss: 3.6749201113042015e-05, Test Loss: 3.9359905749128606e-05\n",
            "Epoch 613/1000, Training Loss: 3.6667868490789864e-05, Test Loss: 3.9293060985759836e-05\n",
            "Epoch 614/1000, Training Loss: 3.658717346173662e-05, Test Loss: 3.922669239632109e-05\n",
            "Epoch 615/1000, Training Loss: 3.6507106445605505e-05, Test Loss: 3.916079233269964e-05\n",
            "Epoch 616/1000, Training Loss: 3.642765805849932e-05, Test Loss: 3.90953533154508e-05\n",
            "Epoch 617/1000, Training Loss: 3.634881910819168e-05, Test Loss: 3.903036802958982e-05\n",
            "Epoch 618/1000, Training Loss: 3.627058058953932e-05, Test Loss: 3.896582932049318e-05\n",
            "Epoch 619/1000, Training Loss: 3.619293368001697e-05, Test Loss: 3.8901730189906985e-05\n",
            "Epoch 620/1000, Training Loss: 3.611586973536649e-05, Test Loss: 3.8838063792064326e-05\n",
            "Epoch 621/1000, Training Loss: 3.6039380285359055e-05, Test Loss: 3.8774823429900124e-05\n",
            "Epoch 622/1000, Training Loss: 3.5963457029669095e-05, Test Loss: 3.871200255136879e-05\n",
            "Epoch 623/1000, Training Loss: 3.5888091833854024e-05, Test Loss: 3.864959474585623e-05\n",
            "Epoch 624/1000, Training Loss: 3.581327672543735e-05, Test Loss: 3.858759374068851e-05\n",
            "Epoch 625/1000, Training Loss: 3.5739003890095024e-05, Test Loss: 3.85259933977302e-05\n",
            "Epoch 626/1000, Training Loss: 3.566526566793924e-05, Test Loss: 3.846478771007202e-05\n",
            "Epoch 627/1000, Training Loss: 3.559205454989752e-05, Test Loss: 3.8403970798806345e-05\n",
            "Epoch 628/1000, Training Loss: 3.551936317418474e-05, Test Loss: 3.834353690988472e-05\n",
            "Epoch 629/1000, Training Loss: 3.544718432286767e-05, Test Loss: 3.8283480411058746e-05\n",
            "Epoch 630/1000, Training Loss: 3.537551091851471e-05, Test Loss: 3.822379578890197e-05\n",
            "Epoch 631/1000, Training Loss: 3.5304336020933896e-05, Test Loss: 3.816447764590582e-05\n",
            "Epoch 632/1000, Training Loss: 3.5233652823993376e-05, Test Loss: 3.8105520697655784e-05\n",
            "Epoch 633/1000, Training Loss: 3.51634546525233e-05, Test Loss: 3.8046919770077126e-05\n",
            "Epoch 634/1000, Training Loss: 3.509373495929462e-05, Test Loss: 3.7988669796753024e-05\n",
            "Epoch 635/1000, Training Loss: 3.502448732207844e-05, Test Loss: 3.7930765816314e-05\n",
            "Epoch 636/1000, Training Loss: 3.495570544077627e-05, Test Loss: 3.787320296989177e-05\n",
            "Epoch 637/1000, Training Loss: 3.488738313462641e-05, Test Loss: 3.781597649864433e-05\n",
            "Epoch 638/1000, Training Loss: 3.4819514339477096e-05, Test Loss: 3.7759081741336826e-05\n",
            "Epoch 639/1000, Training Loss: 3.475209310513342e-05, Test Loss: 3.7702514131995535e-05\n",
            "Epoch 640/1000, Training Loss: 3.4685113592765894e-05, Test Loss: 3.7646269197611365e-05\n",
            "Epoch 641/1000, Training Loss: 3.461857007238854e-05, Test Loss: 3.759034255591065e-05\n",
            "Epoch 642/1000, Training Loss: 3.4552456920398175e-05, Test Loss: 3.7534729913179875e-05\n",
            "Epoch 643/1000, Training Loss: 3.448676861717596e-05, Test Loss: 3.74794270621458e-05\n",
            "Epoch 644/1000, Training Loss: 3.4421499744749477e-05, Test Loss: 3.7424429879913424e-05\n",
            "Epoch 645/1000, Training Loss: 3.4356644984511324e-05, Test Loss: 3.7369734325953e-05\n",
            "Epoch 646/1000, Training Loss: 3.429219911499806e-05, Test Loss: 3.731533644014005e-05\n",
            "Epoch 647/1000, Training Loss: 3.422815700972169e-05, Test Loss: 3.726123234084762e-05\n",
            "Epoch 648/1000, Training Loss: 3.4164513635055515e-05, Test Loss: 3.7207418223084167e-05\n",
            "Epoch 649/1000, Training Loss: 3.410126404817359e-05, Test Loss: 3.715389035668041e-05\n",
            "Epoch 650/1000, Training Loss: 3.4038403395040314e-05, Test Loss: 3.7100645084522746e-05\n",
            "Epoch 651/1000, Training Loss: 3.3975926908450655e-05, Test Loss: 3.704767882083154e-05\n",
            "Epoch 652/1000, Training Loss: 3.391382990611809e-05, Test Loss: 3.699498804948284e-05\n",
            "Epoch 653/1000, Training Loss: 3.385210778880885e-05, Test Loss: 3.6942569322375044e-05\n",
            "Epoch 654/1000, Training Loss: 3.379075603852508e-05, Test Loss: 3.6890419257832405e-05\n",
            "Epoch 655/1000, Training Loss: 3.372977021672748e-05, Test Loss: 3.6838534539051784e-05\n",
            "Epoch 656/1000, Training Loss: 3.366914596260866e-05, Test Loss: 3.678691191259372e-05\n",
            "Epoch 657/1000, Training Loss: 3.3608878991401855e-05, Test Loss: 3.6735548186899974e-05\n",
            "Epoch 658/1000, Training Loss: 3.3548965092735306e-05, Test Loss: 3.6684440230859873e-05\n",
            "Epoch 659/1000, Training Loss: 3.3489400129025444e-05, Test Loss: 3.6633584972408986e-05\n",
            "Epoch 660/1000, Training Loss: 3.343018003390871e-05, Test Loss: 3.658297939715985e-05\n",
            "Epoch 661/1000, Training Loss: 3.3371300810711595e-05, Test Loss: 3.653262054707191e-05\n",
            "Epoch 662/1000, Training Loss: 3.331275853095992e-05, Test Loss: 3.64825055191523e-05\n",
            "Epoch 663/1000, Training Loss: 3.325454933292055e-05, Test Loss: 3.6432631464191605e-05\n",
            "Epoch 664/1000, Training Loss: 3.319666942018178e-05, Test Loss: 3.6382995585528106e-05\n",
            "Epoch 665/1000, Training Loss: 3.31391150602661e-05, Test Loss: 3.633359513784604e-05\n",
            "Epoch 666/1000, Training Loss: 3.3081882583276086e-05, Test Loss: 3.6284427426003174e-05\n",
            "Epoch 667/1000, Training Loss: 3.3024968380574975e-05, Test Loss: 3.623548980388611e-05\n",
            "Epoch 668/1000, Training Loss: 3.2968368903496055e-05, Test Loss: 3.618677967329674e-05\n",
            "Epoch 669/1000, Training Loss: 3.2912080662085565e-05, Test Loss: 3.613829448286447e-05\n",
            "Epoch 670/1000, Training Loss: 3.285610022387366e-05, Test Loss: 3.609003172698864e-05\n",
            "Epoch 671/1000, Training Loss: 3.280042421267585e-05, Test Loss: 3.604198894479994e-05\n",
            "Epoch 672/1000, Training Loss: 3.2745049307422795e-05, Test Loss: 3.599416371915968e-05\n",
            "Epoch 673/1000, Training Loss: 3.26899722410176e-05, Test Loss: 3.594655367567226e-05\n",
            "Epoch 674/1000, Training Loss: 3.263518979922046e-05, Test Loss: 3.589915648172918e-05\n",
            "Epoch 675/1000, Training Loss: 3.2580698819559683e-05, Test Loss: 3.585196984557324e-05\n",
            "Epoch 676/1000, Training Loss: 3.252649619026799e-05, Test Loss: 3.580499151538811e-05\n",
            "Epoch 677/1000, Training Loss: 3.247257884924522e-05, Test Loss: 3.575821927841028e-05\n",
            "Epoch 678/1000, Training Loss: 3.241894378304345e-05, Test Loss: 3.5711650960061676e-05\n",
            "Epoch 679/1000, Training Loss: 3.236558802587769e-05, Test Loss: 3.566528442310261e-05\n",
            "Epoch 680/1000, Training Loss: 3.23125086586591e-05, Test Loss: 3.56191175668117e-05\n",
            "Epoch 681/1000, Training Loss: 3.225970280805021e-05, Test Loss: 3.557314832617641e-05\n",
            "Epoch 682/1000, Training Loss: 3.2207167645544086e-05, Test Loss: 3.5527374671112116e-05\n",
            "Epoch 683/1000, Training Loss: 3.2154900386562275e-05, Test Loss: 3.548179460569643e-05\n",
            "Epoch 684/1000, Training Loss: 3.210289828957628e-05, Test Loss: 3.543640616742146e-05\n",
            "Epoch 685/1000, Training Loss: 3.205115865524719e-05, Test Loss: 3.539120742646635e-05\n",
            "Epoch 686/1000, Training Loss: 3.19996788255869e-05, Test Loss: 3.53461964849879e-05\n",
            "Epoch 687/1000, Training Loss: 3.1948456183138776e-05, Test Loss: 3.530137147642702e-05\n",
            "Epoch 688/1000, Training Loss: 3.1897488150174996e-05, Test Loss: 3.525673056483082e-05\n",
            "Epoch 689/1000, Training Loss: 3.1846772187915e-05, Test Loss: 3.52122719441967e-05\n",
            "Epoch 690/1000, Training Loss: 3.179630579576073e-05, Test Loss: 3.516799383782472e-05\n",
            "Epoch 691/1000, Training Loss: 3.1746086510548844e-05, Test Loss: 3.512389449769357e-05\n",
            "Epoch 692/1000, Training Loss: 3.169611190582154e-05, Test Loss: 3.50799722038444e-05\n",
            "Epoch 693/1000, Training Loss: 3.164637959111243e-05, Test Loss: 3.50362252637856e-05\n",
            "Epoch 694/1000, Training Loss: 3.159688721124864e-05, Test Loss: 3.4992652011908284e-05\n",
            "Epoch 695/1000, Training Loss: 3.154763244567113e-05, Test Loss: 3.4949250808915866e-05\n",
            "Epoch 696/1000, Training Loss: 3.1498613007767955e-05, Test Loss: 3.490602004127173e-05\n",
            "Epoch 697/1000, Training Loss: 3.144982664422297e-05, Test Loss: 3.486295812065308e-05\n",
            "Epoch 698/1000, Training Loss: 3.140127113437994e-05, Test Loss: 3.482006348342521e-05\n",
            "Epoch 699/1000, Training Loss: 3.135294428962041e-05, Test Loss: 3.477733459012141e-05\n",
            "Epoch 700/1000, Training Loss: 3.130484395275668e-05, Test Loss: 3.4734769924943966e-05\n",
            "Epoch 701/1000, Training Loss: 3.125696799743603e-05, Test Loss: 3.4692367995265864e-05\n",
            "Epoch 702/1000, Training Loss: 3.1209314327561564e-05, Test Loss: 3.465012733115715e-05\n",
            "Epoch 703/1000, Training Loss: 3.116188087672192e-05, Test Loss: 3.460804648491006e-05\n",
            "Epoch 704/1000, Training Loss: 3.1114665607637204e-05, Test Loss: 3.456612403058605e-05\n",
            "Epoch 705/1000, Training Loss: 3.106766651161661e-05, Test Loss: 3.452435856356665e-05\n",
            "Epoch 706/1000, Training Loss: 3.102088160802536e-05, Test Loss: 3.448274870011779e-05\n",
            "Epoch 707/1000, Training Loss: 3.0974308943766645e-05, Test Loss: 3.444129307696418e-05\n",
            "Epoch 708/1000, Training Loss: 3.09279465927738e-05, Test Loss: 3.4399990350873324e-05\n",
            "Epoch 709/1000, Training Loss: 3.088179265551279e-05, Test Loss: 3.435883919824987e-05\n",
            "Epoch 710/1000, Training Loss: 3.0835845258497044e-05, Test Loss: 3.4317838314739855e-05\n",
            "Epoch 711/1000, Training Loss: 3.0790102553812103e-05, Test Loss: 3.427698641484331e-05\n",
            "Epoch 712/1000, Training Loss: 3.0744562718650304e-05, Test Loss: 3.423628223153512e-05\n",
            "Epoch 713/1000, Training Loss: 3.069922395485685e-05, Test Loss: 3.4195724515898516e-05\n",
            "Epoch 714/1000, Training Loss: 3.065408448848428e-05, Test Loss: 3.4155312036762545e-05\n",
            "Epoch 715/1000, Training Loss: 3.0609142569357956e-05, Test Loss: 3.4115043580351826e-05\n",
            "Epoch 716/1000, Training Loss: 3.056439647064918e-05, Test Loss: 3.407491794994061e-05\n",
            "Epoch 717/1000, Training Loss: 3.051984448845958e-05, Test Loss: 3.403493396551968e-05\n",
            "Epoch 718/1000, Training Loss: 3.047548494141279e-05, Test Loss: 3.39950904634672e-05\n",
            "Epoch 719/1000, Training Loss: 3.043131617025594e-05, Test Loss: 3.3955386296228345e-05\n",
            "Epoch 720/1000, Training Loss: 3.038733653746962e-05, Test Loss: 3.391582033200246e-05\n",
            "Epoch 721/1000, Training Loss: 3.0343544426884262e-05, Test Loss: 3.387639145443747e-05\n",
            "Epoch 722/1000, Training Loss: 3.0299938243308278e-05, Test Loss: 3.383709856233281e-05\n",
            "Epoch 723/1000, Training Loss: 3.0256516412159823e-05, Test Loss: 3.3797940569345135e-05\n",
            "Epoch 724/1000, Training Loss: 3.021327737911114e-05, Test Loss: 3.375891640370585e-05\n",
            "Epoch 725/1000, Training Loss: 3.01702196097356e-05, Test Loss: 3.372002500794054e-05\n",
            "Epoch 726/1000, Training Loss: 3.0127341589165314e-05, Test Loss: 3.368126533859894e-05\n",
            "Epoch 727/1000, Training Loss: 3.0084641821754925e-05, Test Loss: 3.364263636598675e-05\n",
            "Epoch 728/1000, Training Loss: 3.0042118830753093e-05, Test Loss: 3.360413707390966e-05\n",
            "Epoch 729/1000, Training Loss: 2.9999771157979215e-05, Test Loss: 3.356576645941531e-05\n",
            "Epoch 730/1000, Training Loss: 2.9957597363508968e-05, Test Loss: 3.3527523532548076e-05\n",
            "Epoch 731/1000, Training Loss: 2.9915596025364725e-05, Test Loss: 3.348940731610656e-05\n",
            "Epoch 732/1000, Training Loss: 2.9873765739213194e-05, Test Loss: 3.345141684540349e-05\n",
            "Epoch 733/1000, Training Loss: 2.9832105118069676e-05, Test Loss: 3.341355116803845e-05\n",
            "Epoch 734/1000, Training Loss: 2.9790612792006955e-05, Test Loss: 3.337580934366989e-05\n",
            "Epoch 735/1000, Training Loss: 2.974928740787175e-05, Test Loss: 3.333819044379245e-05\n",
            "Epoch 736/1000, Training Loss: 2.970812762900648e-05, Test Loss: 3.330069355152283e-05\n",
            "Epoch 737/1000, Training Loss: 2.966713213497559e-05, Test Loss: 3.326331776138679e-05\n",
            "Epoch 738/1000, Training Loss: 2.9626299621299174e-05, Test Loss: 3.32260621791131e-05\n",
            "Epoch 739/1000, Training Loss: 2.958562879919174e-05, Test Loss: 3.3188925921431177e-05\n",
            "Epoch 740/1000, Training Loss: 2.954511839530454e-05, Test Loss: 3.315190811587429e-05\n",
            "Epoch 741/1000, Training Loss: 2.950476715147531e-05, Test Loss: 3.3115007900585816e-05\n",
            "Epoch 742/1000, Training Loss: 2.946457382448169e-05, Test Loss: 3.3078224424129804e-05\n",
            "Epoch 743/1000, Training Loss: 2.9424537185800367e-05, Test Loss: 3.304155684530653e-05\n",
            "Epoch 744/1000, Training Loss: 2.9384656021369587e-05, Test Loss: 3.300500433297249e-05\n",
            "Epoch 745/1000, Training Loss: 2.9344929131358747e-05, Test Loss: 3.296856606586325e-05\n",
            "Epoch 746/1000, Training Loss: 2.9305355329940657e-05, Test Loss: 3.293224123241911e-05\n",
            "Epoch 747/1000, Training Loss: 2.9265933445069482e-05, Test Loss: 3.289602903061886e-05\n",
            "Epoch 748/1000, Training Loss: 2.9226662318261543e-05, Test Loss: 3.285992866781327e-05\n",
            "Epoch 749/1000, Training Loss: 2.9187540804382583e-05, Test Loss: 3.2823939360562135e-05\n",
            "Epoch 750/1000, Training Loss: 2.9148567771437195e-05, Test Loss: 3.278806033447748e-05\n",
            "Epoch 751/1000, Training Loss: 2.910974210036421e-05, Test Loss: 3.2752290824068137e-05\n",
            "Epoch 752/1000, Training Loss: 2.9071062684834787e-05, Test Loss: 3.2716630072589565e-05\n",
            "Epoch 753/1000, Training Loss: 2.9032528431054037e-05, Test Loss: 3.2681077331893155e-05\n",
            "Epoch 754/1000, Training Loss: 2.8994138257568477e-05, Test Loss: 3.264563186228283e-05\n",
            "Epoch 755/1000, Training Loss: 2.8955891095075227e-05, Test Loss: 3.2610292932373265e-05\n",
            "Epoch 756/1000, Training Loss: 2.8917785886235463e-05, Test Loss: 3.257505981895089e-05\n",
            "Epoch 757/1000, Training Loss: 2.887982158549265e-05, Test Loss: 3.253993180683798e-05\n",
            "Epoch 758/1000, Training Loss: 2.8841997158892547e-05, Test Loss: 3.250490818875913e-05\n",
            "Epoch 759/1000, Training Loss: 2.8804311583907256e-05, Test Loss: 3.246998826521118e-05\n",
            "Epoch 760/1000, Training Loss: 2.8766763849263703e-05, Test Loss: 3.243517134433722e-05\n",
            "Epoch 761/1000, Training Loss: 2.8729352954773307e-05, Test Loss: 3.240045674179979e-05\n",
            "Epoch 762/1000, Training Loss: 2.8692077911166572e-05, Test Loss: 3.236584378066077e-05\n",
            "Epoch 763/1000, Training Loss: 2.8654937739929907e-05, Test Loss: 3.23313317912595e-05\n",
            "Epoch 764/1000, Training Loss: 2.8617931473146432e-05, Test Loss: 3.229692011109834e-05\n",
            "Epoch 765/1000, Training Loss: 2.858105815333858e-05, Test Loss: 3.22626080847262e-05\n",
            "Epoch 766/1000, Training Loss: 2.8544316833314436e-05, Test Loss: 3.222839506362652e-05\n",
            "Epoch 767/1000, Training Loss: 2.8507706576017486e-05, Test Loss: 3.2194280406109536e-05\n",
            "Epoch 768/1000, Training Loss: 2.847122645437674e-05, Test Loss: 3.216026347720091e-05\n",
            "Epoch 769/1000, Training Loss: 2.843487555116289e-05, Test Loss: 3.212634364853939e-05\n",
            "Epoch 770/1000, Training Loss: 2.839865295884479e-05, Test Loss: 3.2092520298273226e-05\n",
            "Epoch 771/1000, Training Loss: 2.8362557779449534e-05, Test Loss: 3.205879281095758e-05\n",
            "Epoch 772/1000, Training Loss: 2.832658912442515e-05, Test Loss: 3.202516057745733e-05\n",
            "Epoch 773/1000, Training Loss: 2.8290746114505013e-05, Test Loss: 3.1991622994848085e-05\n",
            "Epoch 774/1000, Training Loss: 2.8255027879575293e-05, Test Loss: 3.195817946632328e-05\n",
            "Epoch 775/1000, Training Loss: 2.821943355854546e-05, Test Loss: 3.1924829401098916e-05\n",
            "Epoch 776/1000, Training Loss: 2.8183962299220232e-05, Test Loss: 3.189157221432502e-05\n",
            "Epoch 777/1000, Training Loss: 2.8148613258173154e-05, Test Loss: 3.185840732699342e-05\n",
            "Epoch 778/1000, Training Loss: 2.8113385600624795e-05, Test Loss: 3.182533416585087e-05\n",
            "Epoch 779/1000, Training Loss: 2.807827850032046e-05, Test Loss: 3.179235216331551e-05\n",
            "Epoch 780/1000, Training Loss: 2.804329113941224e-05, Test Loss: 3.175946075738867e-05\n",
            "Epoch 781/1000, Training Loss: 2.8008422708341475e-05, Test Loss: 3.1726659391576694e-05\n",
            "Epoch 782/1000, Training Loss: 2.7973672405724756e-05, Test Loss: 3.169394751480757e-05\n",
            "Epoch 783/1000, Training Loss: 2.793903943824071e-05, Test Loss: 3.166132458135179e-05\n",
            "Epoch 784/1000, Training Loss: 2.7904523020519593e-05, Test Loss: 3.1628790050744345e-05\n",
            "Epoch 785/1000, Training Loss: 2.7870122375034618e-05, Test Loss: 3.159634338771125e-05\n",
            "Epoch 786/1000, Training Loss: 2.7835836731994954e-05, Test Loss: 3.156398406209182e-05\n",
            "Epoch 787/1000, Training Loss: 2.780166532924046e-05, Test Loss: 3.153171154876627e-05\n",
            "Epoch 788/1000, Training Loss: 2.7767607412139287e-05, Test Loss: 3.149952532758542e-05\n",
            "Epoch 789/1000, Training Loss: 2.7733662233485822e-05, Test Loss: 3.1467424883296686e-05\n",
            "Epoch 790/1000, Training Loss: 2.7699829053401834e-05, Test Loss: 3.143540970548051e-05\n",
            "Epoch 791/1000, Training Loss: 2.7666107139237822e-05, Test Loss: 3.1403479288478125e-05\n",
            "Epoch 792/1000, Training Loss: 2.76324957654768e-05, Test Loss: 3.137163313132611e-05\n",
            "Epoch 793/1000, Training Loss: 2.759899421364032e-05, Test Loss: 3.133987073769232e-05\n",
            "Epoch 794/1000, Training Loss: 2.7565601772195034e-05, Test Loss: 3.1308191615811765e-05\n",
            "Epoch 795/1000, Training Loss: 2.7532317736461612e-05, Test Loss: 3.12765952784221e-05\n",
            "Epoch 796/1000, Training Loss: 2.7499141408525025e-05, Test Loss: 3.124508124270549e-05\n",
            "Epoch 797/1000, Training Loss: 2.7466072097145146e-05, Test Loss: 3.1213649030223575e-05\n",
            "Epoch 798/1000, Training Loss: 2.7433109117671556e-05, Test Loss: 3.118229816686379e-05\n",
            "Epoch 799/1000, Training Loss: 2.7400251791956776e-05, Test Loss: 3.1151028182776066e-05\n",
            "Epoch 800/1000, Training Loss: 2.736749944827282e-05, Test Loss: 3.111983861231859e-05\n",
            "Epoch 801/1000, Training Loss: 2.7334851421228598e-05, Test Loss: 3.1088728994001894e-05\n",
            "Epoch 802/1000, Training Loss: 2.730230705168915e-05, Test Loss: 3.1057698870431426e-05\n",
            "Epoch 803/1000, Training Loss: 2.7269865686694956e-05, Test Loss: 3.102674778825735e-05\n",
            "Epoch 804/1000, Training Loss: 2.7237526679383737e-05, Test Loss: 3.099587529811928e-05\n",
            "Epoch 805/1000, Training Loss: 2.720528938891318e-05, Test Loss: 3.096508095459319e-05\n",
            "Epoch 806/1000, Training Loss: 2.7173153180385496e-05, Test Loss: 3.093436431614256e-05\n",
            "Epoch 807/1000, Training Loss: 2.7141117424771512e-05, Test Loss: 3.090372494506719e-05\n",
            "Epoch 808/1000, Training Loss: 2.7109181498838312e-05, Test Loss: 3.0873162407455256e-05\n",
            "Epoch 809/1000, Training Loss: 2.707734478507549e-05, Test Loss: 3.0842676273131144e-05\n",
            "Epoch 810/1000, Training Loss: 2.7045606671625523e-05, Test Loss: 3.0812266115613394e-05\n",
            "Epoch 811/1000, Training Loss: 2.7013966552212535e-05, Test Loss: 3.0781931512063884e-05\n",
            "Epoch 812/1000, Training Loss: 2.6982423826073534e-05, Test Loss: 3.075167204324315e-05\n",
            "Epoch 813/1000, Training Loss: 2.695097789789164e-05, Test Loss: 3.07214872934665e-05\n",
            "Epoch 814/1000, Training Loss: 2.6919628177727105e-05, Test Loss: 3.0691376850557455e-05\n",
            "Epoch 815/1000, Training Loss: 2.6888374080954283e-05, Test Loss: 3.0661340305807686e-05\n",
            "Epoch 816/1000, Training Loss: 2.6857215028194493e-05, Test Loss: 3.063137725393089e-05\n",
            "Epoch 817/1000, Training Loss: 2.6826150445254047e-05, Test Loss: 3.0601487293023024e-05\n",
            "Epoch 818/1000, Training Loss: 2.6795179763061048e-05, Test Loss: 3.057167002451828e-05\n",
            "Epoch 819/1000, Training Loss: 2.6764302417603377e-05, Test Loss: 3.0541925053153664e-05\n",
            "Epoch 820/1000, Training Loss: 2.6733517849868418e-05, Test Loss: 3.0512251986923954e-05\n",
            "Epoch 821/1000, Training Loss: 2.6702825505783105e-05, Test Loss: 3.048265043704536e-05\n",
            "Epoch 822/1000, Training Loss: 2.6672224836154795e-05, Test Loss: 3.045312001791651e-05\n",
            "Epoch 823/1000, Training Loss: 2.6641715296614e-05, Test Loss: 3.0423660347080236e-05\n",
            "Epoch 824/1000, Training Loss: 2.661129634755656e-05, Test Loss: 3.039427104518692e-05\n",
            "Epoch 825/1000, Training Loss: 2.658096745408766e-05, Test Loss: 3.0364951735956683e-05\n",
            "Epoch 826/1000, Training Loss: 2.655072808596657e-05, Test Loss: 3.0335702046144245e-05\n",
            "Epoch 827/1000, Training Loss: 2.6520577717552342e-05, Test Loss: 3.0306521605504527e-05\n",
            "Epoch 828/1000, Training Loss: 2.6490515827749762e-05, Test Loss: 3.0277410046756903e-05\n",
            "Epoch 829/1000, Training Loss: 2.6460541899956576e-05, Test Loss: 3.0248367005550285e-05\n",
            "Epoch 830/1000, Training Loss: 2.6430655422011728e-05, Test Loss: 3.0219392120430425e-05\n",
            "Epoch 831/1000, Training Loss: 2.6400855886142945e-05, Test Loss: 3.0190485032807017e-05\n",
            "Epoch 832/1000, Training Loss: 2.6371142788918437e-05, Test Loss: 3.0161645386920055e-05\n",
            "Epoch 833/1000, Training Loss: 2.6341515631194186e-05, Test Loss: 3.0132872829808977e-05\n",
            "Epoch 834/1000, Training Loss: 2.631197391806707e-05, Test Loss: 3.0104167011279808e-05\n",
            "Epoch 835/1000, Training Loss: 2.6282517158826068e-05, Test Loss: 3.007552758387563e-05\n",
            "Epoch 836/1000, Training Loss: 2.6253144866903756e-05, Test Loss: 3.0046954202845156e-05\n",
            "Epoch 837/1000, Training Loss: 2.622385655983096e-05, Test Loss: 3.0018446526112766e-05\n",
            "Epoch 838/1000, Training Loss: 2.6194651759188496e-05, Test Loss: 2.9990004214248487e-05\n",
            "Epoch 839/1000, Training Loss: 2.6165529990563288e-05, Test Loss: 2.996162693044058e-05\n",
            "Epoch 840/1000, Training Loss: 2.6136490783502917e-05, Test Loss: 2.9933314340464563e-05\n",
            "Epoch 841/1000, Training Loss: 2.610753367147152e-05, Test Loss: 2.9905066112656957e-05\n",
            "Epoch 842/1000, Training Loss: 2.607865819180502e-05, Test Loss: 2.9876881917886738e-05\n",
            "Epoch 843/1000, Training Loss: 2.6049863885669808e-05, Test Loss: 2.9848761429527197e-05\n",
            "Epoch 844/1000, Training Loss: 2.6021150298019463e-05, Test Loss: 2.982070432343056e-05\n",
            "Epoch 845/1000, Training Loss: 2.5992516977553063e-05, Test Loss: 2.979271027790091e-05\n",
            "Epoch 846/1000, Training Loss: 2.596396347667411e-05, Test Loss: 2.9764778973666812e-05\n",
            "Epoch 847/1000, Training Loss: 2.5935489351449534e-05, Test Loss: 2.9736910093857892e-05\n",
            "Epoch 848/1000, Training Loss: 2.5907094161570113e-05, Test Loss: 2.970910332397776e-05\n",
            "Epoch 849/1000, Training Loss: 2.5878777470311257e-05, Test Loss: 2.9681358351880053e-05\n",
            "Epoch 850/1000, Training Loss: 2.5850538844493187e-05, Test Loss: 2.9653674867742797e-05\n",
            "Epoch 851/1000, Training Loss: 2.5822377854443207e-05, Test Loss: 2.962605256404511e-05\n",
            "Epoch 852/1000, Training Loss: 2.5794294073957867e-05, Test Loss: 2.9598491135543625e-05\n",
            "Epoch 853/1000, Training Loss: 2.5766287080265207e-05, Test Loss: 2.957099027924806e-05\n",
            "Epoch 854/1000, Training Loss: 2.573835645398862e-05, Test Loss: 2.9543549694398884e-05\n",
            "Epoch 855/1000, Training Loss: 2.5710501779109673e-05, Test Loss: 2.951616908244314e-05\n",
            "Epoch 856/1000, Training Loss: 2.5682722642933194e-05, Test Loss: 2.9488848147013646e-05\n",
            "Epoch 857/1000, Training Loss: 2.565501863605138e-05, Test Loss: 2.9461586593906554e-05\n",
            "Epoch 858/1000, Training Loss: 2.5627389352309193e-05, Test Loss: 2.943438413105834e-05\n",
            "Epoch 859/1000, Training Loss: 2.559983438876996e-05, Test Loss: 2.9407240468525393e-05\n",
            "Epoch 860/1000, Training Loss: 2.5572353345681448e-05, Test Loss: 2.938015531846245e-05\n",
            "Epoch 861/1000, Training Loss: 2.554494582644239e-05, Test Loss: 2.9353128395101898e-05\n",
            "Epoch 862/1000, Training Loss: 2.5517611437569954e-05, Test Loss: 2.932615941473166e-05\n",
            "Epoch 863/1000, Training Loss: 2.5490349788666343e-05, Test Loss: 2.929924809567816e-05\n",
            "Epoch 864/1000, Training Loss: 2.5463160492387504e-05, Test Loss: 2.9272394158282107e-05\n",
            "Epoch 865/1000, Training Loss: 2.543604316441059e-05, Test Loss: 2.924559732488277e-05\n",
            "Epoch 866/1000, Training Loss: 2.5408997423403834e-05, Test Loss: 2.9218857319794973e-05\n",
            "Epoch 867/1000, Training Loss: 2.5382022890994806e-05, Test Loss: 2.9192173869293042e-05\n",
            "Epoch 868/1000, Training Loss: 2.5355119191740157e-05, Test Loss: 2.9165546701588862e-05\n",
            "Epoch 869/1000, Training Loss: 2.532828595309569e-05, Test Loss: 2.9138975546815002e-05\n",
            "Epoch 870/1000, Training Loss: 2.5301522805386955e-05, Test Loss: 2.9112460137006834e-05\n",
            "Epoch 871/1000, Training Loss: 2.5274829381779064e-05, Test Loss: 2.908600020608124e-05\n",
            "Epoch 872/1000, Training Loss: 2.5248205318249367e-05, Test Loss: 2.905959548982266e-05\n",
            "Epoch 873/1000, Training Loss: 2.5221650253557174e-05, Test Loss: 2.9033245725862236e-05\n",
            "Epoch 874/1000, Training Loss: 2.5195163829217016e-05, Test Loss: 2.900695065366272e-05\n",
            "Epoch 875/1000, Training Loss: 2.5168745689470238e-05, Test Loss: 2.8980710014498567e-05\n",
            "Epoch 876/1000, Training Loss: 2.514239548125751e-05, Test Loss: 2.8954523551442296e-05\n",
            "Epoch 877/1000, Training Loss: 2.5116112854191922e-05, Test Loss: 2.892839100934396e-05\n",
            "Epoch 878/1000, Training Loss: 2.5089897460532796e-05, Test Loss: 2.8902312134817605e-05\n",
            "Epoch 879/1000, Training Loss: 2.5063748955158068e-05, Test Loss: 2.8876286676223348e-05\n",
            "Epoch 880/1000, Training Loss: 2.5037666995539232e-05, Test Loss: 2.885031438365067e-05\n",
            "Epoch 881/1000, Training Loss: 2.5011651241715616e-05, Test Loss: 2.8824395008904753e-05\n",
            "Epoch 882/1000, Training Loss: 2.498570135626833e-05, Test Loss: 2.8798528305489248e-05\n",
            "Epoch 883/1000, Training Loss: 2.4959817004295898e-05, Test Loss: 2.877271402859003e-05\n",
            "Epoch 884/1000, Training Loss: 2.493399785338854e-05, Test Loss: 2.874695193506107e-05\n",
            "Epoch 885/1000, Training Loss: 2.490824357360544e-05, Test Loss: 2.8721241783408705e-05\n",
            "Epoch 886/1000, Training Loss: 2.4882553837448464e-05, Test Loss: 2.8695583333777897e-05\n",
            "Epoch 887/1000, Training Loss: 2.4856928319839948e-05, Test Loss: 2.866997634793558e-05\n",
            "Epoch 888/1000, Training Loss: 2.4831366698098556e-05, Test Loss: 2.864442058925702e-05\n",
            "Epoch 889/1000, Training Loss: 2.4805868651916046e-05, Test Loss: 2.8618915822710968e-05\n",
            "Epoch 890/1000, Training Loss: 2.478043386333394e-05, Test Loss: 2.8593461814846325e-05\n",
            "Epoch 891/1000, Training Loss: 2.475506201672157e-05, Test Loss: 2.856805833377705e-05\n",
            "Epoch 892/1000, Training Loss: 2.4729752798753164e-05, Test Loss: 2.8542705149170533e-05\n",
            "Epoch 893/1000, Training Loss: 2.4704505898385733e-05, Test Loss: 2.8517402032229036e-05\n",
            "Epoch 894/1000, Training Loss: 2.4679321006837192e-05, Test Loss: 2.849214875568249e-05\n",
            "Epoch 895/1000, Training Loss: 2.465419781756521e-05, Test Loss: 2.8466945093769742e-05\n",
            "Epoch 896/1000, Training Loss: 2.4629136026245366e-05, Test Loss: 2.844179082222884e-05\n",
            "Epoch 897/1000, Training Loss: 2.4604135330749777e-05, Test Loss: 2.841668571828148e-05\n",
            "Epoch 898/1000, Training Loss: 2.457919543112728e-05, Test Loss: 2.8391629560621506e-05\n",
            "Epoch 899/1000, Training Loss: 2.4554316029582033e-05, Test Loss: 2.836662212940256e-05\n",
            "Epoch 900/1000, Training Loss: 2.4529496830453302e-05, Test Loss: 2.834166320622453e-05\n",
            "Epoch 901/1000, Training Loss: 2.4504737540195195e-05, Test Loss: 2.8316752574120743e-05\n",
            "Epoch 902/1000, Training Loss: 2.4480037867357493e-05, Test Loss: 2.829189001754686e-05\n",
            "Epoch 903/1000, Training Loss: 2.4455397522565733e-05, Test Loss: 2.826707532236907e-05\n",
            "Epoch 904/1000, Training Loss: 2.4430816218501114e-05, Test Loss: 2.8242308275848335e-05\n",
            "Epoch 905/1000, Training Loss: 2.4406293669882452e-05, Test Loss: 2.821758866663443e-05\n",
            "Epoch 906/1000, Training Loss: 2.4381829593446286e-05, Test Loss: 2.8192916284749508e-05\n",
            "Epoch 907/1000, Training Loss: 2.435742370792881e-05, Test Loss: 2.8168290921577947e-05\n",
            "Epoch 908/1000, Training Loss: 2.433307573404704e-05, Test Loss: 2.8143712369855262e-05\n",
            "Epoch 909/1000, Training Loss: 2.430878539448066e-05, Test Loss: 2.8119180423657148e-05\n",
            "Epoch 910/1000, Training Loss: 2.4284552413853945e-05, Test Loss: 2.8094694878386764e-05\n",
            "Epoch 911/1000, Training Loss: 2.4260376518717813e-05, Test Loss: 2.8070255530764643e-05\n",
            "Epoch 912/1000, Training Loss: 2.4236257437532293e-05, Test Loss: 2.8045862178817452e-05\n",
            "Epoch 913/1000, Training Loss: 2.4212194900649034e-05, Test Loss: 2.8021514621867335e-05\n",
            "Epoch 914/1000, Training Loss: 2.4188188640293933e-05, Test Loss: 2.7997212660520595e-05\n",
            "Epoch 915/1000, Training Loss: 2.4164238390550363e-05, Test Loss: 2.7972956096658824e-05\n",
            "Epoch 916/1000, Training Loss: 2.4140343887342033e-05, Test Loss: 2.7948744733426116e-05\n",
            "Epoch 917/1000, Training Loss: 2.411650486841641e-05, Test Loss: 2.7924578375219695e-05\n",
            "Epoch 918/1000, Training Loss: 2.409272107332838e-05, Test Loss: 2.7900456827679262e-05\n",
            "Epoch 919/1000, Training Loss: 2.4068992243423785e-05, Test Loss: 2.7876379897679143e-05\n",
            "Epoch 920/1000, Training Loss: 2.40453181218233e-05, Test Loss: 2.7852347393313992e-05\n",
            "Epoch 921/1000, Training Loss: 2.402169845340663e-05, Test Loss: 2.782835912389324e-05\n",
            "Epoch 922/1000, Training Loss: 2.399813298479671e-05, Test Loss: 2.780441489992699e-05\n",
            "Epoch 923/1000, Training Loss: 2.3974621464344332e-05, Test Loss: 2.7780514533120925e-05\n",
            "Epoch 924/1000, Training Loss: 2.39511636421124e-05, Test Loss: 2.775665783636279e-05\n",
            "Epoch 925/1000, Training Loss: 2.3927759269860803e-05, Test Loss: 2.7732844623714995e-05\n",
            "Epoch 926/1000, Training Loss: 2.3904408101031765e-05, Test Loss: 2.770907471040462e-05\n",
            "Epoch 927/1000, Training Loss: 2.3881109890734347e-05, Test Loss: 2.768534791281359e-05\n",
            "Epoch 928/1000, Training Loss: 2.385786439573022e-05, Test Loss: 2.766166404847076e-05\n",
            "Epoch 929/1000, Training Loss: 2.383467137441859e-05, Test Loss: 2.7638022936040913e-05\n",
            "Epoch 930/1000, Training Loss: 2.3811530586822216e-05, Test Loss: 2.7614424395317226e-05\n",
            "Epoch 931/1000, Training Loss: 2.378844179457329e-05, Test Loss: 2.7590868247212067e-05\n",
            "Epoch 932/1000, Training Loss: 2.3765404760899118e-05, Test Loss: 2.7567354313748028e-05\n",
            "Epoch 933/1000, Training Loss: 2.3742419250607867e-05, Test Loss: 2.7543882418049665e-05\n",
            "Epoch 934/1000, Training Loss: 2.371948503007561e-05, Test Loss: 2.7520452384332805e-05\n",
            "Epoch 935/1000, Training Loss: 2.369660186723202e-05, Test Loss: 2.7497064037898805e-05\n",
            "Epoch 936/1000, Training Loss: 2.3673769531546614e-05, Test Loss: 2.747371720512398e-05\n",
            "Epoch 937/1000, Training Loss: 2.365098779401675e-05, Test Loss: 2.7450411713452633e-05\n",
            "Epoch 938/1000, Training Loss: 2.3628256427152994e-05, Test Loss: 2.7427147391387625e-05\n",
            "Epoch 939/1000, Training Loss: 2.3605575204966877e-05, Test Loss: 2.7403924068483057e-05\n",
            "Epoch 940/1000, Training Loss: 2.3582943902957524e-05, Test Loss: 2.73807415753344e-05\n",
            "Epoch 941/1000, Training Loss: 2.3560362298099546e-05, Test Loss: 2.7357599743572858e-05\n",
            "Epoch 942/1000, Training Loss: 2.3537830168829678e-05, Test Loss: 2.733449840585602e-05\n",
            "Epoch 943/1000, Training Loss: 2.3515347295034813e-05, Test Loss: 2.731143739585976e-05\n",
            "Epoch 944/1000, Training Loss: 2.3492913458038883e-05, Test Loss: 2.728841654827156e-05\n",
            "Epoch 945/1000, Training Loss: 2.347052844059204e-05, Test Loss: 2.7265435698780525e-05\n",
            "Epoch 946/1000, Training Loss: 2.3448192026857143e-05, Test Loss: 2.7242494684073196e-05\n",
            "Epoch 947/1000, Training Loss: 2.3425904002398353e-05, Test Loss: 2.7219593341822088e-05\n",
            "Epoch 948/1000, Training Loss: 2.3403664154169293e-05, Test Loss: 2.7196731510680966e-05\n",
            "Epoch 949/1000, Training Loss: 2.338147227050115e-05, Test Loss: 2.717390903027622e-05\n",
            "Epoch 950/1000, Training Loss: 2.3359328141091462e-05, Test Loss: 2.7151125741199186e-05\n",
            "Epoch 951/1000, Training Loss: 2.3337231556992294e-05, Test Loss: 2.7128381485000214e-05\n",
            "Epoch 952/1000, Training Loss: 2.3315182310599017e-05, Test Loss: 2.7105676104180053e-05\n",
            "Epoch 953/1000, Training Loss: 2.329318019563863e-05, Test Loss: 2.708300944218316e-05\n",
            "Epoch 954/1000, Training Loss: 2.3271225007159658e-05, Test Loss: 2.7060381343391244e-05\n",
            "Epoch 955/1000, Training Loss: 2.3249316541519895e-05, Test Loss: 2.703779165311448e-05\n",
            "Epoch 956/1000, Training Loss: 2.32274545963766e-05, Test Loss: 2.701524021758714e-05\n",
            "Epoch 957/1000, Training Loss: 2.3205638970674757e-05, Test Loss: 2.6992726883957307e-05\n",
            "Epoch 958/1000, Training Loss: 2.3183869464637293e-05, Test Loss: 2.6970251500283833e-05\n",
            "Epoch 959/1000, Training Loss: 2.3162145879753948e-05, Test Loss: 2.6947813915526747e-05\n",
            "Epoch 960/1000, Training Loss: 2.3140468018771113e-05, Test Loss: 2.692541397954134e-05\n",
            "Epoch 961/1000, Training Loss: 2.311883568568092e-05, Test Loss: 2.6903051543072002e-05\n",
            "Epoch 962/1000, Training Loss: 2.3097248685711963e-05, Test Loss: 2.6880726457745403e-05\n",
            "Epoch 963/1000, Training Loss: 2.3075706825318268e-05, Test Loss: 2.6858438576063934e-05\n",
            "Epoch 964/1000, Training Loss: 2.305420991216956e-05, Test Loss: 2.683618775139845e-05\n",
            "Epoch 965/1000, Training Loss: 2.3032757755141792e-05, Test Loss: 2.6813973837984047e-05\n",
            "Epoch 966/1000, Training Loss: 2.3011350164306673e-05, Test Loss: 2.679179669091105e-05\n",
            "Epoch 967/1000, Training Loss: 2.2989986950922053e-05, Test Loss: 2.6769656166120083e-05\n",
            "Epoch 968/1000, Training Loss: 2.2968667927422654e-05, Test Loss: 2.6747552120396084e-05\n",
            "Epoch 969/1000, Training Loss: 2.2947392907410308e-05, Test Loss: 2.6725484411362076e-05\n",
            "Epoch 970/1000, Training Loss: 2.2926161705644027e-05, Test Loss: 2.6703452897472273e-05\n",
            "Epoch 971/1000, Training Loss: 2.290497413803157e-05, Test Loss: 2.6681457438005803e-05\n",
            "Epoch 972/1000, Training Loss: 2.2883830021619752e-05, Test Loss: 2.66594978930627e-05\n",
            "Epoch 973/1000, Training Loss: 2.286272917458497e-05, Test Loss: 2.6637574123556458e-05\n",
            "Epoch 974/1000, Training Loss: 2.284167141622493e-05, Test Loss: 2.6615685991207607e-05\n",
            "Epoch 975/1000, Training Loss: 2.2820656566948702e-05, Test Loss: 2.659383335853968e-05\n",
            "Epoch 976/1000, Training Loss: 2.2799684448268315e-05, Test Loss: 2.6572016088870398e-05\n",
            "Epoch 977/1000, Training Loss: 2.2778754882790423e-05, Test Loss: 2.655023404631063e-05\n",
            "Epoch 978/1000, Training Loss: 2.2757867694206832e-05, Test Loss: 2.6528487095753246e-05\n",
            "Epoch 979/1000, Training Loss: 2.2737022707286043e-05, Test Loss: 2.6506775102872294e-05\n",
            "Epoch 980/1000, Training Loss: 2.271621974786507e-05, Test Loss: 2.648509793411459e-05\n",
            "Epoch 981/1000, Training Loss: 2.2695458642840343e-05, Test Loss: 2.6463455456693292e-05\n",
            "Epoch 982/1000, Training Loss: 2.2674739220160134e-05, Test Loss: 2.6441847538587323e-05\n",
            "Epoch 983/1000, Training Loss: 2.2654061308815796e-05, Test Loss: 2.6420274048529862e-05\n",
            "Epoch 984/1000, Training Loss: 2.26334247388331e-05, Test Loss: 2.6398734856007026e-05\n",
            "Epoch 985/1000, Training Loss: 2.261282934126535e-05, Test Loss: 2.6377229831249923e-05\n",
            "Epoch 986/1000, Training Loss: 2.2592274948183917e-05, Test Loss: 2.6355758845232712e-05\n",
            "Epoch 987/1000, Training Loss: 2.2571761392671375e-05, Test Loss: 2.633432176966389e-05\n",
            "Epoch 988/1000, Training Loss: 2.2551288508812725e-05, Test Loss: 2.631291847698219e-05\n",
            "Epoch 989/1000, Training Loss: 2.25308561316885e-05, Test Loss: 2.6291548840353105e-05\n",
            "Epoch 990/1000, Training Loss: 2.2510464097366023e-05, Test Loss: 2.6270212733660858e-05\n",
            "Epoch 991/1000, Training Loss: 2.2490112242892454e-05, Test Loss: 2.6248910031505864e-05\n",
            "Epoch 992/1000, Training Loss: 2.246980040628702e-05, Test Loss: 2.6227640609197994e-05\n",
            "Epoch 993/1000, Training Loss: 2.244952842653295e-05, Test Loss: 2.620640434275272e-05\n",
            "Epoch 994/1000, Training Loss: 2.2429296143571076e-05, Test Loss: 2.6185201108885764e-05\n",
            "Epoch 995/1000, Training Loss: 2.2409103398291616e-05, Test Loss: 2.616403078500843e-05\n",
            "Epoch 996/1000, Training Loss: 2.238895003252692e-05, Test Loss: 2.6142893249221536e-05\n",
            "Epoch 997/1000, Training Loss: 2.236883588904473e-05, Test Loss: 2.6121788380312123e-05\n",
            "Epoch 998/1000, Training Loss: 2.2348760811540416e-05, Test Loss: 2.6100716057747992e-05\n",
            "Epoch 999/1000, Training Loss: 2.2328724644630197e-05, Test Loss: 2.6079676161673573e-05\n",
            "Epoch 1000/1000, Training Loss: 2.230872723384413e-05, Test Loss: 2.605866857290352e-05\n",
            "Epoch 1/1000, Training Loss: 0.003589063564881025, Test Loss: 0.0025965065351558777\n",
            "Epoch 2/1000, Training Loss: 0.0034005327961925858, Test Loss: 0.0024040654441049615\n",
            "Epoch 3/1000, Training Loss: 0.0033601514719132806, Test Loss: 0.002360901857427867\n",
            "Epoch 4/1000, Training Loss: 0.0033452584943003934, Test Loss: 0.0023444885324733974\n",
            "Epoch 5/1000, Training Loss: 0.0033385058625499562, Test Loss: 0.0023368146977675655\n",
            "Epoch 6/1000, Training Loss: 0.0033351103856561837, Test Loss: 0.002332803798033906\n",
            "Epoch 7/1000, Training Loss: 0.0033332508065202545, Test Loss: 0.0023305183969443773\n",
            "Epoch 8/1000, Training Loss: 0.0033321260040570233, Test Loss: 0.0023291066677949583\n",
            "Epoch 9/1000, Training Loss: 0.0033313618303597737, Test Loss: 0.002328165000932765\n",
            "Epoch 10/1000, Training Loss: 0.003330778621851063, Test Loss: 0.0023274911948704827\n",
            "Epoch 11/1000, Training Loss: 0.0033302886503821065, Test Loss: 0.0023269783678038763\n",
            "Epoch 12/1000, Training Loss: 0.003329848521973106, Test Loss: 0.0023265668755416244\n",
            "Epoch 13/1000, Training Loss: 0.0033294365125572088, Test Loss: 0.0023262216496328352\n",
            "Epoch 14/1000, Training Loss: 0.0033290416008574767, Test Loss: 0.0023259210828715064\n",
            "Epoch 15/1000, Training Loss: 0.0033286580972615417, Test Loss: 0.002325651331172942\n",
            "Epoch 16/1000, Training Loss: 0.003328282988644524, Test Loss: 0.002325403247382095\n",
            "Epoch 17/1000, Training Loss: 0.003327914616161032, Test Loss: 0.0023251706466181\n",
            "Epoch 18/1000, Training Loss: 0.003327552012910323, Test Loss: 0.0023249492762216235\n",
            "Epoch 19/1000, Training Loss: 0.0033271945703950766, Test Loss: 0.002324736176818066\n",
            "Epoch 20/1000, Training Loss: 0.003326841869610902, Test Loss: 0.002324529271154042\n",
            "Epoch 21/1000, Training Loss: 0.0033264935948879933, Test Loss: 0.0023243270916864724\n",
            "Epoch 22/1000, Training Loss: 0.003326149489474822, Test Loss: 0.002324128596091011\n",
            "Epoch 23/1000, Training Loss: 0.00332580933226118, Test Loss: 0.0023239330403101548\n",
            "Epoch 24/1000, Training Loss: 0.0033254729252657394, Test Loss: 0.0023237398902161087\n",
            "Epoch 25/1000, Training Loss: 0.0033251400866520574, Test Loss: 0.0023235487596753044\n",
            "Epoch 26/1000, Training Loss: 0.0033248106466230785, Test Loss: 0.0023233593669067187\n",
            "Epoch 27/1000, Training Loss: 0.003324484444847285, Test Loss: 0.002323171503634329\n",
            "Epoch 28/1000, Training Loss: 0.0033241613287271305, Test Loss: 0.00232298501324402\n",
            "Epoch 29/1000, Training Loss: 0.0033238411521526842, Test Loss: 0.0023227997753042955\n",
            "Epoch 30/1000, Training Loss: 0.003323523774551926, Test Loss: 0.0023226156945966247\n",
            "Epoch 31/1000, Training Loss: 0.0033232090601350676, Test Loss: 0.002322432693346652\n",
            "Epoch 32/1000, Training Loss: 0.00332289687727452, Test Loss: 0.002322250705729333\n",
            "Epoch 33/1000, Training Loss: 0.003322587097985348, Test Loss: 0.0023220696739899645\n",
            "Epoch 34/1000, Training Loss: 0.0033222795974835137, Test Loss: 0.0023218895457133657\n",
            "Epoch 35/1000, Training Loss: 0.003321974253806255, Test Loss: 0.002321710271908396\n",
            "Epoch 36/1000, Training Loss: 0.003321670947483069, Test Loss: 0.0023215318056709396\n",
            "Epoch 37/1000, Training Loss: 0.0033213695612484407, Test Loss: 0.0023213541012567186\n",
            "Epoch 38/1000, Training Loss: 0.003321069979789263, Test Loss: 0.002321177113443848\n",
            "Epoch 39/1000, Training Loss: 0.003320772089521164, Test Loss: 0.0023210007970997067\n",
            "Epoch 40/1000, Training Loss: 0.003320475778389026, Test Loss: 0.0023208251068913056\n",
            "Epoch 41/1000, Training Loss: 0.0033201809356877207, Test Loss: 0.0023206499970959173\n",
            "Epoch 42/1000, Training Loss: 0.0033198874518997733, Test Loss: 0.0023204754214812555\n",
            "Epoch 43/1000, Training Loss: 0.003319595218547183, Test Loss: 0.002320301333233397\n",
            "Epoch 44/1000, Training Loss: 0.0033193041280550834, Test Loss: 0.002320127684916994\n",
            "Epoch 45/1000, Training Loss: 0.003319014073625291, Test Loss: 0.0023199544284568667\n",
            "Epoch 46/1000, Training Loss: 0.003318724949118102, Test Loss: 0.0023197815151332697\n",
            "Epoch 47/1000, Training Loss: 0.003318436648940966, Test Loss: 0.0023196088955854356\n",
            "Epoch 48/1000, Training Loss: 0.0033181490679428677, Test Loss: 0.002319436519819629\n",
            "Epoch 49/1000, Training Loss: 0.0033178621013134407, Test Loss: 0.002319264337219097\n",
            "Epoch 50/1000, Training Loss: 0.0033175756444859933, Test Loss: 0.0023190922965541498\n",
            "Epoch 51/1000, Training Loss: 0.0033172895930437332, Test Loss: 0.002318920345991154\n",
            "Epoch 52/1000, Training Loss: 0.0033170038426286047, Test Loss: 0.00231874843309967\n",
            "Epoch 53/1000, Training Loss: 0.0033167182888522357, Test Loss: 0.002318576504857237\n",
            "Epoch 54/1000, Training Loss: 0.0033164328272085497, Test Loss: 0.002318404507651509\n",
            "Epoch 55/1000, Training Loss: 0.0033161473529876827, Test Loss: 0.0023182323872796055\n",
            "Epoch 56/1000, Training Loss: 0.003315861761190875, Test Loss: 0.0023180600889446097\n",
            "Epoch 57/1000, Training Loss: 0.003315575946446068, Test Loss: 0.002317887557249252\n",
            "Epoch 58/1000, Training Loss: 0.0033152898029239473, Test Loss: 0.0023177147361868027\n",
            "Epoch 59/1000, Training Loss: 0.0033150032242542375, Test Loss: 0.0023175415691292912\n",
            "Epoch 60/1000, Training Loss: 0.003314716103442035, Test Loss: 0.0023173679988131323\n",
            "Epoch 61/1000, Training Loss: 0.0033144283327840246, Test Loss: 0.0023171939673222648\n",
            "Epoch 62/1000, Training Loss: 0.0033141398037844203, Test Loss: 0.002317019416068929\n",
            "Epoch 63/1000, Training Loss: 0.003313850407070484, Test Loss: 0.002316844285772172\n",
            "Epoch 64/1000, Training Loss: 0.0033135600323074975, Test Loss: 0.0023166685164341975\n",
            "Epoch 65/1000, Training Loss: 0.0033132685681130695, Test Loss: 0.0023164920473146564\n",
            "Epoch 66/1000, Training Loss: 0.003312975901970655, Test Loss: 0.002316314816902968\n",
            "Epoch 67/1000, Training Loss: 0.003312681920142195, Test Loss: 0.002316136762888756\n",
            "Epoch 68/1000, Training Loss: 0.0033123865075797604, Test Loss: 0.0023159578221304843\n",
            "Epoch 69/1000, Training Loss: 0.003312089547836117, Test Loss: 0.0023157779306223537\n",
            "Epoch 70/1000, Training Loss: 0.003311790922974104, Test Loss: 0.002315597023459525\n",
            "Epoch 71/1000, Training Loss: 0.0033114905134747495, Test Loss: 0.0023154150348017374\n",
            "Epoch 72/1000, Training Loss: 0.0033111881981440295, Test Loss: 0.002315231897835344\n",
            "Epoch 73/1000, Training Loss: 0.003310883854018181, Test Loss: 0.002315047544733837\n",
            "Epoch 74/1000, Training Loss: 0.003310577356267489, Test Loss: 0.002314861906616886\n",
            "Epoch 75/1000, Training Loss: 0.0033102685780984718, Test Loss: 0.002314674913507914\n",
            "Epoch 76/1000, Training Loss: 0.003309957390654369, Test Loss: 0.002314486494290259\n",
            "Epoch 77/1000, Training Loss: 0.0033096436629138685, Test Loss: 0.0023142965766619187\n",
            "Epoch 78/1000, Training Loss: 0.0033093272615879795, Test Loss: 0.002314105087088913\n",
            "Epoch 79/1000, Training Loss: 0.003309008051014988, Test Loss: 0.0023139119507572764\n",
            "Epoch 80/1000, Training Loss: 0.0033086858930534077, Test Loss: 0.002313717091523675\n",
            "Epoch 81/1000, Training Loss: 0.003308360646972856, Test Loss: 0.002313520431864685\n",
            "Epoch 82/1000, Training Loss: 0.003308032169342784, Test Loss: 0.002313321892824696\n",
            "Epoch 83/1000, Training Loss: 0.0033077003139189765, Test Loss: 0.0023131213939624794\n",
            "Epoch 84/1000, Training Loss: 0.00330736493152777, Test Loss: 0.002312918853296402\n",
            "Epoch 85/1000, Training Loss: 0.0033070258699478927, Test Loss: 0.0023127141872482716\n",
            "Epoch 86/1000, Training Loss: 0.003306682973789886, Test Loss: 0.0023125073105858397\n",
            "Epoch 87/1000, Training Loss: 0.0033063360843730138, Test Loss: 0.002312298136363921\n",
            "Epoch 88/1000, Training Loss: 0.003305985039599618, Test Loss: 0.0023120865758641516\n",
            "Epoch 89/1000, Training Loss: 0.0033056296738268372, Test Loss: 0.0023118725385333576\n",
            "Epoch 90/1000, Training Loss: 0.003305269817735642, Test Loss: 0.002311655931920532\n",
            "Epoch 91/1000, Training Loss: 0.00330490529819712, Test Loss: 0.002311436661612415\n",
            "Epoch 92/1000, Training Loss: 0.0033045359381359625, Test Loss: 0.0023112146311676574\n",
            "Epoch 93/1000, Training Loss: 0.0033041615563910846, Test Loss: 0.0023109897420495666\n",
            "Epoch 94/1000, Training Loss: 0.003303781967573351, Test Loss: 0.0023107618935574205\n",
            "Epoch 95/1000, Training Loss: 0.0033033969819203416, Test Loss: 0.00231053098275634\n",
            "Epoch 96/1000, Training Loss: 0.003303006405148124, Test Loss: 0.0023102969044057195\n",
            "Epoch 97/1000, Training Loss: 0.0033026100382999943, Test Loss: 0.00231005955088619\n",
            "Epoch 98/1000, Training Loss: 0.0033022076775921514, Test Loss: 0.002309818812125129\n",
            "Epoch 99/1000, Training Loss: 0.0033017991142562726, Test Loss: 0.0023095745755206914\n",
            "Epoch 100/1000, Training Loss: 0.0033013841343789716, Test Loss: 0.0023093267258643816\n",
            "Epoch 101/1000, Training Loss: 0.0033009625187381186, Test Loss: 0.002309075145262135\n",
            "Epoch 102/1000, Training Loss: 0.0033005340426360144, Test Loss: 0.0023088197130539346\n",
            "Epoch 103/1000, Training Loss: 0.0033000984757294055, Test Loss: 0.0023085603057319496\n",
            "Epoch 104/1000, Training Loss: 0.00329965558185635, Test Loss: 0.002308296796857208\n",
            "Epoch 105/1000, Training Loss: 0.0032992051188599405, Test Loss: 0.0023080290569747973\n",
            "Epoch 106/1000, Training Loss: 0.0032987468384089056, Test Loss: 0.0023077569535276295\n",
            "Epoch 107/1000, Training Loss: 0.003298280485815107, Test Loss: 0.002307480350768743\n",
            "Epoch 108/1000, Training Loss: 0.0032978057998479813, Test Loss: 0.002307199109672203\n",
            "Epoch 109/1000, Training Loss: 0.003297322512545962, Test Loss: 0.0023069130878425807\n",
            "Epoch 110/1000, Training Loss: 0.003296830349024938, Test Loss: 0.0023066221394230594\n",
            "Epoch 111/1000, Training Loss: 0.0032963290272838277, Test Loss: 0.0023063261150021858\n",
            "Epoch 112/1000, Training Loss: 0.003295818258007333, Test Loss: 0.0023060248615193036\n",
            "Epoch 113/1000, Training Loss: 0.003295297744365972, Test Loss: 0.002305718222168697\n",
            "Epoch 114/1000, Training Loss: 0.0032947671818134958, Test Loss: 0.0023054060363024947\n",
            "Epoch 115/1000, Training Loss: 0.0032942262578818035, Test Loss: 0.002305088139332387\n",
            "Epoch 116/1000, Training Loss: 0.0032936746519734946, Test Loss: 0.002304764362630183\n",
            "Epoch 117/1000, Training Loss: 0.003293112035152201, Test Loss: 0.002304434533427305\n",
            "Epoch 118/1000, Training Loss: 0.003292538069930866, Test Loss: 0.0023040984747132527\n",
            "Epoch 119/1000, Training Loss: 0.0032919524100581505, Test Loss: 0.0023037560051331215\n",
            "Epoch 120/1000, Training Loss: 0.0032913547003031647, Test Loss: 0.002303406938884262\n",
            "Epoch 121/1000, Training Loss: 0.0032907445762387424, Test Loss: 0.0023030510856121493\n",
            "Epoch 122/1000, Training Loss: 0.003290121664023492, Test Loss: 0.0023026882503055686\n",
            "Epoch 123/1000, Training Loss: 0.0032894855801828813, Test Loss: 0.002302318233191217\n",
            "Epoch 124/1000, Training Loss: 0.0032888359313896246, Test Loss: 0.002301940829627832\n",
            "Epoch 125/1000, Training Loss: 0.0032881723142436886, Test Loss: 0.0023015558299999592\n",
            "Epoch 126/1000, Training Loss: 0.0032874943150521994, Test Loss: 0.0023011630196115\n",
            "Epoch 127/1000, Training Loss: 0.003286801509609632, Test Loss: 0.0023007621785791723\n",
            "Epoch 128/1000, Training Loss: 0.0032860934629786137, Test Loss: 0.002300353081726033\n",
            "Epoch 129/1000, Training Loss: 0.003285369729271746, Test Loss: 0.0022999354984752193\n",
            "Epoch 130/1000, Training Loss: 0.003284629851434851, Test Loss: 0.0022995091927440967\n",
            "Epoch 131/1000, Training Loss: 0.003283873361032075, Test Loss: 0.002299073922838963\n",
            "Epoch 132/1000, Training Loss: 0.0032830997780333203, Test Loss: 0.0022986294413505494\n",
            "Epoch 133/1000, Training Loss: 0.0032823086106044726, Test Loss: 0.0022981754950504884\n",
            "Epoch 134/1000, Training Loss: 0.0032814993549009634, Test Loss: 0.002297711824788987\n",
            "Epoch 135/1000, Training Loss: 0.003280671494865176, Test Loss: 0.002297238165393941\n",
            "Epoch 136/1000, Training Loss: 0.003279824502028289, Test Loss: 0.0022967542455717364\n",
            "Epoch 137/1000, Training Loss: 0.0032789578353171166, Test Loss: 0.002296259787810013\n",
            "Epoch 138/1000, Training Loss: 0.0032780709408665953, Test Loss: 0.0022957545082826474\n",
            "Epoch 139/1000, Training Loss: 0.003277163251838539, Test Loss: 0.002295238116757288\n",
            "Epoch 140/1000, Training Loss: 0.0032762341882473373, Test Loss: 0.0022947103165057237\n",
            "Epoch 141/1000, Training Loss: 0.003275283156793295, Test Loss: 0.002294170804217428\n",
            "Epoch 142/1000, Training Loss: 0.003274309550704337, Test Loss: 0.0022936192699166457\n",
            "Epoch 143/1000, Training Loss: 0.0032733127495868184, Test Loss: 0.002293055396883356\n",
            "Epoch 144/1000, Training Loss: 0.0032722921192862102, Test Loss: 0.002292478861578525\n",
            "Epoch 145/1000, Training Loss: 0.0032712470117584654, Test Loss: 0.0022918893335740443\n",
            "Epoch 146/1000, Training Loss: 0.0032701767649528676, Test Loss: 0.002291286475487781\n",
            "Epoch 147/1000, Training Loss: 0.0032690807027072146, Test Loss: 0.002290669942924182\n",
            "Epoch 148/1000, Training Loss: 0.0032679581346561856, Test Loss: 0.002290039384420906\n",
            "Epoch 149/1000, Training Loss: 0.003266808356153784, Test Loss: 0.0022893944414019668\n",
            "Epoch 150/1000, Training Loss: 0.0032656306482107384, Test Loss: 0.002288734748137891\n",
            "Epoch 151/1000, Training Loss: 0.0032644242774477936, Test Loss: 0.0022880599317134332\n",
            "Epoch 152/1000, Training Loss: 0.0032631884960658045, Test Loss: 0.002287369612003377\n",
            "Epoch 153/1000, Training Loss: 0.003261922541833594, Test Loss: 0.0022866634016570292\n",
            "Epoch 154/1000, Training Loss: 0.0032606256380945174, Test Loss: 0.002285940906091958\n",
            "Epoch 155/1000, Training Loss: 0.003259296993792708, Test Loss: 0.0022852017234976157\n",
            "Epoch 156/1000, Training Loss: 0.0032579358035199854, Test Loss: 0.0022844454448494858\n",
            "Epoch 157/1000, Training Loss: 0.003256541247584394, Test Loss: 0.0022836716539343747\n",
            "Epoch 158/1000, Training Loss: 0.0032551124921013797, Test Loss: 0.0022828799273875646\n",
            "Epoch 159/1000, Training Loss: 0.0032536486891085853, Test Loss: 0.0022820698347424825\n",
            "Epoch 160/1000, Training Loss: 0.0032521489767052656, Test Loss: 0.0022812409384936126\n",
            "Epoch 161/1000, Training Loss: 0.0032506124792173255, Test Loss: 0.0022803927941733764\n",
            "Epoch 162/1000, Training Loss: 0.003249038307388963, Test Loss: 0.0022795249504437005\n",
            "Epoch 163/1000, Training Loss: 0.0032474255586019286, Test Loss: 0.0022786369492030565\n",
            "Epoch 164/1000, Training Loss: 0.0032457733171233818, Test Loss: 0.002277728325709695\n",
            "Epoch 165/1000, Training Loss: 0.003244080654383351, Test Loss: 0.002276798608721887\n",
            "Epoch 166/1000, Training Loss: 0.0032423466292827614, Test Loss: 0.0022758473206559164\n",
            "Epoch 167/1000, Training Loss: 0.003240570288533032, Test Loss: 0.0022748739777626336\n",
            "Epoch 168/1000, Training Loss: 0.0032387506670282142, Test Loss: 0.0022738780903233365\n",
            "Epoch 169/1000, Training Loss: 0.003236886788250653, Test Loss: 0.0022728591628657755\n",
            "Epoch 170/1000, Training Loss: 0.003234977664711128, Test Loss: 0.0022718166944010673\n",
            "Epoch 171/1000, Training Loss: 0.0032330222984244743, Test Loss: 0.0022707501786822914\n",
            "Epoch 172/1000, Training Loss: 0.003231019681421626, Test Loss: 0.0022696591044855247\n",
            "Epoch 173/1000, Training Loss: 0.0032289687962990844, Test Loss: 0.0022685429559141015\n",
            "Epoch 174/1000, Training Loss: 0.003226868616806759, Test Loss: 0.0022674012127268032\n",
            "Epoch 175/1000, Training Loss: 0.0032247181084752007, Test Loss: 0.0022662333506907415\n",
            "Epoch 176/1000, Training Loss: 0.003222516229283188, Test Loss: 0.0022650388419595945\n",
            "Epoch 177/1000, Training Loss: 0.0032202619303667007, Test Loss: 0.0022638171554779274\n",
            "Epoch 178/1000, Training Loss: 0.0032179541567702754, Test Loss: 0.002262567757412194\n",
            "Epoch 179/1000, Training Loss: 0.003215591848241815, Test Loss: 0.0022612901116090874\n",
            "Epoch 180/1000, Training Loss: 0.0032131739400718742, Test Loss: 0.0022599836800818044\n",
            "Epoch 181/1000, Training Loss: 0.003210699363978543, Test Loss: 0.002258647923524791\n",
            "Epoch 182/1000, Training Loss: 0.003208167049039015, Test Loss: 0.00225728230185747\n",
            "Epoch 183/1000, Training Loss: 0.0032055759226689894, Test Loss: 0.002255886274797442\n",
            "Epoch 184/1000, Training Loss: 0.00320292491165108, Test Loss: 0.0022544593024635897\n",
            "Epoch 185/1000, Training Loss: 0.003200212943213426, Test Loss: 0.0022530008460094455\n",
            "Epoch 186/1000, Training Loss: 0.003197438946159742, Test Loss: 0.002251510368287192\n",
            "Epoch 187/1000, Training Loss: 0.003194601852052068, Test Loss: 0.002249987334542562\n",
            "Epoch 188/1000, Training Loss: 0.003191700596447499, Test Loss: 0.0022484312131408617\n",
            "Epoch 189/1000, Training Loss: 0.0031887341201902114, Test Loss: 0.002246841476324299\n",
            "Epoch 190/1000, Training Loss: 0.003185701370760106, Test Loss: 0.0022452176010007274\n",
            "Epoch 191/1000, Training Loss: 0.003182601303679394, Test Loss: 0.0022435590695638386\n",
            "Epoch 192/1000, Training Loss: 0.0031794328839784476, Test Loss: 0.002241865370744797\n",
            "Epoch 193/1000, Training Loss: 0.0031761950877222294, Test Loss: 0.002240136000495198\n",
            "Epoch 194/1000, Training Loss: 0.003172886903598557, Test Loss: 0.002238370462901195\n",
            "Epoch 195/1000, Training Loss: 0.003169507334569426, Test Loss: 0.002236568271128513\n",
            "Epoch 196/1000, Training Loss: 0.003166055399586514, Test Loss: 0.002234728948398008\n",
            "Epoch 197/1000, Training Loss: 0.003162530135371911, Test Loss: 0.0022328520289912946\n",
            "Epoch 198/1000, Training Loss: 0.0031589305982649483, Test Loss: 0.0022309370592859114\n",
            "Epoch 199/1000, Training Loss: 0.0031552558661358727, Test Loss: 0.002228983598819294\n",
            "Epoch 200/1000, Training Loss: 0.0031515050403668607, Test Loss: 0.0022269912213807434\n",
            "Epoch 201/1000, Training Loss: 0.0031476772479006525, Test Loss: 0.002224959516130448\n",
            "Epoch 202/1000, Training Loss: 0.003143771643356766, Test Loss: 0.0022228880887443865\n",
            "Epoch 203/1000, Training Loss: 0.00313978741121492, Test Loss: 0.0022207765625838296\n",
            "Epoch 204/1000, Training Loss: 0.0031357237680648847, Test Loss: 0.0022186245798879118\n",
            "Epoch 205/1000, Training Loss: 0.0031315799649215335, Test Loss: 0.0022164318029875567\n",
            "Epoch 206/1000, Training Loss: 0.003127355289603323, Test Loss: 0.0022141979155388047\n",
            "Epoch 207/1000, Training Loss: 0.003123049069171893, Test Loss: 0.0022119226237732677\n",
            "Epoch 208/1000, Training Loss: 0.0031186606724297656, Test Loss: 0.002209605657763265\n",
            "Epoch 209/1000, Training Loss: 0.00311418951247247, Test Loss: 0.0022072467726987604\n",
            "Epoch 210/1000, Training Loss: 0.003109635049290549, Test Loss: 0.0022048457501729667\n",
            "Epoch 211/1000, Training Loss: 0.0031049967924161397, Test Loss: 0.002202402399473079\n",
            "Epoch 212/1000, Training Loss: 0.0031002743036077705, Test Loss: 0.002199916558872229\n",
            "Epoch 213/1000, Training Loss: 0.0030954671995661086, Test Loss: 0.002197388096918296\n",
            "Epoch 214/1000, Training Loss: 0.003090575154672246, Test Loss: 0.0021948169137148265\n",
            "Epoch 215/1000, Training Loss: 0.0030855979037389993, Test Loss: 0.002192202942188752\n",
            "Epoch 216/1000, Training Loss: 0.003080535244764471, Test Loss: 0.002189546149339192\n",
            "Epoch 217/1000, Training Loss: 0.003075387041675863, Test Loss: 0.0021868465374610267\n",
            "Epoch 218/1000, Training Loss: 0.003070153227050204, Test Loss: 0.0021841041453364344\n",
            "Epoch 219/1000, Training Loss: 0.0030648338047972847, Test Loss: 0.0021813190493869842\n",
            "Epoch 220/1000, Training Loss: 0.003059428852788717, Test Loss: 0.002178491364778368\n",
            "Epoch 221/1000, Training Loss: 0.0030539385254155903, Test Loss: 0.0021756212464691615\n",
            "Epoch 222/1000, Training Loss: 0.0030483630560557834, Test Loss: 0.002172708890194561\n",
            "Epoch 223/1000, Training Loss: 0.003042702759430571, Test Loss: 0.0021697545333752982\n",
            "Epoch 224/1000, Training Loss: 0.0030369580338287703, Test Loss: 0.0021667584559415007\n",
            "Epoch 225/1000, Training Loss: 0.0030311293631753093, Test Loss: 0.0021637209810606156\n",
            "Epoch 226/1000, Training Loss: 0.00302521731891982, Test Loss: 0.0021606424757579833\n",
            "Epoch 227/1000, Training Loss: 0.0030192225617196554, Test Loss: 0.0021575233514181994\n",
            "Epoch 228/1000, Training Loss: 0.00301314584289064, Test Loss: 0.0021543640641548747\n",
            "Epoch 229/1000, Training Loss: 0.003006988005597882, Test Loss: 0.002151165115036024\n",
            "Epoch 230/1000, Training Loss: 0.003000749985758199, Test Loss: 0.0021479270501519668\n",
            "Epoch 231/1000, Training Loss: 0.0029944328126250736, Test Loss: 0.0021446504605123315\n",
            "Epoch 232/1000, Training Loss: 0.002988037609026664, Test Loss: 0.002141335981758591\n",
            "Epoch 233/1000, Training Loss: 0.0029815655912271964, Test Loss: 0.002137984293678385\n",
            "Epoch 234/1000, Training Loss: 0.0029750180683821774, Test Loss: 0.0021345961195079997\n",
            "Epoch 235/1000, Training Loss: 0.002968396441558236, Test Loss: 0.002131172225009399\n",
            "Epoch 236/1000, Training Loss: 0.002961702202289039, Test Loss: 0.0021277134173084493\n",
            "Epoch 237/1000, Training Loss: 0.002954936930639762, Test Loss: 0.002124220543481417\n",
            "Epoch 238/1000, Training Loss: 0.0029481022927538947, Test Loss: 0.00212069448887723\n",
            "Epoch 239/1000, Training Loss: 0.0029412000378578372, Test Loss: 0.0021171361751637103\n",
            "Epoch 240/1000, Training Loss: 0.002934231994700747, Test Loss: 0.002113546558086721\n",
            "Epoch 241/1000, Training Loss: 0.0029272000674094786, Test Loss: 0.002109926624932156\n",
            "Epoch 242/1000, Training Loss: 0.0029201062307411215, Test Loss: 0.0021062773916817458\n",
            "Epoch 243/1000, Training Loss: 0.0029129525247187154, Test Loss: 0.0021025998998548557\n",
            "Epoch 244/1000, Training Loss: 0.0029057410486389777, Test Loss: 0.0020988952130298675\n",
            "Epoch 245/1000, Training Loss: 0.0028984739544445424, Test Loss: 0.0020951644130400974\n",
            "Epoch 246/1000, Training Loss: 0.002891153439456996, Test Loss: 0.0020914085958409177\n",
            "Epoch 247/1000, Training Loss: 0.0028837817384710297, Test Loss: 0.0020876288670462905\n",
            "Epoch 248/1000, Training Loss: 0.002876361115214198, Test Loss: 0.0020838263371347164\n",
            "Epoch 249/1000, Training Loss: 0.0028688938531809998, Test Loss: 0.0020800021163264347\n",
            "Epoch 250/1000, Training Loss: 0.002861382245854283, Test Loss: 0.002076157309135396\n",
            "Epoch 251/1000, Training Loss: 0.0028538285863311326, Test Loss: 0.0020722930086014214\n",
            "Epoch 252/1000, Training Loss: 0.0028462351563745196, Test Loss: 0.002068410290209668\n",
            "Epoch 253/1000, Training Loss: 0.0028386042149158306, Test Loss: 0.0020645102055061953\n",
            "Epoch 254/1000, Training Loss: 0.0028309379860369886, Test Loss: 0.0020605937754199447\n",
            "Epoch 255/1000, Training Loss: 0.002823238646464079, Test Loss: 0.0020566619833029026\n",
            "Epoch 256/1000, Training Loss: 0.0028155083126071823, Test Loss: 0.002052715767701304\n",
            "Epoch 257/1000, Training Loss: 0.002807749027183352, Test Loss: 0.002048756014871812\n",
            "Epoch 258/1000, Training Loss: 0.002799962745461376, Test Loss: 0.00204478355105717\n",
            "Epoch 259/1000, Training Loss: 0.0027921513211679174, Test Loss: 0.0020407991345362405\n",
            "Epoch 260/1000, Training Loss: 0.0027843164920950346, Test Loss: 0.0020368034474633626\n",
            "Epoch 261/1000, Training Loss: 0.0027764598654485455, Test Loss: 0.0020327970875114814\n",
            "Epoch 262/1000, Training Loss: 0.0027685829029755467, Test Loss: 0.002028780559332775\n",
            "Epoch 263/1000, Training Loss: 0.002760686905907351, Test Loss: 0.0020247542658492087\n",
            "Epoch 264/1000, Training Loss: 0.002752772999751233, Test Loss: 0.0020207184993836207\n",
            "Epoch 265/1000, Training Loss: 0.0027448421189607496, Test Loss: 0.0020166734326397913\n",
            "Epoch 266/1000, Training Loss: 0.0027368949915098687, Test Loss: 0.0020126191095370576\n",
            "Epoch 267/1000, Training Loss: 0.002728932123390965, Test Loss: 0.002008555435901742\n",
            "Epoch 268/1000, Training Loss: 0.0027209537830506633, Test Loss: 0.0020044821700137476\n",
            "Epoch 269/1000, Training Loss: 0.0027129599857708864, Test Loss: 0.0020003989130022303\n",
            "Epoch 270/1000, Training Loss: 0.002704950477995114, Test Loss: 0.0019963050990791345\n",
            "Epoch 271/1000, Training Loss: 0.002696924721591985, Test Loss: 0.001992199985593821\n",
            "Epoch 272/1000, Training Loss: 0.002688881878039925, Test Loss: 0.001988082642885634\n",
            "Epoch 273/1000, Training Loss: 0.002680820792507796, Test Loss: 0.001983951943904536\n",
            "Epoch 274/1000, Training Loss: 0.0026727399777972385, Test Loss: 0.001979806553562245\n",
            "Epoch 275/1000, Training Loss: 0.0026646375981031457, Test Loss: 0.0019756449177683088\n",
            "Epoch 276/1000, Training Loss: 0.0026565114525390126, Test Loss: 0.0019714652520966165\n",
            "Epoch 277/1000, Training Loss: 0.0026483589583643037, Test Loss: 0.001967265530018324\n",
            "Epoch 278/1000, Training Loss: 0.002640177133841278, Test Loss: 0.0019630434706268828\n",
            "Epoch 279/1000, Training Loss: 0.002631962580639164, Test Loss: 0.0019587965257698886\n",
            "Epoch 280/1000, Training Loss: 0.0026237114656941147, Test Loss: 0.0019545218664903763\n",
            "Epoch 281/1000, Training Loss: 0.002615419502424209, Test Loss: 0.0019502163686677355\n",
            "Epoch 282/1000, Training Loss: 0.0026070819311899, Test Loss: 0.001945876597734519\n",
            "Epoch 283/1000, Training Loss: 0.0025986934988818824, Test Loss: 0.0019414987923309702\n",
            "Epoch 284/1000, Training Loss: 0.0025902484375103915, Test Loss: 0.0019370788467435243\n",
            "Epoch 285/1000, Training Loss: 0.002581740441662659, Test Loss: 0.0019326122919570206\n",
            "Epoch 286/1000, Training Loss: 0.002573162644688729, Test Loss: 0.00192809427513303\n",
            "Epoch 287/1000, Training Loss: 0.00256450759347017, Test Loss: 0.0019235195373084211\n",
            "Epoch 288/1000, Training Loss: 0.0025557672216218502, Test Loss: 0.0019188823890896917\n",
            "Epoch 289/1000, Training Loss: 0.0025469328209738805, Test Loss: 0.0019141766840994109\n",
            "Epoch 290/1000, Training Loss: 0.0025379950111797324, Test Loss: 0.0019093957899125136\n",
            "Epoch 291/1000, Training Loss: 0.0025289437072977014, Test Loss: 0.001904532556202075\n",
            "Epoch 292/1000, Training Loss: 0.002519768085197218, Test Loss: 0.0018995792797986252\n",
            "Epoch 293/1000, Training Loss: 0.0025104565446497323, Test Loss: 0.0018945276663540971\n",
            "Epoch 294/1000, Training Loss: 0.0025009966699774343, Test Loss: 0.0018893687882942296\n",
            "Epoch 295/1000, Training Loss: 0.0024913751881534794, Test Loss: 0.0018840930387432283\n",
            "Epoch 296/1000, Training Loss: 0.002481577924276836, Test Loss: 0.0018786900811153776\n",
            "Epoch 297/1000, Training Loss: 0.002471589754386289, Test Loss: 0.0018731487940939675\n",
            "Epoch 298/1000, Training Loss: 0.0024613945556350796, Test Loss: 0.0018674572117638164\n",
            "Epoch 299/1000, Training Loss: 0.002450975153925244, Test Loss: 0.0018616024587362584\n",
            "Epoch 300/1000, Training Loss: 0.002440313269204594, Test Loss: 0.0018555706802131017\n",
            "Epoch 301/1000, Training Loss: 0.0024293894587678905, Test Loss: 0.0018493469670890533\n",
            "Epoch 302/1000, Training Loss: 0.0024181830590863738, Test Loss: 0.001842915276402599\n",
            "Epoch 303/1000, Training Loss: 0.002406672126929023, Test Loss: 0.0018362583477285765\n",
            "Epoch 304/1000, Training Loss: 0.0023948333808495007, Test Loss: 0.0018293576164792147\n",
            "Epoch 305/1000, Training Loss: 0.0023826421445128742, Test Loss: 0.00182219312556485\n",
            "Epoch 306/1000, Training Loss: 0.0023700722938471805, Test Loss: 0.0018147434374841316\n",
            "Epoch 307/1000, Training Loss: 0.0023570962106515296, Test Loss: 0.0018069855496921994\n",
            "Epoch 308/1000, Training Loss: 0.0023436847461019935, Test Loss: 0.0017988948170607345\n",
            "Epoch 309/1000, Training Loss: 0.00232980719859875, Test Loss: 0.0017904448864227976\n",
            "Epoch 310/1000, Training Loss: 0.0023154313116212646, Test Loss: 0.0017816076496106697\n",
            "Epoch 311/1000, Training Loss: 0.0023005232987279084, Test Loss: 0.0017723532230607775\n",
            "Epoch 312/1000, Training Loss: 0.0022850479045665047, Test Loss: 0.001762649963974675\n",
            "Epoch 313/1000, Training Loss: 0.0022689685127476873, Test Loss: 0.0017524645351600266\n",
            "Epoch 314/1000, Training Loss: 0.0022522473136344723, Test Loss: 0.0017417620329601392\n",
            "Epoch 315/1000, Training Loss: 0.0022348455474261795, Test Loss: 0.0017305061949814\n",
            "Epoch 316/1000, Training Loss: 0.0022167238401896382, Test Loss: 0.0017186597064240128\n",
            "Epoch 317/1000, Training Loss: 0.002197842652428288, Test Loss: 0.0017061846253730582\n",
            "Epoch 318/1000, Training Loss: 0.002178162860941338, Test Loss: 0.0016930429479265357\n",
            "Epoch 319/1000, Training Loss: 0.002157646494483939, Test Loss: 0.0016791973328650644\n",
            "Epoch 320/1000, Training Loss: 0.002136257641260717, Test Loss: 0.0016646120018726177\n",
            "Epoch 321/1000, Training Loss: 0.002113963540544725, Test Loss: 0.001649253824138674\n",
            "Epoch 322/1000, Training Loss: 0.0020907358605879492, Test Loss: 0.0016330935825395493\n",
            "Epoch 323/1000, Training Loss: 0.0020665521494520243, Test Loss: 0.0016161074017530398\n",
            "Epoch 324/1000, Training Loss: 0.0020413974238510436, Test Loss: 0.0015982782964079976\n",
            "Epoch 325/1000, Training Loss: 0.002015265833889932, Test Loss: 0.0015795977705166336\n",
            "Epoch 326/1000, Training Loss: 0.0019881623104959735, Test Loss: 0.0015600673702637665\n",
            "Epoch 327/1000, Training Loss: 0.001960104071105316, Test Loss: 0.0015397000648053653\n",
            "Epoch 328/1000, Training Loss: 0.0019311218335680213, Test Loss: 0.0015185213098564897\n",
            "Epoch 329/1000, Training Loss: 0.0019012605756157084, Test Loss: 0.0014965696433561291\n",
            "Epoch 330/1000, Training Loss: 0.0018705796851442185, Test Loss: 0.0014738966778080084\n",
            "Epoch 331/1000, Training Loss: 0.001839152380615331, Test Loss: 0.001450566393968062\n",
            "Epoch 332/1000, Training Loss: 0.0018070643422780698, Test Loss: 0.0014266537046955215\n",
            "Epoch 333/1000, Training Loss: 0.0017744115784607893, Test Loss: 0.0014022423391856088\n",
            "Epoch 334/1000, Training Loss: 0.0017412976448523767, Test Loss: 0.0013774221836221547\n",
            "Epoch 335/1000, Training Loss: 0.0017078304213847119, Test Loss: 0.0013522862879433908\n",
            "Epoch 336/1000, Training Loss: 0.001674118712912543, Test Loss: 0.0013269277937785144\n",
            "Epoch 337/1000, Training Loss: 0.0016402689623328255, Test Loss: 0.0013014370449313572\n",
            "Epoch 338/1000, Training Loss: 0.001606382342656025, Test Loss: 0.0012758991074014094\n",
            "Epoch 339/1000, Training Loss: 0.0015725524329471548, Test Loss: 0.001250391859102703\n",
            "Epoch 340/1000, Training Loss: 0.0015388635961591016, Test Loss: 0.0012249847257436936\n",
            "Epoch 341/1000, Training Loss: 0.0015053900834628563, Test Loss: 0.0011997380566415481\n",
            "Epoch 342/1000, Training Loss: 0.0014721958075600788, Test Loss: 0.001174703067466023\n",
            "Epoch 343/1000, Training Loss: 0.0014393346689534996, Test Loss: 0.0011499222344771296\n",
            "Epoch 344/1000, Training Loss: 0.0014068512889963698, Test Loss: 0.0011254300079732856\n",
            "Epoch 345/1000, Training Loss: 0.001374781999568228, Test Loss: 0.00110125371706531\n",
            "Epoch 346/1000, Training Loss: 0.0013431559547691396, Test Loss: 0.001077414556382824\n",
            "Epoch 347/1000, Training Loss: 0.001311996256853313, Test Loss: 0.0010539285705346638\n",
            "Epoch 348/1000, Training Loss: 0.0012813210192526977, Test Loss: 0.0010308075781945954\n",
            "Epoch 349/1000, Training Loss: 0.0012511443185137968, Test Loss: 0.0010080600007945646\n",
            "Epoch 350/1000, Training Loss: 0.0012214770111387914, Test Loss: 0.0009856915791858184\n",
            "Epoch 351/1000, Training Loss: 0.0011923274094946593, Test Loss: 0.0009637059749335499\n",
            "Epoch 352/1000, Training Loss: 0.0011637018232813468, Test Loss: 0.0009421052616662698\n",
            "Epoch 353/1000, Training Loss: 0.001135604980416525, Test Loss: 0.0009208903170097385\n",
            "Epoch 354/1000, Training Loss: 0.0011080403447551825, Test Loss: 0.0009000611280633257\n",
            "Epoch 355/1000, Training Loss: 0.0010810103489481814, Test Loss: 0.000879617023979212\n",
            "Epoch 356/1000, Training Loss: 0.0010545165599163935, Test Loss: 0.0008595568486638136\n",
            "Epoch 357/1000, Training Loss: 0.0010285597926158272, Test Loss: 0.0008398790854355117\n",
            "Epoch 358/1000, Training Loss: 0.0010031401855226027, Test Loss: 0.0008205819439888174\n",
            "Epoch 359/1000, Training Loss: 0.0009782572489242542, Test Loss: 0.0008016634184550947\n",
            "Epoch 360/1000, Training Loss: 0.0009539098948790203, Test Loss: 0.0007831213238487611\n",
            "Epoch 361/1000, Training Loss: 0.0009300964557125822, Test Loss: 0.0007649533168191615\n",
            "Epoch 362/1000, Training Loss: 0.0009068146962118639, Test Loss: 0.0007471569054251639\n",
            "Epoch 363/1000, Training Loss: 0.0008840618232552534, Test Loss: 0.0007297294516206456\n",
            "Epoch 364/1000, Training Loss: 0.0008618344954716261, Test Loss: 0.0007126681692772676\n",
            "Epoch 365/1000, Training Loss: 0.0008401288346175529, Test Loss: 0.000695970119863069\n",
            "Epoch 366/1000, Training Loss: 0.0008189404396683428, Test Loss: 0.0006796322073220098\n",
            "Epoch 367/1000, Training Loss: 0.0007982644041003021, Test Loss: 0.000663651173243483\n",
            "Epoch 368/1000, Training Loss: 0.0007780953364656322, Test Loss: 0.0006480235930522651\n",
            "Epoch 369/1000, Training Loss: 0.0007584273840995585, Test Loss: 0.0006327458736730886\n",
            "Epoch 370/1000, Training Loss: 0.0007392542596266334, Test Loss: 0.0006178142529145576\n",
            "Epoch 371/1000, Training Loss: 0.0007205692698288271, Test Loss: 0.0006032248006616365\n",
            "Epoch 372/1000, Training Loss: 0.0007023653463852792, Test Loss: 0.0005889734218537504\n",
            "Epoch 373/1000, Training Loss: 0.0006846350779783827, Test Loss: 0.0005750558611478043\n",
            "Epoch 374/1000, Training Loss: 0.0006673707432724518, Test Loss: 0.0005614677091137671\n",
            "Epoch 375/1000, Training Loss: 0.0006505643443010556, Test Loss: 0.0005482044097796447\n",
            "Epoch 376/1000, Training Loss: 0.000634207639840391, Test Loss: 0.0005352612693268514\n",
            "Epoch 377/1000, Training Loss: 0.0006182921783941869, Test Loss: 0.0005226334657330388\n",
            "Epoch 378/1000, Training Loss: 0.0006028093304663282, Test Loss: 0.0005103160591630525\n",
            "Epoch 379/1000, Training Loss: 0.000587750319848627, Test Loss: 0.0004983040029186306\n",
            "Epoch 380/1000, Training Loss: 0.0005731062537003942, Test Loss: 0.00048659215477064936\n",
            "Epoch 381/1000, Training Loss: 0.0005588681512429829, Test Loss: 0.000475175288513561\n",
            "Epoch 382/1000, Training Loss: 0.000545026970934932, Test Loss: 0.00046404810559842497\n",
            "Epoch 383/1000, Training Loss: 0.0005315736360319394, Test Loss: 0.0004532052467181222\n",
            "Epoch 384/1000, Training Loss: 0.0005184990584696584, Test Loss: 0.00044264130323525675\n",
            "Epoch 385/1000, Training Loss: 0.0005057941610371877, Test Loss: 0.000432350828359508\n",
            "Epoch 386/1000, Training Loss: 0.000493449897834288, Test Loss: 0.0004223283479963919\n",
            "Epoch 387/1000, Training Loss: 0.0004814572730269906, Test Loss: 0.0004125683712035695\n",
            "Epoch 388/1000, Training Loss: 0.000469807357933884, Test Loss: 0.0004030654002036979\n",
            "Epoch 389/1000, Training Loss: 0.00045849130648979415, Test Loss: 0.0003938139399144772\n",
            "Epoch 390/1000, Training Loss: 0.0004475003691450269, Test Loss: 0.00038480850696688947\n",
            "Epoch 391/1000, Training Loss: 0.0004368259052669887, Test Loss: 0.00037604363819177975\n",
            "Epoch 392/1000, Training Loss: 0.0004264593941175126, Test Loss: 0.00036751389856297845\n",
            "Epoch 393/1000, Training Loss: 0.00041639244448357074, Test Loss: 0.0003592138885919871\n",
            "Epoch 394/1000, Training Loss: 0.0004066168030417361, Test Loss: 0.0003511382511752623\n",
            "Epoch 395/1000, Training Loss: 0.0003971243615380437, Test Loss: 0.00034328167790010413\n",
            "Epoch 396/1000, Training Loss: 0.0003879071628648651, Test Loss: 0.0003356389148192315\n",
            "Epoch 397/1000, Training Loss: 0.0003789574061154577, Test Loss: 0.00032820476770773356\n",
            "Epoch 398/1000, Training Loss: 0.00037026745069515375, Test Loss: 0.00032097410681868646\n",
            "Epoch 399/1000, Training Loss: 0.00036182981956560975, Test Loss: 0.0003139418711559656\n",
            "Epoch 400/1000, Training Loss: 0.00035363720169581285, Test Loss: 0.0003071030722844698\n",
            "Epoch 401/1000, Training Loss: 0.0003456824537901479, Test Loss: 0.00030045279769902846\n",
            "Epoch 402/1000, Training Loss: 0.0003379586013604793, Test Loss: 0.00029398621377436603\n",
            "Epoch 403/1000, Training Loss: 0.0003304588392053315, Test Loss: 0.00028769856831855896\n",
            "Epoch 404/1000, Training Loss: 0.00032317653135578283, Test Loss: 0.0002815851927530588\n",
            "Epoch 405/1000, Training Loss: 0.0003161052105436237, Test Loss: 0.00027564150394192885\n",
            "Epoch 406/1000, Training Loss: 0.0003092385772438251, Test Loss: 0.00026986300569292764\n",
            "Epoch 407/1000, Training Loss: 0.0003025704983395015, Test Loss: 0.00026424528995253163\n",
            "Epoch 408/1000, Training Loss: 0.0002960950054541641, Test Loss: 0.0002587840377165172\n",
            "Epoch 409/1000, Training Loss: 0.0002898062929924198, Test Loss: 0.00025347501967695296\n",
            "Epoch 410/1000, Training Loss: 0.00028369871592710247, Test Loss: 0.00024831409662581295\n",
            "Epoch 411/1000, Training Loss: 0.0002777667873676746, Test Loss: 0.00024329721963461587\n",
            "Epoch 412/1000, Training Loss: 0.00027200517594161216, Test Loss: 0.00023842043002847996\n",
            "Epoch 413/1000, Training Loss: 0.0002664087030178861, Test Loss: 0.00023367985917235998\n",
            "Epoch 414/1000, Training Loss: 0.00026097233979884476, Test Loss: 0.00022907172808609398\n",
            "Epoch 415/1000, Training Loss: 0.00025569120430441345, Test Loss: 0.00022459234690416603\n",
            "Epoch 416/1000, Training Loss: 0.0002505605582702138, Test Loss: 0.00022023811419503785\n",
            "Epoch 417/1000, Training Loss: 0.00024557580397904127, Test Loss: 0.0002160055161541766\n",
            "Epoch 418/1000, Training Loss: 0.00024073248104323605, Test Loss: 0.00021189112568389208\n",
            "Epoch 419/1000, Training Loss: 0.00023602626315353482, Test Loss: 0.00020789160137233028\n",
            "Epoch 420/1000, Training Loss: 0.00023145295480843462, Test Loss: 0.0002040036863831342\n",
            "Epoch 421/1000, Training Loss: 0.0002270084880364727, Test Loss: 0.00020022420726648732\n",
            "Epoch 422/1000, Training Loss: 0.0002226889191224422, Test Loss: 0.0001965500727015236\n",
            "Epoch 423/1000, Training Loss: 0.0002184904253472377, Test Loss: 0.00019297827217926503\n",
            "Epoch 424/1000, Training Loss: 0.0002144093017499217, Test Loss: 0.0001895058746347349\n",
            "Epoch 425/1000, Training Loss: 0.0002104419579194006, Test Loss: 0.00018613002703604848\n",
            "Epoch 426/1000, Training Loss: 0.00020658491482225755, Test Loss: 0.0001828479529377958\n",
            "Epoch 427/1000, Training Loss: 0.00020283480167229776, Test Loss: 0.00017965695100535854\n",
            "Epoch 428/1000, Training Loss: 0.00019918835284663154, Test Loss: 0.00017655439351634155\n",
            "Epoch 429/1000, Training Loss: 0.000195642404852344, Test Loss: 0.00017353772484468983\n",
            "Epoch 430/1000, Training Loss: 0.00019219389334719346, Test Loss: 0.0001706044599325778\n",
            "Epoch 431/1000, Training Loss: 0.0001888398502171626, Test Loss: 0.00016775218275482212\n",
            "Epoch 432/1000, Training Loss: 0.00018557740071316247, Test Loss: 0.00016497854477995222\n",
            "Epoch 433/1000, Training Loss: 0.00018240376064877816, Test Loss: 0.0001622812634318777\n",
            "Epoch 434/1000, Training Loss: 0.0001793162336604046, Test Loss: 0.00015965812055553942\n",
            "Epoch 435/1000, Training Loss: 0.000176312208530869, Test Loss: 0.0001571069608896911\n",
            "Epoch 436/1000, Training Loss: 0.00017338915657723458, Test Loss: 0.00015462569054964343\n",
            "Epoch 437/1000, Training Loss: 0.0001705446291032263, Test Loss: 0.00015221227552244724\n",
            "Epoch 438/1000, Training Loss: 0.00016777625491637725, Test Loss: 0.00014986474017673375\n",
            "Epoch 439/1000, Training Loss: 0.0001650817379099153, Test Loss: 0.00014758116578923562\n",
            "Epoch 440/1000, Training Loss: 0.00016245885470904628, Test Loss: 0.00014535968908970827\n",
            "Epoch 441/1000, Training Loss: 0.00015990545238121857, Test Loss: 0.0001431985008257742\n",
            "Epoch 442/1000, Training Loss: 0.00015741944620976825, Test Loss: 0.00014109584434901856\n",
            "Epoch 443/1000, Training Loss: 0.00015499881753021735, Test Loss: 0.00013905001422351773\n",
            "Epoch 444/1000, Training Loss: 0.0001526416116283704, Test Loss: 0.00013705935485774706\n",
            "Epoch 445/1000, Training Loss: 0.00015034593569930054, Test Loss: 0.0001351222591607027\n",
            "Epoch 446/1000, Training Loss: 0.00014810995686615714, Test Loss: 0.0001332371672229396\n",
            "Epoch 447/1000, Training Loss: 0.00014593190025775655, Test Loss: 0.0001314025650230747\n",
            "Epoch 448/1000, Training Loss: 0.0001438100471437961, Test Loss: 0.00012961698316021298\n",
            "Epoch 449/1000, Training Loss: 0.00014174273312647884, Test Loss: 0.0001278789956126049\n",
            "Epoch 450/1000, Training Loss: 0.00013972834638735978, Test Loss: 0.00012618721852279642\n",
            "Epoch 451/1000, Training Loss: 0.00013776532598814472, Test Loss: 0.00012454030900945843\n",
            "Epoch 452/1000, Training Loss: 0.0001358521602241739, Test Loss: 0.00012293696400590145\n",
            "Epoch 453/1000, Training Loss: 0.0001339873850293013, Test Loss: 0.00012137591912533066\n",
            "Epoch 454/1000, Training Loss: 0.000132169582430884, Test Loss: 0.00011985594755277023\n",
            "Epoch 455/1000, Training Loss: 0.00013039737905359174, Test Loss: 0.00011837585896354023\n",
            "Epoch 456/1000, Training Loss: 0.00012866944467072479, Test Loss: 0.00011693449846810885\n",
            "Epoch 457/1000, Training Loss: 0.0001269844908017844, Test Loss: 0.00011553074558312206\n",
            "Epoch 458/1000, Training Loss: 0.00012534126935499808, Test Loss: 0.0001141635132283399\n",
            "Epoch 459/1000, Training Loss: 0.0001237385713135411, Test Loss: 0.00011283174674920122\n",
            "Epoch 460/1000, Training Loss: 0.00012217522546421512, Test Loss: 0.0001115344229646879\n",
            "Epoch 461/1000, Training Loss: 0.00012065009716734065, Test Loss: 0.00011027054924014193\n",
            "Epoch 462/1000, Training Loss: 0.00011916208716666768, Test Loss: 0.0001090391625846615\n",
            "Epoch 463/1000, Training Loss: 0.00011771013043809476, Test Loss: 0.0001078393287726857\n",
            "Epoch 464/1000, Training Loss: 0.00011629319507605558, Test Loss: 0.00010667014148935031\n",
            "Epoch 465/1000, Training Loss: 0.00011491028121639176, Test Loss: 0.00010553072149917986\n",
            "Epoch 466/1000, Training Loss: 0.00011356041999463002, Test Loss: 0.00010442021583771775\n",
            "Epoch 467/1000, Training Loss: 0.00011224267253853413, Test Loss: 0.00010333779702557883\n",
            "Epoch 468/1000, Training Loss: 0.0001109561289938833, Test Loss: 0.00010228266230453573\n",
            "Epoch 469/1000, Training Loss: 0.00010969990758242779, Test Loss: 0.00010125403289514026\n",
            "Epoch 470/1000, Training Loss: 0.0001084731536909829, Test Loss: 0.00010025115327541157\n",
            "Epoch 471/1000, Training Loss: 0.00010727503899070562, Test Loss: 9.927329048014512e-05\n",
            "Epoch 472/1000, Training Loss: 0.00010610476058553409, Test Loss: 9.831973342034234e-05\n",
            "Epoch 473/1000, Training Loss: 0.00010496154018891483, Test Loss: 9.738979222232368e-05\n",
            "Epoch 474/1000, Training Loss: 0.0001038446233278322, Test Loss: 9.648279758603719e-05\n",
            "Epoch 475/1000, Training Loss: 0.00010275327857330095, Test Loss: 9.559810016208939e-05\n",
            "Epoch 476/1000, Training Loss: 0.00010168679679643186, Test Loss: 9.473506994705331e-05\n",
            "Epoch 477/1000, Training Loss: 0.00010064449044924797, Test Loss: 9.389309569658808e-05\n",
            "Epoch 478/1000, Training Loss: 9.962569286940766e-05, Test Loss: 9.307158435589572e-05\n",
            "Epoch 479/1000, Training Loss: 9.86297576080719e-05, Test Loss: 9.22699605071073e-05\n",
            "Epoch 480/1000, Training Loss: 9.765605778014292e-05, Test Loss: 9.148766583310857e-05\n",
            "Epoch 481/1000, Training Loss: 9.670398543611292e-05, Test Loss: 9.0724158597404e-05\n",
            "Epoch 482/1000, Training Loss: 9.577295095482055e-05, Test Loss: 8.997891313955702e-05\n",
            "Epoch 483/1000, Training Loss: 9.486238245640726e-05, Test Loss: 8.925141938581694e-05\n",
            "Epoch 484/1000, Training Loss: 9.397172523480393e-05, Test Loss: 8.854118237448329e-05\n",
            "Epoch 485/1000, Training Loss: 9.310044120908385e-05, Test Loss: 8.78477217956308e-05\n",
            "Epoch 486/1000, Training Loss: 9.224800839305122e-05, Test Loss: 8.717057154475184e-05\n",
            "Epoch 487/1000, Training Loss: 9.141392038244848e-05, Test Loss: 8.650927928997577e-05\n",
            "Epoch 488/1000, Training Loss: 9.05976858592067e-05, Test Loss: 8.58634060524356e-05\n",
            "Epoch 489/1000, Training Loss: 8.979882811212355e-05, Test Loss: 8.523252579943264e-05\n",
            "Epoch 490/1000, Training Loss: 8.901688457345706e-05, Test Loss: 8.461622505001744e-05\n",
            "Epoch 491/1000, Training Loss: 8.825140637087447e-05, Test Loss: 8.401410249263764e-05\n",
            "Epoch 492/1000, Training Loss: 8.750195789422905e-05, Test Loss: 8.342576861448053e-05\n",
            "Epoch 493/1000, Training Loss: 8.676811637668785e-05, Test Loss: 8.28508453422049e-05\n",
            "Epoch 494/1000, Training Loss: 8.604947148969489e-05, Test Loss: 8.228896569368407e-05\n",
            "Epoch 495/1000, Training Loss: 8.534562495132533e-05, Test Loss: 8.173977344046391e-05\n",
            "Epoch 496/1000, Training Loss: 8.465619014755235e-05, Test Loss: 8.120292278060617e-05\n",
            "Epoch 497/1000, Training Loss: 8.39807917660202e-05, Test Loss: 8.067807802160972e-05\n",
            "Epoch 498/1000, Training Loss: 8.331906544185431e-05, Test Loss: 8.016491327309587e-05\n",
            "Epoch 499/1000, Training Loss: 8.267065741515311e-05, Test Loss: 7.966311214900154e-05\n",
            "Epoch 500/1000, Training Loss: 8.203522419970756e-05, Test Loss: 7.917236747892092e-05\n",
            "Epoch 501/1000, Training Loss: 8.141243226262539e-05, Test Loss: 7.869238102841615e-05\n",
            "Epoch 502/1000, Training Loss: 8.080195771443657e-05, Test Loss: 7.822286322792657e-05\n",
            "Epoch 503/1000, Training Loss: 8.020348600936396e-05, Test Loss: 7.77635329100878e-05\n",
            "Epoch 504/1000, Training Loss: 7.96167116553977e-05, Test Loss: 7.73141170551634e-05\n",
            "Epoch 505/1000, Training Loss: 7.904133793382962e-05, Test Loss: 7.687435054435243e-05\n",
            "Epoch 506/1000, Training Loss: 7.847707662795962e-05, Test Loss: 7.644397592074447e-05\n",
            "Epoch 507/1000, Training Loss: 7.792364776062459e-05, Test Loss: 7.60227431576473e-05\n",
            "Epoch 508/1000, Training Loss: 7.738077934028128e-05, Test Loss: 7.561040943411419e-05\n",
            "Epoch 509/1000, Training Loss: 7.684820711533761e-05, Test Loss: 7.520673891740091e-05\n",
            "Epoch 510/1000, Training Loss: 7.632567433646069e-05, Test Loss: 7.481150255216901e-05\n",
            "Epoch 511/1000, Training Loss: 7.581293152658796e-05, Test Loss: 7.44244778562048e-05\n",
            "Epoch 512/1000, Training Loss: 7.530973625837797e-05, Test Loss: 7.404544872246416e-05\n",
            "Epoch 513/1000, Training Loss: 7.481585293886513e-05, Test Loss: 7.36742052272529e-05\n",
            "Epoch 514/1000, Training Loss: 7.433105260105699e-05, Test Loss: 7.331054344433038e-05\n",
            "Epoch 515/1000, Training Loss: 7.38551127022474e-05, Test Loss: 7.29542652647639e-05\n",
            "Epoch 516/1000, Training Loss: 7.338781692883246e-05, Test Loss: 7.260517822236894e-05\n",
            "Epoch 517/1000, Training Loss: 7.29289550073794e-05, Test Loss: 7.226309532453055e-05\n",
            "Epoch 518/1000, Training Loss: 7.247832252177453e-05, Test Loss: 7.192783488825633e-05\n",
            "Epoch 519/1000, Training Loss: 7.203572073622202e-05, Test Loss: 7.159922038130396e-05\n",
            "Epoch 520/1000, Training Loss: 7.16009564239093e-05, Test Loss: 7.127708026820831e-05\n",
            "Epoch 521/1000, Training Loss: 7.117384170114021e-05, Test Loss: 7.096124786105656e-05\n",
            "Epoch 522/1000, Training Loss: 7.075419386675976e-05, Test Loss: 7.06515611748774e-05\n",
            "Epoch 523/1000, Training Loss: 7.034183524669431e-05, Test Loss: 7.034786278748086e-05\n",
            "Epoch 524/1000, Training Loss: 6.993659304343529e-05, Test Loss: 7.004999970362887e-05\n",
            "Epoch 525/1000, Training Loss: 6.953829919029664e-05, Test Loss: 6.975782322337834e-05\n",
            "Epoch 526/1000, Training Loss: 6.91467902102954e-05, Test Loss: 6.94711888144828e-05\n",
            "Epoch 527/1000, Training Loss: 6.876190707949691e-05, Test Loss: 6.918995598872067e-05\n",
            "Epoch 528/1000, Training Loss: 6.838349509467764e-05, Test Loss: 6.891398818202763e-05\n",
            "Epoch 529/1000, Training Loss: 6.801140374516401e-05, Test Loss: 6.864315263830438e-05\n",
            "Epoch 530/1000, Training Loss: 6.764548658870194e-05, Test Loss: 6.837732029680206e-05\n",
            "Epoch 531/1000, Training Loss: 6.728560113124022e-05, Test Loss: 6.811636568295576e-05\n",
            "Epoch 532/1000, Training Loss: 6.693160871047608e-05, Test Loss: 6.786016680257284e-05\n",
            "Epoch 533/1000, Training Loss: 6.658337438306281e-05, Test Loss: 6.760860503925526e-05\n",
            "Epoch 534/1000, Training Loss: 6.624076681533438e-05, Test Loss: 6.736156505496854e-05\n",
            "Epoch 535/1000, Training Loss: 6.59036581774561e-05, Test Loss: 6.711893469364977e-05\n",
            "Epoch 536/1000, Training Loss: 6.557192404086622e-05, Test Loss: 6.688060488776134e-05\n",
            "Epoch 537/1000, Training Loss: 6.52454432789226e-05, Test Loss: 6.664646956770775e-05\n",
            "Epoch 538/1000, Training Loss: 6.49240979706271e-05, Test Loss: 6.641642557400066e-05\n",
            "Epoch 539/1000, Training Loss: 6.460777330734944e-05, Test Loss: 6.61903725721228e-05\n",
            "Epoch 540/1000, Training Loss: 6.42963575024296e-05, Test Loss: 6.596821296996434e-05\n",
            "Epoch 541/1000, Training Loss: 6.398974170358818e-05, Test Loss: 6.574985183779267e-05\n",
            "Epoch 542/1000, Training Loss: 6.368781990803626e-05, Test Loss: 6.553519683064408e-05\n",
            "Epoch 543/1000, Training Loss: 6.33904888802033e-05, Test Loss: 6.53241581130773e-05\n",
            "Epoch 544/1000, Training Loss: 6.309764807200173e-05, Test Loss: 6.511664828621241e-05\n",
            "Epoch 545/1000, Training Loss: 6.280919954553828e-05, Test Loss: 6.491258231698413e-05\n",
            "Epoch 546/1000, Training Loss: 6.252504789819939e-05, Test Loss: 6.471187746953326e-05\n",
            "Epoch 547/1000, Training Loss: 6.224510019003011e-05, Test Loss: 6.451445323868852e-05\n",
            "Epoch 548/1000, Training Loss: 6.196926587333721e-05, Test Loss: 6.432023128544519e-05\n",
            "Epoch 549/1000, Training Loss: 6.16974567244393e-05, Test Loss: 6.412913537440409e-05\n",
            "Epoch 550/1000, Training Loss: 6.142958677749656e-05, Test Loss: 6.394109131309772e-05\n",
            "Epoch 551/1000, Training Loss: 6.116557226035927e-05, Test Loss: 6.375602689314432e-05\n",
            "Epoch 552/1000, Training Loss: 6.0905331532360423e-05, Test Loss: 6.35738718331773e-05\n",
            "Epoch 553/1000, Training Loss: 6.064878502399882e-05, Test Loss: 6.339455772349397e-05\n",
            "Epoch 554/1000, Training Loss: 6.0395855178448875e-05, Test Loss: 6.32180179723643e-05\n",
            "Epoch 555/1000, Training Loss: 6.014646639483587e-05, Test Loss: 6.304418775396273e-05\n",
            "Epoch 556/1000, Training Loss: 5.9900544973226484e-05, Test Loss: 6.287300395785064e-05\n",
            "Epoch 557/1000, Training Loss: 5.9658019061278066e-05, Test Loss: 6.270440513998378e-05\n",
            "Epoch 558/1000, Training Loss: 5.94188186024886e-05, Test Loss: 6.25383314751834e-05\n",
            "Epoch 559/1000, Training Loss: 5.918287528600603e-05, Test Loss: 6.237472471102715e-05\n",
            "Epoch 560/1000, Training Loss: 5.895012249794046e-05, Test Loss: 6.221352812313008e-05\n",
            "Epoch 561/1000, Training Loss: 5.872049527413383e-05, Test Loss: 6.205468647174422e-05\n",
            "Epoch 562/1000, Training Loss: 5.84939302543422e-05, Test Loss: 6.189814595966542e-05\n",
            "Epoch 563/1000, Training Loss: 5.827036563778326e-05, Test Loss: 6.174385419139245e-05\n",
            "Epoch 564/1000, Training Loss: 5.804974114001171e-05, Test Loss: 6.159176013350032e-05\n",
            "Epoch 565/1000, Training Loss: 5.783199795107737e-05, Test Loss: 6.14418140761943e-05\n",
            "Epoch 566/1000, Training Loss: 5.7617078694916984e-05, Test Loss: 6.129396759600156e-05\n",
            "Epoch 567/1000, Training Loss: 5.740492738996187e-05, Test Loss: 6.114817351957938e-05\n",
            "Epoch 568/1000, Training Loss: 5.719548941090549e-05, Test Loss: 6.1004385888584596e-05\n",
            "Epoch 569/1000, Training Loss: 5.6988711451598884e-05, Test Loss: 6.0862559925595724e-05\n",
            "Epoch 570/1000, Training Loss: 5.678454148904581e-05, Test Loss: 6.0722652001034115e-05\n",
            "Epoch 571/1000, Training Loss: 5.6582928748453064e-05, Test Loss: 6.058461960106695e-05\n",
            "Epoch 572/1000, Training Loss: 5.638382366930948e-05, Test Loss: 6.0448421296455554e-05\n",
            "Epoch 573/1000, Training Loss: 5.618717787246513e-05, Test Loss: 6.031401671232727e-05\n",
            "Epoch 574/1000, Training Loss: 5.599294412816733e-05, Test Loss: 6.018136649883224e-05\n",
            "Epoch 575/1000, Training Loss: 5.580107632503905e-05, Test Loss: 6.005043230266466e-05\n",
            "Epoch 576/1000, Training Loss: 5.5611529439959145e-05, Test Loss: 5.992117673942446e-05\n",
            "Epoch 577/1000, Training Loss: 5.542425950882161e-05, Test Loss: 5.979356336678871e-05\n",
            "Epoch 578/1000, Training Loss: 5.523922359814536e-05, Test Loss: 5.9667556658465345e-05\n",
            "Epoch 579/1000, Training Loss: 5.505637977751062e-05, Test Loss: 5.954312197891854e-05\n",
            "Epoch 580/1000, Training Loss: 5.48756870927937e-05, Test Loss: 5.942022555882788e-05\n",
            "Epoch 581/1000, Training Loss: 5.469710554017565e-05, Test Loss: 5.9298834471265607e-05\n",
            "Epoch 582/1000, Training Loss: 5.4520596040906e-05, Test Loss: 5.9178916608567166e-05\n",
            "Epoch 583/1000, Training Loss: 5.434612041679177e-05, Test Loss: 5.9060440659884925e-05\n",
            "Epoch 584/1000, Training Loss: 5.417364136639344e-05, Test Loss: 5.894337608938456e-05\n",
            "Epoch 585/1000, Training Loss: 5.400312244190357e-05, Test Loss: 5.882769311508025e-05\n",
            "Epoch 586/1000, Training Loss: 5.38345280266962e-05, Test Loss: 5.871336268829189e-05\n",
            "Epoch 587/1000, Training Loss: 5.366782331351125e-05, Test Loss: 5.860035647368449e-05\n",
            "Epoch 588/1000, Training Loss: 5.3502974283268656e-05, Test Loss: 5.848864682989866e-05\n",
            "Epoch 589/1000, Training Loss: 5.3339947684486304e-05, Test Loss: 5.837820679073638e-05\n",
            "Epoch 590/1000, Training Loss: 5.317871101328444e-05, Test Loss: 5.8269010046890587e-05\n",
            "Epoch 591/1000, Training Loss: 5.301923249395966e-05, Test Loss: 5.816103092820428e-05\n",
            "Epoch 592/1000, Training Loss: 5.286148106011053e-05, Test Loss: 5.8054244386439434e-05\n",
            "Epoch 593/1000, Training Loss: 5.270542633630093e-05, Test Loss: 5.794862597854878e-05\n",
            "Epoch 594/1000, Training Loss: 5.2551038620238726e-05, Test Loss: 5.784415185041811e-05\n",
            "Epoch 595/1000, Training Loss: 5.2398288865461894e-05, Test Loss: 5.774079872108458e-05\n",
            "Epoch 596/1000, Training Loss: 5.224714866451105e-05, Test Loss: 5.7638543867404196e-05\n",
            "Epoch 597/1000, Training Loss: 5.2097590232577975e-05, Test Loss: 5.753736510916242e-05\n",
            "Epoch 598/1000, Training Loss: 5.194958639160972e-05, Test Loss: 5.743724079460637e-05\n",
            "Epoch 599/1000, Training Loss: 5.180311055486661e-05, Test Loss: 5.73381497863943e-05\n",
            "Epoch 600/1000, Training Loss: 5.165813671190387e-05, Test Loss: 5.724007144794374e-05\n",
            "Epoch 601/1000, Training Loss: 5.151463941398048e-05, Test Loss: 5.71429856301734e-05\n",
            "Epoch 602/1000, Training Loss: 5.137259375987139e-05, Test Loss: 5.704687265861584e-05\n",
            "Epoch 603/1000, Training Loss: 5.123197538207734e-05, Test Loss: 5.6951713320905196e-05\n",
            "Epoch 604/1000, Training Loss: 5.1092760433414566e-05, Test Loss: 5.6857488854611706e-05\n",
            "Epoch 605/1000, Training Loss: 5.095492557398397e-05, Test Loss: 5.676418093542982e-05\n",
            "Epoch 606/1000, Training Loss: 5.081844795849265e-05, Test Loss: 5.667177166569208e-05\n",
            "Epoch 607/1000, Training Loss: 5.068330522393382e-05, Test Loss: 5.6580243563217504e-05\n",
            "Epoch 608/1000, Training Loss: 5.054947547760209e-05, Test Loss: 5.6489579550468956e-05\n",
            "Epoch 609/1000, Training Loss: 5.041693728544053e-05, Test Loss: 5.639976294401371e-05\n",
            "Epoch 610/1000, Training Loss: 5.028566966070828e-05, Test Loss: 5.6310777444290946e-05\n",
            "Epoch 611/1000, Training Loss: 5.015565205296052e-05, Test Loss: 5.622260712565827e-05\n",
            "Epoch 612/1000, Training Loss: 5.0026864337327275e-05, Test Loss: 5.613523642671846e-05\n",
            "Epoch 613/1000, Training Loss: 4.9899286804089204e-05, Test Loss: 5.604865014092843e-05\n",
            "Epoch 614/1000, Training Loss: 4.977290014853526e-05, Test Loss: 5.5962833407453005e-05\n",
            "Epoch 615/1000, Training Loss: 4.9647685461097375e-05, Test Loss: 5.5877771702289805e-05\n",
            "Epoch 616/1000, Training Loss: 4.952362421775398e-05, Test Loss: 5.579345082963629e-05\n",
            "Epoch 617/1000, Training Loss: 4.9400698270692584e-05, Test Loss: 5.5709856913496404e-05\n",
            "Epoch 618/1000, Training Loss: 4.92788898392279e-05, Test Loss: 5.56269763895229e-05\n",
            "Epoch 619/1000, Training Loss: 4.915818150096307e-05, Test Loss: 5.5544795997087426e-05\n",
            "Epoch 620/1000, Training Loss: 4.903855618318834e-05, Test Loss: 5.5463302771566886e-05\n",
            "Epoch 621/1000, Training Loss: 4.891999715451503e-05, Test Loss: 5.538248403684954e-05\n",
            "Epoch 622/1000, Training Loss: 4.8802488016730974e-05, Test Loss: 5.530232739804183e-05\n",
            "Epoch 623/1000, Training Loss: 4.868601269687628e-05, Test Loss: 5.522282073438123e-05\n",
            "Epoch 624/1000, Training Loss: 4.857055543952923e-05, Test Loss: 5.514395219234382e-05\n",
            "Epoch 625/1000, Training Loss: 4.845610079929994e-05, Test Loss: 5.506571017894021e-05\n",
            "Epoch 626/1000, Training Loss: 4.834263363352421e-05, Test Loss: 5.49880833551981e-05\n",
            "Epoch 627/1000, Training Loss: 4.82301390951481e-05, Test Loss: 5.4911060629821174e-05\n",
            "Epoch 628/1000, Training Loss: 4.8118602625806585e-05, Test Loss: 5.4834631153023424e-05\n",
            "Epoch 629/1000, Training Loss: 4.8008009949081135e-05, Test Loss: 5.475878431053289e-05\n",
            "Epoch 630/1000, Training Loss: 4.789834706393736e-05, Test Loss: 5.468350971775443e-05\n",
            "Epoch 631/1000, Training Loss: 4.778960023833653e-05, Test Loss: 5.460879721410108e-05\n",
            "Epoch 632/1000, Training Loss: 4.76817560030115e-05, Test Loss: 5.453463685746847e-05\n",
            "Epoch 633/1000, Training Loss: 4.757480114541033e-05, Test Loss: 5.4461018918867414e-05\n",
            "Epoch 634/1000, Training Loss: 4.746872270379678e-05, Test Loss: 5.4387933877200105e-05\n",
            "Epoch 635/1000, Training Loss: 4.736350796150303e-05, Test Loss: 5.43153724141726e-05\n",
            "Epoch 636/1000, Training Loss: 4.725914444133444e-05, Test Loss: 5.4243325409357706e-05\n",
            "Epoch 637/1000, Training Loss: 4.7155619900120986e-05, Test Loss: 5.41717839353724e-05\n",
            "Epoch 638/1000, Training Loss: 4.705292232340658e-05, Test Loss: 5.4100739253203764e-05\n",
            "Epoch 639/1000, Training Loss: 4.695103992027828e-05, Test Loss: 5.4030182807646526e-05\n",
            "Epoch 640/1000, Training Loss: 4.684996111832868e-05, Test Loss: 5.396010622287049e-05\n",
            "Epoch 641/1000, Training Loss: 4.674967455874879e-05, Test Loss: 5.389050129810578e-05\n",
            "Epoch 642/1000, Training Loss: 4.6650169091543124e-05, Test Loss: 5.3821360003439885e-05\n",
            "Epoch 643/1000, Training Loss: 4.6551433770874866e-05, Test Loss: 5.3752674475730614e-05\n",
            "Epoch 644/1000, Training Loss: 4.645345785052368e-05, Test Loss: 5.368443701462469e-05\n",
            "Epoch 645/1000, Training Loss: 4.6356230779461975e-05, Test Loss: 5.36166400786866e-05\n",
            "Epoch 646/1000, Training Loss: 4.6259742197543115e-05, Test Loss: 5.354927628162546e-05\n",
            "Epoch 647/1000, Training Loss: 4.61639819313008e-05, Test Loss: 5.3482338388621215e-05\n",
            "Epoch 648/1000, Training Loss: 4.606893998985258e-05, Test Loss: 5.341581931275385e-05\n",
            "Epoch 649/1000, Training Loss: 4.597460656090965e-05, Test Loss: 5.334971211152166e-05\n",
            "Epoch 650/1000, Training Loss: 4.588097200688296e-05, Test Loss: 5.3284009983450624e-05\n",
            "Epoch 651/1000, Training Loss: 4.5788026861092075e-05, Test Loss: 5.321870626479625e-05\n",
            "Epoch 652/1000, Training Loss: 4.569576182406729e-05, Test Loss: 5.315379442632936e-05\n",
            "Epoch 653/1000, Training Loss: 4.5604167759942527e-05, Test Loss: 5.3089268070204726e-05\n",
            "Epoch 654/1000, Training Loss: 4.5513235692941044e-05, Test Loss: 5.302512092691724e-05\n",
            "Epoch 655/1000, Training Loss: 4.5422956803946965e-05, Test Loss: 5.296134685232717e-05\n",
            "Epoch 656/1000, Training Loss: 4.533332242716145e-05, Test Loss: 5.289793982477388e-05\n",
            "Epoch 657/1000, Training Loss: 4.524432404684479e-05, Test Loss: 5.2834893942254994e-05\n",
            "Epoch 658/1000, Training Loss: 4.515595329413321e-05, Test Loss: 5.277220341968341e-05\n",
            "Epoch 659/1000, Training Loss: 4.5068201943941986e-05, Test Loss: 5.270986258621731e-05\n",
            "Epoch 660/1000, Training Loss: 4.498106191193646e-05, Test Loss: 5.26478658826469e-05\n",
            "Epoch 661/1000, Training Loss: 4.4894525251583006e-05, Test Loss: 5.2586207858867035e-05\n",
            "Epoch 662/1000, Training Loss: 4.480858415127134e-05, Test Loss: 5.252488317139828e-05\n",
            "Epoch 663/1000, Training Loss: 4.472323093150283e-05, Test Loss: 5.246388658097612e-05\n",
            "Epoch 664/1000, Training Loss: 4.463845804215229e-05, Test Loss: 5.240321295020823e-05\n",
            "Epoch 665/1000, Training Loss: 4.45542580597928e-05, Test Loss: 5.234285724127991e-05\n",
            "Epoch 666/1000, Training Loss: 4.447062368508766e-05, Test Loss: 5.228281451372746e-05\n",
            "Epoch 667/1000, Training Loss: 4.438754774024297e-05, Test Loss: 5.222307992226315e-05\n",
            "Epoch 668/1000, Training Loss: 4.430502316652145e-05, Test Loss: 5.216364871465514e-05\n",
            "Epoch 669/1000, Training Loss: 4.422304302181902e-05, Test Loss: 5.2104516229662046e-05\n",
            "Epoch 670/1000, Training Loss: 4.414160047829514e-05, Test Loss: 5.2045677895017905e-05\n",
            "Epoch 671/1000, Training Loss: 4.4060688820061626e-05, Test Loss: 5.1987129225470835e-05\n",
            "Epoch 672/1000, Training Loss: 4.39803014409276e-05, Test Loss: 5.1928865820866115e-05\n",
            "Epoch 673/1000, Training Loss: 4.39004318421948e-05, Test Loss: 5.187088336428164e-05\n",
            "Epoch 674/1000, Training Loss: 4.382107363050814e-05, Test Loss: 5.181317762020423e-05\n",
            "Epoch 675/1000, Training Loss: 4.3742220515755255e-05, Test Loss: 5.175574443275756e-05\n",
            "Epoch 676/1000, Training Loss: 4.366386630901352e-05, Test Loss: 5.1698579723969844e-05\n",
            "Epoch 677/1000, Training Loss: 4.358600492054957e-05, Test Loss: 5.16416794920841e-05\n",
            "Epoch 678/1000, Training Loss: 4.350863035786213e-05, Test Loss: 5.158503980991075e-05\n",
            "Epoch 679/1000, Training Loss: 4.343173672377205e-05, Test Loss: 5.152865682322277e-05\n",
            "Epoch 680/1000, Training Loss: 4.3355318214554764e-05, Test Loss: 5.147252674918796e-05\n",
            "Epoch 681/1000, Training Loss: 4.327936911811777e-05, Test Loss: 5.1416645874836026e-05\n",
            "Epoch 682/1000, Training Loss: 4.320388381222026e-05, Test Loss: 5.136101055557242e-05\n",
            "Epoch 683/1000, Training Loss: 4.3128856762731884e-05, Test Loss: 5.130561721371808e-05\n",
            "Epoch 684/1000, Training Loss: 4.3054282521933424e-05, Test Loss: 5.125046233709087e-05\n",
            "Epoch 685/1000, Training Loss: 4.298015572685452e-05, Test Loss: 5.119554247761981e-05\n",
            "Epoch 686/1000, Training Loss: 4.2906471097650296e-05, Test Loss: 5.114085424998903e-05\n",
            "Epoch 687/1000, Training Loss: 4.283322343601601e-05, Test Loss: 5.108639433032183e-05\n",
            "Epoch 688/1000, Training Loss: 4.276040762363445e-05, Test Loss: 5.103215945488803e-05\n",
            "Epoch 689/1000, Training Loss: 4.2688018620661106e-05, Test Loss: 5.09781464188482e-05\n",
            "Epoch 690/1000, Training Loss: 4.261605146424237e-05, Test Loss: 5.092435207502498e-05\n",
            "Epoch 691/1000, Training Loss: 4.254450126706752e-05, Test Loss: 5.0870773332703544e-05\n",
            "Epoch 692/1000, Training Loss: 4.24733632159513e-05, Test Loss: 5.081740715646144e-05\n",
            "Epoch 693/1000, Training Loss: 4.2402632570451076e-05, Test Loss: 5.0764250565026034e-05\n",
            "Epoch 694/1000, Training Loss: 4.233230466151207e-05, Test Loss: 5.071130063015756e-05\n",
            "Epoch 695/1000, Training Loss: 4.226237489014407e-05, Test Loss: 5.0658554475559923e-05\n",
            "Epoch 696/1000, Training Loss: 4.219283872612605e-05, Test Loss: 5.0606009275816656e-05\n",
            "Epoch 697/1000, Training Loss: 4.212369170674174e-05, Test Loss: 5.0553662255350225e-05\n",
            "Epoch 698/1000, Training Loss: 4.2054929435539316e-05, Test Loss: 5.050151068740674e-05\n",
            "Epoch 699/1000, Training Loss: 4.198654758112098e-05, Test Loss: 5.044955189306606e-05\n",
            "Epoch 700/1000, Training Loss: 4.1918541875958405e-05, Test Loss: 5.0397783240269976e-05\n",
            "Epoch 701/1000, Training Loss: 4.185090811523373e-05, Test Loss: 5.034620214287911e-05\n",
            "Epoch 702/1000, Training Loss: 4.1783642155703696e-05, Test Loss: 5.029480605974408e-05\n",
            "Epoch 703/1000, Training Loss: 4.171673991459253e-05, Test Loss: 5.024359249380529e-05\n",
            "Epoch 704/1000, Training Loss: 4.1650197368504125e-05, Test Loss: 5.01925589912089e-05\n",
            "Epoch 705/1000, Training Loss: 4.158401055236051e-05, Test Loss: 5.0141703140442145e-05\n",
            "Epoch 706/1000, Training Loss: 4.151817555836226e-05, Test Loss: 5.009102257149332e-05\n",
            "Epoch 707/1000, Training Loss: 4.145268853497013e-05, Test Loss: 5.004051495502634e-05\n",
            "Epoch 708/1000, Training Loss: 4.1387545685909364e-05, Test Loss: 4.9990178001574385e-05\n",
            "Epoch 709/1000, Training Loss: 4.132274326919441e-05, Test Loss: 4.994000946075258e-05\n",
            "Epoch 710/1000, Training Loss: 4.125827759617456e-05, Test Loss: 4.9890007120489845e-05\n",
            "Epoch 711/1000, Training Loss: 4.119414503059883e-05, Test Loss: 4.984016880627451e-05\n",
            "Epoch 712/1000, Training Loss: 4.1130341987702176e-05, Test Loss: 4.9790492380420205e-05\n",
            "Epoch 713/1000, Training Loss: 4.106686493330998e-05, Test Loss: 4.9740975741345084e-05\n",
            "Epoch 714/1000, Training Loss: 4.100371038295848e-05, Test Loss: 4.9691616822869903e-05\n",
            "Epoch 715/1000, Training Loss: 4.094087490103817e-05, Test Loss: 4.9642413593528265e-05\n",
            "Epoch 716/1000, Training Loss: 4.087835509995162e-05, Test Loss: 4.959336405589732e-05\n",
            "Epoch 717/1000, Training Loss: 4.081614763928945e-05, Test Loss: 4.954446624593506e-05\n",
            "Epoch 718/1000, Training Loss: 4.0754249225023516e-05, Test Loss: 4.949571823234108e-05\n",
            "Epoch 719/1000, Training Loss: 4.0692656608716e-05, Test Loss: 4.944711811592544e-05\n",
            "Epoch 720/1000, Training Loss: 4.0631366586744993e-05, Test Loss: 4.939866402899288e-05\n",
            "Epoch 721/1000, Training Loss: 4.057037599954631e-05, Test Loss: 4.935035413474123e-05\n",
            "Epoch 722/1000, Training Loss: 4.050968173086922e-05, Test Loss: 4.930218662667116e-05\n",
            "Epoch 723/1000, Training Loss: 4.0449280707048756e-05, Test Loss: 4.925415972800937e-05\n",
            "Epoch 724/1000, Training Loss: 4.038916989629208e-05, Test Loss: 4.9206271691147515e-05\n",
            "Epoch 725/1000, Training Loss: 4.0329346307978496e-05, Test Loss: 4.91585207970847e-05\n",
            "Epoch 726/1000, Training Loss: 4.026980699197319e-05, Test Loss: 4.911090535489295e-05\n",
            "Epoch 727/1000, Training Loss: 4.021054903795755e-05, Test Loss: 4.9063423701183935e-05\n",
            "Epoch 728/1000, Training Loss: 4.015156957476918e-05, Test Loss: 4.901607419959466e-05\n",
            "Epoch 729/1000, Training Loss: 4.0092865769755985e-05, Test Loss: 4.896885524027915e-05\n",
            "Epoch 730/1000, Training Loss: 4.003443482814432e-05, Test Loss: 4.8921765239413206e-05\n",
            "Epoch 731/1000, Training Loss: 3.997627399241838e-05, Test Loss: 4.887480263870958e-05\n",
            "Epoch 732/1000, Training Loss: 3.99183805417114e-05, Test Loss: 4.88279659049422e-05\n",
            "Epoch 733/1000, Training Loss: 3.986075179121007e-05, Test Loss: 4.878125352947974e-05\n",
            "Epoch 734/1000, Training Loss: 3.9803385091569464e-05, Test Loss: 4.8734664027833985e-05\n",
            "Epoch 735/1000, Training Loss: 3.974627782833909e-05, Test Loss: 4.8688195939210615e-05\n",
            "Epoch 736/1000, Training Loss: 3.96894274214007e-05, Test Loss: 4.864184782607207e-05\n",
            "Epoch 737/1000, Training Loss: 3.963283132441739e-05, Test Loss: 4.859561827371297e-05\n",
            "Epoch 738/1000, Training Loss: 3.957648702429181e-05, Test Loss: 4.854950588983762e-05\n",
            "Epoch 739/1000, Training Loss: 3.9520392040636175e-05, Test Loss: 4.850350930415306e-05\n",
            "Epoch 740/1000, Training Loss: 3.9464543925250035e-05, Test Loss: 4.845762716796511e-05\n",
            "Epoch 741/1000, Training Loss: 3.940894026161164e-05, Test Loss: 4.841185815378631e-05\n",
            "Epoch 742/1000, Training Loss: 3.9353578664376433e-05, Test Loss: 4.8366200954948676e-05\n",
            "Epoch 743/1000, Training Loss: 3.929845677888436e-05, Test Loss: 4.8320654285228554e-05\n",
            "Epoch 744/1000, Training Loss: 3.924357228067918e-05, Test Loss: 4.827521687847357e-05\n",
            "Epoch 745/1000, Training Loss: 3.9188922875033716e-05, Test Loss: 4.8229887488243e-05\n",
            "Epoch 746/1000, Training Loss: 3.913450629648604e-05, Test Loss: 4.818466488745008e-05\n",
            "Epoch 747/1000, Training Loss: 3.90803203083841e-05, Test Loss: 4.813954786801772e-05\n",
            "Epoch 748/1000, Training Loss: 3.902636270243724e-05, Test Loss: 4.809453524053136e-05\n",
            "Epoch 749/1000, Training Loss: 3.897263129827846e-05, Test Loss: 4.804962583391205e-05\n",
            "Epoch 750/1000, Training Loss: 3.8919123943032505e-05, Test Loss: 4.800481849508315e-05\n",
            "Epoch 751/1000, Training Loss: 3.8865838510892923e-05, Test Loss: 4.796011208865254e-05\n",
            "Epoch 752/1000, Training Loss: 3.8812772902706856e-05, Test Loss: 4.791550549659637e-05\n",
            "Epoch 753/1000, Training Loss: 3.875992504556745e-05, Test Loss: 4.787099761795106e-05\n",
            "Epoch 754/1000, Training Loss: 3.870729289241395e-05, Test Loss: 4.782658736851355e-05\n",
            "Epoch 755/1000, Training Loss: 3.865487442163724e-05, Test Loss: 4.7782273680540115e-05\n",
            "Epoch 756/1000, Training Loss: 3.860266763669566e-05, Test Loss: 4.773805550246058e-05\n",
            "Epoch 757/1000, Training Loss: 3.8550670565734594e-05, Test Loss: 4.769393179859027e-05\n",
            "Epoch 758/1000, Training Loss: 3.8498881261215284e-05, Test Loss: 4.764990154885351e-05\n",
            "Epoch 759/1000, Training Loss: 3.844729779954866e-05, Test Loss: 4.760596374850723e-05\n",
            "Epoch 760/1000, Training Loss: 3.839591828073668e-05, Test Loss: 4.7562117407874916e-05\n",
            "Epoch 761/1000, Training Loss: 3.834474082801967e-05, Test Loss: 4.751836155208121e-05\n",
            "Epoch 762/1000, Training Loss: 3.829376358753055e-05, Test Loss: 4.7474695220795605e-05\n",
            "Epoch 763/1000, Training Loss: 3.824298472795358e-05, Test Loss: 4.7431117467979266e-05\n",
            "Epoch 764/1000, Training Loss: 3.8192402440190754e-05, Test Loss: 4.73876273616356e-05\n",
            "Epoch 765/1000, Training Loss: 3.81420149370337e-05, Test Loss: 4.734422398356877e-05\n",
            "Epoch 766/1000, Training Loss: 3.809182045284084e-05, Test Loss: 4.730090642914085e-05\n",
            "Epoch 767/1000, Training Loss: 3.804181724322043e-05, Test Loss: 4.725767380704465e-05\n",
            "Epoch 768/1000, Training Loss: 3.799200358471929e-05, Test Loss: 4.721452523906583e-05\n",
            "Epoch 769/1000, Training Loss: 3.794237777451681e-05, Test Loss: 4.717145985986466e-05\n",
            "Epoch 770/1000, Training Loss: 3.789293813012402e-05, Test Loss: 4.712847681675038e-05\n",
            "Epoch 771/1000, Training Loss: 3.784368298908918e-05, Test Loss: 4.708557526946755e-05\n",
            "Epoch 772/1000, Training Loss: 3.77946107087059e-05, Test Loss: 4.70427543899805e-05\n",
            "Epoch 773/1000, Training Loss: 3.774571966572836e-05, Test Loss: 4.700001336226539e-05\n",
            "Epoch 774/1000, Training Loss: 3.769700825609114e-05, Test Loss: 4.695735138210813e-05\n",
            "Epoch 775/1000, Training Loss: 3.764847489463352e-05, Test Loss: 4.691476765690075e-05\n",
            "Epoch 776/1000, Training Loss: 3.760011801482772e-05, Test Loss: 4.687226140544397e-05\n",
            "Epoch 777/1000, Training Loss: 3.75519360685125e-05, Test Loss: 4.682983185775611e-05\n",
            "Epoch 778/1000, Training Loss: 3.750392752563261e-05, Test Loss: 4.678747825487991e-05\n",
            "Epoch 779/1000, Training Loss: 3.745609087398015e-05, Test Loss: 4.674519984870108e-05\n",
            "Epoch 780/1000, Training Loss: 3.7408424618941115e-05, Test Loss: 4.670299590176193e-05\n",
            "Epoch 781/1000, Training Loss: 3.736092728324744e-05, Test Loss: 4.666086568708175e-05\n",
            "Epoch 782/1000, Training Loss: 3.731359740673201e-05, Test Loss: 4.661880848798353e-05\n",
            "Epoch 783/1000, Training Loss: 3.726643354608734e-05, Test Loss: 4.657682359791953e-05\n",
            "Epoch 784/1000, Training Loss: 3.721943427462998e-05, Test Loss: 4.6534910320300896e-05\n",
            "Epoch 785/1000, Training Loss: 3.7172598182066986e-05, Test Loss: 4.649306796833297e-05\n",
            "Epoch 786/1000, Training Loss: 3.712592387426806e-05, Test Loss: 4.645129586485122e-05\n",
            "Epoch 787/1000, Training Loss: 3.707940997303891e-05, Test Loss: 4.640959334216083e-05\n",
            "Epoch 788/1000, Training Loss: 3.703305511590152e-05, Test Loss: 4.636795974187975e-05\n",
            "Epoch 789/1000, Training Loss: 3.6986857955875596e-05, Test Loss: 4.6326394414782976e-05\n",
            "Epoch 790/1000, Training Loss: 3.694081716126484e-05, Test Loss: 4.62848967206528e-05\n",
            "Epoch 791/1000, Training Loss: 3.689493141544486e-05, Test Loss: 4.624346602812913e-05\n",
            "Epoch 792/1000, Training Loss: 3.6849199416658654e-05, Test Loss: 4.6202101714562245e-05\n",
            "Epoch 793/1000, Training Loss: 3.6803619877810635e-05, Test Loss: 4.61608031658702e-05\n",
            "Epoch 794/1000, Training Loss: 3.675819152626674e-05, Test Loss: 4.6119569776399066e-05\n",
            "Epoch 795/1000, Training Loss: 3.6712913103657305e-05, Test Loss: 4.607840094878284e-05\n",
            "Epoch 796/1000, Training Loss: 3.666778336568285e-05, Test Loss: 4.603729609380831e-05\n",
            "Epoch 797/1000, Training Loss: 3.662280108192267e-05, Test Loss: 4.599625463028242e-05\n",
            "Epoch 798/1000, Training Loss: 3.65779650356478e-05, Test Loss: 4.5955275984899643e-05\n",
            "Epoch 799/1000, Training Loss: 3.653327402363555e-05, Test Loss: 4.5914359592115966e-05\n",
            "Epoch 800/1000, Training Loss: 3.648872685598762e-05, Test Loss: 4.587350489402025e-05\n",
            "Epoch 801/1000, Training Loss: 3.644432235595131e-05, Test Loss: 4.5832711340212067e-05\n",
            "Epoch 802/1000, Training Loss: 3.640005935974355e-05, Test Loss: 4.579197838767917e-05\n",
            "Epoch 803/1000, Training Loss: 3.635593671637675e-05, Test Loss: 4.575130550067728e-05\n",
            "Epoch 804/1000, Training Loss: 3.631195328748815e-05, Test Loss: 4.571069215061529e-05\n",
            "Epoch 805/1000, Training Loss: 3.626810794717328e-05, Test Loss: 4.567013781593679e-05\n",
            "Epoch 806/1000, Training Loss: 3.622439958181887e-05, Test Loss: 4.5629641982007776e-05\n",
            "Epoch 807/1000, Training Loss: 3.6180827089941216e-05, Test Loss: 4.5589204141009205e-05\n",
            "Epoch 808/1000, Training Loss: 3.613738938202655e-05, Test Loss: 4.554882379182322e-05\n",
            "Epoch 809/1000, Training Loss: 3.6094085380371566e-05, Test Loss: 4.5508500439926104e-05\n",
            "Epoch 810/1000, Training Loss: 3.605091401893035e-05, Test Loss: 4.546823359728741e-05\n",
            "Epoch 811/1000, Training Loss: 3.6007874243159835e-05, Test Loss: 4.542802278226112e-05\n",
            "Epoch 812/1000, Training Loss: 3.596496500987044e-05, Test Loss: 4.5387867519488474e-05\n",
            "Epoch 813/1000, Training Loss: 3.592218528707846e-05, Test Loss: 4.534776733979509e-05\n",
            "Epoch 814/1000, Training Loss: 3.587953405385865e-05, Test Loss: 4.530772178009514e-05\n",
            "Epoch 815/1000, Training Loss: 3.583701030020268e-05, Test Loss: 4.526773038329357e-05\n",
            "Epoch 816/1000, Training Loss: 3.5794613026876396e-05, Test Loss: 4.522779269819474e-05\n",
            "Epoch 817/1000, Training Loss: 3.5752341245281245e-05, Test Loss: 4.518790827940434e-05\n",
            "Epoch 818/1000, Training Loss: 3.571019397731796e-05, Test Loss: 4.5148076687242035e-05\n",
            "Epoch 819/1000, Training Loss: 3.566817025524997e-05, Test Loss: 4.510829748765255e-05\n",
            "Epoch 820/1000, Training Loss: 3.562626912157262e-05, Test Loss: 4.506857025211476e-05\n",
            "Epoch 821/1000, Training Loss: 3.5584489628880683e-05, Test Loss: 4.5028894557556716e-05\n",
            "Epoch 822/1000, Training Loss: 3.5542830839740654e-05, Test Loss: 4.498926998627098e-05\n",
            "Epoch 823/1000, Training Loss: 3.550129182656378e-05, Test Loss: 4.4949696125832094e-05\n",
            "Epoch 824/1000, Training Loss: 3.54598716714804e-05, Test Loss: 4.491017256901294e-05\n",
            "Epoch 825/1000, Training Loss: 3.541856946621744e-05, Test Loss: 4.487069891370492e-05\n",
            "Epoch 826/1000, Training Loss: 3.537738431197789e-05, Test Loss: 4.4831274762839324e-05\n",
            "Epoch 827/1000, Training Loss: 3.533631531931983e-05, Test Loss: 4.4791899724309526e-05\n",
            "Epoch 828/1000, Training Loss: 3.529536160804064e-05, Test Loss: 4.475257341089443e-05\n",
            "Epoch 829/1000, Training Loss: 3.52545223070599e-05, Test Loss: 4.4713295440184616e-05\n",
            "Epoch 830/1000, Training Loss: 3.521379655430643e-05, Test Loss: 4.467406543450795e-05\n",
            "Epoch 831/1000, Training Loss: 3.517318349660464e-05, Test Loss: 4.4634883020855986e-05\n",
            "Epoch 832/1000, Training Loss: 3.5132682289565166e-05, Test Loss: 4.459574783081598e-05\n",
            "Epoch 833/1000, Training Loss: 3.509229209747511e-05, Test Loss: 4.4556659500498766e-05\n",
            "Epoch 834/1000, Training Loss: 3.5052012093190973e-05, Test Loss: 4.451761767047131e-05\n",
            "Epoch 835/1000, Training Loss: 3.5011841458032464e-05, Test Loss: 4.447862198568888e-05\n",
            "Epoch 836/1000, Training Loss: 3.497177938167868e-05, Test Loss: 4.443967209542976e-05\n",
            "Epoch 837/1000, Training Loss: 3.493182506206509e-05, Test Loss: 4.440076765322813e-05\n",
            "Epoch 838/1000, Training Loss: 3.489197770528236e-05, Test Loss: 4.4361908316812895e-05\n",
            "Epoch 839/1000, Training Loss: 3.4852236525476963e-05, Test Loss: 4.4323093748044215e-05\n",
            "Epoch 840/1000, Training Loss: 3.481260074475312e-05, Test Loss: 4.428432361285135e-05\n",
            "Epoch 841/1000, Training Loss: 3.477306959307546e-05, Test Loss: 4.424559758117225e-05\n",
            "Epoch 842/1000, Training Loss: 3.473364230817335e-05, Test Loss: 4.42069153268931e-05\n",
            "Epoch 843/1000, Training Loss: 3.4694318135448036e-05, Test Loss: 4.416827652779288e-05\n",
            "Epoch 844/1000, Training Loss: 3.465509632787937e-05, Test Loss: 4.41296808654839e-05\n",
            "Epoch 845/1000, Training Loss: 3.461597614593479e-05, Test Loss: 4.4091128025355e-05\n",
            "Epoch 846/1000, Training Loss: 3.4576956857478575e-05, Test Loss: 4.405261769651711e-05\n",
            "Epoch 847/1000, Training Loss: 3.453803773768493e-05, Test Loss: 4.40141495717505e-05\n",
            "Epoch 848/1000, Training Loss: 3.4499218068948705e-05, Test Loss: 4.397572334744669e-05\n",
            "Epoch 849/1000, Training Loss: 3.446049714080073e-05, Test Loss: 4.393733872356014e-05\n",
            "Epoch 850/1000, Training Loss: 3.442187424982257e-05, Test Loss: 4.38989954035561e-05\n",
            "Epoch 851/1000, Training Loss: 3.438334869956245e-05, Test Loss: 4.386069309435916e-05\n",
            "Epoch 852/1000, Training Loss: 3.434491980045349e-05, Test Loss: 4.382243150630174e-05\n",
            "Epoch 853/1000, Training Loss: 3.430658686973254e-05, Test Loss: 4.3784210353080365e-05\n",
            "Epoch 854/1000, Training Loss: 3.426834923135932e-05, Test Loss: 4.374602935170019e-05\n",
            "Epoch 855/1000, Training Loss: 3.423020621593859e-05, Test Loss: 4.3707888222434975e-05\n",
            "Epoch 856/1000, Training Loss: 3.419215716064117e-05, Test Loss: 4.366978668877673e-05\n",
            "Epoch 857/1000, Training Loss: 3.415420140912853e-05, Test Loss: 4.363172447739084e-05\n",
            "Epoch 858/1000, Training Loss: 3.411633831147638e-05, Test Loss: 4.35937013180731e-05\n",
            "Epoch 859/1000, Training Loss: 3.4078567224100075e-05, Test Loss: 4.355571694370201e-05\n",
            "Epoch 860/1000, Training Loss: 3.404088750968205e-05, Test Loss: 4.3517771090200016e-05\n",
            "Epoch 861/1000, Training Loss: 3.400329853709859e-05, Test Loss: 4.34798634964882e-05\n",
            "Epoch 862/1000, Training Loss: 3.396579968134878e-05, Test Loss: 4.344199390444503e-05\n",
            "Epoch 863/1000, Training Loss: 3.39283903234839e-05, Test Loss: 4.340416205886604e-05\n",
            "Epoch 864/1000, Training Loss: 3.3891069850538444e-05, Test Loss: 4.336636770742343e-05\n",
            "Epoch 865/1000, Training Loss: 3.3853837655461564e-05, Test Loss: 4.33286106006279e-05\n",
            "Epoch 866/1000, Training Loss: 3.38166931370493e-05, Test Loss: 4.329089049178626e-05\n",
            "Epoch 867/1000, Training Loss: 3.3779635699878374e-05, Test Loss: 4.325320713696749e-05\n",
            "Epoch 868/1000, Training Loss: 3.374266475424102e-05, Test Loss: 4.3215560294962375e-05\n",
            "Epoch 869/1000, Training Loss: 3.370577971607939e-05, Test Loss: 4.317794972724822e-05\n",
            "Epoch 870/1000, Training Loss: 3.36689800069228e-05, Test Loss: 4.3140375197952334e-05\n",
            "Epoch 871/1000, Training Loss: 3.363226505382413e-05, Test Loss: 4.3102836473815216e-05\n",
            "Epoch 872/1000, Training Loss: 3.359563428929833e-05, Test Loss: 4.3065333324160525e-05\n",
            "Epoch 873/1000, Training Loss: 3.355908715126109e-05, Test Loss: 4.302786552085327e-05\n",
            "Epoch 874/1000, Training Loss: 3.352262308296827e-05, Test Loss: 4.2990432838272365e-05\n",
            "Epoch 875/1000, Training Loss: 3.348624153295735e-05, Test Loss: 4.295303505327426e-05\n",
            "Epoch 876/1000, Training Loss: 3.344994195498812e-05, Test Loss: 4.291567194516389e-05\n",
            "Epoch 877/1000, Training Loss: 3.341372380798463e-05, Test Loss: 4.287834329565732e-05\n",
            "Epoch 878/1000, Training Loss: 3.337758655597904e-05, Test Loss: 4.2841048888857364e-05\n",
            "Epoch 879/1000, Training Loss: 3.334152966805498e-05, Test Loss: 4.2803788511216095e-05\n",
            "Epoch 880/1000, Training Loss: 3.3305552618291677e-05, Test Loss: 4.2766561951509465e-05\n",
            "Epoch 881/1000, Training Loss: 3.3269654885709886e-05, Test Loss: 4.272936900080653e-05\n",
            "Epoch 882/1000, Training Loss: 3.323383595421753e-05, Test Loss: 4.2692209452437826e-05\n",
            "Epoch 883/1000, Training Loss: 3.319809531255701e-05, Test Loss: 4.265508310196943e-05\n",
            "Epoch 884/1000, Training Loss: 3.316243245425186e-05, Test Loss: 4.261798974717401e-05\n",
            "Epoch 885/1000, Training Loss: 3.312684687755628e-05, Test Loss: 4.258092918800406e-05\n",
            "Epoch 886/1000, Training Loss: 3.3091338085402866e-05, Test Loss: 4.254390122656273e-05\n",
            "Epoch 887/1000, Training Loss: 3.305590558535275e-05, Test Loss: 4.2506905667079346e-05\n",
            "Epoch 888/1000, Training Loss: 3.302054888954553e-05, Test Loss: 4.246994231588195e-05\n",
            "Epoch 889/1000, Training Loss: 3.298526751465129e-05, Test Loss: 4.2433010981371904e-05\n",
            "Epoch 890/1000, Training Loss: 3.295006098182156e-05, Test Loss: 4.239611147399989e-05\n",
            "Epoch 891/1000, Training Loss: 3.2914928816640764e-05, Test Loss: 4.2359243606237456e-05\n",
            "Epoch 892/1000, Training Loss: 3.287987054908068e-05, Test Loss: 4.232240719255778e-05\n",
            "Epoch 893/1000, Training Loss: 3.284488571345336e-05, Test Loss: 4.228560204940782e-05\n",
            "Epoch 894/1000, Training Loss: 3.280997384836515e-05, Test Loss: 4.2248827995186296e-05\n",
            "Epoch 895/1000, Training Loss: 3.2775134496672124e-05, Test Loss: 4.221208485022097e-05\n",
            "Epoch 896/1000, Training Loss: 3.2740367205434145e-05, Test Loss: 4.217537243674613e-05\n",
            "Epoch 897/1000, Training Loss: 3.270567152587302e-05, Test Loss: 4.2138690578877796e-05\n",
            "Epoch 898/1000, Training Loss: 3.267104701332728e-05, Test Loss: 4.210203910259621e-05\n",
            "Epoch 899/1000, Training Loss: 3.2636493227209804e-05, Test Loss: 4.2065417835720956e-05\n",
            "Epoch 900/1000, Training Loss: 3.26020097309663e-05, Test Loss: 4.202882660789049e-05\n",
            "Epoch 901/1000, Training Loss: 3.2567596092032446e-05, Test Loss: 4.1992265250542566e-05\n",
            "Epoch 902/1000, Training Loss: 3.253325188179376e-05, Test Loss: 4.195573359689372e-05\n",
            "Epoch 903/1000, Training Loss: 3.2498976675544394e-05, Test Loss: 4.1919231481917743e-05\n",
            "Epoch 904/1000, Training Loss: 3.2464770052447575e-05, Test Loss: 4.188275874232868e-05\n",
            "Epoch 905/1000, Training Loss: 3.2430631595495305e-05, Test Loss: 4.184631521655826e-05\n",
            "Epoch 906/1000, Training Loss: 3.2396560891470225e-05, Test Loss: 4.1809900744741116e-05\n",
            "Epoch 907/1000, Training Loss: 3.236255753090681e-05, Test Loss: 4.177351516869174e-05\n",
            "Epoch 908/1000, Training Loss: 3.2328621108052834e-05, Test Loss: 4.173715833189049e-05\n",
            "Epoch 909/1000, Training Loss: 3.2294751220832616e-05, Test Loss: 4.1700830079462927e-05\n",
            "Epoch 910/1000, Training Loss: 3.2260947470809525e-05, Test Loss: 4.1664530258163034e-05\n",
            "Epoch 911/1000, Training Loss: 3.2227209463149525e-05, Test Loss: 4.16282587163552e-05\n",
            "Epoch 912/1000, Training Loss: 3.219353680658594e-05, Test Loss: 4.159201530399982e-05\n",
            "Epoch 913/1000, Training Loss: 3.215992911338212e-05, Test Loss: 4.1555799872633004e-05\n",
            "Epoch 914/1000, Training Loss: 3.212638599929799e-05, Test Loss: 4.1519612275352604e-05\n",
            "Epoch 915/1000, Training Loss: 3.209290708355458e-05, Test Loss: 4.148345236680233e-05\n",
            "Epoch 916/1000, Training Loss: 3.205949198879957e-05, Test Loss: 4.144732000315599e-05\n",
            "Epoch 917/1000, Training Loss: 3.202614034107429e-05, Test Loss: 4.141121504209857e-05\n",
            "Epoch 918/1000, Training Loss: 3.199285176977985e-05, Test Loss: 4.137513734281566e-05\n",
            "Epoch 919/1000, Training Loss: 3.195962590764422e-05, Test Loss: 4.133908676597717e-05\n",
            "Epoch 920/1000, Training Loss: 3.1926462390689715e-05, Test Loss: 4.130306317372082e-05\n",
            "Epoch 921/1000, Training Loss: 3.189336085820137e-05, Test Loss: 4.126706642964086e-05\n",
            "Epoch 922/1000, Training Loss: 3.186032095269436e-05, Test Loss: 4.123109639876943e-05\n",
            "Epoch 923/1000, Training Loss: 3.182734231988363e-05, Test Loss: 4.1195152947567264e-05\n",
            "Epoch 924/1000, Training Loss: 3.1794424608652344e-05, Test Loss: 4.1159235943908844e-05\n",
            "Epoch 925/1000, Training Loss: 3.1761567471022046e-05, Test Loss: 4.112334525706701e-05\n",
            "Epoch 926/1000, Training Loss: 3.1728770562121826e-05, Test Loss: 4.108748075770201e-05\n",
            "Epoch 927/1000, Training Loss: 3.169603354015911e-05, Test Loss: 4.105164231784587e-05\n",
            "Epoch 928/1000, Training Loss: 3.166335606639016e-05, Test Loss: 4.1015829810893935e-05\n",
            "Epoch 929/1000, Training Loss: 3.1630737805090996e-05, Test Loss: 4.098004311158899e-05\n",
            "Epoch 930/1000, Training Loss: 3.1598178423528744e-05, Test Loss: 4.094428209601065e-05\n",
            "Epoch 931/1000, Training Loss: 3.156567759193341e-05, Test Loss: 4.090854664156207e-05\n",
            "Epoch 932/1000, Training Loss: 3.1533234983470403e-05, Test Loss: 4.087283662695895e-05\n",
            "Epoch 933/1000, Training Loss: 3.150085027421222e-05, Test Loss: 4.0837151932217935e-05\n",
            "Epoch 934/1000, Training Loss: 3.146852314311173e-05, Test Loss: 4.080149243864575e-05\n",
            "Epoch 935/1000, Training Loss: 3.143625327197495e-05, Test Loss: 4.0765858028827005e-05\n",
            "Epoch 936/1000, Training Loss: 3.140404034543515e-05, Test Loss: 4.073024858661323e-05\n",
            "Epoch 937/1000, Training Loss: 3.1371884050925755e-05, Test Loss: 4.069466399711371e-05\n",
            "Epoch 938/1000, Training Loss: 3.133978407865486e-05, Test Loss: 4.065910414668204e-05\n",
            "Epoch 939/1000, Training Loss: 3.1307740121579634e-05, Test Loss: 4.0623568922909306e-05\n",
            "Epoch 940/1000, Training Loss: 3.127575187538121e-05, Test Loss: 4.058805821460871e-05\n",
            "Epoch 941/1000, Training Loss: 3.124381903843903e-05, Test Loss: 4.0552571911811895e-05\n",
            "Epoch 942/1000, Training Loss: 3.121194131180702e-05, Test Loss: 4.0517109905755705e-05\n",
            "Epoch 943/1000, Training Loss: 3.118011839918866e-05, Test Loss: 4.048167208887038e-05\n",
            "Epoch 944/1000, Training Loss: 3.114835000691337e-05, Test Loss: 4.044625835477482e-05\n",
            "Epoch 945/1000, Training Loss: 3.1116635843912405e-05, Test Loss: 4.04108685982636e-05\n",
            "Epoch 946/1000, Training Loss: 3.108497562169485e-05, Test Loss: 4.037550271529887e-05\n",
            "Epoch 947/1000, Training Loss: 3.1053369054325907e-05, Test Loss: 4.034016060300126e-05\n",
            "Epoch 948/1000, Training Loss: 3.1021815858402185e-05, Test Loss: 4.0304842159642646e-05\n",
            "Epoch 949/1000, Training Loss: 3.099031575303046e-05, Test Loss: 4.026954728463301e-05\n",
            "Epoch 950/1000, Training Loss: 3.0958868459804156e-05, Test Loss: 4.0234275878517264e-05\n",
            "Epoch 951/1000, Training Loss: 3.092747370278204e-05, Test Loss: 4.019902784296408e-05\n",
            "Epoch 952/1000, Training Loss: 3.089613120846621e-05, Test Loss: 4.016380308075794e-05\n",
            "Epoch 953/1000, Training Loss: 3.086484070577968e-05, Test Loss: 4.012860149578837e-05\n",
            "Epoch 954/1000, Training Loss: 3.0833601926046374e-05, Test Loss: 4.009342299304795e-05\n",
            "Epoch 955/1000, Training Loss: 3.080241460296858e-05, Test Loss: 4.0058267478618615e-05\n",
            "Epoch 956/1000, Training Loss: 3.077127847260712e-05, Test Loss: 4.002313485966808e-05\n",
            "Epoch 957/1000, Training Loss: 3.074019327336082e-05, Test Loss: 3.998802504443947e-05\n",
            "Epoch 958/1000, Training Loss: 3.0709158745944836e-05, Test Loss: 3.995293794224434e-05\n",
            "Epoch 959/1000, Training Loss: 3.0678174633372176e-05, Test Loss: 3.991787346345736e-05\n",
            "Epoch 960/1000, Training Loss: 3.064724068093268e-05, Test Loss: 3.9882831519506206e-05\n",
            "Epoch 961/1000, Training Loss: 3.0616356636173365e-05, Test Loss: 3.984781202286616e-05\n",
            "Epoch 962/1000, Training Loss: 3.0585522248879626e-05, Test Loss: 3.981281488705365e-05\n",
            "Epoch 963/1000, Training Loss: 3.0554737271055593e-05, Test Loss: 3.977784002661741e-05\n",
            "Epoch 964/1000, Training Loss: 3.052400145690495e-05, Test Loss: 3.974288735713333e-05\n",
            "Epoch 965/1000, Training Loss: 3.0493314562812404e-05, Test Loss: 3.970795679519639e-05\n",
            "Epoch 966/1000, Training Loss: 3.0462676347324847e-05, Test Loss: 3.967304825841656e-05\n",
            "Epoch 967/1000, Training Loss: 3.0432086571133152e-05, Test Loss: 3.96381616654102e-05\n",
            "Epoch 968/1000, Training Loss: 3.040154499705401e-05, Test Loss: 3.960329693579435e-05\n",
            "Epoch 969/1000, Training Loss: 3.037105139001194e-05, Test Loss: 3.9568453990179236e-05\n",
            "Epoch 970/1000, Training Loss: 3.0340605517021324e-05, Test Loss: 3.953363275016559e-05\n",
            "Epoch 971/1000, Training Loss: 3.0310207147169307e-05, Test Loss: 3.949883313833496e-05\n",
            "Epoch 972/1000, Training Loss: 3.0279856051597877e-05, Test Loss: 3.946405507824554e-05\n",
            "Epoch 973/1000, Training Loss: 3.024955200348648e-05, Test Loss: 3.9429298494425905e-05\n",
            "Epoch 974/1000, Training Loss: 3.0219294778035928e-05, Test Loss: 3.9394563312368955e-05\n",
            "Epoch 975/1000, Training Loss: 3.0189084152450855e-05, Test Loss: 3.9359849458527553e-05\n",
            "Epoch 976/1000, Training Loss: 3.0158919905923352e-05, Test Loss: 3.93251568603072e-05\n",
            "Epoch 977/1000, Training Loss: 3.0128801819616247e-05, Test Loss: 3.9290485446062016e-05\n",
            "Epoch 978/1000, Training Loss: 3.009872967664726e-05, Test Loss: 3.925583514508774e-05\n",
            "Epoch 979/1000, Training Loss: 3.006870326207227e-05, Test Loss: 3.922120588761701e-05\n",
            "Epoch 980/1000, Training Loss: 3.0038722362870027e-05, Test Loss: 3.918659760481576e-05\n",
            "Epoch 981/1000, Training Loss: 3.000878676792609e-05, Test Loss: 3.915201022877475e-05\n",
            "Epoch 982/1000, Training Loss: 2.9978896268017282e-05, Test Loss: 3.911744369250807e-05\n",
            "Epoch 983/1000, Training Loss: 2.9949050655795924e-05, Test Loss: 3.908289792994469e-05\n",
            "Epoch 984/1000, Training Loss: 2.9919249725775143e-05, Test Loss: 3.9048372875925594e-05\n",
            "Epoch 985/1000, Training Loss: 2.9889493274313433e-05, Test Loss: 3.901386846619879e-05\n",
            "Epoch 986/1000, Training Loss: 2.9859781099599334e-05, Test Loss: 3.8979384637413595e-05\n",
            "Epoch 987/1000, Training Loss: 2.9830113001637392e-05, Test Loss: 3.894492132711573e-05\n",
            "Epoch 988/1000, Training Loss: 2.9800488782233464e-05, Test Loss: 3.891047847374479e-05\n",
            "Epoch 989/1000, Training Loss: 2.9770908244979196e-05, Test Loss: 3.887605601662502e-05\n",
            "Epoch 990/1000, Training Loss: 2.974137119523877e-05, Test Loss: 3.884165389596678e-05\n",
            "Epoch 991/1000, Training Loss: 2.9711877440134267e-05, Test Loss: 3.880727205285647e-05\n",
            "Epoch 992/1000, Training Loss: 2.9682426788531377e-05, Test Loss: 3.877291042925533e-05\n",
            "Epoch 993/1000, Training Loss: 2.9653019051025906e-05, Test Loss: 3.8738568967994485e-05\n",
            "Epoch 994/1000, Training Loss: 2.962365403992972e-05, Test Loss: 3.870424761276896e-05\n",
            "Epoch 995/1000, Training Loss: 2.9594331569257674e-05, Test Loss: 3.866994630813542e-05\n",
            "Epoch 996/1000, Training Loss: 2.9565051454712814e-05, Test Loss: 3.8635664999507336e-05\n",
            "Epoch 997/1000, Training Loss: 2.9535813513674347e-05, Test Loss: 3.860140363314897e-05\n",
            "Epoch 998/1000, Training Loss: 2.950661756518357e-05, Test Loss: 3.8567162156173416e-05\n",
            "Epoch 999/1000, Training Loss: 2.9477463429931233e-05, Test Loss: 3.853294051653877e-05\n",
            "Epoch 1000/1000, Training Loss: 2.94483509302444e-05, Test Loss: 3.849873866304148e-05\n",
            "Epoch 1/1000, Training Loss: 0.003449940134554709, Test Loss: 0.003509046547648503\n",
            "Epoch 2/1000, Training Loss: 0.003244828737763738, Test Loss: 0.0034007605602134677\n",
            "Epoch 3/1000, Training Loss: 0.0031949746344152306, Test Loss: 0.0033824587281485797\n",
            "Epoch 4/1000, Training Loss: 0.00317194908338696, Test Loss: 0.00337363505621372\n",
            "Epoch 5/1000, Training Loss: 0.003158147778506872, Test Loss: 0.003367348654019587\n",
            "Epoch 6/1000, Training Loss: 0.003148925960632388, Test Loss: 0.003362667373522947\n",
            "Epoch 7/1000, Training Loss: 0.0031424119170186645, Test Loss: 0.0033592602325212646\n",
            "Epoch 8/1000, Training Loss: 0.0031376170563747097, Test Loss: 0.003356822928941539\n",
            "Epoch 9/1000, Training Loss: 0.0031339506546536446, Test Loss: 0.0033550766392514285\n",
            "Epoch 10/1000, Training Loss: 0.003131044409259924, Test Loss: 0.0033537990472934354\n",
            "Epoch 11/1000, Training Loss: 0.00312866501302409, Test Loss: 0.0033528296996292854\n",
            "Epoch 12/1000, Training Loss: 0.0031266626291892, Test Loss: 0.003352059993410965\n",
            "Epoch 13/1000, Training Loss: 0.0031249391329342693, Test Loss: 0.0033514193515413833\n",
            "Epoch 14/1000, Training Loss: 0.003123428545492356, Test Loss: 0.003350863000079832\n",
            "Epoch 15/1000, Training Loss: 0.0031220850717333325, Test Loss: 0.0033503628247455526\n",
            "Epoch 16/1000, Training Loss: 0.0031208758104645466, Test Loss: 0.003349901108248761\n",
            "Epoch 17/1000, Training Loss: 0.00311977629791346, Test Loss: 0.003349466470620359\n",
            "Epoch 18/1000, Training Loss: 0.0031187677572746698, Test Loss: 0.0033490513380849867\n",
            "Epoch 19/1000, Training Loss: 0.0031178353749545047, Test Loss: 0.0033486504124934985\n",
            "Epoch 20/1000, Training Loss: 0.0031169671973973762, Test Loss: 0.0033482597696444143\n",
            "Epoch 21/1000, Training Loss: 0.003116153406283517, Test Loss: 0.0033478763402964695\n",
            "Epoch 22/1000, Training Loss: 0.003115385827392249, Test Loss: 0.003347497617344895\n",
            "Epoch 23/1000, Training Loss: 0.0031146575862745535, Test Loss: 0.003347121492765583\n",
            "Epoch 24/1000, Training Loss: 0.0031139628582232967, Test Loss: 0.0033467461666174436\n",
            "Epoch 25/1000, Training Loss: 0.0031132966804735948, Test Loss: 0.003346370094523621\n",
            "Epoch 26/1000, Training Loss: 0.0031126548067855137, Test Loss: 0.0033459919547119583\n",
            "Epoch 27/1000, Training Loss: 0.003112033591905773, Test Loss: 0.0033456106243778595\n",
            "Epoch 28/1000, Training Loss: 0.0031114298978522485, Test Loss: 0.0033452251601352495\n",
            "Epoch 29/1000, Training Loss: 0.0031108410166847023, Test Loss: 0.003344834780108436\n",
            "Epoch 30/1000, Training Loss: 0.0031102646061108344, Test Loss: 0.003344438846701767\n",
            "Epoch 31/1000, Training Loss: 0.0031096986353403655, Test Loss: 0.003344036849821523\n",
            "Epoch 32/1000, Training Loss: 0.0031091413392866246, Test Loss: 0.003343628390648689\n",
            "Epoch 33/1000, Training Loss: 0.0031085911796713936, Test Loss: 0.0033432131661668895\n",
            "Epoch 34/1000, Training Loss: 0.0031080468119021576, Test Loss: 0.003342790954648503\n",
            "Epoch 35/1000, Training Loss: 0.0031075070568141443, Test Loss: 0.0033423616022551683\n",
            "Epoch 36/1000, Training Loss: 0.003106970876534402, Test Loss: 0.0033419250108491253\n",
            "Epoch 37/1000, Training Loss: 0.003106437353851316, Test Loss: 0.003341481127054901\n",
            "Epoch 38/1000, Training Loss: 0.0031059056745722387, Test Loss: 0.0033410299325635304\n",
            "Epoch 39/1000, Training Loss: 0.0031053751124321296, Test Loss: 0.003340571435635446\n",
            "Epoch 40/1000, Training Loss: 0.003104845016182006, Test Loss: 0.003340105663733064\n",
            "Epoch 41/1000, Training Loss: 0.003104314798541013, Test Loss: 0.0033396326571981127\n",
            "Epoch 42/1000, Training Loss: 0.0031037839267422044, Test Loss: 0.0033391524638802343\n",
            "Epoch 43/1000, Training Loss: 0.0031032519144413794, Test Loss: 0.0033386651346204903\n",
            "Epoch 44/1000, Training Loss: 0.0031027183147917057, Test Loss: 0.0033381707194945183\n",
            "Epoch 45/1000, Training Loss: 0.00310218271451542, Test Loss: 0.0033376692647240774\n",
            "Epoch 46/1000, Training Loss: 0.003101644728828237, Test Loss: 0.0033371608101714694\n",
            "Epoch 47/1000, Training Loss: 0.0031011039970930236, Test Loss: 0.003336645387338099\n",
            "Epoch 48/1000, Training Loss: 0.0031005601790971334, Test Loss: 0.003336123017795777\n",
            "Epoch 49/1000, Training Loss: 0.003100012951863137, Test Loss: 0.0033355937119866784\n",
            "Epoch 50/1000, Training Loss: 0.0030994620069157704, Test Loss: 0.0033350574683351304\n",
            "Epoch 51/1000, Training Loss: 0.0030989070479391805, Test Loss: 0.0033345142726211195\n",
            "Epoch 52/1000, Training Loss: 0.0030983477887681276, Test Loss: 0.0033339640975718634\n",
            "Epoch 53/1000, Training Loss: 0.0030977839516650647, Test Loss: 0.0033334069026334746\n",
            "Epoch 54/1000, Training Loss: 0.0030972152658420176, Test Loss: 0.0033328426338900836\n",
            "Epoch 55/1000, Training Loss: 0.003096641466192241, Test Loss: 0.0033322712241024405\n",
            "Epoch 56/1000, Training Loss: 0.0030960622922017508, Test Loss: 0.003331692592842167\n",
            "Epoch 57/1000, Training Loss: 0.00309547748701525, Test Loss: 0.003331106646701554\n",
            "Epoch 58/1000, Training Loss: 0.003094886796634719, Test Loss: 0.00333051327956196\n",
            "Epoch 59/1000, Training Loss: 0.0030942899692321603, Test Loss: 0.0033299123729066617\n",
            "Epoch 60/1000, Training Loss: 0.0030936867545607242, Test Loss: 0.0033293037961664874\n",
            "Epoch 61/1000, Training Loss: 0.0030930769034508065, Test Loss: 0.0033286874070885576\n",
            "Epoch 62/1000, Training Loss: 0.003092460167379686, Test Loss: 0.003328063052120307\n",
            "Epoch 63/1000, Training Loss: 0.003091836298104997, Test Loss: 0.0033274305668024562\n",
            "Epoch 64/1000, Training Loss: 0.003091205047353784, Test Loss: 0.0033267897761658782\n",
            "Epoch 65/1000, Training Loss: 0.003090566166560123, Test Loss: 0.003326140495128439\n",
            "Epoch 66/1000, Training Loss: 0.0030899194066453628, Test Loss: 0.0033254825288887655\n",
            "Epoch 67/1000, Training Loss: 0.0030892645178359278, Test Loss: 0.00332481567331466\n",
            "Epoch 68/1000, Training Loss: 0.003088601249514401, Test Loss: 0.0033241397153245555\n",
            "Epoch 69/1000, Training Loss: 0.0030879293501002597, Test Loss: 0.00332345443326087\n",
            "Epoch 70/1000, Training Loss: 0.003087248566957167, Test Loss: 0.0033227595972545893\n",
            "Epoch 71/1000, Training Loss: 0.00308655864632422, Test Loss: 0.003322054969580722\n",
            "Epoch 72/1000, Training Loss: 0.003085859333268931, Test Loss: 0.003321340305004577\n",
            "Epoch 73/1000, Training Loss: 0.00308515037166004, Test Loss: 0.0033206153511189575\n",
            "Epoch 74/1000, Training Loss: 0.003084431504158581, Test Loss: 0.0033198798486726417\n",
            "Epoch 75/1000, Training Loss: 0.0030837024722257843, Test Loss: 0.003319133531890514\n",
            "Epoch 76/1000, Training Loss: 0.0030829630161466675, Test Loss: 0.0033183761287858813\n",
            "Epoch 77/1000, Training Loss: 0.003082212875068261, Test Loss: 0.0033176073614655356\n",
            "Epoch 78/1000, Training Loss: 0.0030814517870515904, Test Loss: 0.0033168269464281305\n",
            "Epoch 79/1000, Training Loss: 0.0030806794891366147, Test Loss: 0.003316034594856495\n",
            "Epoch 80/1000, Training Loss: 0.0030798957174194, Test Loss: 0.0033152300129044303\n",
            "Epoch 81/1000, Training Loss: 0.0030791002071408947, Test Loss: 0.0033144129019785796\n",
            "Epoch 82/1000, Training Loss: 0.0030782926927867002, Test Loss: 0.003313582959015872\n",
            "Epoch 83/1000, Training Loss: 0.0030774729081972656, Test Loss: 0.0033127398767570206\n",
            "Epoch 84/1000, Training Loss: 0.003076640586687959, Test Loss: 0.003311883344016497\n",
            "Epoch 85/1000, Training Loss: 0.0030757954611784935, Test Loss: 0.0033110130459493496\n",
            "Epoch 86/1000, Training Loss: 0.0030749372643311687, Test Loss: 0.003310128664315173\n",
            "Epoch 87/1000, Training Loss: 0.003074065728697398, Test Loss: 0.003309229877739454\n",
            "Epoch 88/1000, Training Loss: 0.003073180586871989, Test Loss: 0.003308316361972505\n",
            "Epoch 89/1000, Training Loss: 0.0030722815716546187, Test Loss: 0.0033073877901460685\n",
            "Epoch 90/1000, Training Loss: 0.0030713684162179448, Test Loss: 0.0033064438330276703\n",
            "Epoch 91/1000, Training Loss: 0.0030704408542817766, Test Loss: 0.003305484159272713\n",
            "Epoch 92/1000, Training Loss: 0.0030694986202927, Test Loss: 0.0033045084356742643\n",
            "Epoch 93/1000, Training Loss: 0.0030685414496085485, Test Loss: 0.0033035163274104207\n",
            "Epoch 94/1000, Training Loss: 0.003067569078687091, Test Loss: 0.003302507498289113\n",
            "Epoch 95/1000, Training Loss: 0.0030665812452782995, Test Loss: 0.0033014816109901597\n",
            "Epoch 96/1000, Training Loss: 0.003065577688619525, Test Loss: 0.0033004383273043405\n",
            "Epoch 97/1000, Training Loss: 0.003064558149632939, Test Loss: 0.003299377308369277\n",
            "Epoch 98/1000, Training Loss: 0.0030635223711245625, Test Loss: 0.0032982982149018184\n",
            "Epoch 99/1000, Training Loss: 0.0030624700979842105, Test Loss: 0.003297200707426681\n",
            "Epoch 100/1000, Training Loss: 0.0030614010773856943, Test Loss: 0.0032960844465010574\n",
            "Epoch 101/1000, Training Loss: 0.003060315058986622, Test Loss: 0.0032949490929349346\n",
            "Epoch 102/1000, Training Loss: 0.0030592117951271473, Test Loss: 0.0032937943080068345\n",
            "Epoch 103/1000, Training Loss: 0.0030580910410270535, Test Loss: 0.0032926197536747707\n",
            "Epoch 104/1000, Training Loss: 0.0030569525549805536, Test Loss: 0.0032914250927821628\n",
            "Epoch 105/1000, Training Loss: 0.003055796098548255, Test Loss: 0.0032902099892585675\n",
            "Epoch 106/1000, Training Loss: 0.003054621436745727, Test Loss: 0.003288974108315048\n",
            "Epoch 107/1000, Training Loss: 0.0030534283382281915, Test Loss: 0.003287717116634112\n",
            "Epoch 108/1000, Training Loss: 0.003052216575470867, Test Loss: 0.003286438682554156\n",
            "Epoch 109/1000, Training Loss: 0.0030509859249445654, Test Loss: 0.003285138476248419\n",
            "Epoch 110/1000, Training Loss: 0.003049736167286178, Test Loss: 0.003283816169898534\n",
            "Epoch 111/1000, Training Loss: 0.00304846708746376, Test Loss: 0.0032824714378627724\n",
            "Epoch 112/1000, Training Loss: 0.0030471784749359597, Test Loss: 0.0032811039568392025\n",
            "Epoch 113/1000, Training Loss: 0.0030458701238056138, Test Loss: 0.003279713406024008\n",
            "Epoch 114/1000, Training Loss: 0.003044541832967395, Test Loss: 0.0032782994672652765\n",
            "Epoch 115/1000, Training Loss: 0.003043193406249445, Test Loss: 0.003276861825212654\n",
            "Epoch 116/1000, Training Loss: 0.003041824652548998, Test Loss: 0.0032754001674633083\n",
            "Epoch 117/1000, Training Loss: 0.003040435385962074, Test Loss: 0.0032739141847047105\n",
            "Epoch 118/1000, Training Loss: 0.003039025425907364, Test Loss: 0.0032724035708547854\n",
            "Epoch 119/1000, Training Loss: 0.0030375945972444898, Test Loss: 0.0032708680232000575\n",
            "Epoch 120/1000, Training Loss: 0.0030361427303869045, Test Loss: 0.0032693072425324333\n",
            "Epoch 121/1000, Training Loss: 0.003034669661409718, Test Loss: 0.00326772093328534\n",
            "Epoch 122/1000, Training Loss: 0.0030331752321528026, Test Loss: 0.0032661088036699355\n",
            "Epoch 123/1000, Training Loss: 0.003031659290319583, Test Loss: 0.003264470565812164\n",
            "Epoch 124/1000, Training Loss: 0.0030301216895719397, Test Loss: 0.00326280593589141\n",
            "Epoch 125/1000, Training Loss: 0.0030285622896217093, Test Loss: 0.0032611146342815876\n",
            "Epoch 126/1000, Training Loss: 0.0030269809563192784, Test Loss: 0.0032593963856954113\n",
            "Epoch 127/1000, Training Loss: 0.0030253775617398093, Test Loss: 0.0032576509193326586\n",
            "Epoch 128/1000, Training Loss: 0.0030237519842676395, Test Loss: 0.003255877969033225\n",
            "Epoch 129/1000, Training Loss: 0.003022104108679421, Test Loss: 0.0032540772734356866\n",
            "Epoch 130/1000, Training Loss: 0.003020433826226557, Test Loss: 0.003252248576142122\n",
            "Epoch 131/1000, Training Loss: 0.0030187410347175233, Test Loss: 0.0032503916258899103\n",
            "Epoch 132/1000, Training Loss: 0.0030170256386006047, Test Loss: 0.0032485061767311\n",
            "Epoch 133/1000, Training Loss: 0.003015287549047626, Test Loss: 0.003246591988220015\n",
            "Epoch 134/1000, Training Loss: 0.0030135266840391867, Test Loss: 0.003244648825609595\n",
            "Epoch 135/1000, Training Loss: 0.0030117429684519063, Test Loss: 0.0032426764600569837\n",
            "Epoch 136/1000, Training Loss: 0.0030099363341481765, Test Loss: 0.003240674668838753\n",
            "Epoch 137/1000, Training Loss: 0.0030081067200688327, Test Loss: 0.0032386432355761513\n",
            "Epoch 138/1000, Training Loss: 0.0030062540723291772, Test Loss: 0.003236581950470594\n",
            "Epoch 139/1000, Training Loss: 0.0030043783443186897, Test Loss: 0.003234490610549625\n",
            "Epoch 140/1000, Training Loss: 0.0030024794968047564, Test Loss: 0.0032323690199234333\n",
            "Epoch 141/1000, Training Loss: 0.003000557498040668, Test Loss: 0.0032302169900519473\n",
            "Epoch 142/1000, Training Loss: 0.0029986123238780996, Test Loss: 0.003228034340022456\n",
            "Epoch 143/1000, Training Loss: 0.002996643957884218, Test Loss: 0.003225820896837575\n",
            "Epoch 144/1000, Training Loss: 0.002994652391463521, Test Loss: 0.003223576495713345\n",
            "Epoch 145/1000, Training Loss: 0.002992637623984427, Test Loss: 0.0032213009803870934\n",
            "Epoch 146/1000, Training Loss: 0.002990599662910592, Test Loss: 0.003218994203434672\n",
            "Epoch 147/1000, Training Loss: 0.0029885385239368603, Test Loss: 0.0032166560265965275\n",
            "Epoch 148/1000, Training Loss: 0.00298645423112969, Test Loss: 0.0032142863211120226\n",
            "Epoch 149/1000, Training Loss: 0.0029843468170718374, Test Loss: 0.003211884968061326\n",
            "Epoch 150/1000, Training Loss: 0.002982216323011001, Test Loss: 0.0032094518587140746\n",
            "Epoch 151/1000, Training Loss: 0.002980062799012103, Test Loss: 0.0032069868948839835\n",
            "Epoch 152/1000, Training Loss: 0.0029778863041127687, Test Loss: 0.0032044899892884598\n",
            "Epoch 153/1000, Training Loss: 0.002975686906481548, Test Loss: 0.003201961065912197\n",
            "Epoch 154/1000, Training Loss: 0.0029734646835783436, Test Loss: 0.003199400060373676\n",
            "Epoch 155/1000, Training Loss: 0.0029712197223164425, Test Loss: 0.0031968069202934217\n",
            "Epoch 156/1000, Training Loss: 0.002968952119225518, Test Loss: 0.0031941816056627716\n",
            "Epoch 157/1000, Training Loss: 0.0029666619806148777, Test Loss: 0.0031915240892118712\n",
            "Epoch 158/1000, Training Loss: 0.0029643494227362195, Test Loss: 0.0031888343567755494\n",
            "Epoch 159/1000, Training Loss: 0.0029620145719450636, Test Loss: 0.0031861124076556567\n",
            "Epoch 160/1000, Training Loss: 0.0029596575648600217, Test Loss: 0.003183358254978413\n",
            "Epoch 161/1000, Training Loss: 0.002957278548518997, Test Loss: 0.0031805719260452254\n",
            "Epoch 162/1000, Training Loss: 0.0029548776805313594, Test Loss: 0.0031777534626754624\n",
            "Epoch 163/1000, Training Loss: 0.0029524551292251307, Test Loss: 0.00317490292153955\n",
            "Epoch 164/1000, Training Loss: 0.002950011073788139, Test Loss: 0.003172020374480766\n",
            "Epoch 165/1000, Training Loss: 0.0029475457044021167, Test Loss: 0.0031691059088240943\n",
            "Epoch 166/1000, Training Loss: 0.002945059222368632, Test Loss: 0.0031661596276704174\n",
            "Epoch 167/1000, Training Loss: 0.0029425518402257833, Test Loss: 0.0031631816501743624\n",
            "Epoch 168/1000, Training Loss: 0.0029400237818545056, Test Loss: 0.003160172111804093\n",
            "Epoch 169/1000, Training Loss: 0.0029374752825733646, Test Loss: 0.0031571311645812823\n",
            "Epoch 170/1000, Training Loss: 0.0029349065892206837, Test Loss: 0.0031540589772996075\n",
            "Epoch 171/1000, Training Loss: 0.002932317960222842, Test Loss: 0.0031509557357199715\n",
            "Epoch 172/1000, Training Loss: 0.0029297096656475904, Test Loss: 0.0031478216427408036\n",
            "Epoch 173/1000, Training Loss: 0.0029270819872412343, Test Loss: 0.003144656918541735\n",
            "Epoch 174/1000, Training Loss: 0.0029244352184485153, Test Loss: 0.003141461800698981\n",
            "Epoch 175/1000, Training Loss: 0.0029217696644140865, Test Loss: 0.0031382365442708254\n",
            "Epoch 176/1000, Training Loss: 0.0029190856419644452, Test Loss: 0.0031349814218516402\n",
            "Epoch 177/1000, Training Loss: 0.0029163834795692426, Test Loss: 0.003131696723592857\n",
            "Epoch 178/1000, Training Loss: 0.0029136635172809164, Test Loss: 0.0031283827571895123\n",
            "Epoch 179/1000, Training Loss: 0.0029109261066516162, Test Loss: 0.0031250398478308945\n",
            "Epoch 180/1000, Training Loss: 0.002908171610626447, Test Loss: 0.003121668338114009\n",
            "Epoch 181/1000, Training Loss: 0.0029054004034120996, Test Loss: 0.003118268587918612\n",
            "Epoch 182/1000, Training Loss: 0.002902612870319995, Test Loss: 0.0031148409742426737\n",
            "Epoch 183/1000, Training Loss: 0.0028998094075831215, Test Loss: 0.0031113858909972637\n",
            "Epoch 184/1000, Training Loss: 0.002896990422145813, Test Loss: 0.0031079037487598544\n",
            "Epoch 185/1000, Training Loss: 0.002894156331425804, Test Loss: 0.0031043949744853235\n",
            "Epoch 186/1000, Training Loss: 0.0028913075630479412, Test Loss: 0.0031008600111739034\n",
            "Epoch 187/1000, Training Loss: 0.0028884445545490594, Test Loss: 0.003097299317495572\n",
            "Epoch 188/1000, Training Loss: 0.002885567753053562, Test Loss: 0.0030937133673704536\n",
            "Epoch 189/1000, Training Loss: 0.002882677614919405, Test Loss: 0.003090102649504986\n",
            "Epoch 190/1000, Training Loss: 0.0028797746053542215, Test Loss: 0.003086467666883733\n",
            "Epoch 191/1000, Training Loss: 0.002876859198001466, Test Loss: 0.0030828089362169245\n",
            "Epoch 192/1000, Training Loss: 0.002873931874496538, Test Loss: 0.003079126987343913\n",
            "Epoch 193/1000, Training Loss: 0.002870993123992983, Test Loss: 0.003075422362592962\n",
            "Epoch 194/1000, Training Loss: 0.0028680434426589484, Test Loss: 0.003071695616097904\n",
            "Epoch 195/1000, Training Loss: 0.0028650833331442084, Test Loss: 0.0030679473130724062\n",
            "Epoch 196/1000, Training Loss: 0.0028621133040181855, Test Loss: 0.0030641780290427767\n",
            "Epoch 197/1000, Training Loss: 0.0028591338691795103, Test Loss: 0.0030603883490403546\n",
            "Epoch 198/1000, Training Loss: 0.0028561455472377786, Test Loss: 0.003056578866754757\n",
            "Epoch 199/1000, Training Loss: 0.0028531488608682827, Test Loss: 0.0030527501836494423\n",
            "Epoch 200/1000, Training Loss: 0.0028501443361406273, Test Loss: 0.0030489029080411206\n",
            "Epoch 201/1000, Training Loss: 0.002847132501822215, Test Loss: 0.0030450376541448503\n",
            "Epoch 202/1000, Training Loss: 0.002844113888657758, Test Loss: 0.0030411550410866617\n",
            "Epoch 203/1000, Training Loss: 0.002841089028626021, Test Loss: 0.0030372556918858468\n",
            "Epoch 204/1000, Training Loss: 0.002838058454175152, Test Loss: 0.003033340232409094\n",
            "Epoch 205/1000, Training Loss: 0.002835022697438034, Test Loss: 0.0030294092902988156\n",
            "Epoch 206/1000, Training Loss: 0.002831982289429197, Test Loss: 0.0030254634938781623\n",
            "Epoch 207/1000, Training Loss: 0.002828937759224909, Test Loss: 0.0030215034710352785\n",
            "Epoch 208/1000, Training Loss: 0.0028258896331281526, Test Loss: 0.00301752984808953\n",
            "Epoch 209/1000, Training Loss: 0.002822838433820268, Test Loss: 0.0030135432486424136\n",
            "Epoch 210/1000, Training Loss: 0.0028197846795011135, Test Loss: 0.003009544292416065\n",
            "Epoch 211/1000, Training Loss: 0.002816728883019627, Test Loss: 0.003005533594082207\n",
            "Epoch 212/1000, Training Loss: 0.0028136715509967597, Test Loss: 0.0030015117620845215\n",
            "Epoch 213/1000, Training Loss: 0.002810613182942746, Test Loss: 0.002997479397457396\n",
            "Epoch 214/1000, Training Loss: 0.0028075542703707448, Test Loss: 0.0029934370926440407\n",
            "Epoch 215/1000, Training Loss: 0.002804495295908861, Test Loss: 0.0029893854303169346\n",
            "Epoch 216/1000, Training Loss: 0.00280143673241259, Test Loss: 0.00298532498220357\n",
            "Epoch 217/1000, Training Loss: 0.0027983790420797136, Test Loss: 0.0029812563079203817\n",
            "Epoch 218/1000, Training Loss: 0.002795322675569655, Test Loss: 0.002977179953817728\n",
            "Epoch 219/1000, Training Loss: 0.0027922680711292775, Test Loss: 0.0029730964518386934\n",
            "Epoch 220/1000, Training Loss: 0.002789215653727056, Test Loss: 0.0029690063183944048\n",
            "Epoch 221/1000, Training Loss: 0.0027861658341975415, Test Loss: 0.002964910053258438\n",
            "Epoch 222/1000, Training Loss: 0.0027831190083979092, Test Loss: 0.0029608081384827566\n",
            "Epoch 223/1000, Training Loss: 0.0027800755563783764, Test Loss: 0.0029567010373375505\n",
            "Epoch 224/1000, Training Loss: 0.002777035841568154, Test Loss: 0.002952589193277088\n",
            "Epoch 225/1000, Training Loss: 0.0027740002099785146, Test Loss: 0.00294847302893366\n",
            "Epoch 226/1000, Training Loss: 0.0027709689894244533, Test Loss: 0.0029443529451413917\n",
            "Epoch 227/1000, Training Loss: 0.0027679424887663237, Test Loss: 0.0029402293199916416\n",
            "Epoch 228/1000, Training Loss: 0.002764920997172693, Test Loss: 0.0029361025079214093\n",
            "Epoch 229/1000, Training Loss: 0.002761904783405554, Test Loss: 0.002931972838836046\n",
            "Epoch 230/1000, Training Loss: 0.0027588940951288796, Test Loss: 0.002927840617267311\n",
            "Epoch 231/1000, Training Loss: 0.002755889158241395, Test Loss: 0.0029237061215676534\n",
            "Epoch 232/1000, Training Loss: 0.002752890176234278, Test Loss: 0.0029195696031413306\n",
            "Epoch 233/1000, Training Loss: 0.0027498973295743607, Test Loss: 0.002915431285712793\n",
            "Epoch 234/1000, Training Loss: 0.002746910775113257, Test Loss: 0.002911291364632563\n",
            "Epoch 235/1000, Training Loss: 0.0027439306455226973, Test Loss: 0.0029071500062205725\n",
            "Epoch 236/1000, Training Loss: 0.0027409570487561766, Test Loss: 0.002903007347146721\n",
            "Epoch 237/1000, Training Loss: 0.0027379900675368932, Test Loss: 0.002898863493848266\n",
            "Epoch 238/1000, Training Loss: 0.0027350297588717787, Test Loss: 0.002894718521983371\n",
            "Epoch 239/1000, Training Loss: 0.0027320761535912894, Test Loss: 0.002890572475919935\n",
            "Epoch 240/1000, Training Loss: 0.002729129255914459, Test Loss: 0.002886425368258733\n",
            "Epoch 241/1000, Training Loss: 0.002726189043038577, Test Loss: 0.0028822771793895717\n",
            "Epoch 242/1000, Training Loss: 0.002723255464752693, Test Loss: 0.0028781278570790877\n",
            "Epoch 243/1000, Training Loss: 0.0027203284430740186, Test Loss: 0.002873977316088579\n",
            "Epoch 244/1000, Training Loss: 0.0027174078719061647, Test Loss: 0.002869825437820166\n",
            "Epoch 245/1000, Training Loss: 0.002714493616717987, Test Loss: 0.0028656720699893395\n",
            "Epoch 246/1000, Training Loss: 0.002711585514241713, Test Loss: 0.002861517026321887\n",
            "Epoch 247/1000, Training Loss: 0.0027086833721888847, Test Loss: 0.0028573600862730475\n",
            "Epoch 248/1000, Training Loss: 0.0027057869689825235, Test Loss: 0.0028532009947665714\n",
            "Epoch 249/1000, Training Loss: 0.0027028960535038227, Test Loss: 0.002849039461951336\n",
            "Epoch 250/1000, Training Loss: 0.0027000103448515424, Test Loss: 0.0028448751629730107\n",
            "Epoch 251/1000, Training Loss: 0.002697129532112214, Test Loss: 0.002840707737758214\n",
            "Epoch 252/1000, Training Loss: 0.0026942532741391014, Test Loss: 0.002836536790808532\n",
            "Epoch 253/1000, Training Loss: 0.0026913811993378323, Test Loss: 0.0028323618910016885\n",
            "Epoch 254/1000, Training Loss: 0.0026885129054564968, Test Loss: 0.0028281825713971365\n",
            "Epoch 255/1000, Training Loss: 0.0026856479593779076, Test Loss: 0.0028239983290432908\n",
            "Epoch 256/1000, Training Loss: 0.002682785896911676, Test Loss: 0.0028198086247835934\n",
            "Epoch 257/1000, Training Loss: 0.002679926222583636, Test Loss: 0.002815612883058598\n",
            "Epoch 258/1000, Training Loss: 0.002677068409420132, Test Loss: 0.0028114104917012646\n",
            "Epoch 259/1000, Training Loss: 0.0026742118987245402, Test Loss: 0.0028072008017226086\n",
            "Epoch 260/1000, Training Loss: 0.002671356099843413, Test Loss: 0.002802983127084978\n",
            "Epoch 261/1000, Training Loss: 0.002668500389919491, Test Loss: 0.002798756744460128\n",
            "Epoch 262/1000, Training Loss: 0.002665644113628816, Test Loss: 0.0027945208929693642\n",
            "Epoch 263/1000, Training Loss: 0.0026627865828990948, Test Loss: 0.002790274773903108\n",
            "Epoch 264/1000, Training Loss: 0.0026599270766064008, Test Loss: 0.0027860175504171914\n",
            "Epoch 265/1000, Training Loss: 0.0026570648402472375, Test Loss: 0.00278174834720335\n",
            "Epoch 266/1000, Training Loss: 0.002654199085582917, Test Loss: 0.0027774662501313663\n",
            "Epoch 267/1000, Training Loss: 0.002651328990253159, Test Loss: 0.0027731703058604695\n",
            "Epoch 268/1000, Training Loss: 0.0026484536973557004, Test Loss: 0.0027688595214176155\n",
            "Epoch 269/1000, Training Loss: 0.0026455723149886863, Test Loss: 0.002764532863740416\n",
            "Epoch 270/1000, Training Loss: 0.0026426839157524915, Test Loss: 0.002760189259182523\n",
            "Epoch 271/1000, Training Loss: 0.002639787536207541, Test Loss: 0.002755827592979473\n",
            "Epoch 272/1000, Training Loss: 0.002636882176284622, Test Loss: 0.0027514467086729795\n",
            "Epoch 273/1000, Training Loss: 0.002633966798644065, Test Loss: 0.0027470454074919127\n",
            "Epoch 274/1000, Training Loss: 0.0026310403279800588, Test Loss: 0.0027426224476882074\n",
            "Epoch 275/1000, Training Loss: 0.002628101650266262, Test Loss: 0.0027381765438261813\n",
            "Epoch 276/1000, Training Loss: 0.002625149611938735, Test Loss: 0.0027337063660237792\n",
            "Epoch 277/1000, Training Loss: 0.0026221830190120534, Test Loss: 0.0027292105391444575\n",
            "Epoch 278/1000, Training Loss: 0.0026192006361243614, Test Loss: 0.002724687641938579\n",
            "Epoch 279/1000, Training Loss: 0.0026162011855069053, Test Loss: 0.002720136206133257\n",
            "Epoch 280/1000, Training Loss: 0.002613183345873433, Test Loss: 0.0027155547154698347\n",
            "Epoch 281/1000, Training Loss: 0.0026101457512246514, Test Loss: 0.0027109416046882644\n",
            "Epoch 282/1000, Training Loss: 0.002607086989562703, Test Loss: 0.0027062952584578463\n",
            "Epoch 283/1000, Training Loss: 0.0026040056015103987, Test Loss: 0.002701614010253916\n",
            "Epoch 284/1000, Training Loss: 0.002600900078829719, Test Loss: 0.0026968961411802305\n",
            "Epoch 285/1000, Training Loss: 0.0025977688628337882, Test Loss: 0.002692139878736946\n",
            "Epoch 286/1000, Training Loss: 0.002594610342686258, Test Loss: 0.0026873433955342156\n",
            "Epoch 287/1000, Training Loss: 0.0025914228535817508, Test Loss: 0.002682504807951588\n",
            "Epoch 288/1000, Training Loss: 0.0025882046748006296, Test Loss: 0.0026776221747434347\n",
            "Epoch 289/1000, Training Loss: 0.0025849540276310632, Test Loss: 0.002672693495590844\n",
            "Epoch 290/1000, Training Loss: 0.0025816690731509453, Test Loss: 0.002667716709600384\n",
            "Epoch 291/1000, Training Loss: 0.002578347909861829, Test Loss: 0.002662689693750271\n",
            "Epoch 292/1000, Training Loss: 0.0025749885711666507, Test Loss: 0.0026576102612844254\n",
            "Epoch 293/1000, Training Loss: 0.002571589022682505, Test Loss: 0.0026524761600549385\n",
            "Epoch 294/1000, Training Loss: 0.0025681471593793407, Test Loss: 0.002647285070813339\n",
            "Epoch 295/1000, Training Loss: 0.0025646608025348787, Test Loss: 0.002642034605450984\n",
            "Epoch 296/1000, Training Loss: 0.0025611276964956032, Test Loss: 0.0026367223051886033\n",
            "Epoch 297/1000, Training Loss: 0.002557545505233095, Test Loss: 0.0026313456387148434\n",
            "Epoch 298/1000, Training Loss: 0.00255391180868443, Test Loss: 0.0026259020002732036\n",
            "Epoch 299/1000, Training Loss: 0.0025502240988648224, Test Loss: 0.002620388707696311\n",
            "Epoch 300/1000, Training Loss: 0.002546479775740062, Test Loss: 0.002614803000385859\n",
            "Epoch 301/1000, Training Loss: 0.002542676142845718, Test Loss: 0.0026091420372357666\n",
            "Epoch 302/1000, Training Loss: 0.0025388104026395, Test Loss: 0.0026034028944951822\n",
            "Epoch 303/1000, Training Loss: 0.002534879651572522, Test Loss: 0.0025975825635669186\n",
            "Epoch 304/1000, Training Loss: 0.002530880874864644, Test Loss: 0.0025916779487354207\n",
            "Epoch 305/1000, Training Loss: 0.0025268109409685028, Test Loss: 0.002585685864817002\n",
            "Epoch 306/1000, Training Loss: 0.0025226665957062437, Test Loss: 0.002579603034723093\n",
            "Epoch 307/1000, Training Loss: 0.0025184444560624897, Test Loss: 0.002573426086925177\n",
            "Epoch 308/1000, Training Loss: 0.002514141003616591, Test Loss: 0.002567151552807624\n",
            "Epoch 309/1000, Training Loss: 0.0025097525775968025, Test Loss: 0.002560775863891751\n",
            "Epoch 310/1000, Training Loss: 0.002505275367538726, Test Loss: 0.0025542953489112365\n",
            "Epoch 311/1000, Training Loss: 0.0025007054055301205, Test Loss: 0.0025477062307152996\n",
            "Epoch 312/1000, Training Loss: 0.0024960385580241377, Test Loss: 0.002541004622971896\n",
            "Epoch 313/1000, Training Loss: 0.0024912705172030923, Test Loss: 0.002534186526638505\n",
            "Epoch 314/1000, Training Loss: 0.002486396791875153, Test Loss: 0.0025272478261628524\n",
            "Epoch 315/1000, Training Loss: 0.002481412697886865, Test Loss: 0.00252018428537012\n",
            "Epoch 316/1000, Training Loss: 0.0024763133480351883, Test Loss: 0.002512991542986858\n",
            "Epoch 317/1000, Training Loss: 0.0024710936414638666, Test Loss: 0.002505665107744805\n",
            "Epoch 318/1000, Training Loss: 0.0024657482525304634, Test Loss: 0.0024982003530003745\n",
            "Epoch 319/1000, Training Loss: 0.0024602716191323955, Test Loss: 0.0024905925107975304\n",
            "Epoch 320/1000, Training Loss: 0.002454657930482879, Test Loss: 0.002482836665293339\n",
            "Epoch 321/1000, Training Loss: 0.0024489011143309014, Test Loss: 0.002474927745456787\n",
            "Epoch 322/1000, Training Loss: 0.0024429948236234346, Test Loss: 0.0024668605169425677\n",
            "Epoch 323/1000, Training Loss: 0.0024369324226130823, Test Loss: 0.0024586295730330263\n",
            "Epoch 324/1000, Training Loss: 0.002430706972420557, Test Loss: 0.002450229324533288\n",
            "Epoch 325/1000, Training Loss: 0.0024243112160689272, Test Loss: 0.0024416539884974937\n",
            "Epoch 326/1000, Training Loss: 0.002417737563015899, Test Loss: 0.0024328975756588147\n",
            "Epoch 327/1000, Training Loss: 0.0024109780732216785, Test Loss: 0.002423953876432886\n",
            "Epoch 328/1000, Training Loss: 0.002404024440803771, Test Loss: 0.002414816445364882\n",
            "Epoch 329/1000, Training Loss: 0.00239686797734691, Test Loss: 0.002405478583895666\n",
            "Epoch 330/1000, Training Loss: 0.002389499594956793, Test Loss: 0.0023959333213339306\n",
            "Epoch 331/1000, Training Loss: 0.002381909789171238, Test Loss: 0.0023861733939405613\n",
            "Epoch 332/1000, Training Loss: 0.002374088621872655, Test Loss: 0.002376191222060884\n",
            "Epoch 333/1000, Training Loss: 0.0023660257043825136, Test Loss: 0.002365978885282404\n",
            "Epoch 334/1000, Training Loss: 0.0023577101809629697, Test Loss: 0.0023555280956524035\n",
            "Epoch 335/1000, Training Loss: 0.002349130713004482, Test Loss: 0.0023448301690650233\n",
            "Epoch 336/1000, Training Loss: 0.002340275464242655, Test Loss: 0.0023338759950233363\n",
            "Epoch 337/1000, Training Loss: 0.0023311320874243747, Test Loss: 0.002322656005102556\n",
            "Epoch 338/1000, Training Loss: 0.0023216877129343355, Test Loss: 0.0023111601405881406\n",
            "Epoch 339/1000, Training Loss: 0.0023119289399996927, Test Loss: 0.0022993778199401666\n",
            "Epoch 340/1000, Training Loss: 0.002301841831214205, Test Loss: 0.002287297906944403\n",
            "Epoch 341/1000, Training Loss: 0.0022914119112643484, Test Loss: 0.0022749086806516834\n",
            "Epoch 342/1000, Training Loss: 0.002280624170898032, Test Loss: 0.002262197808478571\n",
            "Epoch 343/1000, Training Loss: 0.0022694630773497496, Test Loss: 0.0022491523241402363\n",
            "Epoch 344/1000, Training Loss: 0.0022579125926200662, Test Loss: 0.002235758612402675\n",
            "Epoch 345/1000, Training Loss: 0.0022459562011951516, Test Loss: 0.0022220024029641077\n",
            "Epoch 346/1000, Training Loss: 0.0022335769489730934, Test Loss: 0.00220786877608711\n",
            "Epoch 347/1000, Training Loss: 0.0022207574953227046, Test Loss: 0.0021933421828800987\n",
            "Epoch 348/1000, Training Loss: 0.002207480180318119, Test Loss: 0.002178406483339777\n",
            "Epoch 349/1000, Training Loss: 0.0021937271092433834, Test Loss: 0.0021630450053786933\n",
            "Epoch 350/1000, Training Loss: 0.002179480256417093, Test Loss: 0.0021472406280336463\n",
            "Epoch 351/1000, Training Loss: 0.002164721590215568, Test Loss: 0.0021309758918378148\n",
            "Epoch 352/1000, Training Loss: 0.002149433220842206, Test Loss: 0.002114233138900035\n",
            "Epoch 353/1000, Training Loss: 0.0021335975718723694, Test Loss: 0.0020969946845338476\n",
            "Epoch 354/1000, Training Loss: 0.0021171975758782317, Test Loss: 0.0020792430212957917\n",
            "Epoch 355/1000, Training Loss: 0.002100216893503201, Test Loss: 0.0020609610550278287\n",
            "Epoch 356/1000, Training Loss: 0.0020826401542296045, Test Loss: 0.00204213237098447\n",
            "Epoch 357/1000, Training Loss: 0.002064453215812689, Test Loss: 0.002022741526427832\n",
            "Epoch 358/1000, Training Loss: 0.002045643438016069, Test Loss: 0.002002774364300053\n",
            "Epoch 359/1000, Training Loss: 0.002026199964985076, Test Loss: 0.0019822183408731435\n",
            "Epoch 360/1000, Training Loss: 0.002006114009463119, Test Loss: 0.001961062858797511\n",
            "Epoch 361/1000, Training Loss: 0.0019853791312287876, Test Loss: 0.0019392995958940808\n",
            "Epoch 362/1000, Training Loss: 0.001963991501732449, Test Loss: 0.0019169228195150528\n",
            "Epoch 363/1000, Training Loss: 0.001941950147030718, Test Loss: 0.0018939296764428526\n",
            "Epoch 364/1000, Training Loss: 0.0019192571617896955, Test Loss: 0.0018703204491433078\n",
            "Epoch 365/1000, Training Loss: 0.001895917888315829, Test Loss: 0.0018460987706884289\n",
            "Epoch 366/1000, Training Loss: 0.001871941056163806, Test Loss: 0.0018212717926782455\n",
            "Epoch 367/1000, Training Loss: 0.0018473388796851958, Test Loss: 0.0017958503028068318\n",
            "Epoch 368/1000, Training Loss: 0.0018221271126993277, Test Loss: 0.0017698487910772726\n",
            "Epoch 369/1000, Training Loss: 0.0017963250610624356, Test Loss: 0.0017432854658114138\n",
            "Epoch 370/1000, Training Loss: 0.0017699555550882153, Test Loss: 0.0017161822222994283\n",
            "Epoch 371/1000, Training Loss: 0.0017430448844070233, Test Loss: 0.0016885645680419364\n",
            "Epoch 372/1000, Training Loss: 0.0017156226979084242, Test Loss: 0.0016604615090015195\n",
            "Epoch 373/1000, Training Loss: 0.0016877218709589574, Test Loss: 0.001631905401147552\n",
            "Epoch 374/1000, Training Loss: 0.0016593783412792162, Test Loss: 0.0016029317709796026\n",
            "Epoch 375/1000, Training Loss: 0.001630630913917645, Test Loss: 0.0015735791078357307\n",
            "Epoch 376/1000, Training Loss: 0.0016015210349148363, Test Loss: 0.0015438886298387525\n",
            "Epoch 377/1000, Training Loss: 0.001572092532734901, Test Loss: 0.0015139040244966592\n",
            "Epoch 378/1000, Training Loss: 0.0015423913265211215, Test Loss: 0.0014836711644033893\n",
            "Epoch 379/1000, Training Loss: 0.001512465100801878, Test Loss: 0.0014532377982790217\n",
            "Epoch 380/1000, Training Loss: 0.001482362947427371, Test Loss: 0.0014226532177817816\n",
            "Epoch 381/1000, Training Loss: 0.0014521349771640145, Test Loss: 0.0013919679011039473\n",
            "Epoch 382/1000, Training Loss: 0.0014218319053414393, Test Loss: 0.0013612331352737153\n",
            "Epoch 383/1000, Training Loss: 0.0013915046180150166, Test Loss: 0.0013305006202388291\n",
            "Epoch 384/1000, Training Loss: 0.0013612037270328272, Test Loss: 0.0012998220590980727\n",
            "Epoch 385/1000, Training Loss: 0.001330979123948739, Test Loss: 0.001269248740153475\n",
            "Epoch 386/1000, Training Loss: 0.001300879543712552, Test Loss: 0.0012388311176531402\n",
            "Epoch 387/1000, Training Loss: 0.00127095214936719, Test Loss: 0.0012086183990593262\n",
            "Epoch 388/1000, Training Loss: 0.0012412421485410916, Test Loss: 0.0011786581472967097\n",
            "Epoch 389/1000, Training Loss: 0.0012117924513729093, Test Loss: 0.0011489959066260306\n",
            "Epoch 390/1000, Training Loss: 0.0011826433777492491, Test Loss: 0.0011196748604961877\n",
            "Epoch 391/1000, Training Loss: 0.0011538324195360463, Test Loss: 0.001090735528946603\n",
            "Epoch 392/1000, Training Loss: 0.001125394061037097, Test Loss: 0.0010622155119014076\n",
            "Epoch 393/1000, Training Loss: 0.001097359658423978, Test Loss: 0.001034149283102052\n",
            "Epoch 394/1000, Training Loss: 0.001069757376541276, Test Loss: 0.0010065680375845383\n",
            "Epoch 395/1000, Training Loss: 0.0010426121794552892, Test Loss: 0.0009794995936603986\n",
            "Epoch 396/1000, Training Loss: 0.0010159458694933795, Test Loss: 0.0009529683484494092\n",
            "Epoch 397/1000, Training Loss: 0.0009897771683719098, Test Loss: 0.0009269952842653527\n",
            "Epoch 398/1000, Training Loss: 0.0009641218333433279, Test Loss: 0.000901598021678633\n",
            "Epoch 399/1000, Training Loss: 0.000938992801074022, Test Loss: 0.0008767909139397237\n",
            "Epoch 400/1000, Training Loss: 0.000914400352132488, Test Loss: 0.0008525851766794315\n",
            "Epoch 401/1000, Training Loss: 0.0008903522894406131, Test Loss: 0.0008289890464037934\n",
            "Epoch 402/1000, Training Loss: 0.0008668541247326818, Test Loss: 0.0008060079612433732\n",
            "Epoch 403/1000, Training Loss: 0.0008439092678916318, Test Loss: 0.0007836447576480595\n",
            "Epoch 404/1000, Training Loss: 0.0008215192149144541, Test Loss: 0.0007618998771759495\n",
            "Epoch 405/1000, Training Loss: 0.0007996837311366187, Test Loss: 0.0007407715781422325\n",
            "Epoch 406/1000, Training Loss: 0.0007784010271715783, Test Loss: 0.0007202561476066935\n",
            "Epoch 407/1000, Training Loss: 0.0007576679257639583, Test Loss: 0.0007003481099297601\n",
            "Epoch 408/1000, Training Loss: 0.0007374800183957053, Test Loss: 0.0006810404288710759\n",
            "Epoch 409/1000, Training Loss: 0.000717831811015748, Test Loss: 0.0006623247009059441\n",
            "Epoch 410/1000, Training Loss: 0.0006987168586874715, Test Loss: 0.0006441913380703587\n",
            "Epoch 411/1000, Training Loss: 0.0006801278892720654, Test Loss: 0.0006266297392009601\n",
            "Epoch 412/1000, Training Loss: 0.0006620569165010344, Test Loss: 0.0006096284489066366\n",
            "Epoch 413/1000, Training Loss: 0.0006444953429513391, Test Loss: 0.0005931753039945205\n",
            "Epoch 414/1000, Training Loss: 0.0006274340535354493, Test Loss: 0.0005772575673801228\n",
            "Epoch 415/1000, Training Loss: 0.0006108635001694994, Test Loss: 0.0005618620497474828\n",
            "Epoch 416/1000, Training Loss: 0.0005947737782973069, Test Loss: 0.000546975219399511\n",
            "Epoch 417/1000, Training Loss: 0.0005791546959367572, Test Loss: 0.0005325833008615619\n",
            "Epoch 418/1000, Training Loss: 0.0005639958358863855, Test Loss: 0.0005186723628815075\n",
            "Epoch 419/1000, Training Loss: 0.000549286611690502, Test Loss: 0.0005052283965163213\n",
            "Epoch 420/1000, Training Loss: 0.0005350163179162297, Test Loss: 0.0004922373840156317\n",
            "Epoch 421/1000, Training Loss: 0.0005211741752489755, Test Loss: 0.00047968535921346706\n",
            "Epoch 422/1000, Training Loss: 0.0005077493708667759, Test Loss: 0.00046755846012570677\n",
            "Epoch 423/1000, Training Loss: 0.0004947310945103123, Test Loss: 0.00045584297442680826\n",
            "Epoch 424/1000, Training Loss: 0.0004821085706252548, Test Loss: 0.00044452537844860023\n",
            "Epoch 425/1000, Training Loss: 0.00046987108691724913, Test Loss: 0.0004335923703088387\n",
            "Epoch 426/1000, Training Loss: 0.00045800801962749183, Test Loss: 0.0004230308977397698\n",
            "Epoch 427/1000, Training Loss: 0.0004465088558083321, Test Loss: 0.0004128281811485072\n",
            "Epoch 428/1000, Training Loss: 0.0004353632128532942, Test Loss: 0.0004029717324027968\n",
            "Epoch 429/1000, Training Loss: 0.00042456085551393784, Test Loss: 0.000393449369798129\n",
            "Epoch 430/1000, Training Loss: 0.00041409171061676465, Test Loss: 0.00038424922962601403\n",
            "Epoch 431/1000, Training Loss: 0.0004039458796765188, Test Loss: 0.00037535977472859494\n",
            "Epoch 432/1000, Training Loss: 0.0003941136495871145, Test Loss: 0.000366769800391937\n",
            "Epoch 433/1000, Training Loss: 0.00038458550155815824, Test Loss: 0.00035846843789942443\n",
            "Epoch 434/1000, Training Loss: 0.00037535211845287107, Test Loss: 0.0003504451560376522\n",
            "Epoch 435/1000, Training Loss: 0.00036640439067229803, Test Loss: 0.00034268976082016606\n",
            "Epoch 436/1000, Training Loss: 0.00035773342072056966, Test Loss: 0.0003351923936691469\n",
            "Epoch 437/1000, Training Loss: 0.0003493305265766524, Test Loss: 0.0003279435282718402\n",
            "Epoch 438/1000, Training Loss: 0.00034118724398926793, Test Loss: 0.00032093396630678456\n",
            "Epoch 439/1000, Training Loss: 0.00033329532780349035, Test Loss: 0.00031415483221505823\n",
            "Epoch 440/1000, Training Loss: 0.0003256467524198037, Test Loss: 0.00030759756717333966\n",
            "Epoch 441/1000, Training Loss: 0.0003182337114790453, Test Loss: 0.0003012539224087396\n",
            "Epoch 442/1000, Training Loss: 0.0003110486168597898, Test Loss: 0.0002951159519799337\n",
            "Epoch 443/1000, Training Loss: 0.00030408409706807795, Test Loss: 0.00028917600513500195\n",
            "Epoch 444/1000, Training Loss: 0.00029733299509322514, Test Loss: 0.0002834267183435565\n",
            "Epoch 445/1000, Training Loss: 0.0002907883657974694, Test Loss: 0.0002778610070890317\n",
            "Epoch 446/1000, Training Loss: 0.000284443472901725, Test Loss: 0.00027247205749644413\n",
            "Epoch 447/1000, Training Loss: 0.00027829178562426926, Test Loss: 0.00026725331786125827\n",
            "Epoch 448/1000, Training Loss: 0.00027232697502441806, Test Loss: 0.00026219849013646456\n",
            "Epoch 449/1000, Training Loss: 0.0002665429100984128, Test Loss: 0.00025730152142708346\n",
            "Epoch 450/1000, Training Loss: 0.0002609336536704693, Test Loss: 0.00025255659553436203\n",
            "Epoch 451/1000, Training Loss: 0.0002554934581177541, Test Loss: 0.0002479581245856385\n",
            "Epoch 452/1000, Training Loss: 0.0002502167609643655, Test Loss: 0.00024350074078030213\n",
            "Epoch 453/1000, Training Loss: 0.0002450981803757072, Test Loss: 0.0002391792882772361\n",
            "Epoch 454/1000, Training Loss: 0.0002401325105815235, Test Loss: 0.00023498881524477305\n",
            "Epoch 455/1000, Training Loss: 0.00023531471725267078, Test Loss: 0.00023092456609023093\n",
            "Epoch 456/1000, Training Loss: 0.00023063993285400835, Test Loss: 0.00022698197388267133\n",
            "Epoch 457/1000, Training Loss: 0.0002261034519931943, Test Loss: 0.00022315665297948918\n",
            "Epoch 458/1000, Training Loss: 0.00022170072678280776, Test Loss: 0.0002194443918647827\n",
            "Epoch 459/1000, Training Loss: 0.00021742736223105745, Test Loss: 0.00021584114620511755\n",
            "Epoch 460/1000, Training Loss: 0.0002132791116743836, Test Loss: 0.0002123430321263326\n",
            "Epoch 461/1000, Training Loss: 0.00020925187226346893, Test Loss: 0.000208946319713243\n",
            "Epoch 462/1000, Training Loss: 0.00020534168051258972, Test Loss: 0.00020564742673266712\n",
            "Epoch 463/1000, Training Loss: 0.0002015447079206595, Test Loss: 0.0002024429125788332\n",
            "Epoch 464/1000, Training Loss: 0.00019785725667113873, Test Loss: 0.00019932947243926595\n",
            "Epoch 465/1000, Training Loss: 0.00019427575541669637, Test Loss: 0.0001963039316782198\n",
            "Epoch 466/1000, Training Loss: 0.00019079675515345091, Test Loss: 0.00019336324043405607\n",
            "Epoch 467/1000, Training Loss: 0.0001874169251887065, Test Loss: 0.00019050446842630375\n",
            "Epoch 468/1000, Training Loss: 0.00018413304920518464, Test Loss: 0.00018772479996760527\n",
            "Epoch 469/1000, Training Loss: 0.00018094202142404964, Test Loss: 0.000185021529175449\n",
            "Epoch 470/1000, Training Loss: 0.00017784084286830582, Test Loss: 0.000182392055378143\n",
            "Epoch 471/1000, Training Loss: 0.00017482661772758306, Test Loss: 0.00017983387870936657\n",
            "Epoch 472/1000, Training Loss: 0.00017189654982476755, Test Loss: 0.00017734459588539616\n",
            "Epoch 473/1000, Training Loss: 0.00016904793918452916, Test Loss: 0.00017492189615904378\n",
            "Epoch 474/1000, Training Loss: 0.00016627817870335543, Test Loss: 0.00017256355744427497\n",
            "Epoch 475/1000, Training Loss: 0.0001635847509203433, Test Loss: 0.00017026744260542935\n",
            "Epoch 476/1000, Training Loss: 0.00016096522488775184, Test Loss: 0.00016803149590506226\n",
            "Epoch 477/1000, Training Loss: 0.0001584172531400184, Test Loss: 0.00016585373960441004\n",
            "Epoch 478/1000, Training Loss: 0.00015593856875974248, Test Loss: 0.00016373227071059382\n",
            "Epoch 479/1000, Training Loss: 0.00015352698253894543, Test Loss: 0.00016166525786480434\n",
            "Epoch 480/1000, Training Loss: 0.0001511803802337976, Test Loss: 0.0001596509383657726\n",
            "Epoch 481/1000, Training Loss: 0.00014889671991080507, Test Loss: 0.0001576876153230151\n",
            "Epoch 482/1000, Training Loss: 0.00014667402938243468, Test Loss: 0.00015577365493446698\n",
            "Epoch 483/1000, Training Loss: 0.00014451040373001257, Test Loss: 0.00015390748388327338\n",
            "Epoch 484/1000, Training Loss: 0.00014240400291172295, Test Loss: 0.0001520875868486514\n",
            "Epoch 485/1000, Training Loss: 0.00014035304945341886, Test Loss: 0.00015031250412596422\n",
            "Epoch 486/1000, Training Loss: 0.0001383558262200244, Test Loss: 0.00014858082935119895\n",
            "Epoch 487/1000, Training Loss: 0.00013641067426519853, Test Loss: 0.00014689120732533264\n",
            "Epoch 488/1000, Training Loss: 0.00013451599075700498, Test Loss: 0.00014524233193416718\n",
            "Epoch 489/1000, Training Loss: 0.00013267022697727756, Test Loss: 0.00014363294415938228\n",
            "Epoch 490/1000, Training Loss: 0.00013087188639242947, Test Loss: 0.00014206183017674895\n",
            "Epoch 491/1000, Training Loss: 0.00012911952279345156, Test Loss: 0.00014052781953760663\n",
            "Epoch 492/1000, Training Loss: 0.00012741173850286958, Test Loss: 0.00013902978342980875\n",
            "Epoch 493/1000, Training Loss: 0.00012574718264651017, Test Loss: 0.00013756663301461804\n",
            "Epoch 494/1000, Training Loss: 0.00012412454948788735, Test Loss: 0.00013613731783602745\n",
            "Epoch 495/1000, Training Loss: 0.0001225425768231196, Test Loss: 0.00013474082429929083\n",
            "Epoch 496/1000, Training Loss: 0.00012100004443432941, Test Loss: 0.00013337617421544396\n",
            "Epoch 497/1000, Training Loss: 0.0001194957725994918, Test Loss: 0.00013204242340886596\n",
            "Epoch 498/1000, Training Loss: 0.00011802862065676564, Test Loss: 0.00013073866038494303\n",
            "Epoch 499/1000, Training Loss: 0.00011659748562142802, Test Loss: 0.0001294640050551604\n",
            "Epoch 500/1000, Training Loss: 0.00011520130085348729, Test Loss: 0.00012821760751691383\n",
            "Epoch 501/1000, Training Loss: 0.00011383903477421859, Test Loss: 0.00012699864688560896\n",
            "Epoch 502/1000, Training Loss: 0.00011250968962983466, Test Loss: 0.0001258063301766149\n",
            "Epoch 503/1000, Training Loss: 0.0001112123003005939, Test Loss: 0.00012463989123481719\n",
            "Epoch 504/1000, Training Loss: 0.00010994593315368555, Test Loss: 0.00012349858970958316\n",
            "Epoch 505/1000, Training Loss: 0.0001087096849382919, Test Loss: 0.0001223817100730716\n",
            "Epoch 506/1000, Training Loss: 0.00010750268172128886, Test Loss: 0.00012128856067991033\n",
            "Epoch 507/1000, Training Loss: 0.00010632407786205678, Test Loss: 0.00012021847286638104\n",
            "Epoch 508/1000, Training Loss: 0.00010517305502498192, Test Loss: 0.00011917080008726979\n",
            "Epoch 509/1000, Training Loss: 0.00010404882122823009, Test Loss: 0.0001181449170887294\n",
            "Epoch 510/1000, Training Loss: 0.0001029506099274363, Test Loss: 0.00011714021911547688\n",
            "Epoch 511/1000, Training Loss: 0.00010187767913302426, Test Loss: 0.00011615612115081272\n",
            "Epoch 512/1000, Training Loss: 0.00010082931055987164, Test Loss: 0.00011519205718796551\n",
            "Epoch 513/1000, Training Loss: 9.98048088081264e-05, Test Loss: 0.00011424747953135118\n",
            "Epoch 514/1000, Training Loss: 9.880350057398674e-05, Test Loss: 0.0001133218581264131\n",
            "Epoch 515/1000, Training Loss: 9.782473388932669e-05, Test Loss: 0.00011241467991676078\n",
            "Epoch 516/1000, Training Loss: 9.686787738907796e-05, Test Loss: 0.00011152544822740436\n",
            "Epoch 517/1000, Training Loss: 9.593231960531634e-05, Test Loss: 0.00011065368217290525\n",
            "Epoch 518/1000, Training Loss: 9.501746828704228e-05, Test Loss: 0.00010979891608934209\n",
            "Epoch 519/1000, Training Loss: 9.412274974470049e-05, Test Loss: 0.00010896069898906147\n",
            "Epoch 520/1000, Training Loss: 9.324760821847539e-05, Test Loss: 0.00010813859403718289\n",
            "Epoch 521/1000, Training Loss: 9.23915052694899e-05, Test Loss: 0.00010733217804891536\n",
            "Epoch 522/1000, Training Loss: 9.155391919302722e-05, Test Loss: 0.00010654104100677405\n",
            "Epoch 523/1000, Training Loss: 9.073434445294435e-05, Test Loss: 0.00010576478559683887\n",
            "Epoch 524/1000, Training Loss: 8.993229113648735e-05, Test Loss: 0.00010500302676321623\n",
            "Epoch 525/1000, Training Loss: 8.914728442871338e-05, Test Loss: 0.00010425539127990359\n",
            "Epoch 526/1000, Training Loss: 8.837886410581345e-05, Test Loss: 0.00010352151733935063\n",
            "Epoch 527/1000, Training Loss: 8.762658404658433e-05, Test Loss: 0.00010280105415695341\n",
            "Epoch 528/1000, Training Loss: 8.689001176139999e-05, Test Loss: 0.00010209366159081971\n",
            "Epoch 529/1000, Training Loss: 8.616872793799832e-05, Test Loss: 0.00010139900977615246\n",
            "Epoch 530/1000, Training Loss: 8.54623260034645e-05, Test Loss: 0.0001007167787736369\n",
            "Epoch 531/1000, Training Loss: 8.47704117017974e-05, Test Loss: 0.00010004665823122057\n",
            "Epoch 532/1000, Training Loss: 8.409260268647095e-05, Test Loss: 9.938834705875758e-05\n",
            "Epoch 533/1000, Training Loss: 8.342852812743664e-05, Test Loss: 9.874155311491187e-05\n",
            "Epoch 534/1000, Training Loss: 8.277782833201502e-05, Test Loss: 9.810599290590045e-05\n",
            "Epoch 535/1000, Training Loss: 8.214015437916509e-05, Test Loss: 9.748139129548395e-05\n",
            "Epoch 536/1000, Training Loss: 8.151516776662783e-05, Test Loss: 9.686748122581501e-05\n",
            "Epoch 537/1000, Training Loss: 8.090254007047171e-05, Test Loss: 9.626400344865664e-05\n",
            "Epoch 538/1000, Training Loss: 8.030195261656128e-05, Test Loss: 9.567070626655346e-05\n",
            "Epoch 539/1000, Training Loss: 7.971309616352236e-05, Test Loss: 9.50873452835518e-05\n",
            "Epoch 540/1000, Training Loss: 7.913567059677746e-05, Test Loss: 9.451368316506508e-05\n",
            "Epoch 541/1000, Training Loss: 7.856938463322097e-05, Test Loss: 9.394948940654688e-05\n",
            "Epoch 542/1000, Training Loss: 7.801395553616706e-05, Test Loss: 9.339454011055988e-05\n",
            "Epoch 543/1000, Training Loss: 7.746910884017141e-05, Test Loss: 9.284861777195653e-05\n",
            "Epoch 544/1000, Training Loss: 7.693457808537393e-05, Test Loss: 9.231151107081511e-05\n",
            "Epoch 545/1000, Training Loss: 7.641010456101213e-05, Test Loss: 9.178301467283306e-05\n",
            "Epoch 546/1000, Training Loss: 7.589543705775688e-05, Test Loss: 9.126292903687167e-05\n",
            "Epoch 547/1000, Training Loss: 7.539033162856414e-05, Test Loss: 9.075106022937676e-05\n",
            "Epoch 548/1000, Training Loss: 7.489455135771574e-05, Test Loss: 9.024721974540611e-05\n",
            "Epoch 549/1000, Training Loss: 7.44078661377663e-05, Test Loss: 8.97512243359802e-05\n",
            "Epoch 550/1000, Training Loss: 7.393005245409648e-05, Test Loss: 8.926289584155511e-05\n",
            "Epoch 551/1000, Training Loss: 7.346089317679331e-05, Test Loss: 8.878206103132431e-05\n",
            "Epoch 552/1000, Training Loss: 7.300017735960884e-05, Test Loss: 8.830855144816562e-05\n",
            "Epoch 553/1000, Training Loss: 7.254770004572618e-05, Test Loss: 8.784220325899097e-05\n",
            "Epoch 554/1000, Training Loss: 7.210326208009008e-05, Test Loss: 8.738285711030543e-05\n",
            "Epoch 555/1000, Training Loss: 7.16666699280683e-05, Test Loss: 8.693035798875838e-05\n",
            "Epoch 556/1000, Training Loss: 7.123773550021884e-05, Test Loss: 8.648455508651936e-05\n",
            "Epoch 557/1000, Training Loss: 7.081627598293196e-05, Test Loss: 8.604530167126722e-05\n",
            "Epoch 558/1000, Training Loss: 7.040211367475366e-05, Test Loss: 8.561245496064595e-05\n",
            "Epoch 559/1000, Training Loss: 6.999507582817671e-05, Test Loss: 8.518587600099797e-05\n",
            "Epoch 560/1000, Training Loss: 6.959499449670397e-05, Test Loss: 8.476542955022447e-05\n",
            "Epoch 561/1000, Training Loss: 6.920170638700155e-05, Test Loss: 8.43509839646101e-05\n",
            "Epoch 562/1000, Training Loss: 6.881505271595955e-05, Test Loss: 8.394241108947118e-05\n",
            "Epoch 563/1000, Training Loss: 6.843487907248634e-05, Test Loss: 8.353958615348446e-05\n",
            "Epoch 564/1000, Training Loss: 6.806103528386792e-05, Test Loss: 8.314238766655737e-05\n",
            "Epoch 565/1000, Training Loss: 6.769337528653608e-05, Test Loss: 8.275069732111806e-05\n",
            "Epoch 566/1000, Training Loss: 6.733175700108406e-05, Test Loss: 8.236439989667186e-05\n",
            "Epoch 567/1000, Training Loss: 6.697604221138372e-05, Test Loss: 8.198338316754899e-05\n",
            "Epoch 568/1000, Training Loss: 6.662609644766262e-05, Test Loss: 8.160753781368879e-05\n",
            "Epoch 569/1000, Training Loss: 6.628178887340043e-05, Test Loss: 8.123675733436069e-05\n",
            "Epoch 570/1000, Training Loss: 6.594299217591223e-05, Test Loss: 8.087093796473879e-05\n",
            "Epoch 571/1000, Training Loss: 6.560958246048782e-05, Test Loss: 8.050997859518831e-05\n",
            "Epoch 572/1000, Training Loss: 6.528143914797024e-05, Test Loss: 8.01537806931903e-05\n",
            "Epoch 573/1000, Training Loss: 6.495844487565043e-05, Test Loss: 7.980224822782075e-05\n",
            "Epoch 574/1000, Training Loss: 6.464048540135753e-05, Test Loss: 7.945528759664874e-05\n",
            "Epoch 575/1000, Training Loss: 6.432744951064878e-05, Test Loss: 7.911280755503297e-05\n",
            "Epoch 576/1000, Training Loss: 6.40192289269783e-05, Test Loss: 7.877471914766487e-05\n",
            "Epoch 577/1000, Training Loss: 6.371571822475518e-05, Test Loss: 7.844093564233549e-05\n",
            "Epoch 578/1000, Training Loss: 6.341681474517836e-05, Test Loss: 7.811137246579864e-05\n",
            "Epoch 579/1000, Training Loss: 6.312241851476379e-05, Test Loss: 7.778594714169839e-05\n",
            "Epoch 580/1000, Training Loss: 6.283243216647283e-05, Test Loss: 7.746457923046732e-05\n",
            "Epoch 581/1000, Training Loss: 6.254676086334285e-05, Test Loss: 7.71471902711357e-05\n",
            "Epoch 582/1000, Training Loss: 6.226531222454948e-05, Test Loss: 7.683370372496124e-05\n",
            "Epoch 583/1000, Training Loss: 6.19879962538024e-05, Test Loss: 7.652404492086381e-05\n",
            "Epoch 584/1000, Training Loss: 6.171472527000935e-05, Test Loss: 7.62181410025587e-05\n",
            "Epoch 585/1000, Training Loss: 6.144541384012903e-05, Test Loss: 7.591592087734325e-05\n",
            "Epoch 586/1000, Training Loss: 6.117997871412967e-05, Test Loss: 7.561731516650584e-05\n",
            "Epoch 587/1000, Training Loss: 6.091833876199891e-05, Test Loss: 7.532225615724526e-05\n",
            "Epoch 588/1000, Training Loss: 6.0660414912716284e-05, Test Loss: 7.503067775612385e-05\n",
            "Epoch 589/1000, Training Loss: 6.0406130095145123e-05, Test Loss: 7.474251544392925e-05\n",
            "Epoch 590/1000, Training Loss: 6.0155409180761684e-05, Test Loss: 7.445770623195038e-05\n",
            "Epoch 591/1000, Training Loss: 5.990817892816813e-05, Test Loss: 7.417618861958911e-05\n",
            "Epoch 592/1000, Training Loss: 5.966436792933004e-05, Test Loss: 7.389790255328366e-05\n",
            "Epoch 593/1000, Training Loss: 5.942390655748078e-05, Test Loss: 7.362278938667947e-05\n",
            "Epoch 594/1000, Training Loss: 5.918672691663773e-05, Test Loss: 7.335079184202956e-05\n",
            "Epoch 595/1000, Training Loss: 5.8952762792675345e-05, Test Loss: 7.308185397275677e-05\n",
            "Epoch 596/1000, Training Loss: 5.872194960590366e-05, Test Loss: 7.281592112715823e-05\n",
            "Epoch 597/1000, Training Loss: 5.849422436511174e-05, Test Loss: 7.255293991322297e-05\n",
            "Epoch 598/1000, Training Loss: 5.826952562300926e-05, Test Loss: 7.229285816448628e-05\n",
            "Epoch 599/1000, Training Loss: 5.804779343304137e-05, Test Loss: 7.203562490693613e-05\n",
            "Epoch 600/1000, Training Loss: 5.7828969307518374e-05, Test Loss: 7.178119032689493e-05\n",
            "Epoch 601/1000, Training Loss: 5.7612996177024026e-05, Test Loss: 7.152950573988604e-05\n",
            "Epoch 602/1000, Training Loss: 5.739981835105078e-05, Test Loss: 7.128052356041015e-05\n",
            "Epoch 603/1000, Training Loss: 5.7189381479841433e-05, Test Loss: 7.103419727263392e-05\n",
            "Epoch 604/1000, Training Loss: 5.698163251737585e-05, Test Loss: 7.079048140194506e-05\n",
            "Epoch 605/1000, Training Loss: 5.6776519685481965e-05, Test Loss: 7.05493314873478e-05\n",
            "Epoch 606/1000, Training Loss: 5.657399243902632e-05, Test Loss: 7.031070405469482e-05\n",
            "Epoch 607/1000, Training Loss: 5.6374001432154524e-05, Test Loss: 7.007455659067036e-05\n",
            "Epoch 608/1000, Training Loss: 5.617649848554404e-05, Test Loss: 6.984084751758318e-05\n",
            "Epoch 609/1000, Training Loss: 5.5981436554635796e-05, Test Loss: 6.960953616886745e-05\n",
            "Epoch 610/1000, Training Loss: 5.578876969882014e-05, Test Loss: 6.938058276531105e-05\n",
            "Epoch 611/1000, Training Loss: 5.559845305153869e-05, Test Loss: 6.915394839197833e-05\n",
            "Epoch 612/1000, Training Loss: 5.541044279127857e-05, Test Loss: 6.892959497580552e-05\n",
            "Epoch 613/1000, Training Loss: 5.5224696113427804e-05, Test Loss: 6.870748526384498e-05\n",
            "Epoch 614/1000, Training Loss: 5.5041171202963024e-05, Test Loss: 6.848758280214091e-05\n",
            "Epoch 615/1000, Training Loss: 5.485982720794875e-05, Test Loss: 6.826985191520974e-05\n",
            "Epoch 616/1000, Training Loss: 5.468062421381475e-05, Test Loss: 6.805425768612105e-05\n",
            "Epoch 617/1000, Training Loss: 5.4503523218396035e-05, Test Loss: 6.784076593715147e-05\n",
            "Epoch 618/1000, Training Loss: 5.43284861077022e-05, Test Loss: 6.762934321097577e-05\n",
            "Epoch 619/1000, Training Loss: 5.4155475632400435e-05, Test Loss: 6.74199567524222e-05\n",
            "Epoch 620/1000, Training Loss: 5.398445538498471e-05, Test Loss: 6.721257449072484e-05\n",
            "Epoch 621/1000, Training Loss: 5.381538977761171e-05, Test Loss: 6.700716502230077e-05\n",
            "Epoch 622/1000, Training Loss: 5.3648244020586214e-05, Test Loss: 6.680369759399676e-05\n",
            "Epoch 623/1000, Training Loss: 5.348298410146635e-05, Test Loss: 6.6602142086826e-05\n",
            "Epoch 624/1000, Training Loss: 5.33195767647805e-05, Test Loss: 6.640246900015896e-05\n",
            "Epoch 625/1000, Training Loss: 5.315798949232682e-05, Test Loss: 6.620464943635579e-05\n",
            "Epoch 626/1000, Training Loss: 5.2998190484046264e-05, Test Loss: 6.600865508583025e-05\n",
            "Epoch 627/1000, Training Loss: 5.284014863944352e-05, Test Loss: 6.581445821254789e-05\n",
            "Epoch 628/1000, Training Loss: 5.2683833539543415e-05, Test Loss: 6.562203163990428e-05\n",
            "Epoch 629/1000, Training Loss: 5.2529215429366754e-05, Test Loss: 6.543134873701597e-05\n",
            "Epoch 630/1000, Training Loss: 5.2376265200904587e-05, Test Loss: 6.524238340538796e-05\n",
            "Epoch 631/1000, Training Loss: 5.2224954376582374e-05, Test Loss: 6.505511006595329e-05\n",
            "Epoch 632/1000, Training Loss: 5.2075255093193183e-05, Test Loss: 6.486950364645815e-05\n",
            "Epoch 633/1000, Training Loss: 5.192714008628898e-05, Test Loss: 6.46855395692144e-05\n",
            "Epoch 634/1000, Training Loss: 5.178058267501333e-05, Test Loss: 6.450319373917904e-05\n",
            "Epoch 635/1000, Training Loss: 5.1635556747369096e-05, Test Loss: 6.432244253235891e-05\n",
            "Epoch 636/1000, Training Loss: 5.149203674589213e-05, Test Loss: 6.414326278453943e-05\n",
            "Epoch 637/1000, Training Loss: 5.134999765373848e-05, Test Loss: 6.396563178032084e-05\n",
            "Epoch 638/1000, Training Loss: 5.1209414981158734e-05, Test Loss: 6.378952724244467e-05\n",
            "Epoch 639/1000, Training Loss: 5.107026475235192e-05, Test Loss: 6.36149273214233e-05\n",
            "Epoch 640/1000, Training Loss: 5.093252349268732e-05, Test Loss: 6.34418105854391e-05\n",
            "Epoch 641/1000, Training Loss: 5.079616821628545e-05, Test Loss: 6.327015601052809e-05\n",
            "Epoch 642/1000, Training Loss: 5.066117641394072e-05, Test Loss: 6.309994297101418e-05\n",
            "Epoch 643/1000, Training Loss: 5.052752604138437e-05, Test Loss: 6.293115123021605e-05\n",
            "Epoch 644/1000, Training Loss: 5.039519550786965e-05, Test Loss: 6.276376093139287e-05\n",
            "Epoch 645/1000, Training Loss: 5.026416366507603e-05, Test Loss: 6.259775258893701e-05\n",
            "Epoch 646/1000, Training Loss: 5.0134409796315654e-05, Test Loss: 6.243310707979809e-05\n",
            "Epoch 647/1000, Training Loss: 5.0005913606041066e-05, Test Loss: 6.22698056351425e-05\n",
            "Epoch 648/1000, Training Loss: 4.98786552096378e-05, Test Loss: 6.210782983222944e-05\n",
            "Epoch 649/1000, Training Loss: 4.975261512349959e-05, Test Loss: 6.19471615864937e-05\n",
            "Epoch 650/1000, Training Loss: 4.962777425537267e-05, Test Loss: 6.178778314386358e-05\n",
            "Epoch 651/1000, Training Loss: 4.950411389496456e-05, Test Loss: 6.162967707324955e-05\n",
            "Epoch 652/1000, Training Loss: 4.938161570480831e-05, Test Loss: 6.147282625924876e-05\n",
            "Epoch 653/1000, Training Loss: 4.926026171137418e-05, Test Loss: 6.131721389504037e-05\n",
            "Epoch 654/1000, Training Loss: 4.914003429642293e-05, Test Loss: 6.116282347545804e-05\n",
            "Epoch 655/1000, Training Loss: 4.9020916188592185e-05, Test Loss: 6.100963879025827e-05\n",
            "Epoch 656/1000, Training Loss: 4.890289045520868e-05, Test Loss: 6.085764391753855e-05\n",
            "Epoch 657/1000, Training Loss: 4.878594049432074e-05, Test Loss: 6.070682321735443e-05\n",
            "Epoch 658/1000, Training Loss: 4.867005002694549e-05, Test Loss: 6.0557161325480145e-05\n",
            "Epoch 659/1000, Training Loss: 4.8555203089523035e-05, Test Loss: 6.040864314734711e-05\n",
            "Epoch 660/1000, Training Loss: 4.844138402657089e-05, Test Loss: 6.0261253852114816e-05\n",
            "Epoch 661/1000, Training Loss: 4.832857748353407e-05, Test Loss: 6.011497886692354e-05\n",
            "Epoch 662/1000, Training Loss: 4.8216768399825655e-05, Test Loss: 5.9969803871273944e-05\n",
            "Epoch 663/1000, Training Loss: 4.810594200205168e-05, Test Loss: 5.9825714791552815e-05\n",
            "Epoch 664/1000, Training Loss: 4.799608379741341e-05, Test Loss: 5.9682697795697454e-05\n",
            "Epoch 665/1000, Training Loss: 4.78871795672836e-05, Test Loss: 5.9540739288007374e-05\n",
            "Epoch 666/1000, Training Loss: 4.7779215360954314e-05, Test Loss: 5.9399825904068157e-05\n",
            "Epoch 667/1000, Training Loss: 4.7672177489543625e-05, Test Loss: 5.925994450582224e-05\n",
            "Epoch 668/1000, Training Loss: 4.756605252006536e-05, Test Loss: 5.9121082176741633e-05\n",
            "Epoch 669/1000, Training Loss: 4.746082726965257e-05, Test Loss: 5.898322621715023e-05\n",
            "Epoch 670/1000, Training Loss: 4.735648879992921e-05, Test Loss: 5.8846364139632474e-05\n",
            "Epoch 671/1000, Training Loss: 4.725302441152904e-05, Test Loss: 5.871048366458041e-05\n",
            "Epoch 672/1000, Training Loss: 4.7150421638757153e-05, Test Loss: 5.857557271583839e-05\n",
            "Epoch 673/1000, Training Loss: 4.704866824438742e-05, Test Loss: 5.8441619416461785e-05\n",
            "Epoch 674/1000, Training Loss: 4.694775221459447e-05, Test Loss: 5.830861208457698e-05\n",
            "Epoch 675/1000, Training Loss: 4.6847661754013946e-05, Test Loss: 5.817653922933535e-05\n",
            "Epoch 676/1000, Training Loss: 4.674838528093207e-05, Test Loss: 5.8045389546995034e-05\n",
            "Epoch 677/1000, Training Loss: 4.6649911422595456e-05, Test Loss: 5.7915151917061386e-05\n",
            "Epoch 678/1000, Training Loss: 4.655222901064107e-05, Test Loss: 5.778581539854632e-05\n",
            "Epoch 679/1000, Training Loss: 4.645532707664079e-05, Test Loss: 5.765736922631525e-05\n",
            "Epoch 680/1000, Training Loss: 4.635919484776023e-05, Test Loss: 5.752980280751085e-05\n",
            "Epoch 681/1000, Training Loss: 4.626382174252705e-05, Test Loss: 5.740310571807857e-05\n",
            "Epoch 682/1000, Training Loss: 4.616919736670347e-05, Test Loss: 5.7277267699367645e-05\n",
            "Epoch 683/1000, Training Loss: 4.6075311509264044e-05, Test Loss: 5.715227865480996e-05\n",
            "Epoch 684/1000, Training Loss: 4.598215413847317e-05, Test Loss: 5.7028128646692423e-05\n",
            "Epoch 685/1000, Training Loss: 4.588971539806153e-05, Test Loss: 5.690480789299028e-05\n",
            "Epoch 686/1000, Training Loss: 4.579798560349545e-05, Test Loss: 5.678230676428707e-05\n",
            "Epoch 687/1000, Training Loss: 4.5706955238340196e-05, Test Loss: 5.666061578075923e-05\n",
            "Epoch 688/1000, Training Loss: 4.561661495071312e-05, Test Loss: 5.653972560924531e-05\n",
            "Epoch 689/1000, Training Loss: 4.5526955549823055e-05, Test Loss: 5.641962706036846e-05\n",
            "Epoch 690/1000, Training Loss: 4.5437968002596224e-05, Test Loss: 5.630031108573496e-05\n",
            "Epoch 691/1000, Training Loss: 4.5349643430382334e-05, Test Loss: 5.6181768775194644e-05\n",
            "Epoch 692/1000, Training Loss: 4.5261973105743675e-05, Test Loss: 5.606399135417867e-05\n",
            "Epoch 693/1000, Training Loss: 4.517494844932214e-05, Test Loss: 5.594697018107637e-05\n",
            "Epoch 694/1000, Training Loss: 4.508856102677909e-05, Test Loss: 5.583069674468989e-05\n",
            "Epoch 695/1000, Training Loss: 4.500280254581336e-05, Test Loss: 5.5715162661742065e-05\n",
            "Epoch 696/1000, Training Loss: 4.491766485324848e-05, Test Loss: 5.560035967444722e-05\n",
            "Epoch 697/1000, Training Loss: 4.483313993219097e-05, Test Loss: 5.548627964812329e-05\n",
            "Epoch 698/1000, Training Loss: 4.474921989925569e-05, Test Loss: 5.537291456888119e-05\n",
            "Epoch 699/1000, Training Loss: 4.466589700186052e-05, Test Loss: 5.526025654134326e-05\n",
            "Epoch 700/1000, Training Loss: 4.4583163615580906e-05, Test Loss: 5.514829778642883e-05\n",
            "Epoch 701/1000, Training Loss: 4.450101224157088e-05, Test Loss: 5.5037030639189775e-05\n",
            "Epoch 702/1000, Training Loss: 4.4419435504043624e-05, Test Loss: 5.4926447546684305e-05\n",
            "Epoch 703/1000, Training Loss: 4.433842614781147e-05, Test Loss: 5.4816541065907666e-05\n",
            "Epoch 704/1000, Training Loss: 4.425797703588387e-05, Test Loss: 5.470730386177101e-05\n",
            "Epoch 705/1000, Training Loss: 4.4178081147122736e-05, Test Loss: 5.459872870511496e-05\n",
            "Epoch 706/1000, Training Loss: 4.409873157395043e-05, Test Loss: 5.449080847077633e-05\n",
            "Epoch 707/1000, Training Loss: 4.401992152011288e-05, Test Loss: 5.4383536135702536e-05\n",
            "Epoch 708/1000, Training Loss: 4.3941644298495413e-05, Test Loss: 5.4276904777087236e-05\n",
            "Epoch 709/1000, Training Loss: 4.386389332898612e-05, Test Loss: 5.417090757056946e-05\n",
            "Epoch 710/1000, Training Loss: 4.3786662136391374e-05, Test Loss: 5.406553778846585e-05\n",
            "Epoch 711/1000, Training Loss: 4.3709944348398604e-05, Test Loss: 5.396078879803186e-05\n",
            "Epoch 712/1000, Training Loss: 4.363373369358495e-05, Test Loss: 5.38566540597701e-05\n",
            "Epoch 713/1000, Training Loss: 4.355802399947186e-05, Test Loss: 5.3753127125776e-05\n",
            "Epoch 714/1000, Training Loss: 4.348280919062418e-05, Test Loss: 5.365020163812041e-05\n",
            "Epoch 715/1000, Training Loss: 4.3408083286793425e-05, Test Loss: 5.354787132725465e-05\n",
            "Epoch 716/1000, Training Loss: 4.3333840401100246e-05, Test Loss: 5.3446130010462604e-05\n",
            "Epoch 717/1000, Training Loss: 4.326007473826187e-05, Test Loss: 5.334497159035017e-05\n",
            "Epoch 718/1000, Training Loss: 4.318678059285561e-05, Test Loss: 5.3244390053345716e-05\n",
            "Epoch 719/1000, Training Loss: 4.3113952347623605e-05, Test Loss: 5.314437946825547e-05\n",
            "Epoch 720/1000, Training Loss: 4.304158447181573e-05, Test Loss: 5.304493398484065e-05\n",
            "Epoch 721/1000, Training Loss: 4.2969671519566944e-05, Test Loss: 5.2946047832420456e-05\n",
            "Epoch 722/1000, Training Loss: 4.2898208128312634e-05, Test Loss: 5.284771531850906e-05\n",
            "Epoch 723/1000, Training Loss: 4.2827189017239206e-05, Test Loss: 5.274993082747689e-05\n",
            "Epoch 724/1000, Training Loss: 4.275660898576832e-05, Test Loss: 5.265268881925132e-05\n",
            "Epoch 725/1000, Training Loss: 4.268646291207367e-05, Test Loss: 5.255598382803028e-05\n",
            "Epoch 726/1000, Training Loss: 4.261674575163102e-05, Test Loss: 5.2459810461031864e-05\n",
            "Epoch 727/1000, Training Loss: 4.2547452535801723e-05, Test Loss: 5.2364163397264606e-05\n",
            "Epoch 728/1000, Training Loss: 4.2478578370442626e-05, Test Loss: 5.226903738633025e-05\n",
            "Epoch 729/1000, Training Loss: 4.241011843455003e-05, Test Loss: 5.217442724724112e-05\n",
            "Epoch 730/1000, Training Loss: 4.234206797893224e-05, Test Loss: 5.2080327867264936e-05\n",
            "Epoch 731/1000, Training Loss: 4.227442232490908e-05, Test Loss: 5.198673420080427e-05\n",
            "Epoch 732/1000, Training Loss: 4.220717686304122e-05, Test Loss: 5.189364126828163e-05\n",
            "Epoch 733/1000, Training Loss: 4.2140327051885526e-05, Test Loss: 5.180104415505639e-05\n",
            "Epoch 734/1000, Training Loss: 4.2073868416776954e-05, Test Loss: 5.1708938010361726e-05\n",
            "Epoch 735/1000, Training Loss: 4.200779654863801e-05, Test Loss: 5.161731804627201e-05\n",
            "Epoch 736/1000, Training Loss: 4.19421071028117e-05, Test Loss: 5.15261795366679e-05\n",
            "Epoch 737/1000, Training Loss: 4.187679579791816e-05, Test Loss: 5.143551781624741e-05\n",
            "Epoch 738/1000, Training Loss: 4.181185841473958e-05, Test Loss: 5.1345328279544564e-05\n",
            "Epoch 739/1000, Training Loss: 4.174729079512321e-05, Test Loss: 5.125560637995876e-05\n",
            "Epoch 740/1000, Training Loss: 4.16830888409114e-05, Test Loss: 5.1166347628831534e-05\n",
            "Epoch 741/1000, Training Loss: 4.161924851289217e-05, Test Loss: 5.107754759450421e-05\n",
            "Epoch 742/1000, Training Loss: 4.1555765829771605e-05, Test Loss: 5.098920190142544e-05\n",
            "Epoch 743/1000, Training Loss: 4.1492636867168406e-05, Test Loss: 5.090130622925918e-05\n",
            "Epoch 744/1000, Training Loss: 4.1429857756626956e-05, Test Loss: 5.0813856312017764e-05\n",
            "Epoch 745/1000, Training Loss: 4.136742468465367e-05, Test Loss: 5.07268479371986e-05\n",
            "Epoch 746/1000, Training Loss: 4.130533389177226e-05, Test Loss: 5.064027694495997e-05\n",
            "Epoch 747/1000, Training Loss: 4.1243581671596006e-05, Test Loss: 5.05541392272954e-05\n",
            "Epoch 748/1000, Training Loss: 4.118216436992134e-05, Test Loss: 5.0468430727227104e-05\n",
            "Epoch 749/1000, Training Loss: 4.112107838384028e-05, Test Loss: 5.038314743801473e-05\n",
            "Epoch 750/1000, Training Loss: 4.1060320160868604e-05, Test Loss: 5.029828540238668e-05\n",
            "Epoch 751/1000, Training Loss: 4.099988619809196e-05, Test Loss: 5.0213840711778245e-05\n",
            "Epoch 752/1000, Training Loss: 4.093977304133224e-05, Test Loss: 5.01298095055784e-05\n",
            "Epoch 753/1000, Training Loss: 4.0879977284325506e-05, Test Loss: 5.00461879704143e-05\n",
            "Epoch 754/1000, Training Loss: 4.0820495567921035e-05, Test Loss: 4.996297233941344e-05\n",
            "Epoch 755/1000, Training Loss: 4.076132457929381e-05, Test Loss: 4.988015889151593e-05\n",
            "Epoch 756/1000, Training Loss: 4.0702461051173575e-05, Test Loss: 4.979774395077767e-05\n",
            "Epoch 757/1000, Training Loss: 4.064390176108711e-05, Test Loss: 4.9715723885683506e-05\n",
            "Epoch 758/1000, Training Loss: 4.058564353061967e-05, Test Loss: 4.963409510849708e-05\n",
            "Epoch 759/1000, Training Loss: 4.052768322468556e-05, Test Loss: 4.955285407459147e-05\n",
            "Epoch 760/1000, Training Loss: 4.0470017750817753e-05, Test Loss: 4.9471997281819336e-05\n",
            "Epoch 761/1000, Training Loss: 4.0412644058468144e-05, Test Loss: 4.9391521269874516e-05\n",
            "Epoch 762/1000, Training Loss: 4.03555591383241e-05, Test Loss: 4.931142261967923e-05\n",
            "Epoch 763/1000, Training Loss: 4.029876002163471e-05, Test Loss: 4.9231697952777235e-05\n",
            "Epoch 764/1000, Training Loss: 4.024224377955394e-05, Test Loss: 4.915234393072629e-05\n",
            "Epoch 765/1000, Training Loss: 4.0186007522493605e-05, Test Loss: 4.907335725452821e-05\n",
            "Epoch 766/1000, Training Loss: 4.013004839948981e-05, Test Loss: 4.899473466404333e-05\n",
            "Epoch 767/1000, Training Loss: 4.007436359758207e-05, Test Loss: 4.8916472937431795e-05\n",
            "Epoch 768/1000, Training Loss: 4.001895034120291e-05, Test Loss: 4.883856889059156e-05\n",
            "Epoch 769/1000, Training Loss: 3.9963805891579717e-05, Test Loss: 4.8761019376616295e-05\n",
            "Epoch 770/1000, Training Loss: 3.990892754614861e-05, Test Loss: 4.8683821285266975e-05\n",
            "Epoch 771/1000, Training Loss: 3.9854312637979146e-05, Test Loss: 4.8606971542437925e-05\n",
            "Epoch 772/1000, Training Loss: 3.9799958535208254e-05, Test Loss: 4.853046710963896e-05\n",
            "Epoch 773/1000, Training Loss: 3.974586264048681e-05, Test Loss: 4.8454304983496296e-05\n",
            "Epoch 774/1000, Training Loss: 3.9692022390435276e-05, Test Loss: 4.837848219524414e-05\n",
            "Epoch 775/1000, Training Loss: 3.9638435255110934e-05, Test Loss: 4.830299581023891e-05\n",
            "Epoch 776/1000, Training Loss: 3.9585098737482716e-05, Test Loss: 4.82278429274835e-05\n",
            "Epoch 777/1000, Training Loss: 3.953201037291834e-05, Test Loss: 4.8153020679142045e-05\n",
            "Epoch 778/1000, Training Loss: 3.9479167728678974e-05, Test Loss: 4.807852623008264e-05\n",
            "Epoch 779/1000, Training Loss: 3.9426568403423814e-05, Test Loss: 4.800435677742021e-05\n",
            "Epoch 780/1000, Training Loss: 3.9374210026724364e-05, Test Loss: 4.793050955006084e-05\n",
            "Epoch 781/1000, Training Loss: 3.932209025858679e-05, Test Loss: 4.78569818082675e-05\n",
            "Epoch 782/1000, Training Loss: 3.927020678898341e-05, Test Loss: 4.778377084321775e-05\n",
            "Epoch 783/1000, Training Loss: 3.921855733739256e-05, Test Loss: 4.77108739765871e-05\n",
            "Epoch 784/1000, Training Loss: 3.916713965234605e-05, Test Loss: 4.763828856011696e-05\n",
            "Epoch 785/1000, Training Loss: 3.911595151098682e-05, Test Loss: 4.7566011975214526e-05\n",
            "Epoch 786/1000, Training Loss: 3.906499071863244e-05, Test Loss: 4.749404163253318e-05\n",
            "Epoch 787/1000, Training Loss: 3.90142551083478e-05, Test Loss: 4.742237497158639e-05\n",
            "Epoch 788/1000, Training Loss: 3.8963742540524354e-05, Test Loss: 4.7351009460347776e-05\n",
            "Epoch 789/1000, Training Loss: 3.8913450902467685e-05, Test Loss: 4.7279942594863754e-05\n",
            "Epoch 790/1000, Training Loss: 3.886337810799283e-05, Test Loss: 4.720917189888141e-05\n",
            "Epoch 791/1000, Training Loss: 3.8813522097024857e-05, Test Loss: 4.713869492346282e-05\n",
            "Epoch 792/1000, Training Loss: 3.8763880835208864e-05, Test Loss: 4.7068509246633195e-05\n",
            "Epoch 793/1000, Training Loss: 3.8714452313525836e-05, Test Loss: 4.699861247300504e-05\n",
            "Epoch 794/1000, Training Loss: 3.866523454791386e-05, Test Loss: 4.692900223343404e-05\n",
            "Epoch 795/1000, Training Loss: 3.861622557889882e-05, Test Loss: 4.6859676184660056e-05\n",
            "Epoch 796/1000, Training Loss: 3.8567423471228975e-05, Test Loss: 4.6790632008969984e-05\n",
            "Epoch 797/1000, Training Loss: 3.851882631351794e-05, Test Loss: 4.672186741385537e-05\n",
            "Epoch 798/1000, Training Loss: 3.8470432217892e-05, Test Loss: 4.6653380131680265e-05\n",
            "Epoch 799/1000, Training Loss: 3.842223931964497e-05, Test Loss: 4.6585167919350986e-05\n",
            "Epoch 800/1000, Training Loss: 3.837424577689787e-05, Test Loss: 4.6517228557998444e-05\n",
            "Epoch 801/1000, Training Loss: 3.83264497702654e-05, Test Loss: 4.644955985265905e-05\n",
            "Epoch 802/1000, Training Loss: 3.8278849502528294e-05, Test Loss: 4.638215963196089e-05\n",
            "Epoch 803/1000, Training Loss: 3.823144319831002e-05, Test Loss: 4.631502574781027e-05\n",
            "Epoch 804/1000, Training Loss: 3.818422910375964e-05, Test Loss: 4.624815607510862e-05\n",
            "Epoch 805/1000, Training Loss: 3.813720548624178e-05, Test Loss: 4.6181548511428386e-05\n",
            "Epoch 806/1000, Training Loss: 3.809037063402736e-05, Test Loss: 4.611520097673726e-05\n",
            "Epoch 807/1000, Training Loss: 3.804372285599555e-05, Test Loss: 4.604911141310908e-05\n",
            "Epoch 808/1000, Training Loss: 3.799726048133517e-05, Test Loss: 4.5983277784424576e-05\n",
            "Epoch 809/1000, Training Loss: 3.795098185925531e-05, Test Loss: 4.591769807611398e-05\n",
            "Epoch 810/1000, Training Loss: 3.790488535869872e-05, Test Loss: 4.585237029486225e-05\n",
            "Epoch 811/1000, Training Loss: 3.785896936805964e-05, Test Loss: 4.578729246834815e-05\n",
            "Epoch 812/1000, Training Loss: 3.781323229490749e-05, Test Loss: 4.5722462644971026e-05\n",
            "Epoch 813/1000, Training Loss: 3.7767672565714885e-05, Test Loss: 4.5657878893596105e-05\n",
            "Epoch 814/1000, Training Loss: 3.772228862559087e-05, Test Loss: 4.559353930328085e-05\n",
            "Epoch 815/1000, Training Loss: 3.7677078938015526e-05, Test Loss: 4.55294419830407e-05\n",
            "Epoch 816/1000, Training Loss: 3.7632041984583326e-05, Test Loss: 4.54655850615765e-05\n",
            "Epoch 817/1000, Training Loss: 3.758717626474706e-05, Test Loss: 4.5401966687041946e-05\n",
            "Epoch 818/1000, Training Loss: 3.7542480295567756e-05, Test Loss: 4.53385850267929e-05\n",
            "Epoch 819/1000, Training Loss: 3.7497952611469786e-05, Test Loss: 4.52754382671456e-05\n",
            "Epoch 820/1000, Training Loss: 3.745359176399619e-05, Test Loss: 4.521252461314976e-05\n",
            "Epoch 821/1000, Training Loss: 3.740939632157249e-05, Test Loss: 4.514984228834485e-05\n",
            "Epoch 822/1000, Training Loss: 3.73653648692706e-05, Test Loss: 4.508738953453346e-05\n",
            "Epoch 823/1000, Training Loss: 3.7321496008579164e-05, Test Loss: 4.502516461155979e-05\n",
            "Epoch 824/1000, Training Loss: 3.7277788357176126e-05, Test Loss: 4.496316579708382e-05\n",
            "Epoch 825/1000, Training Loss: 3.723424054870608e-05, Test Loss: 4.4901391386356195e-05\n",
            "Epoch 826/1000, Training Loss: 3.719085123255922e-05, Test Loss: 4.4839839692014524e-05\n",
            "Epoch 827/1000, Training Loss: 3.7147619073656266e-05, Test Loss: 4.477850904385656e-05\n",
            "Epoch 828/1000, Training Loss: 3.710454275223635e-05, Test Loss: 4.47173977886404e-05\n",
            "Epoch 829/1000, Training Loss: 3.7061620963645626e-05, Test Loss: 4.4656504289873444e-05\n",
            "Epoch 830/1000, Training Loss: 3.701885241813317e-05, Test Loss: 4.459582692760559e-05\n",
            "Epoch 831/1000, Training Loss: 3.6976235840646735e-05, Test Loss: 4.453536409823494e-05\n",
            "Epoch 832/1000, Training Loss: 3.693376997063441e-05, Test Loss: 4.4475114214304386e-05\n",
            "Epoch 833/1000, Training Loss: 3.6891453561847974e-05, Test Loss: 4.4415075704307414e-05\n",
            "Epoch 834/1000, Training Loss: 3.684928538214889e-05, Test Loss: 4.435524701249506e-05\n",
            "Epoch 835/1000, Training Loss: 3.680726421331941e-05, Test Loss: 4.429562659869103e-05\n",
            "Epoch 836/1000, Training Loss: 3.6765388850873556e-05, Test Loss: 4.423621293809938e-05\n",
            "Epoch 837/1000, Training Loss: 3.672365810387403e-05, Test Loss: 4.4177004521118784e-05\n",
            "Epoch 838/1000, Training Loss: 3.6682070794750215e-05, Test Loss: 4.4117999853167916e-05\n",
            "Epoch 839/1000, Training Loss: 3.6640625759119375e-05, Test Loss: 4.405919745449757e-05\n",
            "Epoch 840/1000, Training Loss: 3.6599321845611685e-05, Test Loss: 4.400059586002425e-05\n",
            "Epoch 841/1000, Training Loss: 3.655815791569631e-05, Test Loss: 4.394219361914539e-05\n",
            "Epoch 842/1000, Training Loss: 3.6517132843510256e-05, Test Loss: 4.388398929557581e-05\n",
            "Epoch 843/1000, Training Loss: 3.647624551569175e-05, Test Loss: 4.382598146717381e-05\n",
            "Epoch 844/1000, Training Loss: 3.643549483121434e-05, Test Loss: 4.376816872577636e-05\n",
            "Epoch 845/1000, Training Loss: 3.639487970122425e-05, Test Loss: 4.3710549677032786e-05\n",
            "Epoch 846/1000, Training Loss: 3.635439904887988e-05, Test Loss: 4.365312294024474e-05\n",
            "Epoch 847/1000, Training Loss: 3.6314051809194675e-05, Test Loss: 4.3595887148201694e-05\n",
            "Epoch 848/1000, Training Loss: 3.6273836928880335e-05, Test Loss: 4.353884094703384e-05\n",
            "Epoch 849/1000, Training Loss: 3.623375336619513e-05, Test Loss: 4.348198299603883e-05\n",
            "Epoch 850/1000, Training Loss: 3.6193800090792925e-05, Test Loss: 4.342531196754421e-05\n",
            "Epoch 851/1000, Training Loss: 3.61539760835735e-05, Test Loss: 4.336882654675036e-05\n",
            "Epoch 852/1000, Training Loss: 3.611428033653789e-05, Test Loss: 4.3312525431575346e-05\n",
            "Epoch 853/1000, Training Loss: 3.607471185264317e-05, Test Loss: 4.325640733251902e-05\n",
            "Epoch 854/1000, Training Loss: 3.603526964566134e-05, Test Loss: 4.320047097251022e-05\n",
            "Epoch 855/1000, Training Loss: 3.5995952740038904e-05, Test Loss: 4.3144715086763816e-05\n",
            "Epoch 856/1000, Training Loss: 3.595676017076002e-05, Test Loss: 4.308913842263652e-05\n",
            "Epoch 857/1000, Training Loss: 3.5917690983210545e-05, Test Loss: 4.303373973949787e-05\n",
            "Epoch 858/1000, Training Loss: 3.587874423304339e-05, Test Loss: 4.2978517808584104e-05\n",
            "Epoch 859/1000, Training Loss: 3.5839918986048765e-05, Test Loss: 4.2923471412859976e-05\n",
            "Epoch 860/1000, Training Loss: 3.5801214318023274e-05, Test Loss: 4.286859934689298e-05\n",
            "Epoch 861/1000, Training Loss: 3.576262931464229e-05, Test Loss: 4.2813900416714034e-05\n",
            "Epoch 862/1000, Training Loss: 3.57241630713343e-05, Test Loss: 4.275937343969591e-05\n",
            "Epoch 863/1000, Training Loss: 3.568581469315684e-05, Test Loss: 4.270501724441432e-05\n",
            "Epoch 864/1000, Training Loss: 3.56475832946742e-05, Test Loss: 4.2650830670521415e-05\n",
            "Epoch 865/1000, Training Loss: 3.5609467999837706e-05, Test Loss: 4.2596812568633354e-05\n",
            "Epoch 866/1000, Training Loss: 3.557146794186552e-05, Test Loss: 4.2542961800195063e-05\n",
            "Epoch 867/1000, Training Loss: 3.553358226312702e-05, Test Loss: 4.248927723735704e-05\n",
            "Epoch 868/1000, Training Loss: 3.549581011502696e-05, Test Loss: 4.2435757762863595e-05\n",
            "Epoch 869/1000, Training Loss: 3.545815065789284e-05, Test Loss: 4.2382402269926735e-05\n",
            "Epoch 870/1000, Training Loss: 3.542060306086137e-05, Test Loss: 4.2329209662112806e-05\n",
            "Epoch 871/1000, Training Loss: 3.538316650176972e-05, Test Loss: 4.227617885322549e-05\n",
            "Epoch 872/1000, Training Loss: 3.5345840167046064e-05, Test Loss: 4.2223308767191074e-05\n",
            "Epoch 873/1000, Training Loss: 3.530862325160244e-05, Test Loss: 4.2170598337947335e-05\n",
            "Epoch 874/1000, Training Loss: 3.5271514958730045e-05, Test Loss: 4.211804650932865e-05\n",
            "Epoch 875/1000, Training Loss: 3.523451449999338e-05, Test Loss: 4.2065652234962476e-05\n",
            "Epoch 876/1000, Training Loss: 3.519762109512892e-05, Test Loss: 4.201341447815505e-05\n",
            "Epoch 877/1000, Training Loss: 3.516083397194379e-05, Test Loss: 4.1961332211786584e-05\n",
            "Epoch 878/1000, Training Loss: 3.5124152366215955e-05, Test Loss: 4.1909404418206024e-05\n",
            "Epoch 879/1000, Training Loss: 3.508757552159632e-05, Test Loss: 4.185763008912612e-05\n",
            "Epoch 880/1000, Training Loss: 3.5051102689511564e-05, Test Loss: 4.180600822552118e-05\n",
            "Epoch 881/1000, Training Loss: 3.5014733129067945e-05, Test Loss: 4.1754537837523616e-05\n",
            "Epoch 882/1000, Training Loss: 3.4978466106958446e-05, Test Loss: 4.170321794432506e-05\n",
            "Epoch 883/1000, Training Loss: 3.494230089736916e-05, Test Loss: 4.165204757407957e-05\n",
            "Epoch 884/1000, Training Loss: 3.4906236781887975e-05, Test Loss: 4.1601025763800426e-05\n",
            "Epoch 885/1000, Training Loss: 3.487027304941397e-05, Test Loss: 4.1550151559268086e-05\n",
            "Epoch 886/1000, Training Loss: 3.483440899606866e-05, Test Loss: 4.1499424014933343e-05\n",
            "Epoch 887/1000, Training Loss: 3.479864392510831e-05, Test Loss: 4.1448842193824425e-05\n",
            "Epoch 888/1000, Training Loss: 3.4762977146837914e-05, Test Loss: 4.1398405167448954e-05\n",
            "Epoch 889/1000, Training Loss: 3.472740797852515e-05, Test Loss: 4.13481120157085e-05\n",
            "Epoch 890/1000, Training Loss: 3.4691935744316475e-05, Test Loss: 4.1297961826806786e-05\n",
            "Epoch 891/1000, Training Loss: 3.4656559775154034e-05, Test Loss: 4.1247953697155285e-05\n",
            "Epoch 892/1000, Training Loss: 3.46212794086947e-05, Test Loss: 4.1198086731292905e-05\n",
            "Epoch 893/1000, Training Loss: 3.45860939892283e-05, Test Loss: 4.114836004178937e-05\n",
            "Epoch 894/1000, Training Loss: 3.4551002867598115e-05, Test Loss: 4.109877274916608e-05\n",
            "Epoch 895/1000, Training Loss: 3.451600540112365e-05, Test Loss: 4.104932398180982e-05\n",
            "Epoch 896/1000, Training Loss: 3.448110095352176e-05, Test Loss: 4.100001287588636e-05\n",
            "Epoch 897/1000, Training Loss: 3.444628889483168e-05, Test Loss: 4.095083857525841e-05\n",
            "Epoch 898/1000, Training Loss: 3.4411568601338356e-05, Test Loss: 4.090180023140535e-05\n",
            "Epoch 899/1000, Training Loss: 3.437693945549919e-05, Test Loss: 4.085289700333671e-05\n",
            "Epoch 900/1000, Training Loss: 3.434240084587099e-05, Test Loss: 4.0804128057519305e-05\n",
            "Epoch 901/1000, Training Loss: 3.430795216703674e-05, Test Loss: 4.075549256779833e-05\n",
            "Epoch 902/1000, Training Loss: 3.4273592819534756e-05, Test Loss: 4.070698971530847e-05\n",
            "Epoch 903/1000, Training Loss: 3.4239322209788624e-05, Test Loss: 4.0658618688409616e-05\n",
            "Epoch 904/1000, Training Loss: 3.4205139750038156e-05, Test Loss: 4.061037868260599e-05\n",
            "Epoch 905/1000, Training Loss: 3.417104485826998e-05, Test Loss: 4.056226890046541e-05\n",
            "Epoch 906/1000, Training Loss: 3.413703695815096e-05, Test Loss: 4.051428855155593e-05\n",
            "Epoch 907/1000, Training Loss: 3.410311547896126e-05, Test Loss: 4.046643685236019e-05\n",
            "Epoch 908/1000, Training Loss: 3.4069279855529215e-05, Test Loss: 4.0418713026214276e-05\n",
            "Epoch 909/1000, Training Loss: 3.4035529528165506e-05, Test Loss: 4.037111630322933e-05\n",
            "Epoch 910/1000, Training Loss: 3.400186394259996e-05, Test Loss: 4.032364592022341e-05\n",
            "Epoch 911/1000, Training Loss: 3.396828254991792e-05, Test Loss: 4.0276301120646544e-05\n",
            "Epoch 912/1000, Training Loss: 3.3934784806499096e-05, Test Loss: 4.022908115451738e-05\n",
            "Epoch 913/1000, Training Loss: 3.390137017395541e-05, Test Loss: 4.018198527835536e-05\n",
            "Epoch 914/1000, Training Loss: 3.386803811906988e-05, Test Loss: 4.0135012755109836e-05\n",
            "Epoch 915/1000, Training Loss: 3.383478811373805e-05, Test Loss: 4.00881628540927e-05\n",
            "Epoch 916/1000, Training Loss: 3.3801619634907746e-05, Test Loss: 4.004143485091408e-05\n",
            "Epoch 917/1000, Training Loss: 3.3768532164522145e-05, Test Loss: 3.9994828027423466e-05\n",
            "Epoch 918/1000, Training Loss: 3.373552518946167e-05, Test Loss: 3.994834167163141e-05\n",
            "Epoch 919/1000, Training Loss: 3.370259820148683e-05, Test Loss: 3.990197507765829e-05\n",
            "Epoch 920/1000, Training Loss: 3.366975069718356e-05, Test Loss: 3.985572754566871e-05\n",
            "Epoch 921/1000, Training Loss: 3.3636982177906044e-05, Test Loss: 3.980959838180512e-05\n",
            "Epoch 922/1000, Training Loss: 3.360429214972479e-05, Test Loss: 3.976358689813095e-05\n",
            "Epoch 923/1000, Training Loss: 3.3571680123371075e-05, Test Loss: 3.9717692412566695e-05\n",
            "Epoch 924/1000, Training Loss: 3.353914561418355e-05, Test Loss: 3.967191424883581e-05\n",
            "Epoch 925/1000, Training Loss: 3.3506688142057985e-05, Test Loss: 3.962625173639774e-05\n",
            "Epoch 926/1000, Training Loss: 3.347430723139286e-05, Test Loss: 3.9580704210393656e-05\n",
            "Epoch 927/1000, Training Loss: 3.344200241104107e-05, Test Loss: 3.953527101158996e-05\n",
            "Epoch 928/1000, Training Loss: 3.34097732142572e-05, Test Loss: 3.948995148632094e-05\n",
            "Epoch 929/1000, Training Loss: 3.337761917864916e-05, Test Loss: 3.9444744986428725e-05\n",
            "Epoch 930/1000, Training Loss: 3.3345539846129144e-05, Test Loss: 3.939965086920628e-05\n",
            "Epoch 931/1000, Training Loss: 3.331353476286446e-05, Test Loss: 3.9354668497352244e-05\n",
            "Epoch 932/1000, Training Loss: 3.3281603479230335e-05, Test Loss: 3.930979723890571e-05\n",
            "Epoch 933/1000, Training Loss: 3.3249745549762666e-05, Test Loss: 3.926503646719637e-05\n",
            "Epoch 934/1000, Training Loss: 3.321796053311101e-05, Test Loss: 3.9220385560795565e-05\n",
            "Epoch 935/1000, Training Loss: 3.318624799199292e-05, Test Loss: 3.9175843903454635e-05\n",
            "Epoch 936/1000, Training Loss: 3.315460749314894e-05, Test Loss: 3.9131410884058326e-05\n",
            "Epoch 937/1000, Training Loss: 3.3123038607296716e-05, Test Loss: 3.908708589657561e-05\n",
            "Epoch 938/1000, Training Loss: 3.3091540909088034e-05, Test Loss: 3.9042868340003624e-05\n",
            "Epoch 939/1000, Training Loss: 3.306011397706409e-05, Test Loss: 3.899875761831985e-05\n",
            "Epoch 940/1000, Training Loss: 3.30287573936132e-05, Test Loss: 3.895475314043222e-05\n",
            "Epoch 941/1000, Training Loss: 3.2997470744927166e-05, Test Loss: 3.891085432013118e-05\n",
            "Epoch 942/1000, Training Loss: 3.296625362096079e-05, Test Loss: 3.886706057603938e-05\n",
            "Epoch 943/1000, Training Loss: 3.293510561538882e-05, Test Loss: 3.8823371331564656e-05\n",
            "Epoch 944/1000, Training Loss: 3.290402632556538e-05, Test Loss: 3.8779786014851326e-05\n",
            "Epoch 945/1000, Training Loss: 3.287301535248423e-05, Test Loss: 3.873630405873763e-05\n",
            "Epoch 946/1000, Training Loss: 3.284207230073889e-05, Test Loss: 3.8692924900700286e-05\n",
            "Epoch 947/1000, Training Loss: 3.2811196778481186e-05, Test Loss: 3.864964798281584e-05\n",
            "Epoch 948/1000, Training Loss: 3.278038839738481e-05, Test Loss: 3.8606472751720586e-05\n",
            "Epoch 949/1000, Training Loss: 3.274964677260544e-05, Test Loss: 3.856339865854868e-05\n",
            "Epoch 950/1000, Training Loss: 3.271897152274317e-05, Test Loss: 3.852042515890576e-05\n",
            "Epoch 951/1000, Training Loss: 3.268836226980445e-05, Test Loss: 3.847755171281282e-05\n",
            "Epoch 952/1000, Training Loss: 3.265781863916621e-05, Test Loss: 3.8434777784668104e-05\n",
            "Epoch 953/1000, Training Loss: 3.262734025953825e-05, Test Loss: 3.839210284320611e-05\n",
            "Epoch 954/1000, Training Loss: 3.259692676292727e-05, Test Loss: 3.8349526361452086e-05\n",
            "Epoch 955/1000, Training Loss: 3.256657778460147e-05, Test Loss: 3.830704781667896e-05\n",
            "Epoch 956/1000, Training Loss: 3.253629296305508e-05, Test Loss: 3.826466669036974e-05\n",
            "Epoch 957/1000, Training Loss: 3.2506071939973886e-05, Test Loss: 3.82223824681752e-05\n",
            "Epoch 958/1000, Training Loss: 3.247591436020095e-05, Test Loss: 3.818019463986713e-05\n",
            "Epoch 959/1000, Training Loss: 3.2445819871701304e-05, Test Loss: 3.813810269931006e-05\n",
            "Epoch 960/1000, Training Loss: 3.2415788125530455e-05, Test Loss: 3.809610614441271e-05\n",
            "Epoch 961/1000, Training Loss: 3.238581877579954e-05, Test Loss: 3.80542044770937e-05\n",
            "Epoch 962/1000, Training Loss: 3.2355911479643983e-05, Test Loss: 3.8012397203238685e-05\n",
            "Epoch 963/1000, Training Loss: 3.232606589719047e-05, Test Loss: 3.797068383266531e-05\n",
            "Epoch 964/1000, Training Loss: 3.229628169152441e-05, Test Loss: 3.792906387908269e-05\n",
            "Epoch 965/1000, Training Loss: 3.226655852865992e-05, Test Loss: 3.7887536860054674e-05\n",
            "Epoch 966/1000, Training Loss: 3.223689607750758e-05, Test Loss: 3.784610229696672e-05\n",
            "Epoch 967/1000, Training Loss: 3.220729400984398e-05, Test Loss: 3.7804759714985656e-05\n",
            "Epoch 968/1000, Training Loss: 3.2177752000281416e-05, Test Loss: 3.776350864301918e-05\n",
            "Epoch 969/1000, Training Loss: 3.2148269726237776e-05, Test Loss: 3.772234861368954e-05\n",
            "Epoch 970/1000, Training Loss: 3.2118846867906306e-05, Test Loss: 3.768127916329036e-05\n",
            "Epoch 971/1000, Training Loss: 3.2089483108227926e-05, Test Loss: 3.764029983175641e-05\n",
            "Epoch 972/1000, Training Loss: 3.206017813286048e-05, Test Loss: 3.759941016262355e-05\n",
            "Epoch 973/1000, Training Loss: 3.203093163015125e-05, Test Loss: 3.755860970300177e-05\n",
            "Epoch 974/1000, Training Loss: 3.2001743291108344e-05, Test Loss: 3.751789800353616e-05\n",
            "Epoch 975/1000, Training Loss: 3.197261280937311e-05, Test Loss: 3.747727461837013e-05\n",
            "Epoch 976/1000, Training Loss: 3.194353988119165e-05, Test Loss: 3.743673910512236e-05\n",
            "Epoch 977/1000, Training Loss: 3.191452420538865e-05, Test Loss: 3.739629102484542e-05\n",
            "Epoch 978/1000, Training Loss: 3.188556548334017e-05, Test Loss: 3.735592994199597e-05\n",
            "Epoch 979/1000, Training Loss: 3.185666341894664e-05, Test Loss: 3.7315655424404126e-05\n",
            "Epoch 980/1000, Training Loss: 3.182781771860681e-05, Test Loss: 3.7275467043236304e-05\n",
            "Epoch 981/1000, Training Loss: 3.179902809119188e-05, Test Loss: 3.723536437297011e-05\n",
            "Epoch 982/1000, Training Loss: 3.1770294248020415e-05, Test Loss: 3.719534699136117e-05\n",
            "Epoch 983/1000, Training Loss: 3.174161590283149e-05, Test Loss: 3.71554144794115e-05\n",
            "Epoch 984/1000, Training Loss: 3.1712992771760874e-05, Test Loss: 3.7115566421340556e-05\n",
            "Epoch 985/1000, Training Loss: 3.168442457331637e-05, Test Loss: 3.707580240455026e-05\n",
            "Epoch 986/1000, Training Loss: 3.1655911028352584e-05, Test Loss: 3.703612201960332e-05\n",
            "Epoch 987/1000, Training Loss: 3.162745186004687e-05, Test Loss: 3.699652486018629e-05\n",
            "Epoch 988/1000, Training Loss: 3.159904679387598e-05, Test Loss: 3.695701052308671e-05\n",
            "Epoch 989/1000, Training Loss: 3.157069555759242e-05, Test Loss: 3.6917578608156436e-05\n",
            "Epoch 990/1000, Training Loss: 3.154239788120088e-05, Test Loss: 3.687822871829189e-05\n",
            "Epoch 991/1000, Training Loss: 3.1514153496934554e-05, Test Loss: 3.683896045940146e-05\n",
            "Epoch 992/1000, Training Loss: 3.148596213923369e-05, Test Loss: 3.6799773440374204e-05\n",
            "Epoch 993/1000, Training Loss: 3.145782354472216e-05, Test Loss: 3.676066727305948e-05\n",
            "Epoch 994/1000, Training Loss: 3.1429737452185486e-05, Test Loss: 3.672164157223346e-05\n",
            "Epoch 995/1000, Training Loss: 3.1401703602548735e-05, Test Loss: 3.668269595557746e-05\n",
            "Epoch 996/1000, Training Loss: 3.1373721738854715e-05, Test Loss: 3.664383004364439e-05\n",
            "Epoch 997/1000, Training Loss: 3.1345791606242876e-05, Test Loss: 3.660504345983875e-05\n",
            "Epoch 998/1000, Training Loss: 3.131791295192807e-05, Test Loss: 3.65663358303858e-05\n",
            "Epoch 999/1000, Training Loss: 3.1290085525179346e-05, Test Loss: 3.652770678430985e-05\n",
            "Epoch 1000/1000, Training Loss: 3.12623090772986e-05, Test Loss: 3.648915595340515e-05\n",
            "Optimal Number of Input Samples (N*): 1600\n",
            "Corresponding Optimal Epoch: 1000\n",
            "Minimum Test Loss: 2.5192067359128402e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# MLP class with one hidden layer\n",
        "class MLP:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        # Initialize the number of nodes in each layer\n",
        "        self.input_nodes = input_nodes + 1  # Including bias\n",
        "        self.hidden_nodes = hidden_nodes + 1  # Including bias\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights_input_hidden = np.random.rand(self.input_nodes, self.hidden_nodes - 1) - 0.5\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_nodes, self.output_nodes) - 0.5\n",
        "\n",
        "        # Include bias in the inputs\n",
        "        self.bias_input = np.random.rand(self.input_nodes) - 0.5\n",
        "        self.bias_hidden = np.random.rand(self.hidden_nodes) - 0.5\n",
        "\n",
        "        # Initialize lists to store training and test losses\n",
        "        self.training_loss_history = []\n",
        "        self.test_loss_history = []\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # Add bias to inputs\n",
        "        inputs_with_bias = np.concatenate((inputs, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the hidden layer\n",
        "        self.hidden_input = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        # Add bias to hidden layer outputs\n",
        "        hidden_output_with_bias = np.concatenate((self.hidden_output, [1]), axis=0)\n",
        "\n",
        "        # Compute input and output for the output layer\n",
        "        self.final_input = np.dot(hidden_output_with_bias, self.weights_hidden_output)\n",
        "        self.final_output = sigmoid(self.final_input)\n",
        "\n",
        "        return self.final_output\n",
        "\n",
        "    def backward_pass(self, inputs, expected_output, output, learning_rate):\n",
        "        # Compute error\n",
        "        error = expected_output - output\n",
        "\n",
        "        # Gradient for output weights\n",
        "        d_weights_hidden_output = np.dot(np.concatenate((self.hidden_output, [1]), axis=0).reshape(-1,1),\n",
        "                                         error * sigmoid_derivative(output).reshape(1, -1))\n",
        "\n",
        "        # Error for hidden layer\n",
        "        hidden_error = np.dot(self.weights_hidden_output, error * sigmoid_derivative(output))[:-1]\n",
        "\n",
        "        # Gradient for input weights\n",
        "        d_weights_input_hidden = np.dot(np.concatenate((inputs, [1]), axis=0).reshape(-1,1),\n",
        "                                        hidden_error * sigmoid_derivative(self.hidden_output).reshape(1, -1))\n",
        "\n",
        "        # Update the weights\n",
        "        self.weights_hidden_output += learning_rate * d_weights_hidden_output\n",
        "        self.weights_input_hidden += learning_rate * d_weights_input_hidden\n",
        "\n",
        "    def train_and_evaluate(self, dataset, test_inputs, test_expected_output, max_epochs, learning_rate):\n",
        "        # Split dataset into inputs and expected outputs\n",
        "        inputs = dataset[:, :2]\n",
        "        expected_output = dataset[:, 2:]\n",
        "\n",
        "        # Initialize variables to track optimal epoch and minimum test loss\n",
        "        optimal_epoch = None\n",
        "        min_test_loss = float('inf')\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(max_epochs):\n",
        "            for j in range(inputs.shape[0]):\n",
        "                input_sample = inputs[j]\n",
        "                output = self.forward_pass(input_sample)\n",
        "                self.backward_pass(input_sample, expected_output[j], output, learning_rate)\n",
        "\n",
        "            # Calculate training loss\n",
        "            training_loss = np.mean(np.square(expected_output - self.predict(inputs)))\n",
        "            self.training_loss_history.append(training_loss)\n",
        "\n",
        "            # Calculate test loss\n",
        "            test_loss = np.mean(np.square(test_expected_output - self.predict(test_inputs)))\n",
        "            self.test_loss_history.append(test_loss)\n",
        "\n",
        "            # Check if test loss is minimized\n",
        "            if test_loss < min_test_loss:\n",
        "                min_test_loss = test_loss\n",
        "                optimal_epoch = epoch + 1\n",
        "\n",
        "            # Print out progress\n",
        "            print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {training_loss}, Test Loss: {test_loss}\")\n",
        "\n",
        "        return optimal_epoch\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        outputs = np.array([self.forward_pass(input_sample) for input_sample in inputs])\n",
        "        return outputs\n",
        "\n",
        "# Function to calculate the multivariate normal density\n",
        "def multivariate_gaussian_density(x, mu, cov):\n",
        "    n = mu.shape[0]\n",
        "    diff = x - mu\n",
        "    return (1. / (np.sqrt((2 * np.pi)**n * np.linalg.det(cov)))) * \\\n",
        "           np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))\n",
        "\n",
        "# Parameters for the Gaussian\n",
        "mu_x = np.array([0, 0])\n",
        "cov_x = np.array([[0.3, -0.5],\n",
        "                  [-0.5, 2]])\n",
        "\n",
        "# Set the range of input sample sizes\n",
        "input_sample_sizes = range(100, 2001, 100)  # From 100 to 1000 samples with step size 100\n",
        "\n",
        "# Initialize lists to store test loss and optimal epochs\n",
        "test_loss_values = []\n",
        "training_loss_values = []\n",
        "optimal_epochs = []\n",
        "\n",
        "for N in input_sample_sizes:\n",
        "    # Generate N training samples randomly\n",
        "    samples = np.zeros((N, 3))\n",
        "    samples[:, 0] = np.random.uniform(-2, 2, N)  # x1\n",
        "    samples[:, 1] = np.random.uniform(-4, 4, N)  # x2\n",
        "\n",
        "    # Calculate the function value for each sample\n",
        "    for i in range(N):\n",
        "        samples[i, 2] = multivariate_gaussian_density(samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Generate new test data\n",
        "    N_test = 100  # Change this to your dataset size for testing\n",
        "    test_samples = np.zeros((N_test, 3))\n",
        "    test_samples[:, 0] = np.random.uniform(-2, 2, N_test)  # x1 range\n",
        "    test_samples[:, 1] = np.random.uniform(-4, 4, N_test)  # x2 range\n",
        "\n",
        "    for i in range(N_test):\n",
        "        test_samples[i, 2] = multivariate_gaussian_density(test_samples[i, :2], mu_x, cov_x)\n",
        "\n",
        "    # Initialize the MLP\n",
        "    mlp = MLP(input_nodes=2, hidden_nodes=10, output_nodes=1)\n",
        "    optimal_epoch = mlp.train_and_evaluate(samples, test_samples[:, :2], test_samples[:, 2:], max_epochs=1000, learning_rate=0.1)\n",
        "\n",
        "    # Record test loss, training loss, and optimal epoch\n",
        "    test_loss_values.append(mlp.test_loss_history[optimal_epoch - 1])\n",
        "    training_loss_values.append(mlp.training_loss_history[optimal_epoch - 1])\n",
        "    optimal_epochs.append(optimal_epoch)\n",
        "\n",
        "# Find the index of the minimum test loss value\n",
        "optimal_N_index = np.argmin(test_loss_values)\n",
        "optimal_N = input_sample_sizes[optimal_N_index]\n",
        "min_test_loss = test_loss_values[optimal_N_index]\n",
        "optimal_epoch_for_optimal_N = optimal_epochs[optimal_N_index]\n",
        "\n",
        "print(f\"Optimal Number of Input Samples (N*): {optimal_N}\")\n",
        "print(f\"Corresponding Optimal Epoch: {optimal_epoch_for_optimal_N}\")\n",
        "print(f\"Minimum Test Loss: {min_test_loss}\")\n",
        "\n",
        "# Plot test loss and training loss against the number of input samples\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(input_sample_sizes, training_loss_values, marker='o', linestyle='-', label='Training Loss')\n",
        "plt.plot(input_sample_sizes, test_loss_values, marker='o', linestyle='-', label='Test Loss')\n",
        "plt.title('Loss vs Number of Input Samples')\n",
        "plt.xlabel('Number of Input Samples (N)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.axvline(x=optimal_N, color='r', linestyle='--', label=f'Optimal N: {optimal_N}')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}